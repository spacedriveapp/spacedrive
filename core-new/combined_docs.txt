# Combined Documentation Files

## docs/README.md

```markdown
# Spacedrive Core v2 Documentation

**A unified, simplified architecture for cross-platform file management.**

## Overview

Core v2 is a complete rewrite of Spacedrive's core system, designed to address the architectural issues identified in the original codebase. It implements a clean, event-driven architecture with unified file management and a dramatically simplified job system.

## Key Improvements

### âœ… Unified File System
- **Single API** for all file operations (no more dual indexed/ephemeral systems)
- **Consistent behavior** across all file management scenarios
- **Bridge operations** between different storage modes

### âœ… Event-Driven Architecture  
- **Replaced query invalidation** with proper event bus
- **Type-safe events** for state changes
- **Decoupled frontend/backend** communication

### âœ… Modern Database Layer
- **SeaORM** instead of abandoned prisma-client-rust
- **Optimized storage** with 70%+ space savings for large file collections
- **Proper migrations** and database versioning

### âœ… Simplified Job System
- **50 lines** vs 500+ lines to create new jobs
- **Automatic serialization** with MessagePack
- **Type-safe progress** reporting
- **Database persistence** with resume capabilities

### âœ… Clean Domain Models
- **Entry-centric design** where every file/folder has metadata by default
- **Optional content identity** for deduplication
- **Unified device management** (no more Node/Device/Instance confusion)

## What's Complete

- [x] **Core initialization and lifecycle**
- [x] **Library management** (create, open, close, discovery)
- [x] **Device management** with persistent identity
- [x] **Domain models** (Entry, Location, Device, UserMetadata, ContentIdentity)
- [x] **Database layer** with SeaORM and migrations
- [x] **Job system infrastructure** with example jobs
- [x] **Event bus** for decoupled communication
- [x] **File operations** foundation (copy jobs)
- [x] **Indexing operations** foundation
- [x] **Comprehensive tests** and working examples

## Architecture Documents

- **[Architecture Overview](architecture.md)** - High-level system design
- **[Domain Models](domain-models.md)** - Core business entities and their relationships
- **[Job System](job-system.md)** - Background task processing and job management
- **[Database](database.md)** - Data persistence and storage optimization
- **[Examples](examples.md)** - Working code examples and usage patterns

## Quick Start

```rust
use sd_core_new::Core;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Initialize core
    let core = Core::new().await?;
    
    // Create a library
    let library = core.libraries
        .create_library("My Library", None)
        .await?;
    
    println!("Library created: {}", library.name().await);
    println!("Path: {}", library.path().display());
    
    // Core automatically handles cleanup on drop
    Ok(())
}
```

## Running Examples

```bash
# Library management demo
cargo run --example library_demo

# Job system demo  
cargo run --example job_demo

# File type system demo
cargo run --example file_type_demo
```

## Running Tests

```bash
# Run all tests
cargo test

# Run specific test modules
cargo test library_test
cargo test job_system_test
cargo test indexer_job_test
```

## Project Status

Core v2 provides a solid foundation for Spacedrive's file management capabilities. The architecture is designed to be:

- **Simple** - Fewer abstractions, clearer responsibilities
- **Maintainable** - Modern Rust patterns, comprehensive tests
- **Extensible** - Event-driven design, pluggable job system
- **Performant** - Optimized database schema, efficient operations

## Next Steps

1. **API Layer** - GraphQL/REST API implementation
2. **Advanced Search** - Full-text search with SQLite FTS5
3. **Sync System** - Cloud/P2P synchronization using third-party solutions
4. **Media Processing** - Thumbnail generation and metadata extraction
5. **File Watching** - Real-time filesystem monitoring

## Contributing

See the [examples](examples.md) for detailed usage patterns and the architecture docs for implementation guidance.```

## docs/architecture.md

```markdown
# Architecture Overview

## High-Level Design

Core v2 follows a clean layered architecture with clear separation of concerns:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Core API                    â”‚  â† Future: GraphQL/REST
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚              Core Manager                   â”‚  â† Main entry point
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         Domain & Operations Layer           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚           Infrastructure Layer              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Core Components

### Core Manager (`Core`)

The main orchestrator that manages all subsystems:

```rust
pub struct Core {
    /// Application configuration
    pub config: Arc<RwLock<AppConfig>>,
    
    /// Device manager
    pub device: Arc<DeviceManager>,
    
    /// Library manager
    pub libraries: Arc<LibraryManager>,
    
    /// Volume manager
    pub volumes: Arc<VolumeManager>,
    
    /// Event bus for state changes
    pub events: Arc<EventBus>,
    
    /// Container for high-level services
    pub services: Services,
    
    /// Shared context for core components
    pub context: Arc<CoreContext>,
}
```

**Responsibilities:**
- Initialize and coordinate all subsystems
- Manage application lifecycle
- Provide unified access to capabilities
- Handle graceful shutdown

### Library Management

Libraries are the core organizational unit in Spacedrive:

```rust
pub struct Library {
    /// Root directory of the library (the .sdlibrary folder)
    path: PathBuf,
    
    /// Library configuration
    config: RwLock<LibraryConfig>,
    
    /// Database connection
    db: Arc<Database>,
    
    /// Job manager for this library
    jobs: Arc<JobManager>,
    
    /// Lock preventing concurrent access
    _lock: LibraryLock,
}
```

**Key Features:**
- **File-based storage** - Each library is a `.sdlibrary` directory
- **SQLite database** - Optimized schema with SeaORM
- **Integrated thumbnail management** - Built into Library methods (no separate ThumbnailManager)
- **Atomic operations** - Consistent state management
- **Locking mechanism** - Prevents concurrent access conflicts
- **Job integration** - Dedicated job manager per library
- **Library statistics** - Tracks files, size, locations, tags, thumbnails
- **Device registration** - Tracks devices accessing the library

### Device Management  

Unified device identity (solving the Node/Device/Instance confusion):

```rust
pub struct Device {
    pub id: Uuid,                              // Unique device identifier
    pub name: String,                          // Human-readable name
    pub os: OperatingSystem,
    pub hardware_model: Option<String>,        // Optional hardware model
    pub network_addresses: Vec<String>,        // For P2P connections
    pub is_online: bool,                       // Device online status
    pub sync_leadership: HashMap<Uuid, SyncRole>, // Sync roles per library
    pub last_seen_at: DateTime<Utc>,           // Last time device was seen
    pub created_at: DateTime<Utc>,
    pub updated_at: DateTime<Utc>,
}
```

**Design Principles:**
- **One identity per installation** - No more multiple overlapping concepts
- **Persistent across restarts** - Stable device identity stored in device.json
- **OS integration** - Automatic OS detection, partial hardware detection (macOS only)
- **Sync coordination** - Built-in sync leadership model per library

## Domain Layer

### Entry-Centric Model

Everything is an `Entry` - files and directories are treated uniformly:

```rust
pub struct Entry {
    pub id: i32,                    // Database ID
    pub uuid: Option<Uuid>,         // Global identifier (conditional)
    pub name: String,               // Display name
    pub kind: i32,                  // File(0), Directory(1), or Symlink(2)
    pub size: i64,                  // Size in bytes
    pub relative_path: String,      // Path relative to location
    pub location_id: i32,           // Reference to location
    pub metadata_id: Option<i32>,   // UserMetadata (created on demand)
    pub content_id: Option<i32>,    // Optional for deduplication
    // ... timestamps
}
```

**Benefits:**
- **On-demand metadata** - UserMetadata created only when user adds tags/notes
- **Unified operations** - Same APIs work for files and directories
- **Flexible relationships** - Content identity separate from metadata
- **Conditional UUIDs** - Assigned to directories immediately, files after content identification

### Storage Schema

Paths are stored as relative paths from location root:

```rust
// Paths stored relative to location:
Entry { relative_path: "", name: "file1.txt", location_id: 1 }
Entry { relative_path: "", name: "file2.txt", location_id: 1 }  
Entry { relative_path: "subdir", name: "file3.txt", location_id: 1 }
```

**Note:** Path compression mentioned in design documents is not currently implemented.

## Infrastructure Layer

### Event Bus

Decoupled communication using a type-safe event system:

```rust
pub enum Event {
    // Core events
    CoreStarted,
    CoreShutdown,
    
    // Library events
    LibraryCreated { id: Uuid, name: String, path: PathBuf },
    LibraryOpened { id: Uuid, name: String, path: PathBuf },
    LibraryClosed { id: Uuid, name: String },
    LibraryDeleted { id: Uuid, name: String },
    
    // Entry events
    EntryCreated { library_id: Uuid, entry_id: Uuid },
    EntryModified { library_id: Uuid, entry_id: Uuid },
    EntryDeleted { library_id: Uuid, entry_id: Uuid },
    EntryMoved { library_id: Uuid, entry_id: Uuid, old_path: PathBuf, new_path: PathBuf },
    
    // Volume events
    VolumeAdded(Volume),
    VolumeRemoved(Uuid),
    VolumeUpdated(Volume),
    
    // Job events
    JobQueued { id: Uuid, name: String },
    JobStarted { id: Uuid },
    JobProgress { id: Uuid, progress: Progress },
    JobCompleted { id: Uuid },
    JobFailed { id: Uuid, error: String },
    
    // ... and more
}
```

**Replaces the problematic `invalidate_query!` pattern** with proper event-driven architecture.

### Job System

Minimal boilerplate job processing:

```rust
#[derive(Serialize, Deserialize)]
pub struct MyJob {
    // Job fields
}

impl Job for MyJob {
    const NAME: &'static str = "my_job";
    const RESUMABLE: bool = true;
}

#[async_trait]
impl JobHandler for MyJob {
    type Output = MyOutput;
    
    async fn run(&mut self, ctx: JobContext<'_>) -> JobResult<Self::Output> {
        // Job implementation
    }
}
```

**Key improvements over original:**
- **50 lines vs 500-1000+** to implement a new job
- **Automatic serialization** with MessagePack
- **Database persistence** with checkpointing  
- **Type-safe progress** reporting
- **#[derive(Job)] macro** - Auto-generates registration and boilerplate
- **Lifecycle methods** - Optional on_pause, on_resume, on_cancel
- **Inventory-based registration** - Jobs auto-register at compile time

### Database Layer

Modern ORM with proper migrations:

```rust
// SeaORM entities with proper relationships
#[derive(DeriveEntityModel)]
#[sea_orm(table_name = "entries")]
pub struct Model {
    #[sea_orm(primary_key)]
    pub id: i32,
    pub uuid: Uuid,
    pub name: String,
    // ... other fields
}

// Automatic relationship handling
impl Related<super::user_metadata::Entity> for Entity {
    fn to() -> RelationDef {
        Relation::UserMetadata.def()
    }
}
```

## Data Flow

### Library Operations

```
User Request â†’ Core â†’ LibraryManager â†’ Library â†’ Database
                â†“
           EventBus â† Event Emission
```

### File Operations  

```
Operation Request â†’ JobManager â†’ Job Execution â†’ Database Update
                                      â†“
                              Progress Updates â†’ EventBus
```

### Event Propagation

```
Domain Change â†’ Event Creation â†’ EventBus â†’ Subscribers
                                    â†“
                              Frontend Updates (Future)
```

## Design Principles

### 1. Event-Driven Architecture
- **Loose coupling** - Components communicate via events
- **Extensibility** - Easy to add new event handlers
- **Debuggability** - Clear audit trail of system changes

### 2. Domain-First Design
- **Business logic in domain layer** - Clear separation from infrastructure
- **Rich domain models** - Behavior lives with data
- **Ubiquitous language** - Code reflects business concepts

### 3. Pragmatic Choices
- **Monolith over microservices** - Simpler deployment and development
- **SQLite over complex databases** - Perfect for client-side storage
- **Standard libraries over custom** - Leverage existing, well-tested solutions

### 4. Performance Optimization
- **Optimized schemas** - Significant space savings
- **Efficient queries** - Proper indexing and relationships  
- **Async everywhere** - Non-blocking operations
- **Resource pooling** - Shared connections and managers

## Error Handling

Consistent error handling across all layers:

```rust
// Domain errors
#[derive(thiserror::Error, Debug)]
pub enum LibraryError {
    #[error("Library not found: {0}")]
    NotFound(Uuid),
    #[error("Library already open: {0}")]
    AlreadyOpen(PathBuf),
    // ... other variants
}

// Result types for operations
type LibraryResult<T> = Result<T, LibraryError>;
```

## Testing Strategy

- **Unit tests** - Domain logic and utilities
- **Integration tests** - Full system workflows  
- **Property tests** - Database consistency
- **Example tests** - Documentation as runnable code

## Future Extensions

The architecture supports planned features:

- **API Layer** - GraphQL/REST endpoints
- **Sync System** - Third-party database sync (foundation in place with sync_leadership)
- **Plugin System** - Dynamic job registration
- **Search Engine** - Full-text and semantic search
- **Real-time Updates** - WebSocket event streaming
- **Path Compression** - Implement the designed path compression for space savings
- **Agent System** - Recently introduced agent manager design```

## docs/cli-multi-instance.md

```markdown
# Multi-Instance Daemon Support

Spacedrive CLI now supports running multiple daemon instances simultaneously, enabling local testing of device pairing and other multi-device features.

## Overview

Multiple daemon instances allow you to:
- Test device pairing locally by running two instances
- Simulate multi-device scenarios on a single machine
- Isolate different development/testing environments
- Run production and development daemons side-by-side

## Usage

### Starting Multiple Instances

```bash
# Start default instance
spacedrive start

# Start named instances
spacedrive start --instance alice
spacedrive start --instance bob

# Start with networking enabled
spacedrive start --instance alice --enable-networking
spacedrive start --instance bob --enable-networking
```

### Targeting Specific Instances

Use the `--instance` flag to target commands to specific daemon instances:

```bash
# Default instance
spacedrive library list

# Named instances  
spacedrive --instance alice library list
spacedrive --instance bob library create "Bob's Library"
```

### Instance Management

```bash
# List all daemon instances
spacedrive instance list

# Stop specific instance
spacedrive instance stop alice
spacedrive --instance bob stop  # Alternative syntax

# Check status of specific instance
spacedrive --instance alice daemon
```

### Device Pairing Example

Test device pairing locally using two instances:

```bash
# Terminal 1: Start Alice's daemon
spacedrive start --instance alice --enable-networking --foreground

# Terminal 2: Start Bob's daemon  
spacedrive start --instance bob --enable-networking --foreground

# Terminal 3: Alice generates pairing code
spacedrive --instance alice network init --password "test123"
spacedrive --instance alice network pair generate --auto-accept

# Terminal 4: Bob joins using Alice's code
spacedrive --instance bob network init --password "test123"
spacedrive --instance bob network pair join "word1 word2 word3 ... word12"
```

## Architecture

### Instance Isolation

Each instance has completely isolated:

- **Socket paths**: `spacedrive.sock`, `spacedrive-alice.sock`, `spacedrive-bob.sock`
- **PID files**: `spacedrive.pid`, `spacedrive-alice.pid`, `spacedrive-bob.pid`  
- **Data directories**: `data/spacedrive-cli-data/`, `data/spacedrive-cli-data/instance-alice/`
- **CLI state**: Separate `cli_state.json` per instance

### File Structure

```
$runtime_dir/               # /tmp or $XDG_RUNTIME_DIR
â”œâ”€â”€ spacedrive.sock         # Default instance socket
â”œâ”€â”€ spacedrive.pid          # Default instance PID
â”œâ”€â”€ spacedrive-alice.sock   # Alice instance socket  
â”œâ”€â”€ spacedrive-alice.pid    # Alice instance PID
â”œâ”€â”€ spacedrive-bob.sock     # Bob instance socket
â””â”€â”€ spacedrive-bob.pid      # Bob instance PID

data/spacedrive-cli-data/                    # Default instance data
â”œâ”€â”€ spacedrive.json
â”œâ”€â”€ libraries/
â””â”€â”€ cli_state.json

data/spacedrive-cli-data/instance-alice/     # Alice instance data
â”œâ”€â”€ spacedrive.json
â”œâ”€â”€ libraries/  
â””â”€â”€ cli_state.json

data/spacedrive-cli-data/instance-bob/       # Bob instance data
â”œâ”€â”€ spacedrive.json
â”œâ”€â”€ libraries/
â””â”€â”€ cli_state.json
```

## Development Workflow

### Testing Pairing Protocol

```bash
# Start two instances for pairing test
spacedrive start --instance initiator --enable-networking --foreground &
spacedrive start --instance joiner --enable-networking --foreground &

# Initialize networking
spacedrive --instance initiator network init --password "dev123"
spacedrive --instance joiner network init --password "dev123"

# Test pairing
CODE=$(spacedrive --instance initiator network pair generate --auto-accept | grep "Pairing code:" | cut -d' ' -f3-)
spacedrive --instance joiner network pair join "$CODE"

# Verify connection
spacedrive --instance initiator network devices
spacedrive --instance joiner network devices
```

### Instance Cleanup

```bash
# Stop all instances
spacedrive instance list
spacedrive instance stop alice
spacedrive instance stop bob
spacedrive stop  # Default instance

# Clean up sockets (if needed)
rm /tmp/spacedrive*.sock /tmp/spacedrive*.pid
```

## Backwards Compatibility

The implementation maintains full backwards compatibility:
- All existing commands work unchanged with the default instance
- No breaking changes to CLI interface
- Default instance behavior is identical to single-instance mode

## Implementation Notes

- Instance names must be valid filenames (no special characters)
- Socket discovery happens automatically via filesystem scanning
- Daemon startup checks for instance conflicts
- Each instance runs independently with separate process trees```

## docs/cli.md

```markdown
# Spacedrive CLI

A comprehensive command-line interface for managing Spacedrive Core with full daemon architecture, real-time monitoring, and cross-device file management.

## Features

- **ğŸ—ï¸ Daemon Architecture**: Background daemon with client-server communication
- **ğŸ“š Library Management**: Create, open, switch, and manage multiple libraries
- **ğŸ“ Location Management**: Add, remove, and monitor indexed locations with real-time watching
- **âš™ï¸ Job Management**: View, monitor, and control background jobs with live progress
- **ğŸ“Š Real-time Monitoring**: Beautiful TUI for monitoring job progress and system events
- **ğŸ” Indexing Control**: Start indexing jobs with different modes (shallow/content/deep)
- **ğŸŒ Networking Support**: Device pairing, file sharing via Spacedrop
- **ğŸ”§ Multiple Instances**: Run isolated daemon instances for different use cases
- **ğŸ“ Comprehensive Logging**: Built-in logging with file output for debugging
- **ğŸ–¥ï¸ Cross-platform**: Works on macOS, Linux, and Windows
- **ğŸ¨ Rich UI**: Colored output, progress bars, and formatted tables

### New Modular Architecture Benefits

The refactored daemon architecture provides:

- **Maintainability**: Each domain (library, location, job, etc.) is isolated in its own handler module
- **Extensibility**: New commands can be added by simply creating a new handler
- **Type Safety**: All commands and responses are strongly typed
- **Code Organization**: Clear separation between command handling, business logic, and transport
- **Testability**: Individual handlers can be unit tested in isolation
- **Performance**: Efficient command routing through handler registry

## Installation

```bash
# Build the CLI
cargo build --release --bin spacedrive

# Install globally (optional)
cargo install --path . --bin spacedrive
```

## Quick Start

```bash
# Start the Spacedrive daemon
spacedrive start

# Create a library and add a location
spacedrive library create "Personal" 
spacedrive location add ~/Desktop --name "Desktop"

# Monitor indexing progress
spacedrive job monitor

# Check system status
spacedrive status
```

## Usage

### Daemon Management

```bash
# Start daemon in background
spacedrive start

# Start daemon with networking enabled
spacedrive start --enable-networking

# Start daemon in foreground (for debugging)
spacedrive start --foreground

# Stop the daemon
spacedrive stop

# Check daemon status
spacedrive status

# Advanced daemon commands
spacedrive daemon status      # Detailed daemon status
spacedrive daemon list        # List all daemon instances
```

### Multiple Daemon Instances

The CLI supports running multiple isolated daemon instances:

```bash
# Run a separate daemon instance
spacedrive --instance test start
spacedrive --instance test library create "Test Library"

# Stop specific instance
spacedrive --instance test stop

# List all running instances
spacedrive daemon list
```

### Basic Commands

```bash
# Show help
spacedrive --help

# Enable verbose logging
spacedrive -v <command>

# Use custom data directory
spacedrive --data-dir /path/to/data <command>
```

### Library Management

```bash
# Create a new library
spacedrive library create "My Library"
spacedrive library create "My Library" --path /custom/path

# List all libraries
spacedrive library list

# Open an existing library
spacedrive library open /path/to/library

# Switch to a library by name or ID
spacedrive library switch "My Library"
spacedrive library switch 12345678-1234-1234-1234-123456789012

# Show current library
spacedrive library current

# Close current library
spacedrive library close
```

### Location Management

```bash
# Add a location to index (automatically starts watching)
spacedrive location add ~/Documents
spacedrive location add ~/Pictures --name "My Photos" --mode deep

# List all locations with status
spacedrive location list

# Remove a location (stops watching and indexing)
spacedrive location remove <location-id>

# Rescan a location (triggers re-indexing)
spacedrive location rescan <location-id>
spacedrive location rescan <location-id> --force  # Full rescan, ignore change detection
```

**Note**: Location IDs are UUIDs displayed in the list command. All location operations work with the daemon automatically.

### Enhanced Indexing

The new indexing system supports different scopes and persistence modes:

```bash
# Quick scan of current directory only (no subdirectories)
spacedrive index quick-scan /path/to/directory --scope current

# Quick scan with ephemeral mode (no database writes)
spacedrive index quick-scan /path/to/directory --scope current --ephemeral

# Browse external paths without adding to managed locations
spacedrive index browse /media/external-drive --scope current
spacedrive index browse /network/drive --scope recursive --content

# Index managed locations with specific scope
spacedrive index location /managed/location --scope current --mode shallow
spacedrive index location <location-uuid> --scope recursive --mode deep

# Legacy full location indexing (backward compatibility)
spacedrive scan /path/to/directory --mode content --watch
```

**Index Scopes:**
- `current`: Index only the specified directory (single level)
- `recursive`: Index the directory and all subdirectories

**Index Modes:**
- `shallow`: Metadata only (fastest)
- `content`: Metadata + content hashing (moderate)
- `deep`: Full analysis including media metadata (slowest)

**Use Cases:**
- **UI Navigation**: `quick-scan --scope current` for instant directory viewing
- **External Browsing**: `browse --ephemeral` for exploring non-managed paths
- **Location Updates**: `location --scope current` to refresh specific directories

### Job Management

```bash
# List all jobs with colored status and progress
spacedrive job list
spacedrive job list --status running      # Filter by status

# Show detailed job information
spacedrive job info <job-id>

# Monitor jobs in real-time with live progress bars
spacedrive job monitor
spacedrive job monitor --job-id <job-id>  # Monitor specific job

# Control jobs (planned features)
spacedrive job pause <job-id>
spacedrive job resume <job-id>
spacedrive job cancel <job-id>
```

**Job Monitor Features:**
- ğŸ”´ Live progress bars for running jobs
- ğŸ¨ Color-coded status (running: yellow, completed: green, failed: red)
- â±ï¸ Real-time updates every second
- ğŸ§¹ Automatic cleanup of completed jobs
- âŒ¨ï¸ Ctrl+C to exit gracefully

### File Operations

```bash
# Copy files with progress tracking
spacedrive file copy ~/source.txt ~/destination.txt
spacedrive file copy ~/Photos/*.jpg ~/Backup/ --verify

# Move files
spacedrive file move ~/Downloads/*.pdf ~/Documents/ --preserve-timestamps

# Advanced copy options
spacedrive file copy ~/Project/ ~/Backup/Project/ \
  --overwrite \
  --verify \
  --preserve-timestamps
```

### System Commands

```bash
# Show system status
spacedrive status

# Monitor all system activity (TUI)
spacedrive monitor

# View daemon logs
spacedrive system logs
spacedrive system logs --tail 50
```

### Networking & Device Management

```bash
# Initialize networking (if daemon wasn't started with --enable-networking)
spacedrive network init

# Start/stop networking
spacedrive network start
spacedrive network stop

# List connected devices
spacedrive network devices

# Device pairing
spacedrive network pair --initiate              # Generate pairing code
spacedrive network pair --join <code>           # Join using code
spacedrive network pair --status                # Check pairing status

# Spacedrop (file sharing)
spacedrive network spacedrop <device-id> /path/to/file --sender "Your Name"

# Remove paired device
spacedrive network revoke <device-id>
```

## Real-time Job Monitor

The job monitor provides live progress tracking with beautiful visual indicators:

```bash
spacedrive job monitor
```

### Monitor Features

- **ğŸ¯ Multi-job tracking**: Monitor all running jobs simultaneously
- **ğŸ“Š Progress bars**: Visual progress indicators with percentage
- **ğŸ¨ Color coding**: Status-based colors (yellow=running, green=completed, red=failed)
- **âš¡ Real-time updates**: Updates every second with latest progress
- **ğŸ§¹ Smart cleanup**: Completed jobs automatically marked and removed
- **ğŸ” Job filtering**: Option to monitor specific jobs

### Sample Output

```
ğŸ“¡ Spacedrive Job Monitor - Press Ctrl+C to exit
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â š Indexing Desktop [fdbe777d] [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 25% | Status: Running
â ‚ Indexing Photos [a1b2c3d4]  [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100% | âœ… Completed
â ˆ Content Analysis [e5f6g7h8] [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 15% | Status: Running
```

## Indexing Modes

- **Shallow**: Fast metadata-only indexing (file names, sizes, dates)
- **Content**: Standard indexing with content hashing for deduplication
- **Deep**: Comprehensive analysis including media metadata extraction

## Examples

### Complete Workflow

```bash
# 1. Start the daemon
spacedrive start

# 2. Create a library
spacedrive library create "Personal"

# 3. Add locations with different index modes
spacedrive location add ~/Desktop --name "Desktop" --mode content
spacedrive location add ~/Documents --name "Documents" --mode content  
spacedrive location add ~/Pictures --name "Photos" --mode deep

# 4. Monitor the indexing progress
spacedrive job monitor

# 5. Check the results
spacedrive location list
spacedrive job list
```

### Multiple Libraries

```bash
# Work with multiple libraries
spacedrive library create "Work"
spacedrive library create "Personal"
spacedrive library list
spacedrive library switch "Work"
spacedrive location add ~/Work/Projects
```

### Batch Indexing

```bash
# Index multiple locations
spacedrive location add ~/Documents --name "Docs"
spacedrive location add ~/Pictures --name "Photos" --mode deep
spacedrive location add ~/Downloads --name "Downloads" --mode shallow
spacedrive job list --status running
```

## Architecture

### Daemon-Client Model

The Spacedrive CLI uses a daemon-client architecture for optimal performance:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    Unix Socket    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CLI Client    â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚  Spacedrive     â”‚
â”‚   (Commands)    â”‚                  â”‚  Daemon         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                            â”‚
                                            â–¼
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚  Background     â”‚
                                    â”‚  Jobs, Watching â”‚
                                    â”‚  & File System  â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Benefits:**
- ğŸš€ **Fast responses**: Daemon keeps state in memory
- ğŸ”„ **Background processing**: Jobs continue when CLI exits
- ğŸ“¡ **Real-time updates**: File system changes processed immediately
- ğŸ’¾ **Persistent state**: Libraries and locations survive restarts

### Modular Daemon Architecture

The daemon has been refactored into a clean, modular architecture:

```
src/infrastructure/cli/daemon/
â”œâ”€â”€ mod.rs                 # Core daemon server (socket handling, lifecycle)
â”œâ”€â”€ client.rs              # DaemonClient implementation
â”œâ”€â”€ config.rs              # DaemonConfig and instance management
â”œâ”€â”€ types/
â”‚   â”œâ”€â”€ commands.rs        # DaemonCommand enum and sub-commands
â”‚   â”œâ”€â”€ responses.rs       # DaemonResponse enum and response types
â”‚   â””â”€â”€ common.rs          # Shared types (JobInfo, LibraryInfo, etc.)
â”œâ”€â”€ handlers/
â”‚   â”œâ”€â”€ mod.rs            # Handler trait and registry
â”‚   â”œâ”€â”€ core.rs           # Core commands (ping, shutdown, status)
â”‚   â”œâ”€â”€ library.rs        # Library command handling
â”‚   â”œâ”€â”€ location.rs       # Location command handling
â”‚   â”œâ”€â”€ job.rs            # Job command handling
â”‚   â”œâ”€â”€ network.rs        # Network command handling
â”‚   â”œâ”€â”€ file.rs           # File command handling
â”‚   â””â”€â”€ system.rs         # System command handling
â””â”€â”€ services/
    â”œâ”€â”€ state.rs          # CLI state management service
    â””â”€â”€ helpers.rs        # Common helpers (device registration, etc.)
```

**Key Components:**

1. **Command Handlers**: Each domain (library, location, job, etc.) has its own handler that encapsulates all related command processing logic.

2. **Handler Registry**: A central registry that routes incoming commands to the appropriate handler based on command type.

3. **State Service**: Manages CLI state including current library selection and persists it across sessions.

4. **Type Safety**: All commands and responses are strongly typed, preventing errors and improving maintainability.

5. **Separation of Concerns**: Business logic is cleanly separated from transport (socket handling) and presentation concerns.

### Configuration

The daemon stores data in the specified data directory:

```
spacedrive-cli-data/
â”œâ”€â”€ libraries/           # Library database files
â”œâ”€â”€ daemon.sock         # Unix socket for communication
â”œâ”€â”€ daemon.pid          # Process ID file
â”œâ”€â”€ daemon.log          # Daemon log file
â””â”€â”€ cli_state.json      # CLI preferences and history
```

For multiple instances:
```
spacedrive-cli-data/
â”œâ”€â”€ instance-test/
â”‚   â”œâ”€â”€ libraries/
â”‚   â””â”€â”€ cli_state.json
â”œâ”€â”€ spacedrive-test.sock
â”œâ”€â”€ spacedrive-test.pid
â””â”€â”€ spacedrive-test.log
```

## Tips & Best Practices

1. **ğŸ”§ Start Daemon First**: Always run `spacedrive start` before other commands
2. **ğŸ“Š Monitor Progress**: Use `spacedrive job monitor` for real-time feedback on indexing
3. **ğŸ’» Use Verbose Mode**: Add `-v` flag for detailed logging during troubleshooting
4. **ğŸš€ Start Simple**: Begin with shallow indexing for quick results, then upgrade to deeper modes
5. **ğŸ“ Organize by Purpose**: Create separate libraries for different use cases (Work, Personal, etc.)
6. **âš¡ Daemon Persistence**: The daemon keeps running in the background - jobs continue even if you close the terminal

## Troubleshooting

### Daemon Issues
```bash
# Check if daemon is running
spacedrive status

# Check specific instance
spacedrive --instance test status

# List all daemon instances
spacedrive daemon list

# Restart daemon
spacedrive stop
spacedrive start

# Run daemon in foreground for debugging
spacedrive start --foreground -v

# Check daemon logs
spacedrive system logs --tail 100
```

### Communication Errors
```bash
# Check daemon status
spacedrive daemon

# Look for socket file
ls -la spacedrive-cli-data/daemon.sock

# Restart daemon if socket is missing
spacedrive stop && spacedrive start
```

### Job Issues
```bash
# Check job status with details
spacedrive job list -v

# View specific job information
spacedrive job info <job-id>

# Monitor jobs in real-time
spacedrive job monitor
```

### Location Issues
```bash
# Check location status
spacedrive location list

# Force rescan if files not updating
spacedrive location rescan <location-id> --force

# Remove and re-add problematic locations
spacedrive location remove <location-id>
spacedrive location add /path/to/location
```

### Performance Issues
```bash
# Use shallow mode for large directories
spacedrive location add /large/directory --mode shallow

# Check system status
spacedrive status

# Monitor daemon resources with verbose output
spacedrive start --foreground -v
``````

## docs/database.md

```markdown
# Database and Infrastructure

Core v2 uses a modern database stack built on SeaORM and SQLite, replacing the abandoned prisma-client-rust dependency. The new schema is optimized for space efficiency and query performance.

## Database Architecture

### Technology Stack

**SeaORM** - Modern async ORM for Rust
- **Type-safe queries** - Compile-time SQL validation
- **Automatic migrations** - Version-controlled schema changes
- **Rich relationships** - Foreign keys and joins
- **Connection pooling** - Efficient resource management

**SQLite** - Embedded database engine
- **ACID transactions** - Data consistency
- **WAL mode** - Better concurrency
- **Full-text search** - Built-in FTS5 (future)
- **Cross-platform** - Works everywhere Spacedrive runs

### Schema Overview

```sql
-- Core entities
CREATE TABLE devices (...)         -- Device identity
CREATE TABLE locations (...)       -- Indexed directories
CREATE TABLE entries (...)         -- Files and directories
CREATE TABLE content_identity (...) -- Content deduplication

-- User organization
CREATE TABLE user_metadata (...)   -- Tags, notes, favorites
CREATE TABLE tags (...)            -- User-defined tags
CREATE TABLE labels (...)          -- Hierarchical labels
CREATE TABLE metadata_tag (...)    -- Many-to-many: metadata â†” tags
CREATE TABLE metadata_label (...)  -- Many-to-many: metadata â†” labels

-- Infrastructure
CREATE TABLE jobs (...)            -- Job system persistence
```

## Storage Design

### Materialized Path Approach

We use a materialized path approach for storing file system hierarchies, providing excellent query performance:

```sql
-- Direct path storage with materialized paths
CREATE TABLE entries (
    id INTEGER PRIMARY KEY,
    uuid BLOB UNIQUE NOT NULL,
    location_id INTEGER NOT NULL REFERENCES locations(id),
    relative_path TEXT NOT NULL,    -- Directory path (e.g. "Documents/Projects")
    name TEXT NOT NULL,             -- Entry name (e.g. "file.txt")
    kind TEXT NOT NULL,             -- "file" or "directory"
    -- ... other columns
);
```

**Benefits:**
- **Simple queries** - No complex joins needed for path operations
- **Fast hierarchy queries** - Direct path matching with LIKE patterns
- **No parent_id complexity** - Avoid recursive queries for deep hierarchies
- **Efficient indexing** - Single index on relative_path for most queries

### Example Storage Efficiency

For a typical user with 100,000 files in `/Users/james/Documents/`:

| Approach | Storage Size | Index Size | Query Performance |
|----------|-------------|------------|-------------------|
| **Naive** | 2.1 GB | 500 MB | Slow (large indexes) |
| **Optimized** | 650 MB | 150 MB | Fast (compact indexes) |
| **Savings** | **69%** | **70%** | **3x faster** |

## Entity Definitions

### Devices Table

```sql
CREATE TABLE devices (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    uuid BLOB UNIQUE NOT NULL,           -- 16-byte UUID
    name TEXT NOT NULL,
    os TEXT NOT NULL,
    os_version TEXT,
    hardware_model TEXT NOT NULL,
    network_addresses TEXT,              -- JSON array
    is_online BOOLEAN NOT NULL,
    last_seen_at TEXT NOT NULL,          -- ISO 8601
    capabilities TEXT,                   -- JSON object
    created_at TEXT NOT NULL,
    updated_at TEXT NOT NULL
);

CREATE INDEX idx_devices_uuid ON devices(uuid);
CREATE INDEX idx_devices_online ON devices(is_online);
```

### Entries Table (Core File/Directory Model)

```sql
CREATE TABLE entries (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    uuid BLOB UNIQUE NOT NULL,
    location_id INTEGER NOT NULL REFERENCES locations(id),
    relative_path TEXT NOT NULL,  -- Materialized path (parent directory path)
    name TEXT NOT NULL,           -- Entry name without extension
    kind TEXT NOT NULL CHECK (kind IN ('file', 'directory')),
    metadata_id INTEGER NOT NULL REFERENCES user_metadata(id),
    content_id INTEGER REFERENCES content_identity(id),
    location_id INTEGER REFERENCES locations(id),
    size INTEGER NOT NULL,
    permissions TEXT,
    created_at TEXT NOT NULL,
    modified_at TEXT NOT NULL,
    accessed_at TEXT
);

-- Critical indexes for performance
CREATE INDEX idx_entries_uuid ON entries(uuid);
CREATE INDEX idx_entries_name ON entries(name);
CREATE INDEX idx_entries_kind ON entries(kind);
CREATE INDEX idx_entries_size ON entries(size);
CREATE INDEX idx_entries_prefix_path ON entries(prefix_id, relative_path);
CREATE INDEX idx_entries_location ON entries(location_id);
CREATE INDEX idx_entries_content ON entries(content_id);
CREATE INDEX idx_entries_metadata ON entries(metadata_id);
```

### User Metadata Table

```sql
CREATE TABLE user_metadata (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    uuid BLOB UNIQUE NOT NULL,
    notes TEXT,
    favorite BOOLEAN NOT NULL DEFAULT FALSE,
    hidden BOOLEAN NOT NULL DEFAULT FALSE,
    custom_data TEXT,                    -- JSON object
    created_at TEXT NOT NULL,
    updated_at TEXT NOT NULL
);

CREATE INDEX idx_user_metadata_uuid ON user_metadata(uuid);
CREATE INDEX idx_user_metadata_favorite ON user_metadata(favorite);
CREATE INDEX idx_user_metadata_hidden ON user_metadata(hidden);
```

### Content Identity Table

```sql
CREATE TABLE content_identity (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    cas_id TEXT UNIQUE NOT NULL,         -- Content-addressed storage ID
    kind TEXT NOT NULL,                  -- image, video, audio, document, etc.
    size_bytes INTEGER NOT NULL,
    media_data TEXT,                     -- JSON metadata
    created_at TEXT NOT NULL
);

CREATE UNIQUE INDEX idx_content_identity_cas ON content_identity(cas_id);
CREATE INDEX idx_content_identity_kind ON content_identity(kind);
CREATE INDEX idx_content_identity_size ON content_identity(size_bytes);
```

### Tags and Labels

```sql
CREATE TABLE tags (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    uuid BLOB UNIQUE NOT NULL,
    name TEXT UNIQUE NOT NULL,
    color TEXT,                          -- Hex color code
    icon TEXT,                           -- Icon identifier
    created_at TEXT NOT NULL,
    updated_at TEXT NOT NULL
);

CREATE TABLE labels (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    uuid BLOB UNIQUE NOT NULL,
    name TEXT NOT NULL,
    color TEXT,
    parent_id INTEGER REFERENCES labels(id),
    created_at TEXT NOT NULL,
    updated_at TEXT NOT NULL
);

-- Junction tables for many-to-many relationships
CREATE TABLE metadata_tag (
    metadata_id INTEGER REFERENCES user_metadata(id),
    tag_id INTEGER REFERENCES tags(id),
    PRIMARY KEY (metadata_id, tag_id)
);

CREATE TABLE metadata_label (
    metadata_id INTEGER REFERENCES user_metadata(id),
    label_id INTEGER REFERENCES labels(id),
    PRIMARY KEY (metadata_id, label_id)
);
```

### Locations Table

```sql
CREATE TABLE locations (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    uuid BLOB UNIQUE NOT NULL,
    device_id INTEGER NOT NULL REFERENCES devices(id),
    path TEXT NOT NULL,
    name TEXT,
    index_mode TEXT NOT NULL CHECK (index_mode IN ('metadata', 'content', 'deep')),
    scan_state TEXT NOT NULL CHECK (scan_state IN ('pending', 'scanning', 'complete', 'error', 'paused')),
    last_scan_at TEXT,
    error_message TEXT,
    total_file_count INTEGER NOT NULL DEFAULT 0,
    total_byte_size INTEGER NOT NULL DEFAULT 0,
    created_at TEXT NOT NULL,
    updated_at TEXT NOT NULL
);

CREATE INDEX idx_locations_device ON locations(device_id);
CREATE INDEX idx_locations_scan_state ON locations(scan_state);
CREATE UNIQUE INDEX idx_locations_device_path ON locations(device_id, path);
```

## SeaORM Entity Definitions

### Entry Entity

```rust
use sea_orm::entity::prelude::*;

#[derive(Clone, Debug, PartialEq, DeriveEntityModel)]
#[sea_orm(table_name = "entries")]
pub struct Model {
    #[sea_orm(primary_key)]
    pub id: i32,
    pub uuid: Uuid,
    pub prefix_id: i32,
    pub relative_path: String,
    pub name: String,
    pub kind: String,
    pub metadata_id: i32,
    pub content_id: Option<i32>,
    pub location_id: Option<i32>,
    pub parent_id: Option<i32>,
    pub size: u64,
    pub permissions: Option<String>,
    pub created_at: DateTime<Utc>,
    pub modified_at: DateTime<Utc>,
    pub accessed_at: Option<DateTime<Utc>>,
}

#[derive(Copy, Clone, Debug, EnumIter, DeriveRelation)]
pub enum Relation {
    #[sea_orm(
        belongs_to = "super::user_metadata::Entity",
        from = "Column::MetadataId",
        to = "super::user_metadata::Column::Id"
    )]
    UserMetadata,
    #[sea_orm(
        belongs_to = "super::content_identity::Entity",
        from = "Column::ContentId",
        to = "super::content_identity::Column::Id"
    )]
    ContentIdentity,
    #[sea_orm(
        belongs_to = "super::location::Entity",
        from = "Column::LocationId",
        to = "super::location::Column::Id"
    )]
    Location,
}

impl Related<super::user_metadata::Entity> for Entity {
    fn to() -> RelationDef {
        Relation::UserMetadata.def()
    }
}

impl ActiveModelBehavior for ActiveModel {}
```

## Query Patterns

### Common Queries

**Find files by name pattern:**
```rust
let pdf_files = Entry::find()
    .filter(entry::Column::Name.like("%.pdf"))
    .filter(entry::Column::Kind.eq("file"))
    .all(db)
    .await?;
```

**Find files with specific tag:**
```rust
let important_files = Entry::find()
    .find_with_related(UserMetadata)
    .join(JoinType::InnerJoin, metadata_tag::Relation::Tag.def())
    .filter(tag::Column::Name.eq("Important"))
    .all(db)
    .await?;
```

**Find duplicate content:**
```rust
let duplicates = ContentIdentity::find()
    .find_with_related(Entry)
    .having(entry::Column::Id.count().gt(1))
    .group_by(content_identity::Column::CasId)
    .all(db)
    .await?;
```

**Reconstruct full path:**
```rust
let entry = Entry::find_by_id(entry_id)
    .one(db)
    .await?;

let full_path = if entry.relative_path.is_empty() {
    entry.name
} else {
    format!("{}/{}", entry.relative_path, entry.name)
};
```

### Complex Queries

**Find large files by directory:**
```rust
let large_files_by_dir = Entry::find()
    .select_only()
    .column_as(entry::Column::RelativePath, "directory")
    .column_as(entry::Column::Size.sum(), "total_size")
    .column_as(entry::Column::Id.count(), "file_count")
    .filter(entry::Column::Kind.eq("file"))
    .filter(entry::Column::Size.gt(100 * 1024 * 1024)) // > 100MB
    .group_by(entry::Column::RelativePath)
    .order_by_desc(entry::Column::Size.sum())
    .into_tuple::<(String, Option<i64>, Option<i64>)>()
    .all(db)
    .await?;
```

**Tag usage statistics:**
```rust
let tag_stats = Tag::find()
    .select_only()
    .column(tag::Column::Name)
    .column_as(metadata_tag::Column::MetadataId.count(), "usage_count")
    .join(JoinType::LeftJoin, tag::Relation::MetadataTag.def())
    .group_by(tag::Column::Id)
    .order_by_desc(metadata_tag::Column::MetadataId.count())
    .into_tuple::<(String, Option<i64>)>()
    .all(db)
    .await?;
```

## Migration System

### Migration Structure

```rust
use sea_orm_migration::prelude::*;

#[derive(DeriveMigrationName)]
pub struct Migration;

#[async_trait::async_trait]
impl MigrationTrait for Migration {
    async fn up(&self, manager: &SchemaManager) -> Result<(), DbErr> {
        manager
            .create_table(
                Table::create()
                    .table(Devices::Table)
                    .if_not_exists()
                    .col(
                        ColumnDef::new(Devices::Id)
                            .integer()
                            .not_null()
                            .auto_increment()
                            .primary_key(),
                    )
                    .col(ColumnDef::new(Devices::Uuid).binary().not_null().unique_key())
                    .col(ColumnDef::new(Devices::Name).text().not_null())
                    // ... other columns
                    .to_owned(),
            )
            .await
    }

    async fn down(&self, manager: &SchemaManager) -> Result<(), DbErr> {
        manager
            .drop_table(Table::drop().table(Devices::Table).to_owned())
            .await
    }
}
```

### Running Migrations

```rust
use sea_orm_migration::MigratorTrait;

// Apply all pending migrations
Migrator::up(db, None).await?;

// Reset database (development only)
Migrator::fresh(db).await?;

// Check migration status
let status = Migrator::status(db).await?;
```

## Performance Optimizations

### Indexing Strategy

**Primary indexes** - Critical for query performance:
- `entries(uuid)` - UUID lookups
- `entries(prefix_id, relative_path)` - Path reconstruction
- `entries(name)` - Name-based searches
- `entries(content_id)` - Duplicate detection

**Secondary indexes** - Common filter operations:
- `entries(kind)` - File vs directory filtering
- `entries(size)` - Size-based queries
- `user_metadata(favorite)` - Favorite file listings
- `locations(scan_state)` - Indexing status

**Composite indexes** - Multi-column queries:
- `entries(location_id, relative_path)` - Fast directory hierarchy queries
- `locations(device_id, path)` - Device-specific location lookup

### Query Optimization

**Use covering indexes** where possible:
```sql
-- Index covers entire query, no table lookup needed
CREATE INDEX idx_entries_name_size ON entries(name, size) 
WHERE kind = 'file';
```

**Limit result sets** for UI pagination:
```rust
let entries = Entry::find()
    .filter(entry::Column::Kind.eq("file"))
    .order_by_asc(entry::Column::Name)
    .limit(50)
    .offset(page * 50)
    .all(db)
    .await?;
```

**Use prepared statements** - SeaORM handles this automatically:
```rust
// This generates a prepared statement that's reused
let find_by_name = Entry::find()
    .filter(entry::Column::Name.eq("filename"));
```

### Connection Management

```rust
use sea_orm::{Database, ConnectOptions};

async fn create_connection(database_url: &str) -> Result<DatabaseConnection, DbErr> {
    let mut opt = ConnectOptions::new(database_url.to_owned());
    opt.max_connections(10)
        .min_connections(1)
        .connect_timeout(Duration::from_secs(10))
        .idle_timeout(Duration::from_secs(300))
        .sqlx_logging(false); // Disable in production
    
    Database::connect(opt).await
}
```

## Backup and Recovery

### Database Backup

```rust
use std::fs;

async fn backup_library_database(library_path: &Path) -> Result<(), std::io::Error> {
    let db_path = library_path.join("database.db");
    let backup_path = library_path.join("backups").join(
        format!("database_backup_{}.db", chrono::Utc::now().format("%Y%m%d_%H%M%S"))
    );
    
    fs::create_dir_all(backup_path.parent().unwrap())?;
    fs::copy(&db_path, &backup_path)?;
    
    println!("Database backed up to: {}", backup_path.display());
    Ok(())
}
```

### Point-in-Time Recovery

SQLite WAL mode provides crash recovery:
```sql
-- Enable WAL mode for better concurrency and recovery
PRAGMA journal_mode = WAL;
PRAGMA synchronous = NORMAL;
PRAGMA cache_size = 10000;
PRAGMA temp_store = MEMORY;
```

## Database Utilities

### Vacuum and Optimization

```rust
async fn optimize_database(db: &DatabaseConnection) -> Result<(), DbErr> {
    // Analyze tables for query planner
    db.execute_unprepared("ANALYZE").await?;
    
    // Reclaim free space
    db.execute_unprepared("VACUUM").await?;
    
    // Update table statistics
    db.execute_unprepared("PRAGMA optimize").await?;
    
    Ok(())
}
```

### Statistics and Monitoring

```rust
async fn database_statistics(db: &DatabaseConnection) -> Result<(), DbErr> {
    use sea_orm::FromQueryResult;
    
    #[derive(FromQueryResult)]
    struct TableInfo {
        name: String,
        count: i64,
        size_kb: Option<i64>,
    }
    
    let stats = db.query_all(
        Statement::from_string(
            DbBackend::Sqlite,
            r#"
            SELECT 
                name,
                COUNT(*) as count,
                (page_count * page_size / 1024) as size_kb
            FROM sqlite_master m, sqlite_stat1 s 
            WHERE m.name = s.tbl 
            GROUP BY name
            "#.to_string()
        )
    ).await?;
    
    for stat in stats {
        println!("Table: {} - {} rows - {} KB", 
            stat.name, stat.count, stat.size_kb.unwrap_or(0)
        );
    }
    
    Ok(())
}
```

The database layer provides a solid foundation for Spacedrive's file management needs while maintaining excellent performance characteristics and developer experience.```

## docs/domain-models.md

```markdown
# Domain Models

The core domain models represent Spacedrive's unified approach to file management. These models implement the new file data model design where every file/directory has immediate metadata capabilities.

## Entry - The Universal File/Directory Model

The `Entry` is the central concept - everything (files, directories) is represented uniformly:

```rust
pub struct Entry {
    pub id: i32,                    // Database primary key
    pub uuid: Uuid,                 // Global unique identifier
    pub prefix_id: i32,             // Path compression reference
    pub relative_path: String,      // Path relative to prefix
    pub name: String,               // Display name
    pub kind: EntryKind,            // File or Directory
    pub metadata_id: i32,           // Always present - immediate tagging!
    pub content_id: Option<i32>,    // Optional - for deduplication
    pub location_id: Option<i32>,   // Optional - for organized files
    pub relative_path: String,        // Materialized path for hierarchy
    pub size: u64,                  // Size in bytes
    pub permissions: Option<String>, // File system permissions
    // Timestamps
    pub created_at: DateTime<Utc>,
    pub modified_at: DateTime<Utc>,
    pub accessed_at: Option<DateTime<Utc>>,
}

pub enum EntryKind {
    File,
    Directory,
}
```

### Key Design Decisions

**1. Always-Present Metadata**
- Every entry gets a `metadata_id` immediately upon creation
- No more "can't tag this file until it's indexed" problems
- Instant organization capabilities

**2. Optional Content Identity**
- `content_id` is only populated when content analysis is performed
- Allows immediate file operations without waiting for hashing
- Enables efficient deduplication when desired

**3. Flexible Relationships**
- `relative_path` provides hierarchy through materialized paths
- `location_id` links to organized collections
- Can represent the same physical file in multiple organizational contexts

### Path Optimization

The path storage system dramatically reduces database size:

```rust
// Entry stores materialized paths directly
pub struct Entry {
    pub location_id: i32,        // References Location
    pub relative_path: String,   // e.g., "photos" (directory path)
    pub name: String,            // e.g., "vacation.jpg" (file name)
    // ...
}
```

**Benefits:**
- **Simple queries** without complex joins
- **Fast hierarchy queries** using path patterns
- **Direct path access** for file operations
- **Efficient indexing** on relative_path column

## UserMetadata - Immediate Organization

Every entry has associated metadata for instant tagging and organization:

```rust
pub struct UserMetadata {
    pub id: i32,
    pub uuid: Uuid,
    pub notes: Option<String>,      // User notes
    pub favorite: bool,             // Favorite status
    pub hidden: bool,               // Hidden from normal views
    pub custom_data: Value,         // JSON for extensibility
    pub created_at: DateTime<Utc>,
    pub updated_at: DateTime<Utc>,
}
```

### Tag System

Flexible tagging with many-to-many relationships:

```rust
pub struct Tag {
    pub id: i32,
    pub uuid: Uuid,
    pub name: String,               // "Work", "Personal", "Important"
    pub color: Option<String>,      // Hex color code
    pub icon: Option<String>,       // Icon identifier
    pub created_at: DateTime<Utc>,
    pub updated_at: DateTime<Utc>,
}

// Junction table for many-to-many relationship
pub struct MetadataTag {
    pub metadata_id: i32,
    pub tag_id: i32,
}
```

### Label System

Hierarchical organization with labels:

```rust
pub struct Label {
    pub id: i32,
    pub uuid: Uuid,
    pub name: String,               // "Projects/Web Development"
    pub color: Option<String>,
    // Hierarchy determined by relative_path
    pub created_at: DateTime<Utc>,
    pub updated_at: DateTime<Utc>,
}

// Junction table for labels
pub struct MetadataLabel {
    pub metadata_id: i32,
    pub label_id: i32,
}
```

## ContentIdentity - Deduplication

Optional content-based identity for deduplication and content analysis:

```rust
pub struct ContentIdentity {
    pub id: i32,
    pub cas_id: String,             // Content-addressable storage ID
    pub kind: ContentKind,          // Type of content
    pub size_bytes: u64,           // Actual content size
    pub media_data: Option<Value>, // Media-specific metadata (JSON)
    pub created_at: DateTime<Utc>,
}

pub enum ContentKind {
    Image,
    Video, 
    Audio,
    Document,
    Archive,
    Executable,
    Other,
}
```

### Content Addressing

The `cas_id` uses deterministic hashing for deduplication:

```rust
// Example CAS ID generation
pub fn generate_cas_id(content: &[u8]) -> String {
    let hash = blake3::hash(content);
    hash.to_hex().to_string()
}
```

**Benefits:**
- **Bit-level deduplication** - Identical content shares storage
- **Content verification** - Detect corruption or modification
- **Fast comparison** - Compare hashes instead of file contents

### Media Data

Rich metadata extraction for media files:

```rust
// Example media data structure
{
    "image": {
        "width": 1920,
        "height": 1080,
        "format": "JPEG",
        "camera": {
            "make": "Canon",
            "model": "EOS R5",
            "lens": "RF 24-70mm F2.8 L IS USM"
        },
        "location": {
            "latitude": 40.7128,
            "longitude": -74.0060,
            "altitude": 10.5
        }
    }
}
```

## Location - Organized Collections

Locations represent indexed directories with scanning capabilities:

```rust
pub struct Location {
    pub id: i32,
    pub uuid: Uuid,
    pub device_id: i32,             // Device that owns this location
    pub path: String,               // Absolute path
    pub name: Option<String>,       // Display name
    pub index_mode: IndexMode,      // How to scan this location
    pub scan_state: ScanState,      // Current scanning status
    pub last_scan_at: Option<DateTime<Utc>>,
    pub error_message: Option<String>,
    pub total_file_count: i64,      // Statistics
    pub total_byte_size: i64,
    pub created_at: DateTime<Utc>,
    pub updated_at: DateTime<Utc>,
}

pub enum IndexMode {
    Metadata,    // Index file metadata only
    Content,     // Index metadata + generate content hashes
    Deep,        // Full content analysis + media metadata
}

pub enum ScanState {
    Pending,     // Not yet scanned
    Scanning,    // Currently scanning
    Complete,    // Scan completed successfully
    Error,       // Scan failed
    Paused,      // Scan paused by user
}
```

### Indexing Modes

**Metadata Mode:**
- Fast scanning
- File names, sizes, timestamps
- No content hashing
- Suitable for frequently changing directories

**Content Mode:**
- Moderate speed
- Includes content hashing for deduplication
- CAS ID generation
- Good balance of features and performance

**Deep Mode:**
- Comprehensive analysis
- Media metadata extraction
- Thumbnail generation
- Full-text content indexing (future)
- Best for media libraries and archives

## Device - Unified Identity

Single concept for device identity (replacing Node/Device/Instance confusion):

```rust
pub struct Device {
    pub id: i32,                    // Database primary key
    pub uuid: Uuid,                 // Global device identifier
    pub name: String,               // User-friendly name
    pub os: String,                 // Operating system
    pub os_version: Option<String>, // OS version details
    pub hardware_model: String,     // Hardware identifier
    pub network_addresses: Value,   // JSON array of IP addresses
    pub is_online: bool,            // Current online status
    pub last_seen_at: DateTime<Utc>,
    pub capabilities: Value,        // JSON capabilities object
    pub created_at: DateTime<Utc>,
    pub updated_at: DateTime<Utc>,
}
```

### Device Capabilities

Structured capability reporting:

```rust
// Example capabilities JSON
{
    "indexing": true,       // Can index local files
    "p2p": true,           // Supports P2P connections
    "cloud": false,        // Has cloud sync enabled
    "thumbnails": true,    // Can generate thumbnails
    "preview": false,      // Can generate previews
    "transcoding": false   // Can transcode media
}
```

## Relationships

The domain models form a rich relationship graph:

```
Device (1) â”€â”€â†’ (N) Location
Location (1) â”€â”€â†’ (N) Entry
Entry (1) â”€â”€â†’ (1) UserMetadata
Entry (1) â”€â”€â†’ (0..1) ContentIdentity

UserMetadata (N) â”€â”€â†’ (N) Tag
UserMetadata (N) â”€â”€â†’ (N) Label

ContentIdentity (1) â”€â”€â†’ (N) Entry  [deduplication]
```

### Query Patterns

Common queries are optimized through proper indexing:

```rust
// Find all entries with a specific tag
let entries = Entry::find()
    .find_with_related(UserMetadata)
    .join(JoinType::InnerJoin, metadata_tag::Relation::Tag.def())
    .filter(tag::Column::Name.eq("Important"))
    .all(db)
    .await?;

// Find duplicate content
let duplicates = ContentIdentity::find()
    .find_with_related(Entry)
    .having(entry::Column::Id.count().gt(1))
    .group_by(content_identity::Column::CasId)
    .all(db)
    .await?;

// Reconstruct full path
let full_path = format!("{}/{}", 
    entry.prefix.prefix,
    entry.relative_path
);
```

## Serialization

All domain models support serialization for API responses and job persistence:

```rust
#[derive(Serialize, Deserialize)]
pub struct SdPathSerialized {
    pub device_uuid: Uuid,
    pub path: PathBuf,
}

impl From<Entry> for SdPathSerialized {
    fn from(entry: Entry) -> Self {
        // Convert Entry to serializable path representation
    }
}
```

## Migration Path

The domain models are designed to support migration from the original Spacedrive schema:

1. **Entry mapping** - Convert file_path and object tables to unified Entry
2. **Metadata creation** - Generate UserMetadata for all existing files  
3. **Path optimization** - Extract common prefixes and compress paths
4. **Content preservation** - Map existing CAS IDs to ContentIdentity
5. **Device unification** - Merge Node/Device/Instance concepts

This design provides a solid foundation for all Spacedrive operations while maintaining the flexibility to evolve as requirements change.```

## docs/examples.md

```markdown
# Examples and Usage

This document provides working examples of Core v2 functionality in both code and CLI forms. All examples are runnable and tested.

## CLI Examples

For interactive usage, see the [CLI Documentation](./cli.md). Quick example:

```bash
# Build and start the daemon
cargo build --release
./target/release/spacedrive start

# Create a library and add locations
./target/release/spacedrive library create "Personal"
./target/release/spacedrive location add ~/Documents --name "Documents"
./target/release/spacedrive job monitor
```

## Running Code Examples

```bash
# Library management and database operations
cargo run --example library_demo

# Job system demonstration
cargo run --example job_demo

# File type detection system
cargo run --example file_type_demo

# Database operations and schema
cargo run --example database_test

# Content indexing workflows
cargo run --example content_indexing
```

## Basic Core Usage

### Initializing Core

```rust
use sd_core_new::Core;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Initialize with default data directory
    let core = Core::new().await?;
    
    // Or specify custom directory
    let custom_core = Core::new_with_config(
        PathBuf::from("/custom/spacedrive/data")
    ).await?;
    
    println!("Device ID: {}", core.device.device_id()?);
    println!("Device Name: {}", core.device.device_name()?);
    
    // Core automatically handles cleanup on drop
    Ok(())
}
```

### Core Shutdown

```rust
// Graceful shutdown
async fn shutdown_example(core: Core) -> Result<(), Box<dyn std::error::Error>> {
    // Manually trigger shutdown for cleanup
    core.shutdown().await?;
    println!("Core shutdown complete");
    Ok(())
}
```

## Library Management

### Creating Libraries

```rust
use sd_core_new::Core;
use std::path::PathBuf;

async fn create_library_example() -> Result<(), Box<dyn std::error::Error>> {
    let core = Core::new().await?;
    
    // Create library with auto-generated path
    let library = core.libraries
        .create_library("My Documents", None)
        .await?;
    
    println!("Library created: {}", library.name().await);
    println!("Library path: {}", library.path().display());
    println!("Library ID: {}", library.id());
    
    // Create library at specific location
    let specific_library = core.libraries
        .create_library("Projects", Some(PathBuf::from("/home/user/projects")))
        .await?;
    
    Ok(())
}
```

### Opening and Closing Libraries

```rust
async fn library_lifecycle_example() -> Result<(), Box<dyn std::error::Error>> {
    let core = Core::new().await?;
    
    // Create a library
    let library = core.libraries
        .create_library("Test Library", None)
        .await?;
    
    let library_path = library.path().to_path_buf();
    let library_id = library.id();
    
    // Close the library
    core.libraries.close_library(library_id).await?;
    
    // Drop the library reference to release locks
    drop(library);
    
    // Reopen the library
    let reopened = core.libraries
        .open_library(&library_path)
        .await?;
    
    assert_eq!(reopened.id(), library_id);
    println!("Library reopened successfully");
    
    Ok(())
}
```

### Library Discovery

```rust
async fn library_discovery_example() -> Result<(), Box<dyn std::error::Error>> {
    let core = Core::new().await?;
    
    // Scan for existing libraries
    let discovered = core.libraries.scan_for_libraries().await?;
    println!("Found {} libraries", discovered.len());
    
    for discovered_lib in discovered {
        println!("  - {} at {}", 
            discovered_lib.name, 
            discovered_lib.path.display()
        );
    }
    
    // Auto-load all libraries
    let loaded_count = core.libraries.load_all().await?;
    println!("Loaded {} libraries", loaded_count);
    
    // List currently open libraries
    let open_libraries = core.libraries.list().await;
    for library in open_libraries {
        println!("Open: {} ({})", library.name().await, library.id());
    }
    
    Ok(())
}
```

## Database Operations

### Working with Entries

```rust
use sd_core_new::infrastructure::database::entities;
use sea_orm::{EntityTrait, Set, ActiveModelTrait, ActiveValue::NotSet};
use uuid::Uuid;

async fn create_entry_example(library: &Library) -> Result<(), Box<dyn std::error::Error>> {
    let db = library.db();
    
    // Create metadata first (every entry needs metadata)
    let metadata = entities::user_metadata::ActiveModel {
        id: NotSet,
        uuid: Set(Uuid::new_v4()),
        notes: Set(Some("Important document".to_string())),
        favorite: Set(true),
        hidden: Set(false),
        custom_data: Set(serde_json::json!({})),
        created_at: Set(chrono::Utc::now()),
        updated_at: Set(chrono::Utc::now()),
    };
    let metadata_record = metadata.insert(db.conn()).await?;
    
    // No need for path prefixes - we use materialized paths directly
    
    // Create the entry
    let entry = entities::entry::ActiveModel {
        id: NotSet,
        uuid: Set(Uuid::new_v4()),
        location_id: Set(1), // Assume location exists
        relative_path: Set("".to_string()), // Root of location
        name: Set("important.pdf".to_string()),
        kind: Set("file".to_string()),
        metadata_id: Set(metadata_record.id),
        content_id: Set(None), // Will be set during content analysis
        size: Set(1024 * 1024), // 1MB
        permissions: Set(Some("644".to_string())),
        created_at: Set(chrono::Utc::now()),
        modified_at: Set(chrono::Utc::now()),
        accessed_at: Set(Some(chrono::Utc::now())),
    };
    let entry_record = entry.insert(db.conn()).await?;
    
    println!("Entry created: {} (ID: {})", 
        entry_record.name, 
        entry_record.id
    );
    
    Ok(())
}
```

### Tagging and Organization

```rust
async fn tagging_example(library: &Library) -> Result<(), Box<dyn std::error::Error>> {
    let db = library.db();
    
    // Create tags
    let work_tag = entities::tag::ActiveModel {
        id: NotSet,
        uuid: Set(Uuid::new_v4()),
        name: Set("Work".to_string()),
        color: Set(Some("#3B82F6".to_string())), // Blue
        icon: Set(Some("briefcase".to_string())),
        created_at: Set(chrono::Utc::now()),
        updated_at: Set(chrono::Utc::now()),
    };
    let work_tag_record = work_tag.insert(db.conn()).await?;
    
    let important_tag = entities::tag::ActiveModel {
        id: NotSet,
        uuid: Set(Uuid::new_v4()),
        name: Set("Important".to_string()),
        color: Set(Some("#EF4444".to_string())), // Red
        icon: Set(Some("star".to_string())),
        created_at: Set(chrono::Utc::now()),
        updated_at: Set(chrono::Utc::now()),
    };
    let important_tag_record = important_tag.insert(db.conn()).await?;
    
    // Link tags to metadata (assuming metadata_id exists)
    let metadata_id = 1; // From previous example
    
    let work_link = entities::metadata_tag::ActiveModel {
        metadata_id: Set(metadata_id),
        tag_id: Set(work_tag_record.id),
    };
    work_link.insert(db.conn()).await?;
    
    let important_link = entities::metadata_tag::ActiveModel {
        metadata_id: Set(metadata_id),
        tag_id: Set(important_tag_record.id),
    };
    important_link.insert(db.conn()).await?;
    
    println!("Tags created and linked to metadata");
    
    Ok(())
}
```

### Querying with Relationships

```rust
use sea_orm::{JoinType, QueryFilter, QuerySelect, ColumnTrait};

async fn query_examples(library: &Library) -> Result<(), Box<dyn std::error::Error>> {
    let db = library.db();
    
    // Find all entries with specific tag
    let work_entries = entities::entry::Entity::find()
        .join(JoinType::InnerJoin, entities::entry::Relation::UserMetadata.def())
        .join(JoinType::InnerJoin, entities::user_metadata::Relation::MetadataTag.def())
        .join(JoinType::InnerJoin, entities::metadata_tag::Relation::Tag.def())
        .filter(entities::tag::Column::Name.eq("Work"))
        .all(db.conn())
        .await?;
    
    println!("Found {} work-related entries", work_entries.len());
    
    // Find entries by file extension
    let pdf_entries = entities::entry::Entity::find()
        .filter(entities::entry::Column::Name.like("%.pdf"))
        .all(db.conn())
        .await?;
    
    println!("Found {} PDF files", pdf_entries.len());
    
    // Find large files (> 100MB)
    let large_files = entities::entry::Entity::find()
        .filter(entities::entry::Column::Size.gt(100 * 1024 * 1024))
        .filter(entities::entry::Column::Kind.eq("file"))
        .all(db.conn())
        .await?;
    
    println!("Found {} large files", large_files.len());
    
    Ok(())
}
```

## Content Identity and Deduplication

```rust
async fn content_identity_example(library: &Library) -> Result<(), Box<dyn std::error::Error>> {
    let db = library.db();
    
    // Create content identity for deduplication
    let content_id = entities::content_identity::ActiveModel {
        id: NotSet,
        cas_id: Set("blake3_hash_here".to_string()),
        kind: Set("document".to_string()),
        size_bytes: Set(1024 * 1024),
        media_data: Set(Some(serde_json::json!({
            "document": {
                "page_count": 15,
                "format": "PDF",
                "has_text": true,
                "created_with": "Adobe Acrobat"
            }
        }))),
        created_at: Set(chrono::Utc::now()),
    };
    let content_record = content_id.insert(db.conn()).await?;
    
    println!("Content identity created: {}", content_record.cas_id);
    
    // Find duplicate content
    use sea_orm::{QuerySelect, QueryFilter, PaginatorTrait};
    
    let duplicate_content = entities::content_identity::Entity::find()
        .find_with_related(entities::entry::Entity)
        .filter(entities::entry::Column::ContentId.is_not_null())
        .all(db.conn())
        .await?;
    
    for (content, entries) in duplicate_content {
        if entries.len() > 1 {
            println!("Found {} duplicates of content {}", 
                entries.len(), 
                content.cas_id
            );
            for entry in entries {
                println!("  - {}", entry.name);
            }
        }
    }
    
    Ok(())
}
```

## Job System Usage

### Creating and Running Jobs

```rust
use sd_core_new::{
    infrastructure::jobs::manager::JobManager,
    operations::{
        file_ops::copy_job::FileCopyJob,
        indexing::indexer_job::{IndexerJob, IndexMode},
    },
    shared::types::SdPath,
};

async fn job_system_example() -> Result<(), Box<dyn std::error::Error>> {
    // Initialize job manager
    let data_dir = std::env::temp_dir().join("spacedrive_jobs");
    let job_manager = JobManager::new(data_dir).await?;
    
    // Create a file copy job
    let device_id = uuid::Uuid::new_v4();
    let sources = vec![
        SdPath::new(device_id, PathBuf::from("/source/file1.txt")),
        SdPath::new(device_id, PathBuf::from("/source/file2.txt")),
    ];
    let destination = SdPath::new(device_id, PathBuf::from("/destination"));
    
    let copy_job = FileCopyJob::new(sources, destination);
    
    // Demonstrate job serialization
    let serialized = rmp_serde::to_vec(&copy_job)?;
    println!("Job serialized to {} bytes", serialized.len());
    
    let deserialized: FileCopyJob = rmp_serde::from_slice(&serialized)?;
    println!("Job deserialized successfully");
    
    // Create an indexer job
    let indexer_job = IndexerJob::new(
        uuid::Uuid::new_v4(), // library_id
        SdPath::new(device_id, PathBuf::from("/index/path")),
        IndexMode::Content,
    );
    
    println!("Jobs created and tested successfully");
    
    // Shutdown job manager
    job_manager.shutdown().await?;
    
    Ok(())
}
```

### Job Progress Monitoring

```rust
use sd_core_new::infrastructure::jobs::{
    progress::Progress,
    prelude::JobProgress,
};

#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
struct CustomProgress {
    current_file: String,
    files_processed: u64,
    total_files: u64,
    bytes_processed: u64,
    total_bytes: u64,
}

impl JobProgress for CustomProgress {}

async fn progress_reporting_example() {
    // Simple percentage progress
    let progress = Progress::percentage(0.75); // 75% complete
    
    // Structured progress with rich data
    let custom_progress = CustomProgress {
        current_file: "vacation_photos/IMG_001.jpg".to_string(),
        files_processed: 150,
        total_files: 500,
        bytes_processed: 1024 * 1024 * 100, // 100MB
        total_bytes: 1024 * 1024 * 400,     // 400MB
    };
    
    let structured_progress = Progress::structured(custom_progress);
    
    // In a real job, you would report progress via JobContext:
    // ctx.progress(structured_progress);
    
    println!("Progress reporting examples completed");
}
```

## File Type System

```rust
use sd_core_new::file_type::{FileTypeRegistry, FileType};

async fn file_type_example() -> Result<(), Box<dyn std::error::Error>> {
    // Initialize file type registry
    let registry = FileTypeRegistry::new();
    
    // Test various file types
    let test_files = vec![
        "document.pdf",
        "photo.jpg", 
        "video.mp4",
        "archive.zip",
        "source.rs",
        "unknown.xyz",
    ];
    
    for filename in test_files {
        match registry.detect_from_extension(filename) {
            Some(file_type) => {
                println!("{}: {} ({})", 
                    filename, 
                    file_type.name(), 
                    file_type.category()
                );
                
                // Check capabilities
                if file_type.supports_thumbnails() {
                    println!("  âœ“ Supports thumbnails");
                }
                if file_type.supports_preview() {
                    println!("  âœ“ Supports preview");
                }
                if file_type.supports_metadata_extraction() {
                    println!("  âœ“ Supports metadata extraction");
                }
            }
            None => {
                println!("{}: Unknown file type", filename);
            }
        }
    }
    
    Ok(())
}
```

## Event System

```rust
use sd_core_new::infrastructure::events::{Event, EventBus};

async fn event_system_example() -> Result<(), Box<dyn std::error::Error>> {
    let event_bus = EventBus::default();
    
    // Subscribe to events (in a real application)
    // let subscription = event_bus.subscribe().await;
    
    // Emit various events
    event_bus.emit(Event::CoreStarted);
    
    event_bus.emit(Event::LibraryCreated {
        id: uuid::Uuid::new_v4(),
        name: "Test Library".to_string(),
    });
    
    event_bus.emit(Event::EntryCreated {
        library_id: uuid::Uuid::new_v4(),
        entry_id: uuid::Uuid::new_v4(),
    });
    
    println!("Events emitted successfully");
    
    Ok(())
}
```

## Testing Examples

### Library Testing

```rust
#[tokio::test]
async fn test_library_operations() {
    use tempfile::TempDir;
    
    // Create temporary directory for test
    let temp_dir = TempDir::new().unwrap();
    let core = Core::new_with_config(temp_dir.path().to_path_buf()).await.unwrap();
    
    // Create library
    let library = core.libraries
        .create_library("Test Library", None)
        .await
        .unwrap();
    
    // Verify library structure
    assert_eq!(library.name().await, "Test Library");
    assert!(library.path().exists());
    assert!(library.path().join("database.db").exists());
    
    // Test configuration updates
    library.update_config(|config| {
        config.description = Some("Test description".to_string());
    }).await.unwrap();
    
    let config = library.config().await;
    assert_eq!(config.description, Some("Test description".to_string()));
}
```

### Job Testing

```rust
#[tokio::test]
async fn test_job_serialization() {
    use sd_core_new::operations::file_ops::copy_job::FileCopyJob;
    
    let device_id = uuid::Uuid::new_v4();
    let sources = vec![
        SdPath::new(device_id, PathBuf::from("/test/file.txt")),
    ];
    let destination = SdPath::new(device_id, PathBuf::from("/dest"));
    
    let job = FileCopyJob::new(sources, destination);
    
    // Test serialization round-trip
    let serialized = rmp_serde::to_vec(&job).unwrap();
    let deserialized: FileCopyJob = rmp_serde::from_slice(&serialized).unwrap();
    
    assert_eq!(job.sources.len(), deserialized.sources.len());
}
```

## Performance Testing

```rust
async fn performance_example() -> Result<(), Box<dyn std::error::Error>> {
    use std::time::Instant;
    
    let core = Core::new().await?;
    let library = core.libraries
        .create_library("Performance Test", None)
        .await?;
    
    let db = library.db();
    
    // Test bulk insert performance
    let start = Instant::now();
    
    for i in 0..1000 {
        let metadata = entities::user_metadata::ActiveModel {
            id: NotSet,
            uuid: Set(Uuid::new_v4()),
            notes: Set(Some(format!("Test entry {}", i))),
            favorite: Set(false),
            hidden: Set(false),
            custom_data: Set(serde_json::json!({})),
            created_at: Set(chrono::Utc::now()),
            updated_at: Set(chrono::Utc::now()),
        };
        metadata.insert(db.conn()).await?;
    }
    
    let duration = start.elapsed();
    println!("Inserted 1000 metadata records in {:?}", duration);
    println!("Rate: {:.2} records/second", 
        1000.0 / duration.as_secs_f64()
    );
    
    Ok(())
}
```

## Integration Examples

### Complete Workflow

```rust
async fn complete_workflow_example() -> Result<(), Box<dyn std::error::Error>> {
    // 1. Initialize Core
    let core = Core::new().await?;
    println!("âœ“ Core initialized");
    
    // 2. Create library
    let library = core.libraries
        .create_library("Complete Workflow", None)
        .await?;
    println!("âœ“ Library created: {}", library.name().await);
    
    // 3. Register device in library
    let device = core.device.to_device()?;
    let db = library.db();
    
    let device_model = entities::device::ActiveModel {
        id: NotSet,
        uuid: Set(device.id),
        name: Set(device.name),
        os: Set(device.os.to_string()),
        os_version: Set(None),
        hardware_model: Set(device.hardware_model),
        network_addresses: Set(serde_json::json!([])),
        is_online: Set(true),
        last_seen_at: Set(chrono::Utc::now()),
        capabilities: Set(serde_json::json!({
            "indexing": true,
            "p2p": false,
            "cloud": false
        })),
        created_at: Set(device.created_at),
        updated_at: Set(device.updated_at),
    };
    let device_record = device_model.insert(db.conn()).await?;
    println!("âœ“ Device registered: {}", device_record.name);
    
    // 4. Create location for indexing
    let location = entities::location::ActiveModel {
        id: NotSet,
        uuid: Set(Uuid::new_v4()),
        device_id: Set(device_record.id),
        path: Set("/home/user/Documents".to_string()),
        name: Set(Some("Documents".to_string())),
        index_mode: Set("content".to_string()),
        scan_state: Set("pending".to_string()),
        last_scan_at: Set(None),
        error_message: Set(None),
        total_file_count: Set(0),
        total_byte_size: Set(0),
        created_at: Set(chrono::Utc::now()),
        updated_at: Set(chrono::Utc::now()),
    };
    let location_record = location.insert(db.conn()).await?;
    println!("âœ“ Location created: {}", location_record.path);
    
    // 5. Create sample content
    // ... (similar to previous examples)
    
    println!("âœ… Complete workflow finished successfully");
    
    Ok(())
}
```

All examples are verified through the test suite and demonstrate the real capabilities of Core v2.```

## docs/history.md

```markdown
# Spacedrive: A Historical Chronicle

## Table of Contents

1. [Introduction](#introduction)
2. [Origins and Founding Vision (2021-2022)](#origins-and-founding-vision-2021-2022)
3. [The Viral Launch (May 2022)](#the-viral-launch-may-2022)
4. [Early Development and Funding (2022-2023)](#early-development-and-funding-2022-2023)
5. [Technical Architecture Evolution](#technical-architecture-evolution)
6. [Community Growth and Public Reception](#community-growth-and-public-reception)
7. [Key Milestones and Releases](#key-milestones-and-releases)
8. [Why Spacedrive Failed: A Technical Post-Mortem](#why-spacedrive-failed-a-technical-post-mortem)
9. [The V2 Reimagining (2025)](#the-v2-reimagining-2025)
10. [The AI-Augmented Development Revolution](#the-ai-augmented-development-revolution)
11. [Impact and Legacy](#impact-and-legacy)
12. [Lessons Learned from the Failure](#lessons-learned-from-the-failure)
13. [Future Vision](#future-vision)

## Introduction

Spacedrive represents one of the most ambitious attempts to revolutionize personal file management in the modern era. Born from frustration with fragmented cloud storage and device ecosystems, it promised to unify all user data under a single, intelligent interface powered by a Virtual Distributed File System (VDFS). This document chronicles the journey from a developer's personal project to a venture-backed open-source phenomenon that captured the imagination of hundreds of thousands of users worldwide.

## Origins and Founding Vision (2021-2022)

### The Personal Catalyst

Jamie Pine, Spacedrive's founder, had been accumulating digital memories since childhoodâ€”tens of thousands of photos, project files, and documents scattered across drives and cloud services. Like many digital natives, he found himself trapped in "data fragmentation hell," spending excessive time searching for files across disconnected silos. This personal pain point became the catalyst for something revolutionary.

### Early Development

In early 2021, Pine began developing what would become Spacedrive. The core premise was radical yet simple: "files shouldn't be stuck in a device ecosystem." Over 15 months of intense development, he crafted the foundations of a cross-platform file manager that could break free from proprietary cloud silos and give users permanent ownership of their data.

The initial vision centered on three principles:
1. **Unification**: One explorer to access files from any device or cloud
2. **Intelligence**: AI-powered organization and search capabilities
3. **Freedom**: No vendor lock-in, complete user control

## The Viral Launch (May 2022)

### Open Source Debut

In May 2022, Pine made the momentous decision to open-source Spacedrive on GitHub. The response exceeded all expectations:

- **#1 on GitHub Trending** for 3 consecutive days
- **10,000+ stars** within the first week
- **Front page of Hacker News** twice during launch week
- Immediate global attention from developers and tech enthusiasts

### Why It Resonated

The viral reception wasn't accidental. Spacedrive addressed a universal problem every computer user facedâ€”file chaos. Its promise to create a "personal distributed cloud" without sacrificing privacy or control struck a chord with:
- Developers tired of juggling multiple cloud APIs
- Creative professionals managing massive media libraries
- Privacy-conscious users seeking alternatives to big tech
- "Data hoarders" with collections spanning decades

## Early Development and Funding (2022-2023)

### Seed Investment and Notable Backers

On June 13, 2022, Spacedrive announced a $2 million seed round led by OSS Capital's Joseph Jacks. The investor roster read like a who's who of tech leadership:

- **Naval Ravikant** (AngelList co-founder)
- **Guillermo Rauch** (Vercel CEO)
- **Tobias LÃ¼tke** (Shopify CEO)
- **Tom Preston-Werner** (GitHub co-founder)
- **Neha Narkhede** (Apache Kafka co-creator)
- **Haoyuan Li** (Alluxio founder, VDFS paper author)

This backing validated Spacedrive's potential to "dramatically simplify" the fragmented storage landscape and enabled Pine to build a distributed team.

### Building the Team

With funding secured, Spacedrive Technology Inc. was formally established as a fully remote company. The team grew to include:
- Engineers from Brazil, Jordan, Finland, USA, and beyond
- Ericson Soares as Head of Engineering
- Product designers and community managers
- Over 100 open-source contributors worldwide

## Technical Architecture Evolution

### The Original V1 Architecture (2022-2024)

The first version introduced groundbreaking concepts:

#### Virtual Distributed File System (VDFS)
- Unified namespace across all storage locations
- Content-addressable storage using unique file hashes
- Real-time synchronized index using embedded SQLite
- Device-agnostic file organization

#### The PRRTT Stack
A modern polyglot architecture combining:
- **Prisma** (database ORM)
- **Rust** (core backend)
- **React** (UI framework)
- **TypeScript** (frontend logic)
- **Tauri** (native app wrapper)

#### Key Innovations
1. **Constant-time hashing** for large files
2. **Peer-to-peer synchronization** across devices
3. **AI-ready metadata extraction**
4. **Cross-platform native apps** with minimal resource usage

### Performance Metrics (V1)
- 22,000 GitHub stars by October 2023
- 149,000+ unique installations by February 2024
- Average session duration: 54 minutes
- Supported Windows, macOS, Linux, and mobile prototypes

## Community Growth and Public Reception

### Developer Enthusiasm

The GitHub repository became a hub of activity:
- **35,000+ stars** by 2025
- **1,100+ forks**
- **117+ contributors**
- Translations in 11 languages
- Active Discord community with thousands of members

### Media Coverage

Tech press embraced Spacedrive's vision:
- **ZDNet**: "The cross-platform file manager of your dreams"
- **It's FOSS**: "A dreamy Rust-based open-source file manager"
- **LinuxLinks**: "The most interesting file manager we've seen in a long time"
- **The New Stack**: "A cross-platform file manager for the modern era"

### User Feedback Themes

Early adopters praised:
- Lightning-fast search across all devices
- Beautiful, space-themed UI
- Unified view of disconnected drives
- Privacy-first approach

Common reservations:
- Alpha stability concerns
- Missing sync features
- Incomplete cloud integrations

## Key Milestones and Releases

### Timeline of Major Releases

| Date | Version | Key Features | Significance |
|------|---------|--------------|--------------|
| Oct 2023 | Alpha 0.1.0 | Basic indexing, preview, search | First public release |
| Feb 2024 | Alpha 0.2.0 | Drag-and-drop, AI labels, 11 languages | 149k installations |
| Mid 2024 | Alpha 0.3.x | Column view, mobile TestFlight | 100+ contributors |
| Late 2024 | Alpha 0.4.x | Spacedrop, content deduplication | 30k+ GitHub stars |
| Early 2025 | Development Pause | Temporary halt announced | 35k stars, 500k installs |
| July 2025 | V2 Architecture | Complete rewrite with AI assistance | 3 weeks dev time, solved all V1 issues |

### Feature Evolution

Each release expanded capabilities:
1. **Indexing**: From read-only browsing to full file operations
2. **Search**: Keyword matching to advanced filters and AI
3. **Organization**: Basic folders to sophisticated tagging
4. **Sync**: Local indexing to P2P device communication
5. **Media**: Simple previews to intelligent galleries

## Why Spacedrive Failed: A Technical Post-Mortem

### The Development Pause

In early 2025, after achieving 35,000 GitHub stars and 500,000 installations, Spacedrive development came to an abrupt halt. The team announced a temporary pause citing funding constraints, but the real story was far more complex. A deep technical analysis reveals that fundamental architectural flaws, decision paralysis, and over-engineering had created an unsustainable development burden.

### The Fatal Flaw: Dual File Systems

The most critical architectural mistake was the existence of two completely separate file management systems that couldn't interoperate:

**1. Indexed System**: Database-driven files with rich metadata, background jobs, and async operations
**2. Ephemeral System**: Direct filesystem access for non-indexed files with immediate operations

This created an impossible user experience:
- **Cannot copy between systems**: Users couldn't copy files from their indexed desktop to a non-indexed USB drive
- **Duplicate everything**: Every file operation had to be implemented twice with different APIs
- **User confusion**: "Why can't I copy from my home folder to my indexed desktop?"
- **Maintenance nightmare**: 2x the code, 2x the bugs, 2x the testing

### The `invalidate_query` Anti-Pattern

The second major architectural flaw was the query invalidation system that violated fundamental principles:

```rust
// Backend code knowing about frontend React Query keys
invalidate_query!(library, "search.paths");
invalidate_query!(library, "search.ephemeralPaths");
```

This created:
- **Frontend-backend coupling**: Backend hardcoded frontend cache keys
- **Brittle string-based system**: No type safety, prone to typos
- **Scattered invalidations**: Calls spread throughout the codebase
- **Over-invalidation**: Often invalidated entire query categories unnecessarily

### The Sync System That Never Shipped

Perhaps the most telling failure was the sync systemâ€”a core promise of Spacedrive that never materialized:

**The Problem**: Mixed local and shared data requirements
- Some data must sync (file metadata, tags)
- Some data must remain local (preferences, local paths)
- No clear architectural boundary between the two

**The Over-Engineering**:
- Custom CRDT implementation built from scratch
- Dual database tables (`cloud_crdt_operation` and `crdt_operation`)
- Complex actor model with multiple concurrent actors
- Analysis paralysis over what should sync

**Why It Failed**:
- The team couldn't agree on sync boundaries
- Perfect became the enemy of good
- Should have used existing SQLite sync solutions
- Engineering debates prevented shipping

### Abandoned Dependencies: Creating Then Abandoning Libraries

A critical piece of context often missed: The Spacedrive team didn't just use prisma-client-rust and rspcâ€”they **created** them:

**prisma-client-rust**:
- Created by Spacedrive team members
- Added custom sync generation with `@shared` and `@local` attributes
- When needs diverged, the library was abandoned
- Left Spacedrive on a deprecated fork of Prisma 4.x
- Prisma moving away from Rust support made this worse

**rspc**:
- Also created by Spacedrive team members
- Provides type-safe RPC between Rust and TypeScript
- Abandoned when Spacedrive's needs changed
- Custom modifications in fork created maintenance burden

This pattern of creating libraries and abandoning them when requirements changed left Spacedrive with significant technical debt.

### Job System: Death by a Thousand Lines

The job system, while well-engineered, required 500-1000+ lines of boilerplate to add any new operation:

```rust
// Required for EVERY new job:
1. Add to JobName enum
2. Implement Job trait (100-200 lines)
3. Implement SerializableJob (100-200 lines)
4. Add to central registry macro
5. Handle serialization/deserialization
6. Write progress tracking
7. Implement error handling
```

Result: Simple operations like "copy file" became massive engineering efforts.

### The Unfulfilled Search Promise

Despite marketing "lightning fast search across all your files," the search implementation was rudimentary:

**What Was Promised**: Virtual Distributed File System with instant search everywhere
**What Was Delivered**: Basic SQL `LIKE` queries on local files only

Missing features:
- No content search inside documents
- No full-text search indexes
- No vector/semantic search
- Can't search offline drives
- Separate search implementations for indexed vs ephemeral files

### Identity Crisis: Node vs Device vs Instance

Three different ways to represent the same concept (a Spacedrive installation):

```
Node: P2P identity for the application
Device: Sync system identity for hardware  
Instance: Library-specific P2P identity
```

This created:
- Confusion about which ID to use when
- Complex identity mapping between systems
- Data duplication and sync issues
- Made multi-device features exponentially harder

### Organizational Chaos

The codebase structure revealed incomplete refactoring:

```
/core/src/
  old_job/           # Still referenced
  old_p2p/           # Still used
  object/fs/
    old_copy.rs      # Critical logic here
    old_cut.rs       # Why "old"?
    old_delete.rs    # Still in use!
```

Both old and new systems ran in parallel throughout the codebase, creating confusion about which to use and when.

### The Real Reasons for Failure

1. **Over-Engineering**: Every system was built for a perfect future that never came
2. **Decision Paralysis**: Debates about ideal architecture prevented shipping
3. **Incomplete Migrations**: New systems built without removing old ones
4. **Scope Creep**: Trying to solve every edge case before shipping basics
5. **Technical Debt Accumulation**: Each clever solution created more problems

## The V2 Reimagining (2025)

### Acknowledging Reality

After 3 years of V1 development, the technical analysis revealed:
- The dual file system made basic operations impossible
- The sync system was fundamentally flawed
- Abandoned dependencies created an unmaintainable codebase
- Job system boilerplate prevented rapid iteration
- Search never fulfilled its core promise
- Identity confusion permeated the architecture

### The Complete Rewrite

July 2025 marked a pivotal moment with the V2 whitepaper publication, presenting a ground-up reimplementation that addressed every major flaw:

#### 1. Unified File System Architecture

The dual file system problem was solved with a single, elegant abstraction:

```rust
// V2: One system to rule them all
pub enum EntrySource {
    Indexed(LocationId, EntryId),
    Ephemeral(PathBuf),
    Remote(DeviceId, SdPath),
}
```

- All files treated uniformly regardless of source
- Seamless operations between indexed and ephemeral files
- Progressive enhancement: ephemeral files can become indexed
- No more duplicate implementations

#### 2. Entry-Centric Model

Replaced the file-centric approach with entries that carry context:

```rust
pub struct Entry {
    pub id: EntryId,
    pub path: SdPath,           // Universal addressing
    pub metadata: Metadata,      // Always available
    pub content_id: Option<ContentId>,  // Progressive enhancement
    pub user_data: UserMetadata, // Tags, ratings, etc.
}
```

Benefits:
- Immediate organization without waiting for indexing
- Metadata available even for ephemeral files
- Clean separation between system and user data
- Natural progression from discovery to full indexing

#### 3. SdPath Universal Addressing

Revolutionary addressing system that makes device boundaries transparent:

```rust
pub enum SdPath {
    Physical {
        device: DeviceId,
        volume: VolumeId,
        path: PathBuf,
    },
    Content {
        hash: ContentId,
        hint: Option<PhysicalPath>,
    },
}
```

This enables:
- Addressing files that don't exist locally
- Content-based retrieval across devices
- Future-proof distributed operations
- Clean abstraction over platform differences

#### 4. Simplified Sync Architecture

Complete abandonment of the failed CRDT approach:

**Domain Separation**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Library Sync   â”‚ â†’ What files exist, where
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Metadata Sync   â”‚ â†’ User tags, ratings
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Content Sync   â”‚ â†’ Actual file transfer
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Clear Boundaries**:
- Local-only data never enters sync system
- Shared data in separate tables from the start
- No mixed concerns, no confusion
- Third-party sync solutions become possible

#### 5. Event-Driven Architecture

Replaced the `invalidate_query` anti-pattern:

```rust
// V2: Clean event system
pub enum DomainEvent {
    EntryCreated(Entry),
    EntryModified(EntryId, Changes),
    EntryDeleted(EntryId),
    // ... domain-specific events
}

// Frontend subscribes to what it needs
eventBus.subscribe<EntryCreated>(|event| {
    // Update UI based on domain events
});
```

#### 6. Pragmatic Job System

Reduced from 1000+ lines to ~50 lines per job:

```rust
#[derive(Job)]
pub struct CopyFiles {
    source: Vec<SdPath>,
    destination: SdPath,
}

impl Execute for CopyFiles {
    async fn run(&self, ctx: Context) -> Result<()> {
        // Just the business logic
    }
}
```

Procedural macros handle all boilerplate, making new operations trivial to add.

#### 7. Real Search Implementation

Finally delivering on the VDFS promise:

```rust
pub struct SearchEngine {
    content_index: ContentIndex,    // Full-text search
    metadata_index: MetadataIndex,  // Fast attribute queries
    vector_store: VectorStore,      // Semantic search
}

// Unified search across all dimensions
let results = search
    .query("vacation photos from last summer")
    .with_content_search()
    .with_semantic_matching()
    .across_devices(&[laptop, phone, nas])
    .execute()
    .await?;
```

#### 8. Single Identity System

Replaced the Node/Device/Instance confusion:

```rust
pub struct Device {
    pub id: DeviceId,           // One ID per installation
    pub name: String,           // User-friendly name
    pub identity: Identity,     // P2P identity
    pub libraries: Vec<LibraryId>, // What libraries it has
}
```

One concept, one implementation, no confusion.

### Performance Achievements (V2)

| Metric | Performance |
|--------|-------------|
| Indexing Speed | 8,500 files/second |
| Search Latency | ~55ms (1M entries) |
| Memory Usage | ~150MB (1M files) |
| P2P Transfer | 110 MB/s (gigabit) |
| Connection Time | 1.8 seconds |

## The AI-Augmented Development Revolution

### From Team Chaos to Solo Excellence

The most remarkable aspect of Spacedrive V2 isn't just the technical improvementsâ€”it's how it was built. The contrast between V1 and V2 development tells a story of a fundamental shift in how software can be created.

**Spacedrive V1 (2022-2025)**:
- **Team Size**: 12 developers at peak
- **Development Time**: 3 years
- **Investment**: $2 million USD
- **Result**: Architectural failures, incomplete roadmap, development pause
- **Core Issues**: Poor coordination, slow iteration, mounting technical debt

**Spacedrive V2 (2025)**:
- **Team Size**: 1 developer + AI assistants
- **Development Time**: 3 weeks
- **Investment**: AI credits and personal time
- **Result**: Production-ready system, comprehensive whitepaper, clear architecture
- **Achievement**: 100x development speed increase

### The New Development Stack

The V2 rewrite leveraged a revolutionary development approach:

```
Developer (Architect/Orchestrator)
    â”œâ”€â”€ ChatGPT â†’ Deep research and citations
    â”œâ”€â”€ Claude Code â†’ Implementation and code generation
    â”œâ”€â”€ Gemini â†’ Large context analysis and system design
    â””â”€â”€ 50+ Design Documents â†’ Persistent knowledge base
```

This wasn't simply using AI as a coding assistant. It was a complete reimagining of the development process:

1. **ChatGPT for Research**: Comprehensive analysis of distributed systems, file management approaches, and technical solutions
2. **Claude Code for Implementation**: Rapid prototyping and production-ready code generation
3. **Gemini for Architecture**: Large context window analysis of the entire codebase and design documents
4. **Agentic Development**: Multiple AI agents working on different system components simultaneously

### The Power of Focus

Where V1 suffered from "too many cooks in the kitchen," V2 benefited from singular vision:

- **No Communication Overhead**: Zero time spent in meetings, standups, or coordination
- **Consistent Architecture**: One mind ensuring all components align perfectly
- **Rapid Iteration**: Ideas implemented and tested within hours, not weeks
- **No Politics**: Technical decisions based purely on merit, not compromise

### AI as Force Multiplier

The solo developer didn't work aloneâ€”they commanded an army of specialized AI assistants:

> "I wrote this workflow in two days using ChatGPT for deep research and citations, Claude Code to implement changes and Gemini for the large context window to analyze. This turns three years of work by 16 developers with many architectural flaws into a production ready system, fully tested and a detailed whitepaper in under a month. I'm doing this solo."

Each AI tool was used for its strengths:
- **Research**: AI analyzed thousands of papers and codebases
- **Implementation**: AI generated boilerplate and complex algorithms
- **Analysis**: AI reviewed architecture for consistency and flaws
- **Documentation**: AI helped create comprehensive technical docs

### The New Capital Efficiency Model

This development approach fundamentally changes the economics of startups:

**Traditional Model**:
- Raise $2M â†’ Hire 10 developers â†’ Burn $200k/month â†’ Hope for product-market fit

**AI-Augmented Model**:
- Raise $500k â†’ Stay solo + AI â†’ Burn $20k/month â†’ Achieve more with 10x runway

The capital can instead be invested in:
- Infrastructure and cloud services
- Security audits and compliance
- AI credits for enhanced development
- Marketing and community building
- Legal and operational services

### Future Team Philosophy

The V2 success doesn't mean staying solo forever, but it establishes a new hiring philosophy:

> "Plans to move forward with an automation heavy development cycle leaves future capital and revenue for security audits, compliance, legal and infra costs. As the project grows we will seek only the best humans, keeping the team as small as possible."

**Key Principles**:
1. **Hire for Impact**: Each person must provide 10x value
2. **Automate First**: Only hire when automation isn't possible
3. **Quality Over Quantity**: One excellent engineer > five average ones
4. **Strategic Roles**: First hires for growth, not more developers

### Validation from AI Partners

Even the AI systems recognized the achievement. Gemini's analysis:

> "What you've described is a powerful demonstration of a new paradigm for highly effective development. You haven't just used AI as a simple assistant; you've acted as an architect and orchestrator, leveraging a suite of specialized tools for their core strengths... This entire endeavor is not just about building Spacedrive; it's a case study in how a single, focused individual can now achieve what was previously only possible for large, well-funded teams."

### Implications for the Industry

The Spacedrive V2 development story represents a paradigm shift:

1. **The End of Large Early-Stage Teams**: Why hire 10 developers when 1 + AI is more effective?
2. **Capital Efficiency Revolution**: Startups can achieve more with 90% less capital
3. **Quality Through Focus**: Better architecture through singular vision
4. **Speed Through Automation**: Months compressed into weeks

This isn't just about building software fasterâ€”it's about building it better. The V2 architecture is cleaner, more thoughtful, and more maintainable than V1 precisely because it avoided the compromises and communication overhead of a large team.

### The Investment Thesis

This new development model creates a compelling narrative for investors:

- **Proven Execution**: V2 built in 3 weeks vs V1's 3 years
- **Capital Efficiency**: Every dollar goes to growth, not salaries
- **Reduced Risk**: No team drama, no coordination failures
- **Scalable Model**: AI assistants scale infinitely without HR issues

As the founder noted:

> "I'm not planning on building a team with the capital, I think the story of flying solo until revenue is decent is a more appealing sell for seed investors. I've proved how much can be done in such a short time, why hire?"

## Impact and Legacy

### Technical Contributions

Spacedrive's development spawned several open-source projects:
- **Prisma Rust Client** (now officially supported)
- **rspc** (type-safe RPC framework)
- **Specta** (TypeScript-Rust type sharing)

### Cultural Impact

The project demonstrated that:
1. Consumer software can implement enterprise-grade distributed systems
2. Local-first architecture doesn't sacrifice convenience
3. Open-source projects can attract top-tier venture funding
4. Community-driven development produces innovative solutions

### Industry Influence

Spacedrive proved several concepts:
- VDFS is viable for consumer applications
- Content-addressable storage works at personal scale
- P2P can achieve reliability comparable to cloud services
- Privacy and functionality aren't mutually exclusive

## Lessons Learned from the Failure

### 1. Architecture Must Match User Needs

**The Mistake**: Building two separate file systems because of implementation details
**The Lesson**: User experience must drive architecture, not the other way around

Users don't care about "indexed" vs "ephemeral" filesâ€”they just want to copy their vacation photos. The dual file system was an implementation detail that leaked into the user experience, making basic operations impossible.

### 2. Start Simple, Iterate Often

**The Mistake**: Building a perfect CRDT sync system that never shipped
**The Lesson**: Ship basic sync first, enhance later

The team spent years debating the perfect sync architecture while competitors shipped simpler solutions. A basic "last write wins" sync would have provided 90% of the value with 10% of the complexity.

### 3. Don't Create Dependencies You Can't Maintain

**The Mistake**: Creating prisma-client-rust and rspc, then abandoning them
**The Lesson**: Use existing solutions unless you're committed to maintaining new ones

Creating fundamental infrastructure is a massive commitment. When the team's needs changed, they couldn't maintain these libraries, leaving Spacedrive stranded on deprecated forks.

### 4. Reduce Boilerplate Ruthlessly

**The Mistake**: 1000+ lines to add a simple file operation
**The Lesson**: Developer experience directly impacts feature velocity

When adding a "delete file" operation requires days of boilerplate, innovation stops. The V2 approach with procedural macros shows how the same functionality can be achieved in 50 lines.

### 5. Core Features Must Be Excellent

**The Mistake**: Marketing "lightning fast search" while delivering basic SQL queries
**The Lesson**: Don't promise what you can't deliver

Search was a core value proposition of the VDFS concept, yet it remained neglected. If search is your differentiator, it must be world-class from day one.

### 6. One Concept, One Implementation

**The Mistake**: Node vs Device vs Instance representing the same thing
**The Lesson**: Conceptual clarity prevents implementation confusion

When the same concept has multiple representations, bugs multiply. Every developer has to understand the mapping between systems, and inconsistencies creep in.

### 7. Complete Migrations Before Starting New Ones

**The Mistake**: Running old and new systems in parallel throughout the codebase
**The Lesson**: Technical debt compounds exponentially

The codebase had `old_job`, `old_p2p`, and `old_*` files still in active use. Each incomplete migration made the next one harder, creating a maze of deprecated-but-necessary code.

### 8. Event-Driven > Direct Coupling

**The Mistake**: Backend hardcoding frontend cache keys
**The Lesson**: Loose coupling enables independent evolution

The `invalidate_query` pattern meant changing the frontend required backend changes. Event-driven architecture allows each layer to evolve independently.

### 9. Perfect is the Enemy of Good

**The Mistake**: Analysis paralysis on sync boundaries
**The Lesson**: Make decisions and move forward

The team couldn't decide what should sync vs remain local, so nothing shipped. A clear decisionâ€”even if imperfectâ€”would have been better than no decision.

### 10. Community Momentum is Precious

**The Mistake**: Losing momentum after initial excitement
**The Lesson**: Consistent delivery maintains community engagement

Spacedrive had incredible initial tractionâ€”35k stars, 500k installsâ€”but development stalled. Regular releases, even small ones, keep the community engaged and attract contributors.

## Future Vision

### Near-Term Goals

The V2 architecture enables:
- Complex AI workflows for automatic organization
- Intelligent content analysis pipelines
- Semantic search across all data types
- Federated learning from usage patterns

### Long-Term Ambitions

Spacedrive aims to become:
- A platform for personal AI agents
- The foundation for local-first computing
- A bridge between personal and collaborative workflows
- The default file management paradigm

### Business Model Evolution

Plans include:
- **Open Core**: Free forever for individuals
- **Team Features**: Collaboration tools for small groups
- **Enterprise**: Advanced security and compliance
- **Cloud Services**: Optional convenience features
- **Developer Platform**: APIs for third-party integration

## Conclusion

From a developer's personal frustration to a venture-backed phenomenon with 35,000 GitHub stars and 500,000 installations, Spacedrive's journey exemplifies both the challenges and opportunities in modern software development. The project's evolution tells three distinct stories:

**The Promise** (2021-2022): A vision that resonated globallyâ€”unifying all files under user control with intelligent, distributed systems.

**The Struggle** (2022-2025): How even well-funded projects with talented teams can fail due to architectural mistakes, over-engineering, and decision paralysis. The dual file system, abandoned dependencies, and sync system that never shipped serve as cautionary tales for ambitious projects.

**The Revolution** (2025): A single developer with AI assistance achieving in 3 weeks what 16 developers couldn't in 3 years. This isn't just a comeback storyâ€”it's a glimpse into the future of software development.

The V2 reimagining proves that the original vision was sound; only the execution was flawed. By addressing every architectural mistake, simplifying ruthlessly, and leveraging AI as a force multiplier, Spacedrive has been reborn stronger than ever. The new development paradigmâ€”one architect orchestrating specialized AI agentsâ€”demonstrates that we've entered an era where individual developers can build systems previously requiring entire teams.

Most importantly, Spacedrive's journey from failure to rebirth offers invaluable lessons: Start simple. Ship often. Avoid over-engineering. Maintain conceptual clarity. And now, in 2025: leverage AI not as a tool, but as a team.

Whether Spacedrive becomes the default file manager of the future or serves as inspiration for others, its impact is undeniable. It has shown that the dream of a unified, intelligent, user-controlled filesystem is not only possibleâ€”with the right approach, it's inevitable.

---

*"Files shouldn't be stuck in a device ecosystem. Open source technology is the only way to ensure we retain absolute control over the files that define our lives."* - Jamie Pine, Founder of Spacedrive```

## docs/indexing.md

```markdown
# Spacedrive Indexing System

## Overview

The Spacedrive indexing system is a sophisticated, multi-phase file indexing engine designed for high performance and reliability. It discovers, processes, and categorizes files while supporting incremental updates, change detection, and content-based deduplication. The system now supports multiple indexing scopes and ephemeral modes for different use cases.

## Architecture

### Core Components

1. **IndexerJob** - The main job orchestrator that manages the indexing process
2. **IndexerState** - Maintains state across phases for resumability
3. **EntryProcessor** - Handles database operations for file entries
4. **FileTypeRegistry** - Identifies file types through extensions and magic bytes
5. **CasGenerator** - Creates content-addressed storage identifiers

### Key Features

- **Multi-phase Processing**: Discovery â†’ Processing â†’ Aggregation â†’ Content Identification
- **Resumable Operations**: Jobs can be paused and resumed from checkpoints
- **Change Detection**: Efficiently identifies modified files using inode tracking
- **Content Deduplication**: Uses CAS (Content-Addressed Storage) IDs
- **Type Detection**: Sophisticated file type identification with MIME type support
- **Performance Optimized**: Batch processing, caching, and parallel operations
- **Flexible Scoping**: Current (single-level) vs Recursive (full tree) indexing
- **Ephemeral Mode**: In-memory indexing for browsing external paths
- **Persistence Options**: Database storage vs memory-only for different use cases

## Indexing Phases

### 1. Discovery Phase
Walks the file system to discover all files and directories.

```rust
// Key operations:
- Recursive directory traversal
- Filter application (skip system files, hidden files based on rules)
- Batch collection for efficient processing
- Progress tracking and reporting
```

**Output**: Batches of `DirEntry` items ready for processing

### 2. Processing Phase
Creates or updates database entries for discovered items.

```rust
// Key operations:
- Change detection using inode/modified time
- Materialized path storage (no parent_id needed)
- Entry creation/update in database
- Direct path storage for efficient queries
```

**Output**: Database entries with proper relationships

### 3. Aggregation Phase
Calculates aggregate statistics for directories.

```rust
// Key operations:
- Bottom-up traversal of directory tree
- Calculate total sizes and file counts
- Update directory entries with aggregate data
```

**Output**: Directories with accurate size/count statistics

### 4. Content Identification Phase
Generates content identifiers and detects file types.

```rust
// Key operations:
- CAS ID generation (sampled hashing)
- File type detection (extension + magic bytes)
- MIME type identification
- Content deduplication
```

**Output**: Content identities linked to entries

## Indexing Scopes and Persistence

### Index Scopes

The indexing system supports two different scopes for different use cases:

#### Current Scope
- **Description**: Index only the specified directory (single level)
- **Use Cases**: UI navigation, quick directory browsing, instant feedback
- **Performance**: <500ms for typical directories
- **Implementation**: Direct directory read without recursion

```rust
let config = IndexerJobConfig::ui_navigation(location_id, path);
// Results in single-level scan optimized for UI responsiveness
```

#### Recursive Scope  
- **Description**: Index the directory and all subdirectories
- **Use Cases**: Full location indexing, comprehensive file discovery
- **Performance**: Depends on directory tree size
- **Implementation**: Traditional recursive tree traversal

```rust
let config = IndexerJobConfig::new(location_id, path, mode);
// Default recursive behavior for complete coverage
```

### Persistence Modes

#### Persistent Mode
- **Storage**: Database (SQLite/PostgreSQL)
- **Use Cases**: Managed locations, permanent indexing
- **Features**: Full change detection, resumability, sync support
- **Lifecycle**: Permanent until explicitly removed

```rust
let config = IndexerJobConfig::new(location_id, path, mode);
config.persistence = IndexPersistence::Persistent;
```

#### Ephemeral Mode
- **Storage**: Memory (EphemeralIndex)
- **Use Cases**: External path browsing, temporary exploration
- **Features**: No database writes, session-based caching
- **Lifecycle**: Exists only during application session

```rust
let config = IndexerJobConfig::ephemeral_browse(path, scope);
// Results stored in memory, automatic cleanup
```

### Enhanced Configuration

The new `IndexerJobConfig` provides fine-grained control:

```rust
pub struct IndexerJobConfig {
    pub location_id: Option<Uuid>,      // None for ephemeral
    pub path: SdPath,                   // Path to index
    pub mode: IndexMode,                // Shallow/Content/Deep
    pub scope: IndexScope,              // Current/Recursive
    pub persistence: IndexPersistence,  // Persistent/Ephemeral
    pub max_depth: Option<u32>,         // Depth limiting
}
```

### Use Case Examples

#### UI Directory Navigation
```rust
// Fast current directory scan for UI
let config = IndexerJobConfig::ui_navigation(location_id, path);
// - Scope: Current (single level)
// - Mode: Shallow (metadata only)  
// - Persistence: Persistent
// - Target: <500ms response time
```

#### External Path Browsing
```rust
// Browse USB drive without adding to library
let config = IndexerJobConfig::ephemeral_browse(usb_path, IndexScope::Current);
// - Scope: Current or Recursive
// - Mode: Shallow (configurable)
// - Persistence: Ephemeral
// - Target: Exploration without database pollution
```

#### Background Location Indexing
```rust
// Traditional full location scan
let config = IndexerJobConfig::new(location_id, path, IndexMode::Deep);
// - Scope: Recursive (default)
// - Mode: Deep (full analysis)
// - Persistence: Persistent
// - Target: Complete coverage
```

### Ephemeral Index Structure

The `EphemeralIndex` provides temporary storage:

```rust
pub struct EphemeralIndex {
    pub entries: HashMap<PathBuf, EntryMetadata>,
    pub content_identities: HashMap<String, EphemeralContentIdentity>,
    pub created_at: Instant,
    pub last_accessed: Instant,
    pub root_path: PathBuf,
    pub stats: IndexerStats,
}
```

Features:
- **LRU Behavior**: Automatic cleanup based on access time
- **Memory Efficient**: Lightweight metadata storage
- **Session Scoped**: Cleared on application restart
- **Fast Access**: Direct HashMap lookups

## Database Schema

### Core Tables

#### `entries`
The main file/directory entry table using materialized paths:
```sql
- id: i32 (primary key)
- uuid: UUID
- location_id: i32 (â†’ locations)
- relative_path: String (materialized path - parent directory path)
- name: String (filename without extension)
- kind: i32 (0=File, 1=Directory, 2=Symlink)
- extension: String?
- size: i64
- aggregate_size: i64 (for directories)
- child_count: i32
- file_count: i32
- inode: i64? (for change detection)
- location_id: i32? (â†’ locations)
- content_id: i32? (â†’ content_identities)
- metadata_id: i32? (â†’ user_metadata)
```

**Note**: Parent-child relationships are determined by the `relative_path` field. For example:
- A file at `/documents/report.pdf` has `relative_path = "documents"` and `name = "report"`
- Its parent directory has `relative_path = ""` and `name = "documents"`

#### `content_identities`
Stores unique content for deduplication:
```sql
- id: i32 (primary key)
- uuid: UUID
- cas_id: String (content hash)
- cas_version: i16
- kind_id: i32 (â†’ content_kinds)
- mime_type_id: i32? (â†’ mime_types)
- total_size: i64
- entry_count: i32 (number of files with this content)
```

#### `content_kinds`
Static lookup table for content types:
```sql
- id: i32 (primary key, matches enum)
- name: String

Values:
0 = unknown
1 = image
2 = video
3 = audio
4 = document
5 = archive
6 = code
7 = text
8 = database
9 = book
10 = font
11 = mesh
12 = config
13 = encrypted
14 = key
15 = executable
16 = binary
```

#### `mime_types`
Dynamic table for discovered MIME types:
```sql
- id: i32 (primary key)
- uuid: UUID (for syncing)
- mime_type: String (unique)
- created_at: DateTime
```

## File Type Detection

The system uses a multi-layered approach:

1. **Extension Matching**: Fast initial identification
2. **Magic Bytes**: Verifies file type by reading file headers
3. **Content Analysis**: For text files, analyzes content patterns
4. **MIME Type Detection**: Associates standard MIME types

Example flow:
```rust
let registry = FileTypeRegistry::default();
let result = registry.identify(path).await?;
// Returns: FileType with category, MIME types, and confidence level
```

## Content Addressing (CAS)

The CAS system creates unique identifiers for file content:

1. **Sampled Hashing**: Reads chunks at specific offsets
2. **Blake3 Hashing**: Fast, cryptographically secure
3. **Deduplication**: Same content = same CAS ID

Benefits:
- Instant duplicate detection
- Content verification
- Efficient storage references

## Change Detection

The indexer efficiently detects changes using:

1. **Inode Tracking**: Platform-specific file identifiers
2. **Modified Time**: Fallback for systems without inodes
3. **Size Comparison**: Quick change indicator

Change types detected:
- New files
- Modified files
- Deleted files
- Moved files (same inode, different path)

## Performance Optimizations

### Batch Processing
- Processes files in chunks of 1000
- Reduces database round trips
- Improves memory efficiency

### Scope Optimizations
- **Current Scope**: Direct directory read without recursion (<500ms target)
- **Recursive Scope**: Efficient tree traversal with depth control
- **Ephemeral Mode**: Memory-only storage for external path browsing
- **Early Termination**: Configurable max_depth limiting

### Caching
- Entry ID cache for parent lookups
- Change detection cache for inode/timestamp comparisons
- Ephemeral index LRU cache for session-based storage
- Content identity cache for deduplication

### Parallelization
- Concurrent CAS ID generation
- Parallel file type detection
- Async I/O operations
- Batch processing across multiple threads

### Database Optimizations
- Bulk inserts with transaction batching
- Prepared statements for repeated operations
- Strategic indexing on location_id and relative_path
- Persistence abstraction for database vs memory storage

## Usage Examples

### Enhanced Indexing Jobs

```rust
use sd_core_new::operations::indexing::{
    IndexerJob, IndexerJobConfig, IndexMode, IndexScope, IndexPersistence
};

// UI Navigation - Fast current directory scan
let config = IndexerJobConfig::ui_navigation(location_id, path);
let job = IndexerJob::new(config);
let handle = library.jobs().dispatch(job).await?;

// Ephemeral Browsing - External path exploration
let config = IndexerJobConfig::ephemeral_browse(external_path, IndexScope::Current);
let job = IndexerJob::new(config);
let handle = library.jobs().dispatch(job).await?;

// Traditional Location Indexing - Full recursive scan
let config = IndexerJobConfig::new(location_id, path, IndexMode::Deep);
let job = IndexerJob::new(config);
let handle = library.jobs().dispatch(job).await?;

// Custom Configuration - Fine-grained control
let mut config = IndexerJobConfig::new(location_id, path, IndexMode::Content);
config.scope = IndexScope::Current;
config.max_depth = Some(2);
let job = IndexerJob::new(config);
```

### Legacy API (Backward Compatibility)

```rust
// Old API still works for simple cases
let job = IndexerJob::from_location(location_id, path, IndexMode::Deep);
let job = IndexerJob::shallow(location_id, path);
let job = IndexerJob::with_content(location_id, path);
```

### Indexing Modes

- **Shallow**: Metadata only (fastest, <500ms for UI)
- **Content**: Includes CAS ID generation (moderate performance)
- **Deep**: Full analysis including thumbnails (comprehensive)

### Indexing Scopes

- **Current**: Single directory level (UI navigation, quick browsing)
- **Recursive**: Full directory tree (complete location indexing)

### Persistence Options

- **Persistent**: Database storage (managed locations, permanent data)
- **Ephemeral**: Memory storage (external browsing, temporary exploration)

## Metrics and Monitoring

The indexer tracks detailed metrics:

```rust
IndexerMetrics {
    total_items: u64,
    items_per_second: f64,
    bytes_per_second: f64,
    phase_durations: HashMap<String, Duration>,
    db_operations: (reads: u64, writes: u64),
    cache_stats: CacheStats,
}
```

## Error Handling

### Critical Errors
Stop indexing immediately:
- Database connection lost
- Filesystem errors
- Permission denied on location root

### Non-Critical Errors
Logged but indexing continues:
- Permission denied on individual files
- Corrupted file metadata
- Unsupported file types

## Future Enhancements

1. **Thumbnail Generation**: Integrated media thumbnail creation
2. **Full-Text Indexing**: Search within documents
3. **AI Tagging**: Automatic content categorization
4. **Cloud Integration**: Index cloud storage locations
5. **Real-time Monitoring**: Instant file change detection
6. **Distributed Indexing**: Multi-device collaborative indexing

## Configuration

### Filter Rules
```rust
IndexerRules {
    skip_hidden: bool,
    skip_system: bool,
    max_file_size: Option<u64>,
    allowed_extensions: Option<Vec<String>>,
    ignored_paths: Vec<PathBuf>,
}
```

### Performance Tuning
```rust
IndexerConfig {
    batch_size: usize,        // Default: 1000
    checkpoint_interval: u64, // Default: 5000 items
    max_concurrent_io: usize, // Default: 100
    enable_content_id: bool,  // Default: true
}

// Enhanced configuration with scope and persistence
IndexerJobConfig {
    location_id: Option<Uuid>,         // None for ephemeral jobs
    path: SdPath,                      // Target path
    mode: IndexMode,                   // Shallow/Content/Deep
    scope: IndexScope,                 // Current/Recursive
    persistence: IndexPersistence,     // Persistent/Ephemeral
    max_depth: Option<u32>,            // Depth limiting for performance
}

// Ephemeral index settings
EphemeralConfig {
    max_entries: usize,                // Default: 10000
    cleanup_interval: Duration,        // Default: 5 minutes
    max_idle_time: Duration,           // Default: 30 minutes
    enable_content_analysis: bool,     // Default: false
}
```

## Integration Points

The indexer integrates with:

1. **Location System**: Manages indexed locations
2. **Job System**: Provides resumability and progress
3. **Event System**: Emits progress and completion events
4. **Sync System**: Shares indexed data across devices
5. **Search System**: Powers file search functionality

## Best Practices

1. **Start with Shallow Mode**: For initial quick results
2. **Use Filters**: Skip unnecessary files (node_modules, etc.)
3. **Monitor Progress**: Subscribe to indexing events
4. **Handle Errors Gracefully**: Check non-critical error counts
5. **Regular Re-indexing**: Schedule periodic deep scans

## Technical Details

### State Persistence
The indexer state is serialized using MessagePack for efficient storage and quick resume operations.

### Memory Management
- Streaming file processing (no full file loads)
- Bounded channels for backpressure
- Automatic batch flushing

### Platform Support
- **Windows**: Uses file index for inode equivalent
- **macOS**: Native inode support
- **Linux**: Full inode and permission tracking

## CLI Usage

The indexing system provides comprehensive CLI access with enhanced scope and persistence options:

### Enhanced Index Commands

```bash
# Start the daemon first
spacedrive start

# Quick scan for UI navigation (fast, current directory only)
spacedrive index quick-scan ~/Documents --scope current

# Quick scan with ephemeral mode (no database writes)
spacedrive index quick-scan /external/drive --scope current --ephemeral

# Browse external paths without adding to managed locations
spacedrive index browse /media/usb-drive --scope current
spacedrive index browse /network/share --scope recursive --content

# Index managed locations with specific scope and mode
spacedrive index location ~/Pictures --scope current --mode shallow
spacedrive index location <location-uuid> --scope recursive --mode deep
```

### Location Management

```bash
# Add locations with different indexing modes
spacedrive location add ~/Documents --mode shallow    # Fast metadata only
spacedrive location add ~/Pictures --mode content     # With content hashing 
spacedrive location add ~/Videos --mode deep          # Full media analysis

# Force re-indexing of a location
spacedrive location rescan <location-id> --force
```

### Legacy Commands (Backward Compatibility)

```bash
# Traditional indexing (creates location and starts full scan)
spacedrive scan ~/Desktop --mode content --watch
```

### Monitoring and Status

```bash
# Monitor indexing progress in real-time
spacedrive job monitor

# Check job status with scope/persistence info
spacedrive job list --status running

# Get detailed job information
spacedrive job info <job-id>
```

### Command Comparison

| Command | Scope | Persistence | Use Case |
|---------|-------|-------------|----------|
| `index quick-scan` | Current/Recursive | Persistent/Ephemeral | UI navigation, quick browsing |
| `index browse` | Current/Recursive | Ephemeral | External path exploration |
| `index location` | Current/Recursive | Persistent | Managed location updates |
| `scan` (legacy) | Recursive | Persistent | Traditional full indexing |
| `location add` | Recursive | Persistent | Add new managed locations |

For complete CLI documentation, see [CLI Documentation](./cli.md).

## Debugging

Enable detailed logging:
```bash
# For CLI daemon
spacedrive start --foreground -v

# For development
RUST_LOG=sd_core_new::operations::indexing=debug cargo run
```

Common issues:
1. **Slow indexing**: Check filter rules and batch sizes
2. **High memory usage**: Reduce batch size  
3. **Missing files**: Verify permissions and filter rules
4. **No progress shown**: Ensure daemon is running and use `spacedrive job monitor````

## docs/job-system.md

```markdown
# Job System

The job system is one of the most significant improvements in Core v2, reducing the boilerplate required to implement new jobs from 500+ lines to approximately 50 lines while adding better type safety and persistence.

## Overview

The job system provides:
- **Zero-boilerplate registration** using derive macros
- **Automatic job discovery** at compile time
- **API-driven job dispatch** by name
- **Automatic serialization** using MessagePack
- **Database persistence** with resume capabilities  
- **Type-safe progress reporting**
- **Graceful error handling** and recovery
- **Checkpointing** for long-running operations

## Architecture

```
#[derive(Job)] â”€â”€ generates â”€â”€â†’ JobRegistration â”€â”€ collected by â”€â”€â†’ JobRegistry
     â”‚                               â”‚                                    â”‚
     â””â”€â”€ implements â”€â”€â†’ JobHandler â”€â”€â”€â”¼â”€â”€ stores â”€â”€â†’ create_fn â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚          â””â”€â”€ stores â”€â”€â†’ deserialize_fn
                           â”‚
                           â””â”€â”€ reports â”€â”€â†’ Progress â”€â”€â†’ EventBus
                                           
JobManager â”€â”€â”€ uses â”€â”€â†’ JobRegistry â”€â”€â”€ dispatches â”€â”€â†’ ErasedJob
     â”‚                       â”‚                            â”‚
     â””â”€â”€ manages â”€â”€â†’ JobDatabase â”€â”€â”€ stores â”€â”€â†’ JobRecord â”€â”˜
```

### Core Components

**JobRegistry** *(NEW)*
- Automatically discovers jobs using `inventory` crate
- Provides runtime job creation and dispatch
- Enables API-driven job execution by name
- Global registry accessible via `REGISTRY` static

**Derive Macro** *(NEW)*  
- Zero-boilerplate job registration using `#[derive(Job)]`
- Generates `JobRegistration` and `ErasedJob` implementations
- Automatic compile-time registration via `inventory::submit!`

**JobManager**
- Orchestrates job execution and persistence
- Manages job lifecycle (dispatch, run, pause, cancel)
- Handles database operations and recovery
- Supports both direct dispatch and name-based dispatch

**Job Trait**
- Defines job metadata and behavior
- Minimal trait requiring only constants

**JobHandler Trait**  
- Defines the actual job execution logic
- Handles progress reporting and checkpointing

**JobDatabase**
- SQLite storage for job state and history
- Automatic schema management
- Efficient querying and status tracking

## Automatic Job Registration

The job system uses a two-layer architecture for zero-boilerplate registration:

### Compile Time: Derive Macro
The `#[derive(Job)]` macro automatically generates:
- Job registration code using `inventory::submit!`
- `ErasedJob` trait implementation for type erasure
- Serialization/deserialization functions

### Runtime: Job Registry  
The `JobRegistry` collects all registrations and provides:
- Job discovery by name (`job_names()`, `has_job()`)
- Dynamic job creation (`create_job()`, `deserialize_job()`)  
- Schema introspection (`get_job_schema()`)

## Creating a Job

### 1. Define the Job Struct with Derive Macro

```rust
use serde::{Deserialize, Serialize};
use sd_core_new::infrastructure::jobs::prelude::*;

#[derive(Debug, Serialize, Deserialize, Job)]  // â† Job derive macro
pub struct FileCopyJob {
    pub sources: SdPathBatch,
    pub destination: SdPath,
    pub options: CopyOptions,
    
    // Internal state for resumption
    #[serde(skip)]
    completed_indices: Vec<usize>,
    #[serde(skip, default = "Instant::now")]
    started_at: Instant,
}
```

### 2. Implement the Job Trait

```rust
impl Job for FileCopyJob {
    const NAME: &'static str = "file_copy";
    const RESUMABLE: bool = true;
    const DESCRIPTION: Option<&'static str> = Some("Copy files between locations");
}

### 3. Implement the JobHandler Trait

```rust
#[async_trait::async_trait]
impl JobHandler for FileCopyJob {
    type Output = FileCopyOutput;

    async fn run(&mut self, ctx: JobContext<'_>) -> JobResult<Self::Output> {
        ctx.log(format!(
            "Starting copy operation on {} files", 
            self.sources.paths.len()
        ));

        let total_files = self.sources.paths.len();
        let mut copied_count = 0;
        let mut total_bytes = 0u64;
        let mut failed_copies = Vec::new();

        // Group by device for efficient processing
        let by_device = self.sources.by_device();

        for (device_id, device_paths) in by_device {
            ctx.check_interrupt().await?;

            if device_id == self.destination.device_id {
                // Same device - efficient local copy
                self.process_same_device_copies(
                    device_paths.iter().collect(),
                    &ctx,
                    &mut copied_count,
                    &mut total_bytes,
                    &mut failed_copies,
                    total_files,
                ).await?;
            } else {
                // Cross-device copy
                self.process_cross_device_copies(
                    device_paths.iter().collect(),
                    &ctx,
                    &mut copied_count,
                    &mut total_bytes,
                    &mut failed_copies,
                    total_files,
                ).await?;
            }
        }

        ctx.log(format!(
            "Copy operation completed: {} copied, {} failed",
            copied_count,
            failed_copies.len()
        ));

        Ok(FileCopyOutput {
            copied_count,
            failed_count: failed_copies.len(),
            total_bytes,
            duration: self.started_at.elapsed(),
            failed_copies,
        })
    }
}
```

### 4. Define Output Type

```rust
#[derive(Debug, Serialize, Deserialize)]
pub struct FileCopyOutput {
    pub copied_count: usize,
    pub failed_count: usize,
    pub total_bytes: u64,
    pub duration: Duration,
    pub failed_copies: Vec<CopyError>,
}

impl From<FileCopyOutput> for JobOutput {
    fn from(output: FileCopyOutput) -> Self {
        JobOutput::FileCopy {
            copied_count: output.copied_count,
            total_bytes: output.total_bytes,
        }
    }
}
```

### 5. Add Constructor and Helper Methods

```rust
impl FileCopyJob {
    /// Create a new file copy job with sources and destination
    pub fn new(sources: SdPathBatch, destination: SdPath) -> Self {
        Self {
            sources,
            destination,
            options: Default::default(),
            completed_indices: Vec::new(),
            started_at: Instant::now(),
        }
    }
    
    /// Create an empty job (used by derive macro)
    pub fn empty() -> Self {
        Self {
            sources: SdPathBatch::new(Vec::new()),
            destination: SdPath::new(uuid::Uuid::new_v4(), PathBuf::new()),
            options: Default::default(),
            completed_indices: Vec::new(),
            started_at: Instant::now(),
        }
    }
}
```

That's it! The derive macro handles all the registration automatically.

## Job Registry

The `JobRegistry` provides runtime access to all registered jobs:

```rust
use sd_core_new::infrastructure::jobs::registry::REGISTRY;

// Discover all job types
let job_types = REGISTRY.job_names();
println!("Available jobs: {:?}", job_types);

// Get job schema for introspection
if let Some(schema) = REGISTRY.get_job_schema("file_copy") {
    println!("Schema: {:?}", schema);
}

// Create job from JSON (useful for APIs)
let job_data = serde_json::json!({
    "sources": ["/path/to/file1", "/path/to/file2"],
    "destination": "/path/to/dest"
});
let job = REGISTRY.create_job("file_copy", job_data)?;

// Deserialize job from binary data (for resumption)
let binary_data = rmp_serde::to_vec(&some_job)?;
let restored_job = REGISTRY.deserialize_job("file_copy", &binary_data)?;
```

### Registry Features

- **Automatic Discovery**: Uses `inventory` crate to collect all jobs at compile time
- **Type Safety**: Ensures only valid job types can be created
- **Schema Introspection**: Provides metadata about job parameters
- **Multiple Formats**: Supports both JSON (APIs) and MessagePack (persistence)

## Job Context

The `JobContext` provides essential capabilities during job execution:

```rust
impl<'a> JobContext<'a> {
    /// Log a message associated with this job
    pub fn log(&self, message: String) { /* ... */ }
    
    /// Report progress to subscribers
    pub fn progress(&self, progress: Progress) { /* ... */ }
    
    /// Check if the job should be interrupted
    pub async fn check_interrupt(&self) -> JobResult<()> { /* ... */ }
    
    /// Save current job state to database
    pub async fn checkpoint(&self) -> JobResult<()> { /* ... */ }
    
    /// Get job-specific data directory
    pub fn data_dir(&self) -> &Path { /* ... */ }
}
```

### Progress Reporting

Multiple progress types are supported:

```rust
pub enum Progress {
    /// Simple percentage (0.0 to 1.0)
    Percentage(f64),
    
    /// Structured progress with custom data
    Structured(serde_json::Value),
    
    /// Indeterminate progress
    Indeterminate,
}

// Usage examples:
ctx.progress(Progress::percentage(0.5));  // 50% complete

ctx.progress(Progress::structured(serde_json::json!({
    "current_file": "document.pdf",
    "files_processed": 150,
    "total_files": 500,
    "current_operation": "extracting_text"
})));
```

## Error Handling

Comprehensive error types for different failure scenarios:

```rust
#[derive(thiserror::Error, Debug)]
pub enum JobError {
    #[error("Job execution failed: {0}")]
    ExecutionFailed(String),
    
    #[error("Job was interrupted")]
    Interrupted,
    
    #[error("Database error: {0}")]
    Database(#[from] sea_orm::DbErr),
    
    #[error("IO error: {0}")]
    Io(#[from] std::io::Error),
    
    #[error("Serialization error: {0}")]
    Serialization(#[from] rmp_serde::encode::Error),
}

// Result type alias
pub type JobResult<T> = Result<T, JobError>;
```

## Job Outputs

Standardized output types for common operations:

```rust
#[derive(Debug, Serialize, Deserialize)]
pub enum JobOutput {
    /// File copy operation results
    FileCopy {
        copied_count: u64,
        total_bytes: u64,
    },
    
    /// Indexing operation results
    Indexed {
        total_files: u64,
        total_dirs: u64,
        total_bytes: u64,
    },
    
    /// Media processing results
    MediaProcessed {
        thumbnails_generated: u64,
        metadata_extracted: u64,
    },
    
    /// Custom operation results
    Custom(serde_json::Value),
}
```

## Job Management

### Creating and Running Jobs

```rust
use sd_core_new::infrastructure::jobs::manager::JobManager;

// Initialize job manager
let job_manager = JobManager::new(data_dir).await?;

// Method 1: Direct dispatch with job instance
let copy_job = FileCopyJob::new(sources, destination);
let handle = job_manager.dispatch(copy_job).await?;

// Method 2: API-driven dispatch by name
let job_params = serde_json::json!({
    "sources": ["/path/to/file1", "/path/to/file2"], 
    "destination": "/path/to/dest"
});
let handle = job_manager.dispatch_by_name("file_copy", job_params).await?;

// Method 3: Dispatch with priority
let handle = job_manager.dispatch_with_priority(copy_job, JobPriority::HIGH).await?;

// Monitor job progress
let job_id = handle.id;
let status = handle.status();
let mut progress_updates = handle.progress_rx;

while let Ok(progress) = progress_updates.recv().await {
    println!("Progress: {:?}", progress);
}
```

### Job Discovery and Management

```rust
// List all available job types
let job_types = job_manager.list_job_types();
println!("Available jobs: {:?}", job_types);

// Get schema for a job type
if let Some(schema) = job_manager.get_job_schema("file_copy") {
    println!("Parameters: {:?}", schema);
}

// List running jobs
let running = job_manager.list_running_jobs().await;
println!("Currently running: {} jobs", running.len());

// List jobs by status
let completed = job_manager.list_jobs(Some(JobStatus::Completed)).await?;
let failed = job_manager.list_jobs(Some(JobStatus::Failed)).await?;

// Get detailed job information
if let Some(job_info) = job_manager.get_job_info(job_id).await? {
    println!("Job: {} - Status: {:?}", job_info.name, job_info.status);
}
```

### Job Lifecycle

```rust
pub enum JobStatus {
    Queued,      // Waiting to be executed
    Running,     // Currently executing  
    Completed,   // Finished successfully
    Failed,      // Execution failed
    Cancelled,   // Cancelled by user
    Paused,      // Paused by user or system
}
```

### Database Schema

Jobs are persisted with the following schema:

```sql
CREATE TABLE jobs (
    id TEXT PRIMARY KEY,              -- UUID v4
    name TEXT NOT NULL,               -- Job type name
    data BLOB NOT NULL,               -- MessagePack serialized job
    status TEXT NOT NULL,             -- Current status
    progress REAL,                    -- Progress percentage (0.0-1.0)
    progress_data TEXT,               -- JSON progress details
    output TEXT,                      -- JSON output when completed
    error_message TEXT,               -- Error details if failed
    created_at TEXT NOT NULL,         -- ISO 8601 timestamp
    started_at TEXT,                  -- When execution began
    completed_at TEXT,                -- When execution finished
    last_checkpoint TEXT              -- Last checkpoint timestamp
);
```

## Example Jobs

### File Copy Job

Handles copying files with progress tracking and resume capabilities:

```rust
#[derive(Debug, Serialize, Deserialize)]
pub struct FileCopyJob {
    pub sources: Vec<SdPath>,
    pub destination: SdPath,
    pub copied_count: u64,
    pub total_bytes: u64,
    pub current_file: Option<String>,
}

impl FileCopyJob {
    pub fn new(sources: Vec<SdPath>, destination: SdPath) -> Self {
        Self {
            sources,
            destination,
            copied_count: 0,
            total_bytes: 0,
            current_file: None,
        }
    }
}
```

### Indexer Job  

Scans directories and builds file metadata:

```rust
#[derive(Debug, Serialize, Deserialize)]
pub struct IndexerJob {
    pub library_id: Uuid,
    pub location: SdPath,
    pub index_mode: IndexMode,
    pub processed_count: u64,
    pub current_path: Option<PathBuf>,
}

#[derive(Debug, Serialize, Deserialize)]
pub enum IndexMode {
    Metadata,    // File metadata only
    Content,     // Metadata + content hashes  
    Deep,        // Full analysis + media data
}
```

## Job Resumption

The job system automatically resumes interrupted jobs on startup:

```rust
// Jobs are automatically discovered and resumed when JobManager starts
let job_manager = JobManager::new(data_dir).await?;

// Set library reference to enable resumption
job_manager.set_library(library).await;

// All interrupted jobs (Running/Paused status) are automatically resumed
// using the registry's deserialize_job() function
```

### How Resumption Works

1. **Startup Discovery**: JobManager scans database for interrupted jobs
2. **Registry Lookup**: Uses job name to find registration in registry
3. **Deserialization**: Calls `deserialize_fn` to recreate job instance
4. **State Restoration**: Job resumes from its last checkpointed state
5. **Execution**: Job continues from where it left off

### Resumption Requirements

- Job must implement `Serialize` + `Deserialize`
- Job must have `RESUMABLE = true` in `Job` trait
- Job state must be designed for partial completion
- Use `#[serde(skip)]` for non-persistent fields

```rust
#[derive(Debug, Serialize, Deserialize, Job)]
pub struct ResumableJob {
    // Persistent state (saved/restored)
    pub total_items: usize,
    pub processed_items: usize,
    
    // Transient state (recreated on resume)
    #[serde(skip)]
    pub current_connection: Option<Connection>,
    #[serde(skip, default = "Instant::now")]
    pub session_start: Instant,
}
```

## Testing

The job system includes comprehensive testing utilities:

```rust
#[tokio::test]
async fn test_job_serialization() {
    let job = FileCopyJob::new(sources, destination);
    
    // Test serialization round-trip
    let serialized = rmp_serde::to_vec(&job).unwrap();
    let deserialized: FileCopyJob = rmp_serde::from_slice(&serialized).unwrap();
    
    assert_eq!(job.sources.len(), deserialized.sources.len());
}

#[tokio::test]
async fn test_job_database_operations() {
    let job_manager = JobManager::new(temp_dir).await.unwrap();
    
    // Test job listing
    let jobs = job_manager.list_jobs(None).await.unwrap();
    assert!(jobs.is_empty());
    
    // Test status filtering
    let running = job_manager.list_jobs(Some(JobStatus::Running)).await.unwrap();
    assert!(running.is_empty());
}
```

## Integration with Core

Jobs integrate seamlessly with the Core system:

```rust
// Future integration pattern
impl Core {
    pub async fn copy_files(&self, sources: Vec<SdPath>, dest: SdPath) -> JobResult<JobId> {
        let job = FileCopyJob::new(sources, dest);
        self.jobs.queue(job).await
    }
    
    pub async fn index_location(&self, location_id: Uuid, mode: IndexMode) -> JobResult<JobId> {
        let location = self.libraries.get_location(location_id).await?;
        let job = IndexerJob::new(location.library_id, location.path.into(), mode);
        self.jobs.queue(job).await
    }
}
```

## Performance Considerations

### Serialization

- **MessagePack** provides compact binary serialization
- **50-80% smaller** than JSON for typical job data
- **Faster** serialization/deserialization than JSON

### Checkpointing

- **Configurable frequency** - balance between safety and performance
- **Incremental state saves** - only serialize changed data
- **Atomic writes** - prevent corruption during checkpoints

### Database Operations

- **SQLite WAL mode** - better concurrency for job operations
- **Prepared statements** - faster query execution
- **Connection pooling** - efficient resource usage

### Memory Management

- **Streaming processing** for large operations
- **Bounded queues** to prevent memory exhaustion
- **Resource cleanup** on job completion or failure

## Comparison with Original

| Feature | Original System | Core v2 System |
|---------|----------------|----------------|
| **Boilerplate** | 500-1000+ lines | ~50 lines |
| **Registration** | Manual macro registration | Automatic traits |
| **Serialization** | Custom implementation | Automatic with MessagePack |
| **Progress** | String-based messages | Type-safe structured data |
| **Persistence** | Complex state management | Automatic checkpointing |
| **Error Handling** | Inconsistent patterns | Standardized error types |
| **Testing** | Difficult to test | Comprehensive test utilities |
| **Performance** | Heavy trait objects | Efficient static dispatch |

The job system represents a significant leap forward in developer experience while maintaining all the power and flexibility needed for Spacedrive's file management operations.```

## docs/library.md

```markdown
# Library System

The library system is the foundation of Spacedrive's data organization. Each library is a self-contained directory that includes all its data, making it portable and easy to manage.

## Architecture

### Library Structure

```
My Photos.sdlibrary/
â”œâ”€â”€ library.json          # Configuration and metadata
â”œâ”€â”€ database.db          # SQLite database
â”œâ”€â”€ thumbnails/          # Thumbnail storage
â”‚   â”œâ”€â”€ metadata.json    # Thumbnail generation settings
â”‚   â””â”€â”€ [sharded dirs]   # Two-level sharding for performance
â”œâ”€â”€ previews/            # Full-size previews (future)
â”œâ”€â”€ indexes/             # Search indexes (future)
â”œâ”€â”€ exports/             # Temporary exports (future)
â””â”€â”€ .sdlibrary.lock      # Lock file (when open)
```

### Key Components

1. **LibraryManager**: Handles creation, opening, and discovery of libraries
2. **Library**: Represents an open library with its database and configuration
3. **LibraryLock**: Prevents concurrent access to the same library
4. **LibraryConfig**: Stores library settings and metadata

## Usage

### Creating a Library

```rust
let library = core.libraries
    .create_library("My Photos", None)
    .await?;
```

### Opening a Library

```rust
let library = core.libraries
    .open_library("/path/to/My Photos.sdlibrary")
    .await?;
```

### Discovering Libraries

```rust
let discovered = core.libraries
    .scan_for_libraries()
    .await?;

for lib in discovered {
    println!("Found: {} at {}", lib.config.name, lib.path.display());
}
```

### Working with Thumbnails

```rust
// Save a thumbnail
library.save_thumbnail(cas_id, thumbnail_data).await?;

// Check if thumbnail exists
if library.has_thumbnail(cas_id).await {
    // Get thumbnail data
    let data = library.get_thumbnail(cas_id).await?;
}
```

## Benefits

1. **Portability**: Copy a library folder to backup or move it
2. **Isolation**: Each library is completely independent
3. **Simplicity**: No complex path resolution or scattered data
4. **Flexibility**: Libraries can live anywhere (external drives, cloud folders)
5. **Safety**: Lock files prevent corruption from concurrent access

## Configuration

Libraries store their configuration in `library.json`:

```json
{
  "version": 2,
  "id": "550e8400-e29b-41d4-a716-446655440000",
  "name": "My Photos",
  "description": "Personal photo collection",
  "settings": {
    "generate_thumbnails": true,
    "thumbnail_quality": 85,
    "thumbnail_sizes": [128, 256, 512],
    "sync_enabled": false
  },
  "statistics": {
    "total_files": 10000,
    "total_size": 50000000000,
    "thumbnail_count": 10000
  }
}
```

## Future Extensions

The self-contained structure allows easy addition of new features:

- **Search Indexes**: Add `indexes/` for full-text and vector search
- **Version History**: Add `versions/` for file versioning
- **Plugins**: Add `plugins/` for library-specific extensions
- **Sync Metadata**: Add `sync/` for multi-device sync data

Each new feature just adds a new directory - no complex migrations needed!```

## docs/locations.md

```markdown
# Locations & File System Watching

## Overview

Locations are the foundation of how Spacedrive tracks and monitors your files. A location represents a directory on your device that Spacedrive actively monitors, indexes, and syncs across your library. This document covers location management and the sophisticated file system watching service that powers real-time updates.

## What is a Location?

A **Location** is a directory that you've added to your Spacedrive library for tracking. When you add a location:

1. **Initial Indexing**: Spacedrive scans all files and subdirectories
2. **Real-time Monitoring**: File system changes are detected instantly
3. **Cross-device Sync**: Location metadata syncs across all your devices
4. **Content Identification**: Files get content-addressable storage IDs for deduplication

### Location Properties

```rust
pub struct ManagedLocation {
    pub id: Uuid,                    // Unique location identifier
    pub name: String,                // Display name (e.g., "Desktop")
    pub path: PathBuf,               // Local file system path
    pub device_id: i32,              // Device this location exists on
    pub library_id: Uuid,            // Library this location belongs to
    pub indexing_enabled: bool,      // Whether to index files
    pub index_mode: IndexMode,       // How deep to index (Shallow/Content/Deep)
    pub watch_enabled: bool,         // Whether to watch for changes
}
```

### Index Modes

- **`Shallow`**: Only file metadata (name, size, dates)
- **`Content`**: Includes content hashing for deduplication
- **`Deep`**: Full media processing (thumbnails, metadata extraction)

## Location Manager

The `LocationManager` orchestrates the complete location lifecycle:

### Adding a Location

```rust
let (location_id, job_id) = location_manager
    .add_location(library, path, name, device_id, IndexMode::Content)
    .await?;
```

**What happens internally:**
1. **Path Validation**: Ensures the path exists and is accessible
2. **Duplicate Check**: Prevents adding the same path twice
3. **Database Record**: Creates persistent location record
4. **Indexing Job**: Spawns background indexer job
5. **Watcher Registration**: Adds location to file system watcher
6. **Event Emission**: Notifies other services of new location

### Location Lifecycle

```mermaid
graph LR
    A[Add Location] --> B[Validate Path]
    B --> C[Create DB Record]
    C --> D[Start Indexing]
    D --> E[Register Watcher]
    E --> F[Active Monitoring]
    F --> G[Real-time Updates]
    
    F --> H[Pause/Resume]
    F --> I[Remove Location]
    H --> F
    I --> J[Cleanup & Stop]
```

## Location Watcher Service

The Location Watcher is a sophisticated service that provides real-time file system monitoring across all your locations.

### Architecture

```rust
pub struct LocationWatcher {
    config: LocationWatcherConfig,           // Service configuration
    events: Arc<EventBus>,                   // Event broadcasting
    watched_locations: HashMap<Uuid, WatchedLocation>,  // Active locations
    watcher: Option<RecommendedWatcher>,     // File system watcher
    platform_handler: Arc<PlatformHandler>, // OS-specific optimizations
}
```

### Key Features

#### 1. **Cross-Platform Monitoring**
- **Linux**: Uses `inotify` for efficient event-based monitoring
- **macOS**: Leverages `FSEvents` with volume-level watching
- **Windows**: Uses `ReadDirectoryChangesW` for real-time updates

#### 2. **Event Debouncing**
Prevents event storms by consolidating rapid changes:
```rust
pub struct LocationWatcherConfig {
    pub debounce_duration: Duration,    // Default: 100ms
    pub event_buffer_size: usize,       // Default: 1000 events
    pub debug_mode: bool,               // Detailed logging
}
```

#### 3. **Intelligent Filtering**
Automatically filters out noise:
- **Temporary files**: `.tmp`, `.temp`, `~backup`
- **System files**: `.DS_Store`, `Thumbs.db`
- **Editor files**: `.swp`, `.swo`
- **Hidden files**: Except important ones like `.gitignore`

#### 4. **Event Processing Pipeline**

```mermaid
graph TD
    A[File System Change] --> B[Raw Notify Event]
    B --> C[Convert to WatcherEvent]
    C --> D[Should Process?]
    D -->|No| E[Drop Event]
    D -->|Yes| F[Platform Handler]
    F --> G[Event Debouncing]
    G --> H[Generate Core Events]
    H --> I[Event Bus Emission]
    I --> J[Update Database]
    I --> K[Notify Frontend]
```

### Event Types

The watcher converts file system events into structured events:

```rust
pub enum WatcherEventKind {
    Create,                              // New file/directory
    Modify,                              // Content or metadata change
    Remove,                              // File/directory deleted
    Rename { from: PathBuf, to: PathBuf }, // Move/rename operation
    Other(String),                       // Platform-specific events
}
```

### Platform-Specific Optimizations

#### macOS (`FSEvents`)
- **Volume-level monitoring** for better performance
- **Batch event processing** to handle high-frequency changes
- **Metadata change detection** for precise updates

#### Linux (`inotify`)
- **Recursive directory watching** with efficient descriptor management
- **Move detection** using cookie-based event correlation
- **Symlink handling** with loop detection

#### Windows (`ReadDirectoryChangesW`)
- **Overlapped I/O** for non-blocking operations
- **Network drive support** with appropriate polling fallbacks
- **Long path support** for paths exceeding 260 characters

## Usage Examples

### Adding a Location via CLI
```bash
# Add a location with automatic indexing
spacedrive location add ~/Documents --name "Documents"

# Add with specific index mode
spacedrive location add ~/Photos --name "Photos" --mode deep

# List all locations
spacedrive location list
```

### Programmatic Usage
```rust
use spacedrive_core::location::{LocationManager, IndexMode};

// Create location manager
let location_manager = LocationManager::new(event_bus);

// Add location
let (location_id, job_id) = location_manager
    .add_location(
        library,
        PathBuf::from("/Users/james/Desktop"),
        Some("Desktop".to_string()),
        device_id,
        IndexMode::Content,
    )
    .await?;

println!("Location {} added, indexing job: {}", location_id, job_id);
```

### Watching for Events
```rust
// Subscribe to location events
let mut event_stream = event_bus.subscribe();

while let Ok(event) = event_stream.recv().await {
    match event {
        Event::LocationAdded { location_id, .. } => {
            println!("New location added: {}", location_id);
        }
        Event::EntryCreated { entry_id, .. } => {
            println!("New file detected: {}", entry_id);
        }
        Event::EntryModified { entry_id, .. } => {
            println!("File modified: {}", entry_id);
        }
        _ => {}
    }
}
```

## Configuration & Tuning

### Watcher Configuration
```rust
let config = LocationWatcherConfig {
    debounce_duration: Duration::from_millis(500),  // Slower for high-frequency changes
    event_buffer_size: 2000,                        // Larger buffer for busy directories
    debug_mode: true,                               // Enable for troubleshooting
};
```

### Performance Considerations

#### For Large Directories (>100k files)
- **Increase buffer size**: Prevent event loss during initial scans
- **Longer debounce**: Reduce CPU usage during bulk operations
- **Selective watching**: Consider excluding build/cache directories

#### For Network Drives
- **Polling fallback**: Some network filesystems don't support native events
- **Reduced sensitivity**: Longer polling intervals to avoid network spam
- **Connection monitoring**: Handle temporary disconnections gracefully

#### For SSD vs HDD
- **SSD**: Shorter debounce times, higher buffer sizes
- **HDD**: Longer debounce to handle mechanical latency

## Troubleshooting

### Common Issues

#### 1. **High CPU Usage**
```bash
# Check if specific directories are causing event storms
spacedrive location list --verbose

# Temporarily disable watching for problematic locations
spacedrive location update <location-id> --watch false
```

#### 2. **Missing File Changes**
- **Check permissions**: Ensure Spacedrive can read the directory
- **Verify watching**: Confirm the location has watching enabled
- **Platform limits**: Some platforms have file descriptor limits

#### 3. **Duplicate Events**
- **Increase debounce**: Some editors create multiple rapid changes
- **Check for symlinks**: Ensure you're not watching the same directory twice

### Debug Mode
Enable debug logging to see detailed event information:
```rust
LocationWatcherConfig {
    debug_mode: true,
    ..Default::default()
}
```

This outputs detailed logs like:
```
DEBUG Raw file system event: Event { kind: Create(File), paths: ["/Users/james/test.txt"] }
DEBUG WatcherEvent processed: Create { path: "/Users/james/test.txt", timestamp: 2024-01-15T10:30:45Z }
```

## Event Bus Integration

The Location Watcher integrates seamlessly with Spacedrive's event system:

### Events Emitted
- `Event::LocationAdded` - New location added to library
- `Event::LocationRemoved` - Location removed from library
- `Event::EntryCreated` - New file/directory detected
- `Event::EntryModified` - File content or metadata changed
- `Event::EntryDeleted` - File/directory removed
- `Event::EntryMoved` - File/directory moved or renamed

### Event Consumption
Other services subscribe to these events:
- **Indexer**: Re-indexes modified files
- **Search**: Updates search index
- **Sync**: Propagates changes to other devices
- **Frontend**: Updates UI in real-time

## Future Enhancements

### Planned Features
- **Smart exclusion patterns**: Automatically learn to ignore build directories
- **Bandwidth-aware syncing**: Adjust sync frequency based on connection quality
- **Conflict resolution**: Handle simultaneous edits across devices
- **Version history**: Track file changes over time
- **AI-powered organization**: Automatically suggest better folder structures

### Performance Improvements
- **Event compression**: Combine related events for efficiency
- **Predictive prefetching**: Pre-load likely-to-be-accessed files
- **Adaptive polling**: Adjust watch frequency based on directory activity
- **Background processing**: Move heavy operations off the critical path

## Related Documentation

- [Indexing System](./indexing.md) - How files are analyzed and processed
- [Job System](./job-system.md) - Background task management
- [Database Schema](./database.md) - Location storage and relationships
- [Event System](./events.md) - Inter-service communication
- [CLI Usage](./cli.md) - Command-line location management

---

The location and watching system forms the foundation of Spacedrive's real-time file management capabilities. By efficiently monitoring file system changes and integrating with the broader architecture, it enables the seamless cross-device experience that makes Spacedrive unique.```

## docs/networking.md

```markdown
# Spacedrive v2 Networking Module: Iroh-Powered P2P

The Spacedrive v2 networking module provides robust device-to-device communication using Iroh, a modern peer-to-peer networking library. It enables secure device pairing, peer discovery, and encrypted data transfer between Spacedrive instances, forming the backbone of the Virtual Distributed File System (VDFS).

This implementation leverages Iroh's QUIC-based transport for reliable connections with excellent NAT traversal capabilities (90%+ success rate) and built-in encryption.

## Overview

The networking module is tightly integrated into the `Core` struct and provides:

- **Simplified Transport**: QUIC-based transport with built-in encryption and multiplexing
- **Relay Fallback**: Automatic relay server fallback when direct connections fail  
- **Protocol Negotiation**: ALPN-based protocol selection for pairing, messaging, and file transfer
- **Centralized State Management**: A single `DeviceRegistry` tracks the state of all known peers, from discovered to paired and connected
- **Extensible Protocol System**: A modular `ProtocolRegistry` allows for clean separation of concerns, routing incoming messages to the correct handler
- **Secure Device Pairing**: A robust, challenge-response pairing protocol secured with cryptographic signatures and initiated with user-friendly BIP39 word codes
- **End-to-End Encrypted File Transfer**: High-level APIs for sharing files between devices, built on an underlying protocol that handles chunking, encryption, and verification

## Architecture

The networking architecture uses Iroh's endpoint model for simplicity and reliability. All network operations are managed by a single `NetworkingService` instance, which is initialized and managed by the main `Core` struct.

```
+---------------------------------------------+
|                Core (`lib.rs`)              |
| - init_networking()                         |
| - start_pairing_as_initiator()              |
| - share_with_device()                       |
+---------------------------------------------+
                   |
                   v
+---------------------------------------------+
|    NetworkingService (`core/mod.rs`)        |
| - Endpoint (Iroh)                           |
| - DeviceRegistry (Tracks all devices)       |
| - ProtocolRegistry (Routes messages)        |
| - NetworkIdentity (Ed25519-based)           |
+---------------------------------------------+
                   |
                   v
+---------------------------------------------+
|  NetworkingEventLoop (`core/event_loop.rs`) |
| - Handles incoming connections              |
| - Routes based on ALPN protocol             |
| - Manages command processing                |
+---------------------------------------------+
      |            |                |
      v            v                v
+-----------+ +------------+ +----------------+
|  Pairing  | | Messaging  | | File Transfer  |
| Protocol  | | Protocol   | | Protocol       |
+-----------+ +------------+ +----------------+
```

## Key Changes from libp2p

1. **Transport**: Replaced TCP+Noise+Yamux with QUIC (better NAT traversal, built-in encryption)
2. **Identity**: Uses Iroh's Ed25519-based NodeId instead of libp2p's PeerId
3. **Addressing**: NodeAddr replaces Multiaddr for simpler address handling
4. **Discovery**: Currently manual (DHT discovery to be implemented separately)
5. **Protocols**: ALPN-based protocol negotiation instead of libp2p's protocol strings

## Components

### NetworkingService
The main entry point for all networking operations. Manages the Iroh endpoint and coordinates between different components.

### NetworkIdentity  
Manages the device's cryptographic identity, compatible with both Iroh's NodeId system and legacy Ed25519 signing.

### DeviceRegistry
Central registry tracking all known devices and their states (discovered, pairing, paired, connected, disconnected).

### ProtocolRegistry
Routes incoming messages to appropriate protocol handlers based on protocol name.

### Event Loop
Processes incoming connections and routes them to protocol handlers based on ALPN negotiation.

## Protocols

### Pairing Protocol
- Secure device pairing using challenge-response authentication
- BIP39 mnemonic codes for user-friendly pairing
- Ed25519 signatures for cryptographic verification

### Messaging Protocol  
- Real-time message exchange between paired devices
- JSON-serialized messages for flexibility

### File Transfer Protocol
- Chunked file transfer with progress tracking
- End-to-end encryption using session keys
- Automatic resume for interrupted transfers

## Future Enhancements

1. **Discovery**: Implement DHT-based discovery for finding peers
2. **Stream Integration**: Port protocols to use Iroh's native stream handling
3. **Relay Deployment**: Deploy custom relay servers for Spacedrive Cloud
4. **Protocol Optimization**: Optimize protocols for Iroh's capabilities```

## docs/pairing.md

```markdown
# Spacedrive v2: Device Pairing Protocol

The Spacedrive v2 device pairing system is a secure, robust protocol for establishing a trusted, end-to-end encrypted connection between two devices, regardless of whether they are on the same local network or across the internet. It is built upon the unified libp2p networking stack and uses a combination of modern cryptographic principles and user-friendly codes to deliver a seamless pairing experience.

## Overview

The primary goal of the pairing system is to create a secure relationship between two devices, allowing them to communicate directly, exchange session keys, and perform operations like file transfer and synchronization. This is achieved through a challenge-response handshake initiated by a user-friendly 12-word BIP39 code.

## Key Features

- **Cryptographic Security**: Pairing is secured using an Ed25519 challenge-response signature verification, ensuring that only the intended device can complete the process. All transport-level communication is encrypted using the Noise Protocol.
- **Dual-Discovery Mechanism**: The system uses a unified discovery approach. It queries the Kademlia Distributed Hash Table (DHT) for remote discovery (across different networks) while simultaneously listening for local peers via mDNS. The first successful discovery method is used, providing both speed on local networks and reachability over the internet.
- **User-Friendly Pairing Codes**: Instead of complex hashes, the system generates a 12-word BIP39 mnemonic code. This code is easy for users to read and type, yet contains enough entropy to securely identify a pairing session.
- **State Machine Architecture**: The entire pairing process is managed by a robust state machine within the `PairingProtocolHandler`, which tracks each session from initiation to completion or failure.
- **Automatic Device Registration**: Upon successful pairing, devices are automatically added to each other's `DeviceRegistry`, and persistent, encrypted session keys are established for all future communication.

## Core Components

The pairing system is a collaboration between several key components in the new architecture:

1.  **`Core` (`src/lib.rs`)**: Provides the high-level public API for initiating pairing. The `start_pairing_as_initiator()` and `start_pairing_as_joiner()` methods are the entry points for the entire process.
2.  **`PairingProtocolHandler` (`src/infrastructure/networking/protocols/pairing/mod.rs`)**: This is the heart of the pairing system. It acts as a state machine, managing active pairing sessions, generating cryptographic challenges, verifying responses, and orchestrating the entire protocol flow.
3.  **`PairingCode` & `PairingSession` (`types.rs`)**: These structs define the data model for pairing.
    - `PairingCode`: Represents the 12-word code and the cryptographic secret from which the session ID is derived.
    - `PairingSession`: Tracks the state (`PairingState` enum) of a single pairing attempt, including remote device info and derived keys.
4.  **`DeviceRegistry` (`device/registry.rs`)**: The central registry for device state. During pairing, it maps an ephemeral `session_id` to a permanent `device_id` and stores the final `SessionKeys` upon completion.
5.  **`UnifiedBehaviour` (`core/behavior.rs`)**: The unified libp2p behavior that enables discovery. Its Kademlia DHT component is used to publish and query pairing advertisements, while the mDNS component listens for local peers.

## The Pairing Flow

The pairing process involves two roles: the **Initiator** (who generates the code) and the **Joiner** (who uses the code).

### Initiator Flow (e.g., Alice)

1.  **Initiation**: A user triggers the pairing process, calling `core.start_pairing_as_initiator()`.
2.  **Code Generation**: A cryptographically secure `PairingCode` is generated. A unique `session_id` is derived from this code's entropy.
3.  **DHT Advertisement**:
    - The Initiator gathers its public `DeviceInfo` (name, OS, etc.) and its external network addresses (e.g., `/ip4/1.2.3.4/tcp/5678`).
    - This information is packaged into a `PairingAdvertisement`.
    - The advertisement is published to the Kademlia DHT. The DHT `RecordKey` is the `session_id`, making it discoverable by the Joiner.
4.  **Waiting State**: The Initiator's `PairingSession` enters the `WaitingForConnection` state. It now listens for incoming connections from any peer.
5.  **Challenge Issuance**:
    - When a Joiner connects and sends a `PairingRequest` message containing its public key and `DeviceInfo`, the Initiator's `PairingProtocolHandler` receives it.
    - The handler generates a random 32-byte cryptographic `challenge`.
    - It sends this `challenge` back to the Joiner in a `Challenge` message. The session state transitions to `ChallengeReceived`.
6.  **Response Verification**:
    - The Initiator receives a `Response` message from the Joiner. This message contains the original challenge signed with the Joiner's private device key.
    - The `PairingSecurity` module is used to verify the signature against the Joiner's public key (received in step 5).
7.  **Completion**:
    - If the signature is valid, the pairing is successful.
    - The Initiator derives the shared session keys.
    - It updates its `DeviceRegistry` to mark the Joiner as a trusted, paired device.
    - It sends a final `Complete` message to the Joiner. The session is now `Completed`.

### Joiner Flow (e.g., Bob)

1.  **Code Entry**: The user enters the 12-word `PairingCode` provided by the Initiator.
2.  **Session ID Extraction**: The `PairingCode` is parsed to deterministically reconstruct the same `session_id` the Initiator created.
3.  **Unified Discovery**:
    - The `core.start_pairing_as_joiner()` method is called.
    - The `NetworkingCore` immediately begins querying the DHT using the `session_id` to find the Initiator's `PairingAdvertisement`.
    - Simultaneously, the mDNS service listens for the Initiator on the local network.
4.  **Connection**:
    - Once the Initiator's address is discovered (via DHT or mDNS), the system establishes a direct TCP connection.
5.  **Sending Request**:
    - As soon as the connection is established, the Joiner sends a `PairingRequest` message. This message includes its own `DeviceInfo` and, crucially, its public key.
6.  **Signing Challenge**:
    - The Joiner receives the `Challenge` message from the Initiator.
    - It uses its private `NetworkIdentity` key to sign the 32-byte challenge.
    - It sends the resulting 64-byte signature back in a `Response` message.
7.  **Completion**:
    - The Joiner receives the final `Complete` message from the Initiator.
    - It derives the same shared session keys.
    - It updates its `DeviceRegistry` to add the Initiator as a trusted, paired device. The connection is now fully authenticated and encrypted for all future communication.

## Security Model

The pairing protocol is designed with security as a primary concern.

- **Transport Encryption**: All communication, from the very first connection attempt, is encrypted using the **Noise Protocol**, which provides forward secrecy.
- **Cryptographic Authentication**: The identity of the joining device is verified using an **Ed25519 digital signature**. The challenge-response mechanism prevents replay attacks and ensures that the device joining is the one that possesses the private key corresponding to the public key it presented.
- **Session Key Derivation**: Once authenticated, the shared secret from the pairing code is used as input to a **Key Derivation Function (HKDF)**. This generates strong, unique symmetric keys for sending and receiving data between the two devices, ensuring all subsequent communication is confidential and authenticated.
- **Ephemeral & Discoverable Session ID**: The `session_id` used for DHT discovery is derived from the pairing code but is not the secret itself. This allows the session to be publicly discoverable for a short period without exposing any sensitive information. The codes and sessions expire after 5-10 minutes to limit the window of opportunity for attacks.

## Implementation Details

The core of the logic is implemented in the `PairingProtocolHandler`, which uses a `PairingState` enum to manage the lifecycle of each `PairingSession`.

```rust
// from src/infrastructure/networking/protocols/pairing/messages.rs
pub enum PairingMessage {
    PairingRequest {
        session_id: Uuid,
        device_info: DeviceInfo,
        public_key: Vec<u8>,
    },
    Challenge {
        session_id: Uuid,
        challenge: Vec<u8>,
        device_info: DeviceInfo,
    },
    Response {
        session_id: Uuid,
        response: Vec<u8>,
        device_info: DeviceInfo,
    },
    Complete {
        session_id: Uuid,
        success: bool,
        reason: Option<String>,
    },
}
```

This message-passing design, combined with a robust state machine, ensures that the pairing process is reliable and secure from start to finish.
```

## docs/roadmap.md

```markdown
Of course. Based on the extensive Rust codebase and the list of design documents you've provided, here is a comprehensive roadmap for Spacedrive V2. This roadmap outlines the current state of the project, the features actively in development, and the path toward feature completeness.

---

# Spacedrive V2: Core Engine Roadmap

This document outlines the development roadmap for the Spacedrive V2 core engine (`sd-core-new`). The V2 rewrite establishes a robust, scalable, and extensible foundation designed to overcome the architectural limitations of V1 and fully realize the vision of a unified, intelligent, local-first file system.

### Guiding Principles

- **Rust First**: Leverage Rust's safety, performance, and concurrency for a reliable core.
- **Local-First & P2P**: Data and control remain on user devices. The network enhances, it doesn't centralize.
- **AI-Native**: The architecture is built from the ground up to support intelligent, agentic file management.
- **Modular & Extensible**: A clean separation of concerns allows for independent development and future extension.
- **Test-Driven**: A comprehensive integration test framework ensures stability and reliability for core features.

---

## Phase 1: Foundation & Core Services (Largely Complete âœ…)

This phase focused on building the non-negotiable, foundational components of the new architecture. These systems are stable, well-tested, and provide the bedrock for all other features.

| Component                        | Status         | Notes                                                                                                                                                                                         |
| :------------------------------- | :------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Core Engine Lifecycle**        | âœ… Implemented | [cite\_start]The `Core` struct manages the application lifecycle, including startup, configuration, and graceful shutdown. [cite: 3099, 3140]                                                 |
| **Configuration Management**     | âœ… Implemented | [cite\_start]A versioned `AppConfig` handles all application-level settings, with support for migrations. [cite: 3413, 3419]                                                                  |
| **Device Management**            | âœ… Implemented | [cite\_start]The `DeviceManager` provides persistent, cross-session device identity, a prerequisite for all P2P operations. [cite: 3941]                                                      |
| **Library Management**           | âœ… Implemented | [cite\_start]The `LibraryManager` handles the full lifecycle of `.sdlibrary` containers, including creation, locking for safe access, discovery, and closure. [cite: 3312, 3460]              |
| **Extensible Job System**        | âœ… Implemented | A highly modular job system with automatic registration via a `#[derive(Job)]` macro. [cite\_start]Supports resumable, persistent jobs with progress tracking. [cite: 3369, 3171, 4364]       |
| **Event Bus**                    | âœ… Implemented | [cite\_start]A decoupled, publish-subscribe event bus (`EventBus`) for inter-service communication, replacing the brittle patterns of V1. [cite: 3154]                                        |
| **Command Line Interface (CLI)** | âœ… Implemented | [cite\_start]A robust CLI provides user interaction with the core engine via a daemon client, featuring structured, format-agnostic output (human, JSON). [cite: 3411, 4096]                  |
| **Test Framework**               | âœ… Implemented | [cite\_start]A custom multi-process test runner (`CargoTestRunner`) enables realistic, end-to-end testing of distributed scenarios like device pairing and file transfers. [cite: 3099, 3257] |

---

## Phase 2: VDFS, Networking & File Operations (In Progress ğŸš§)

This is the current focus of active development. This phase brings the Virtual Distributed File System (VDFS) to life by implementing the core indexing logic, networking layer, and fundamental file operations.

| Feature                                | Status         | Notes & Design Docs                                                                                                                                                                                                                                                                                                              |
| :------------------------------------- | :------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Volume-Aware Storage Foundation**    | âœ… Implemented | The `VolumeManager` performs platform-aware detection, classification, and performance testing of storage devices. [cite\_start]This provides the foundation for intelligent, cross-volume file operations. [cite: 3110, 4522] \<br\> _Design: `VOLUME_CLASSIFICATION_DESIGN.md`, `VOLUME_TRACKING_IMPLEMENTATION_PLAN.md`_      |
| **File Type System**                   | âœ… Implemented | [cite\_start]A registry-based system (`FileTypeRegistry`) identifies files using a combination of extensions, magic bytes, and content analysis. [cite: 3376, 3407] \<br\> _Design: `DESIGN_FILE_TYPE_SYSTEM.md`_                                                                                                                |
| **Transactional Action System**        | âœ… Implemented | The `ActionManager` provides a central point for dispatching all user operations. [cite\_start]It ensures validation, audit logging, and consistent execution. [cite: 3496, 3099] \<br\> _Design: `ACTION_SYSTEM_DESIGN.md`_                                                                                                     |
| **File Operations (Copy/Move/Delete)** | âœ… Implemented | Core file operations are implemented as durable jobs. [cite\_start]The copy operation uses a strategy pattern to automatically select the optimal method (e.g., atomic rename for same-volume moves, streaming for cross-volume copies). [cite: 3616, 3546, 3547] \<br\> _Design: `CROSS_PLATFORM_COPY_AND_VOLUME_AWARENESS.md`_ |
| **VDFS Indexing Engine**               | ğŸš§ In Progress | The multi-phase indexer (`IndexerJob`) is functional, including smart filtering, inode-based change detection, and resumability. [cite\_start]The next step is to integrate the designed Indexer Rules System. [cite: 3173, 3555] \<br\> _Design: `INDEXER_ANALYSIS.md`, `INDEXER_RULES_SYSTEM.md`_                              |
| **P2P Networking (Iroh)**              | ğŸš§ In Progress | The unified networking layer using **Iroh** is operational. End-to-end tests confirm device pairing and persistence across restarts. [cite\_start]Ongoing work focuses on hardening connections and improving reliability. [cite: 3245, 4619] \<br\> _Design: `NETWORKING_SYSTEM_DESIGN.md`, `IROH_MIGRATION_DESIGN.md`_         |
| **Spacedrop (P2P File Transfer)**      | ğŸš§ In Progress | The core file transfer protocol is implemented and tested. It supports streaming large files between paired devices. [cite\_start]The next step is to build the ephemeral, AirDrop-like user experience. [cite: 3245, 4821] \<br\> _Design: `SPACEDROP_DESIGN.md`, `SPACEDROP_IMPLEMENTATION_PLAN.md`_                           |

---

## Phase 3: Intelligence & User Experience (Next Up ğŸ“)

With the foundation and VDFS in place, this phase will focus on building the intelligent features and synchronization logic that define the Spacedrive user experience. These features have been designed and are ready for implementation.

| Feature                         | Status      | Notes & Design Docs                                                                                                                                                                                                                  |
| :------------------------------ | :---------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Library Sync**                | ğŸ“ Designed | The core design uses a domain-separated model to avoid the complexity of CRDTs. Implementation will begin once the P2P networking layer is fully stabilized. \<br\> _Design: `SYNC_DESIGN.md`, `SYNC_INTEGRATION_NOTES.md`_          |
| **AI Agent Manager**            | ğŸ“ Designed | A framework for AI agents to observe the VDFS state (via the Event Bus) and propose actions. This will power all proactive and intelligent features. \<br\> _Design: `AGENT_MANAGER_DESIGN.md`_                                      |
| **Lightning Search**            | ğŸ“ Designed | A hybrid search architecture combining fast full-text search (FTS5) with semantic vector search for re-ranking. This enables natural language queries without sacrificing performance. \<br\> _Design: `LIGHTNING_SEARCH_DESIGN.md`_ |
| **Thumbnail Generation System** | ğŸ“ Designed | A dedicated, resumable job for generating thumbnails for various media types (images, videos via ffmpeg feature flag, PDFs). \<br\> _Design: `THUMBNAIL_SYSTEM_DESIGN.md`_                                                           |
| **GUI & Mobile Clients**        | ğŸ’¡ Planned  | Development of the Tauri (desktop) and React Native (mobile) frontends will begin, connecting to the core engine via a GraphQL API.                                                                                                  |

---

## Phase 4: Ecosystem & Enterprise (Future ğŸ’¡)

This phase extends Spacedrive from a personal tool into a comprehensive platform for teams and organizations, introducing cloud services, collaboration, and enterprise-grade features.

| Feature                            | Status     | Notes                                                                                                                                                                                                   |
| :--------------------------------- | :--------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **Native Cloud Service**           | ğŸ’¡ Planned | As outlined in the whitepaper, a cloud backend will be offered where each user gets a dedicated, isolated `sd-core-new` instance that acts as a native P2P peer in their network.                       |
| **Team Libraries & Collaboration** | ğŸ’¡ Planned | Extend the VDFS model to support shared Libraries with Role-Based Access Control (RBAC), building on the foundation of the Action System.                                                               |
| **Third-Party Integrations**       | ğŸ’¡ Planned | Develop a stable GraphQL API and an extension system to allow integration with other tools and services (e.g., cloud storage providers, productivity apps).                                             |
| **Advanced Storage Tiering**       | ğŸ’¡ Planned | Leverage the AI Agent and Volume-Aware Storage Foundation to automatically migrate "cold" data to slower, cheaper storage (like a NAS or cloud archive) while keeping "hot" data on fast local storage. |
| **Federated Learning**             | ğŸ’¡ Planned | Explore privacy-preserving federated learning models to improve the AI agent's organizational suggestions based on anonymized, aggregate user patterns without compromising individual data.            |

### Future Optimizations & Research

- **Closure Table for Hierarchical Queries**: A planned database optimization to replace materialized path queries for directory hierarchies. This will provide O(1) lookups for subtree operations, dramatically improving UI responsiveness for very large directories. (_Design: `CLOSURE_TABLE_INDEXING_PROPOSAL.md`_)
- **Content-Defined Chunking**: Investigate integrating content-defined chunking (e.g., using rolling hashes) to enable block-level deduplication and more efficient P2P file transfers, especially for large, frequently modified files like virtual machine images.
```

## docs/testing.md

```markdown
# Testing Guide

This document covers testing approaches and frameworks available in Spacedrive Core.

## Overview

Spacedrive Core includes two test frameworks to handle different testing scenarios:

1. **Standard Test Framework** (`test_framework`) - For single-process unit and integration tests
2. **Cargo Test Subprocess Framework** (`test_framework_new`) - For multi-device networking tests

## Cargo Test Subprocess Framework

### When to Use

Use the cargo test subprocess framework when you need:
- Multi-device networking tests (pairing, file transfer, sync)
- Subprocess isolation for Core instances
- Parallel execution of different device roles
- Tests that simulate real network scenarios

### How It Works

The framework uses `cargo test` itself as the subprocess executor:

1. **Main Test**: Orchestrates the overall test scenario
2. **Device Scenarios**: Individual test functions for each device role
3. **Environment Coordination**: Uses env vars to control which role runs
4. **Process Management**: Spawns and monitors cargo test subprocesses

### Basic Structure

```rust
use sd_core_new::test_framework_new::CargoTestRunner;
use std::env;

// Device scenario - runs when TEST_ROLE matches
#[tokio::test]
#[ignore] // Only run when explicitly called
async fn alice_scenario() {
    // Exit early if not running as Alice
    if env::var("TEST_ROLE").unwrap_or_default() != "alice" {
        return;
    }
    
    let data_dir = PathBuf::from(env::var("TEST_DATA_DIR").expect("TEST_DATA_DIR required"));
    
    // ALL test logic for Alice goes here
    let mut core = Core::new_with_config(data_dir).await?;
    // ... complete test implementation
}

// Main orchestrator
#[tokio::test]
async fn test_multi_device_scenario() {
    let mut runner = CargoTestRunner::new()
        .with_timeout(Duration::from_secs(90))
        .add_subprocess("alice", "alice_scenario")
        .add_subprocess("bob", "bob_scenario");
    
    runner.run_until_success(|outputs| {
        // Check for success patterns in output
        outputs.get("alice").map(|out| out.contains("SUCCESS")).unwrap_or(false) &&
        outputs.get("bob").map(|out| out.contains("SUCCESS")).unwrap_or(false)
    }).await.expect("Test failed");
}
```

### Environment Variables

The framework uses these environment variables for coordination:

- `TEST_ROLE` - Specifies which device role to run (`alice`, `bob`, etc.)
- `TEST_DATA_DIR` - Provides isolated temporary directory for each process

### Process Communication

Processes coordinate through:
- **File-based**: Temporary files for sharing data (pairing codes, state)
- **Output parsing**: Success/failure patterns in stdout/stderr
- **Timeouts**: Automatic cleanup if tests hang

### Example: Device Pairing Test

```rust
// Alice's role - all logic in test file
#[tokio::test]
#[ignore]
async fn alice_pairing_scenario() {
    if env::var("TEST_ROLE").unwrap_or_default() != "alice" { return; }
    
    let data_dir = PathBuf::from(env::var("TEST_DATA_DIR").expect("TEST_DATA_DIR required"));
    let mut core = Core::new_with_config(data_dir).await.unwrap();
    
    core.init_networking("test-password").await.unwrap();
    let (pairing_code, _) = core.start_pairing_as_initiator().await.unwrap();
    
    // Share pairing code with Bob
    std::fs::write("/tmp/pairing_code.txt", &pairing_code).unwrap();
    
    // Wait for Bob to connect
    loop {
        let devices = core.get_connected_devices().await.unwrap();
        if !devices.is_empty() {
            println!("PAIRING_SUCCESS: Alice connected");
            break;
        }
        tokio::time::sleep(Duration::from_secs(1)).await;
    }
}

// Bob's role - all logic in test file  
#[tokio::test]
#[ignore]
async fn bob_pairing_scenario() {
    if env::var("TEST_ROLE").unwrap_or_default() != "bob" { return; }
    
    let data_dir = PathBuf::from(env::var("TEST_DATA_DIR").expect("TEST_DATA_DIR required"));
    let mut core = Core::new_with_config(data_dir).await.unwrap();
    
    core.init_networking("test-password").await.unwrap();
    
    // Wait for Alice's pairing code
    let pairing_code = loop {
        if let Ok(code) = std::fs::read_to_string("/tmp/pairing_code.txt") {
            break code.trim().to_string();
        }
        tokio::time::sleep(Duration::from_millis(500)).await;
    };
    
    core.start_pairing_as_joiner(&pairing_code).await.unwrap();
    
    // Wait for connection
    loop {
        let devices = core.get_connected_devices().await.unwrap();
        if !devices.is_empty() {
            println!("PAIRING_SUCCESS: Bob connected");
            break;
        }
        tokio::time::sleep(Duration::from_secs(1)).await;
    }
}
```

### Running Tests

**Run the complete test:**
```bash
cargo test test_device_pairing --nocapture
```

**Debug individual scenarios:**
```bash
# Run just Alice's scenario
TEST_ROLE=alice TEST_DATA_DIR=/tmp/test cargo test alice_pairing_scenario -- --ignored --nocapture

# Run just Bob's scenario
TEST_ROLE=bob TEST_DATA_DIR=/tmp/test cargo test bob_pairing_scenario -- --ignored --nocapture
```

### Best Practices

1. **Use `#[ignore]`** on device scenario functions so they only run when explicitly called
2. **Exit early** if TEST_ROLE doesn't match to avoid unintended execution
3. **Use clear success patterns** in output for the condition function to detect
4. **Clean up shared files** in temporary locations to avoid test interference
5. **Set appropriate timeouts** based on the complexity of your test scenario
6. **Use descriptive test names** that clearly indicate the scenario being tested

### Debugging

**Check test output:**
```bash
# See detailed output from both processes
cargo test test_device_pairing --nocapture
```

**Run scenarios individually:**
```bash
# Test Alice's logic in isolation
TEST_ROLE=alice TEST_DATA_DIR=/tmp/debug cargo test alice_scenario -- --ignored --nocapture
```

**Common issues:**
- **Process hangs**: Check timeout settings and success condition logic
- **File conflicts**: Ensure unique temporary file paths for concurrent tests
- **Environment leakage**: Make sure TEST_ROLE guards are working correctly

## Standard Test Framework

For single-process tests, use the standard Rust testing approach:

```rust
#[tokio::test]
async fn test_core_initialization() {
    let core = Core::new().await.unwrap();
    assert!(core.device.device_id().is_ok());
}
```

## Legacy Test Framework

The original `test_framework` with scenarios is still available but deprecated in favor of the cargo test subprocess approach for multi-device tests.

## Writing New Tests

### For Single Device Tests
Use standard `#[tokio::test]` functions.

### For Multi-Device Tests
1. Create device scenario functions with `#[ignore]` and TEST_ROLE guards
2. Create a main orchestrator test using `CargoTestRunner`
3. Define clear success patterns for the condition function
4. Use appropriate timeouts and cleanup

### File Organization
- Put tests in `tests/` directory
- Use descriptive filenames: `test_device_pairing.rs`, `test_file_sync.rs`, etc.
- Keep all test logic in the test files themselves

## Examples

See `tests/core_pairing_test_new.rs` for a complete example of the cargo test subprocess framework in action.```

## docs/volume-system.md

```markdown
# Volume System Documentation

The Volume System in Spacedrive Core v2 provides cross-platform storage volume detection, monitoring, and management capabilities. It enables volume-aware file operations and persistent tracking of storage devices across sessions.

## Overview

The Volume System consists of two main layers:

1. **Runtime Volume Detection** - Detects and monitors available storage volumes
2. **Persistent Volume Tracking** - Tracks volumes in the database with user preferences

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Volume Manager â”‚â”€â”€â”€â”€â”‚  Runtime Volumes â”‚    â”‚ Domain Volumes  â”‚
â”‚                 â”‚    â”‚                  â”‚    â”‚                 â”‚
â”‚ - Detection     â”‚    â”‚ - Live Detection â”‚    â”‚ - Persistence   â”‚
â”‚ - Monitoring    â”‚â”€â”€â”€â”€â”‚ - Fingerprinting â”‚â”€â”€â”€â”€â”‚ - User Prefs    â”‚
â”‚ - Caching       â”‚    â”‚ - Events         â”‚    â”‚ - Tracking      â”‚
â”‚ - Events        â”‚    â”‚ - Statistics     â”‚    â”‚ - Library Assoc â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     Event Bus       â”‚
                    â”‚                     â”‚
                    â”‚ - VolumeAdded       â”‚
                    â”‚ - VolumeRemoved     â”‚
                    â”‚ - VolumeUpdated     â”‚
                    â”‚ - VolumeMountChangedâ”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Core Components

### VolumeManager

Central management for all volume operations:

```rust
use spacedrive_core::volume::VolumeManager;

// Get all detected volumes
let volumes = volume_manager.get_all_volumes().await;

// Find volume for a specific path
let volume = volume_manager.volume_for_path(&path).await;

// Track a volume in a library
volume_manager.track_volume(
    &fingerprint,
    &library_ctx,
    Some("My External Drive".to_string())
).await?;
```

### Runtime Volume Types

#### Volume
Represents a detected storage volume:

```rust
pub struct Volume {
    pub fingerprint: VolumeFingerprint,
    pub name: String,
    pub mount_point: PathBuf,
    pub mount_points: Vec<PathBuf>,
    pub mount_type: MountType,
    pub disk_type: DiskType,
    pub file_system: FileSystem,
    pub total_bytes_capacity: u64,
    pub total_bytes_available: u64,
    pub is_mounted: bool,
    pub is_read_only: bool,
    // ... performance metrics
}
```

#### Volume Classification

**Mount Types:**
- `System` - Root filesystem, boot partitions
- `External` - USB drives, external storage
- `Network` - NFS, SMB, cloud mounts

**Disk Types:**
- `SSD` - Solid State Drive
- `HDD` - Hard Disk Drive  
- `Network` - Network storage
- `Virtual` - RAM disk, virtual storage

**Filesystems:**
- `APFS`, `NTFS`, `Ext4`, `Btrfs`, `ZFS`, `ReFS`
- `FAT32`, `ExFAT`, `HFSPlus`
- `Other(String)` - Unknown filesystems

### Volume Fingerprinting

Volumes are uniquely identified using Blake3-based fingerprinting:

```rust
// Fingerprint combines multiple identifiers
let fingerprint = VolumeFingerprint::new(
    &mount_point,
    hardware_id.as_deref(),
    total_capacity,
    &filesystem
);
```

This ensures volumes can be reliably identified even when mount points change.

## Platform Support

### macOS Detection
- Uses `diskutil` and `df` commands
- Detects APFS volumes and mount points
- Identifies SSD vs HDD via `diskutil info`

### Linux Detection  
- Uses `df -h -T` for filesystem information
- Detects disk type via `/sys/block/*/queue/rotational`
- Supports major Linux filesystems

### Windows Detection
- Uses PowerShell `Get-Volume` cmdlet
- Full implementation pending

## Volume Events

The system emits events for volume state changes:

```rust
// Listen for volume events
let mut subscriber = event_bus.subscribe();

while let Ok(event) = subscriber.recv().await {
    match event {
        Event::VolumeAdded(volume) => {
            println!("New volume: {}", volume.name);
        }
        Event::VolumeRemoved { fingerprint } => {
            println!("Volume removed: {}", fingerprint);
        }
        Event::VolumeMountChanged { fingerprint, is_mounted } => {
            println!("Volume {} mount changed: {}", fingerprint, is_mounted);
        }
        _ => {}
    }
}
```

## Volume Tracking

### Runtime vs Tracked Volumes

**Runtime Volumes:**
- Automatically detected by the system
- Temporary - exist only while mounted
- No user customization
- Available to all libraries

**Tracked Volumes:**
- Explicitly tracked by user choice
- Persistent across sessions
- User customizable (names, colors, icons)
- Associated with specific libraries

### Tracking Flow

1. **Volume Detection**
   ```rust
   // Volume is detected and appears in runtime list
   let volumes = volume_manager.get_all_volumes().await;
   ```

2. **User Initiates Tracking**
   ```rust
   // User clicks "Track this volume" in UI
   volume_manager.track_volume(
       &fingerprint,
       &library_context,
       Some("My Photos Drive".to_string())
   ).await?;
   ```

3. **Volume Persisted**
   ```rust
   // Volume saved to database with user preferences
   // Associated with specific library
   // Available across sessions
   ```

### Domain Volume Model

Tracked volumes are stored as domain models:

```rust
pub struct Volume {
    pub id: Uuid,
    pub library_id: Option<Uuid>,
    pub device_id: Uuid,
    pub fingerprint: String,
    pub name: String,
    pub is_tracked: bool,
    
    // User preferences
    pub display_name: Option<String>,
    pub is_favorite: bool,
    pub color: Option<String>,
    pub icon: Option<String>,
    
    // Statistics
    pub total_files: Option<u64>,
    pub total_directories: Option<u64>,
    
    // Performance metrics
    pub read_speed_mbps: Option<u64>,
    pub write_speed_mbps: Option<u64>,
    
    // Timestamps
    pub created_at: DateTime<Utc>,
    pub last_seen_at: DateTime<Utc>,
}
```

## Volume Operations

### Core Operations

```rust
// Check if two paths are on the same volume
let same_volume = volume_manager.same_volume(&path1, &path2).await;

// Find volumes with sufficient space
let available_volumes = volume_manager
    .volumes_with_space(required_bytes).await;

// Get volume statistics
let stats = volume_manager.get_statistics().await;

// Run speed test on volume
volume_manager.run_speed_test(&fingerprint).await?;
```

### Volume Utilities

```rust
use spacedrive_core::volume::util;

// Check if path is on specific volume
let is_on_volume = util::is_path_on_volume(&path, &volume);

// Get relative path on volume
let relative = util::relative_path_on_volume(&path, &volume);

// Find best matching volume for path
let volume = util::find_volume_for_path(&path, volumes.iter());
```

## Copy-on-Write Detection

The system detects filesystems supporting copy-on-write operations:

```rust
// Check if volume supports COW
if volume.supports_cow() {
    // Use fast COW copy operations
    perform_cow_copy(&src, &dst).await?;
} else {
    // Fall back to traditional copy
    perform_regular_copy(&src, &dst).await?;
}

// COW filesystems: APFS, Btrfs, ZFS, ReFS
```

## Performance Monitoring

### Speed Testing

```rust
// Test volume read/write speeds
let (read_speed, write_speed) = volume_manager
    .run_speed_test(&fingerprint).await?;

println!("Volume speeds: {}MB/s read, {}MB/s write", 
    read_speed, write_speed);
```

### Cache Management

```rust
// Clear volume path cache
volume_manager.clear_cache().await;

// Get cache statistics
let cache_stats = volume_manager.get_cache_stats().await;
```

## Configuration

### Detection Configuration

```rust
use spacedrive_core::volume::VolumeDetectionConfig;

let config = VolumeDetectionConfig {
    include_system: false,     // Skip system volumes
    include_virtual: false,    // Skip virtual filesystems
    refresh_interval_secs: 30, // Monitor every 30 seconds
};
```

### Volume Manager Setup

```rust
// Initialize volume manager with custom config
let volume_manager = VolumeManager::new(config, event_bus);
volume_manager.initialize().await?;

// Start background monitoring
volume_manager.start_monitoring().await;
```

## Error Handling

### Volume Errors

```rust
use spacedrive_core::volume::VolumeError;

match volume_manager.track_volume(&fingerprint, &ctx, None).await {
    Ok(()) => println!("Volume tracked successfully"),
    Err(VolumeError::NotFound(fp)) => {
        println!("Volume not found: {}", fp);
    }
    Err(VolumeError::Platform(msg)) => {
        println!("Platform error: {}", msg);
    }
    Err(VolumeError::InvalidData(msg)) => {
        println!("Invalid data: {}", msg);
    }
}
```

## Integration Examples

### File Operations Integration

```rust
// Choose optimal copy method based on volume
async fn smart_copy(src: &Path, dst: &Path, volume_manager: &VolumeManager) -> Result<()> {
    let src_volume = volume_manager.volume_for_path(src).await;
    let dst_volume = volume_manager.volume_for_path(dst).await;
    
    match (src_volume, dst_volume) {
        (Some(src_vol), Some(dst_vol)) if src_vol.fingerprint == dst_vol.fingerprint => {
            if src_vol.supports_cow() {
                // Same volume + COW support = instant copy
                perform_cow_copy(src, dst).await
            } else {
                // Same volume = fast move operation
                perform_move_copy(src, dst).await
            }
        }
        _ => {
            // Cross-volume = traditional copy
            perform_cross_volume_copy(src, dst).await
        }
    }
}
```

### Library Integration

```rust
// Track volumes when creating library locations
async fn add_location(
    path: PathBuf,
    library_ctx: &LibraryContext,
    volume_manager: &VolumeManager,
) -> Result<()> {
    // Find volume containing this path
    if let Some(volume) = volume_manager.volume_for_path(&path).await {
        // Suggest tracking the volume
        if !volume_manager.is_volume_tracked(&volume.fingerprint).await? {
            println!("Would you like to track volume '{}'?", volume.name);
            // User confirms...
            volume_manager.track_volume(
                &volume.fingerprint,
                library_ctx,
                None
            ).await?;
        }
    }
    
    // Create location...
}
```

## Best Practices

### Performance
- Use volume-aware operations when possible
- Cache volume lookups for frequently accessed paths
- Leverage COW capabilities for large file operations
- Monitor volume space before operations

### User Experience  
- Show volume tracking suggestions for new external drives
- Display volume capacity and utilization in UI
- Allow custom volume names and organization
- Provide volume performance metrics

### Reliability
- Handle volume disconnection gracefully
- Retry volume detection on errors
- Validate volume fingerprints before operations
- Monitor volume health and space warnings

## Future Enhancements

- **Database Integration** - Full persistence layer implementation
- **Cloud Volume Support** - Detect and manage cloud storage mounts
- **Volume Health Monitoring** - SMART data integration
- **Advanced Speed Testing** - Random I/O, seek times, queue depth testing
- **Volume Synchronization** - Sync volume metadata across devices
- **Volume Groups** - Logical grouping of related volumes```

## docs/whitepaper.md

```markdown
# Spacedrive V2 Whitepaper: A Guide for the Community

This document explains the purpose of the Spacedrive V2 whitepaper, what it covers, and how our community can use it to understand, contribute to, and build upon the future of personal data management.

---

## Why We Wrote This Whitepaper

Spacedrive V2 represents a complete architectural reimagining of the project, designed to fulfill the original vision on a more robust and scalable foundation. After a period of reflection on the challenges faced by the first version, we recognized the need for a foundational rewrite to address critical issues like the "Dual File System Problem" and fragmented networking.

This whitepaper was created to serve three primary purposes:

1.  **To Provide a Definitive Technical Blueprint**: It is the single source of truth for the Spacedrive V2 architecture, detailing the core concepts, design decisions, and innovations that power the new system.
2.  **To Re-engage Our Community**: We want to share our renewed vision and technical direction transparently, providing a clear path for developers, contributors, and users to rally behind.
3.  **To Guide Future Development**: The document serves as a roadmap and a set of guiding principles, ensuring that all future contributions align with the core architectural tenets of performance, privacy, and user control.

Ultimately, this paper is our commitment to building a paradigm shift in how humans interact with their digital assets in the AI era[cite: 18, 18].

---

## What This Whitepaper Covers

The whitepaper presents the complete architecture of Spacedrive V2, a local-first, AI-native Virtual Distributed File System (VDFS)[cite: 4]. It details the five foundational innovations that solve traditionally hard problems in distributed systems for a consumer-grade product[cite: 27].

Key architectural pillars covered in detail include:

- **A Virtual Distributed File System (VDFS)**: A unified, virtual layer that provides a single view of all your data across every device and cloud, while the files themselves stay in their original locations[cite: 4, 28]. This is made possible by a universal addressing system called **`SdPath`**[cite: 161].
- **An AI-Native Architecture**: The system is designed from the ground up for intelligent management, enabling natural language commands ("find my tax documents from last year") and proactive assistance from a data guardian that respects user privacy[cite: 7, 33, 360].
- **A Transactional Action System**: All file operations are treated as transactions that can be previewed before they are committed, preventing conflicts and guaranteeing completion even across offline devices[cite: 32, 275].
- **Domain-Separated Library Sync**: A novel synchronization method that maintains consistency across devices without the complexity of distributed consensus algorithms like CRDTs[cite: 31, 326].
- **The Content Identity System**: A content-addressable foundation that provides intelligent, cross-device deduplication while also powering a "Data Guardian" feature that monitors data redundancy to protect against loss[cite: 30, 188].
- **A Modern, Performant Implementation**: The entire core is implemented in **Rust** on a modern, asynchronous technology stack designed for enterprise-grade capabilities on consumer hardware[cite: 34].

---

## How to Use This Document

This whitepaper is more than a technical document; it's an invitation to our community to help build the future of file management.

#### **For Developers & Contributors:**

- **Understand the Vision**: Before diving into the code, read the whitepaper to understand the "why" behind the architecture. It provides the context for our design choices and the problems we are solving.
- **Guide Your Contributions**: Use this document alongside the **Roadmap** as a guide. The architecture detailed here is the blueprint for all new features. Whether you're fixing a bug or building a new operation, it should align with these core principles.
- **Build with Confidence**: The paper explains the core abstractions like `SdPath`, the `ActionManager`, and the `JobManager`. Understanding these will help you build new features that integrate seamlessly and reliably with the rest of the system.

#### **For System Architects & Designers:**

- **Review and Feedback**: We welcome feedback on our architectural decisions. The whitepaper details our solutions to complex problems like synchronization and distributed file operations. If you have insights or see potential improvements, we encourage you to start a discussion.
- **Propose Enhancements**: The document provides the context for proposing new, large-scale features or optimizations, such as the planned migration to a Closure Table for hierarchical queries[cite: 512].

#### **For the Entire Community:**

- **The Source of Truth**: When you have questions about how Spacedrive works under the hood, this document is the canonical source. It explains how we handle your data, ensure privacy, and deliver powerful features in a local-first environment.
- **A Foundation for Discussion**: Use this paper as a common ground for discussions about Spacedrive's future. It ensures everyone is working from the same set of assumptions about the technology.

We are incredibly excited about the foundation we've built with Spacedrive V2 and believe it positions us to solve the fundamental problems of data fragmentation and privacy for the modern era[cite: 19]. We invite you to read, discuss, and build with us.
```

