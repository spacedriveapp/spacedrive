# Combined Rust Files

## tests/copy_action_test.rs

```rust
//! Simple integration test for the file copy action dispatch
//!
//! This test verifies that the action can be properly dispatched without
//! requiring a full database setup or job execution.

use sd_core_new::{
	infrastructure::actions::{manager::ActionManager, Action},
	operations::files::{
		copy::{
			action::FileCopyAction,
			job::{CopyOptions, MoveMode},
		},
		input::CopyMethod,
	},
};
use std::path::PathBuf;
use tempfile::TempDir;
use tokio::fs;
use uuid::Uuid;

/// Helper to create test files with content
async fn create_test_file(path: &std::path::Path, content: &str) -> Result<(), std::io::Error> {
	if let Some(parent) = path.parent() {
		fs::create_dir_all(parent).await?;
	}
	fs::write(path, content).await
}

#[tokio::test]
async fn test_copy_action_construction() {
	// Setup test environment
	let temp_dir = TempDir::new().unwrap();
	let test_root = temp_dir.path();

	// Create source and destination directories
	let source_dir = test_root.join("source");
	let dest_dir = test_root.join("destination");
	fs::create_dir_all(&source_dir).await.unwrap();
	fs::create_dir_all(&dest_dir).await.unwrap();

	// Create test files
	let source_file1 = source_dir.join("test1.txt");
	let source_file2 = source_dir.join("test2.txt");

	create_test_file(&source_file1, "Hello, World! This is test file 1.")
		.await
		.unwrap();
	create_test_file(&source_file2, "This is the content of test file 2.")
		.await
		.unwrap();

	// Test 1: Basic copy action construction
	let copy_action = FileCopyAction {
		sources: vec![source_file1.clone(), source_file2.clone()],
		destination: dest_dir.clone(),
		options: CopyOptions {
			overwrite: false,
			copy_method: CopyMethod::Auto,
			verify_checksum: true,
			preserve_timestamps: true,
			delete_after_copy: false,
			move_mode: None,
		},
	};

	// Create Action enum
	let library_id = Uuid::new_v4();
	let action = Action::FileCopy {
		library_id,
		action: copy_action,
	};

	// Verify action properties
	assert_eq!(action.library_id(), Some(library_id));
	assert_eq!(action.kind(), "file.copy");
	assert!(action.description().contains("Copy 2 file(s)"));

	let targets = action.targets_summary();
	assert!(targets.get("sources").is_some());
	assert!(targets.get("destination").is_some());

	println!("âœ… Copy action construction test passed!");
}

#[tokio::test]
async fn test_move_action_construction() {
	let temp_dir = TempDir::new().unwrap();
	let source_file = temp_dir.path().join("source.txt");
	let dest_file = temp_dir.path().join("dest.txt");

	create_test_file(&source_file, "Move me!").await.unwrap();

	// Test move action (copy with delete_after_copy)
	let move_action = FileCopyAction {
		sources: vec![source_file.clone()],
		destination: dest_file.clone(),
		options: CopyOptions {
			copy_method: CopyMethod::Auto,
			overwrite: false,
			verify_checksum: false,
			preserve_timestamps: true,
			delete_after_copy: true, // This makes it a move operation
			move_mode: Some(MoveMode::Move),
		},
	};

	let library_id = Uuid::new_v4();
	let action = Action::FileCopy {
		library_id,
		action: move_action,
	};

	// Verify action properties
	assert_eq!(action.library_id(), Some(library_id));
	assert_eq!(action.kind(), "file.copy");
	assert!(action.description().contains("Copy 1 file(s)"));

	println!("âœ… Move action construction test passed!");
}

#[tokio::test]
async fn test_action_validation_logic() {
	// Test empty sources validation
	let copy_action = FileCopyAction {
		sources: vec![], // Empty sources should be invalid
		destination: PathBuf::from("/tmp/dest"),
		options: CopyOptions::default(),
	};

	let library_id = Uuid::new_v4();
	let action = Action::FileCopy {
		library_id,
		action: copy_action,
	};

	// For now, just verify the action is constructed correctly
	// The actual validation happens in the ActionHandler
	assert_eq!(action.library_id(), Some(library_id));
	assert_eq!(action.kind(), "file.copy");

	println!("âœ… Action validation logic test passed!");
}

#[test]
fn test_copy_options_defaults() {
	let options = CopyOptions::default();

	assert!(!options.overwrite);
	assert!(!options.verify_checksum);
	assert!(options.preserve_timestamps);
	assert!(!options.delete_after_copy);
	assert!(options.move_mode.is_none());

	println!("âœ… Copy options defaults test passed!");
}

#[test]
fn test_move_mode_variants() {
	// Test that all move modes can be constructed
	let _move_mode = MoveMode::Move;
	let _rename_mode = MoveMode::Rename;
	let _cut_mode = MoveMode::Cut;

	println!("âœ… Move mode variants test passed!");
}
```

## tests/device_persistence_test.rs

```rust
//! Test device persistence and automatic reconnection after core restart
//!
//! This test verifies that:
//! 1. Devices can pair successfully
//! 2. Pairing information is persisted to disk
//! 3. After both devices restart, they automatically reconnect
//! 4. The reconnection happens without manual intervention

use sd_core_new::test_framework::CargoTestRunner;
use sd_core_new::Core;
use std::env;
use std::path::PathBuf;
use std::time::Duration;
use tokio::time::timeout;

/// Alice's device persistence scenario - handles both initial pairing and restart
#[tokio::test]
#[ignore] // Only run when explicitly called via subprocess
async fn alice_persistence_scenario() {
	let role = env::var("TEST_ROLE").unwrap_or_default();
	if !role.starts_with("alice") {
		return;
	}

	let data_dir = PathBuf::from("/tmp/spacedrive-persistence-test/alice");
	let device_name = "Alice's Persistent Device";

	// Set test directory for file-based discovery
	env::set_var("SPACEDRIVE_TEST_DIR", "/tmp/spacedrive-persistence-test");

	// Determine which phase we're in
	let is_restart = role == "alice_restart";

	if is_restart {
		println!("ðŸ”„ Alice: RESTART PHASE - Testing automatic reconnection");
		println!("ðŸ“ Alice: Data dir: {:?}", data_dir);

		// Initialize Core - this should load persisted devices
		println!("ðŸ”§ Alice: Initializing Core after restart...");
		let mut core = timeout(Duration::from_secs(10), Core::new_with_config(data_dir))
			.await
			.unwrap()
			.unwrap();
		println!("âœ… Alice: Core initialized successfully");

		// Device name should be persisted
		let device_config = core.device.config().unwrap();
		let current_name = device_config.name;
		println!("ðŸ·ï¸ Alice: Device name after restart: {}", current_name);
		assert_eq!(current_name, device_name, "Device name not persisted");

		// Initialize networking - this should trigger auto-reconnection
		println!("ðŸŒ Alice: Initializing networking (should auto-reconnect)...");
		timeout(Duration::from_secs(10), core.init_networking())
			.await
			.unwrap()
			.unwrap();

		// Give time for auto-reconnection to happen
		tokio::time::sleep(Duration::from_secs(5)).await;
		println!("âœ… Alice: Networking initialized, checking for auto-reconnection...");

		// Check if Bob reconnected automatically
		println!("â³ Alice: Waiting for automatic reconnection to Bob...");
		let mut attempts = 0;
		let max_attempts = 30; // 30 seconds

		loop {
			tokio::time::sleep(Duration::from_secs(1)).await;

			let connected_devices = core.get_connected_devices().await.unwrap();
			if !connected_devices.is_empty() {
				println!("ðŸŽ‰ Alice: Auto-reconnection successful!");
				println!(
					"âœ… Alice: Connected {} devices after restart",
					connected_devices.len()
				);

				// Verify it's Bob
				let device_info = core.get_connected_devices_info().await.unwrap();
				let bob_found = device_info.iter().any(|d| d.device_name.contains("Bob"));
				assert!(
					bob_found,
					"Bob not found in connected devices after restart"
				);

				for device in &device_info {
					println!(
						"ðŸ“± Alice sees after restart: {} (ID: {})",
						device.device_name, device.device_id
					);
				}

				// Write success marker
				std::fs::write(
					"/tmp/spacedrive-persistence-test/alice_restart_success.txt",
					"success",
				)
				.unwrap();
				println!("âœ… Alice: Device persistence test completed successfully");
				break;
			}

			attempts += 1;
			if attempts >= max_attempts {
				panic!("Alice: Auto-reconnection timeout - Bob did not reconnect automatically");
			}

			if attempts % 5 == 0 {
				println!(
					"ðŸ” Alice: Auto-reconnection check {} - waiting for Bob",
					attempts / 5
				);
			}
		}
	} else {
		// Initial pairing phase
		println!("ðŸŸ¦ Alice: INITIAL PHASE - Starting pairing");
		println!("ðŸ“ Alice: Data dir: {:?}", data_dir);

		// Initialize Core
		println!("ðŸ”§ Alice: Initializing Core...");
		let mut core = timeout(Duration::from_secs(10), Core::new_with_config(data_dir))
			.await
			.unwrap()
			.unwrap();
		println!("âœ… Alice: Core initialized successfully");

		// Set device name
		println!("ðŸ·ï¸ Alice: Setting device name...");
		core.device.set_name(device_name.to_string()).unwrap();

		// Initialize networking
		println!("ðŸŒ Alice: Initializing networking...");
		timeout(Duration::from_secs(10), core.init_networking())
			.await
			.unwrap()
			.unwrap();

		tokio::time::sleep(Duration::from_secs(3)).await;
		println!("âœ… Alice: Networking initialized successfully");

		// Start pairing as initiator
		println!("ðŸ”‘ Alice: Starting pairing as initiator...");
		let (pairing_code, expires_in) = if let Some(networking) = core.networking() {
			timeout(
				Duration::from_secs(15),
				networking.start_pairing_as_initiator(),
			)
			.await
			.unwrap()
			.unwrap()
		} else {
			panic!("Networking not initialized");
		};

		println!(
			"âœ… Alice: Pairing code generated (expires in {}s)",
			expires_in
		);

		// Write pairing code for Bob
		std::fs::create_dir_all("/tmp/spacedrive-persistence-test").unwrap();
		std::fs::write(
			"/tmp/spacedrive-persistence-test/pairing_code.txt",
			&pairing_code,
		)
		.unwrap();

		// Wait for Bob to connect
		println!("â³ Alice: Waiting for Bob to connect...");
		let mut attempts = 0;
		let max_attempts = 45;

		loop {
			tokio::time::sleep(Duration::from_secs(1)).await;

			let connected_devices = core.get_connected_devices().await.unwrap();
			if !connected_devices.is_empty() {
				println!("ðŸŽ‰ Alice: Initial pairing completed!");
				println!("âœ… Alice: Connected {} devices", connected_devices.len());

				// Verify devices are properly persisted
				if let Some(networking) = core.networking() {
					let registry = networking.device_registry();
					let paired_devices = registry.read().await.get_paired_devices();
					assert!(
						!paired_devices.is_empty(),
						"No paired devices found in registry"
					);
					println!(
						"âœ… Alice: {} devices persisted to registry",
						paired_devices.len()
					);
				}

				// Write success marker
				std::fs::write(
					"/tmp/spacedrive-persistence-test/alice_paired.txt",
					"success",
				)
				.unwrap();

				// Keep running for a bit to ensure persistence completes
				tokio::time::sleep(Duration::from_secs(3)).await;
				break;
			}

			attempts += 1;
			if attempts >= max_attempts {
				panic!("Alice: Initial pairing timeout");
			}
		}

		// Gracefully shutdown to ensure persistence
		println!("ðŸ›‘ Alice: Shutting down gracefully to ensure persistence...");
		drop(core);
		tokio::time::sleep(Duration::from_secs(2)).await;
		println!("âœ… Alice: Initial phase completed");
	}
}

/// Bob's device persistence scenario - handles both initial pairing and restart
#[tokio::test]
#[ignore] // Only run when explicitly called via subprocess
async fn bob_persistence_scenario() {
	let role = env::var("TEST_ROLE").unwrap_or_default();
	if !role.starts_with("bob") {
		return;
	}

	let data_dir = PathBuf::from("/tmp/spacedrive-persistence-test/bob");
	let device_name = "Bob's Persistent Device";

	// Set test directory for file-based discovery
	env::set_var("SPACEDRIVE_TEST_DIR", "/tmp/spacedrive-persistence-test");

	// Determine which phase we're in
	let is_restart = role == "bob_restart";

	if is_restart {
		println!("ðŸ”„ Bob: RESTART PHASE - Testing automatic reconnection");
		println!("ðŸ“ Bob: Data dir: {:?}", data_dir);

		// Initialize Core - this should load persisted devices
		println!("ðŸ”§ Bob: Initializing Core after restart...");
		let mut core = timeout(Duration::from_secs(10), Core::new_with_config(data_dir))
			.await
			.unwrap()
			.unwrap();
		println!("âœ… Bob: Core initialized successfully");

		// Device name should be persisted
		let current_name = core.device.config().unwrap().name;
		println!("ðŸ·ï¸ Bob: Device name after restart: {}", current_name);
		assert_eq!(current_name, device_name, "Device name not persisted");

		// Initialize networking - this should trigger auto-reconnection
		println!("ðŸŒ Bob: Initializing networking (should auto-reconnect)...");
		timeout(Duration::from_secs(10), core.init_networking())
			.await
			.unwrap()
			.unwrap();

		// Give time for auto-reconnection to happen
		tokio::time::sleep(Duration::from_secs(5)).await;
		println!("âœ… Bob: Networking initialized, checking for auto-reconnection...");

		// Check if Alice reconnected automatically
		println!("â³ Bob: Waiting for automatic reconnection to Alice...");
		let mut attempts = 0;
		let max_attempts = 30; // 30 seconds

		loop {
			tokio::time::sleep(Duration::from_secs(1)).await;

			let connected_devices = core.get_connected_devices().await.unwrap();
			if !connected_devices.is_empty() {
				println!("ðŸŽ‰ Bob: Auto-reconnection successful!");
				println!(
					"âœ… Bob: Connected {} devices after restart",
					connected_devices.len()
				);

				// Verify it's Alice
				let device_info = core.get_connected_devices_info().await.unwrap();
				let alice_found = device_info.iter().any(|d| d.device_name.contains("Alice"));
				assert!(
					alice_found,
					"Alice not found in connected devices after restart"
				);

				for device in &device_info {
					println!(
						"ðŸ“± Bob sees after restart: {} (ID: {})",
						device.device_name, device.device_id
					);
				}

				// Write success marker
				std::fs::write(
					"/tmp/spacedrive-persistence-test/bob_restart_success.txt",
					"success",
				)
				.unwrap();
				println!("âœ… Bob: Device persistence test completed successfully");
				break;
			}

			attempts += 1;
			if attempts >= max_attempts {
				panic!("Bob: Auto-reconnection timeout - Alice did not reconnect automatically");
			}

			if attempts % 5 == 0 {
				println!(
					"ðŸ” Bob: Auto-reconnection check {} - waiting for Alice",
					attempts / 5
				);
			}
		}
	} else {
		// Initial pairing phase
		println!("ðŸŸ¦ Bob: INITIAL PHASE - Starting pairing");
		println!("ðŸ“ Bob: Data dir: {:?}", data_dir);

		// Initialize Core
		println!("ðŸ”§ Bob: Initializing Core...");
		let mut core = timeout(Duration::from_secs(10), Core::new_with_config(data_dir))
			.await
			.unwrap()
			.unwrap();
		println!("âœ… Bob: Core initialized successfully");

		// Set device name
		println!("ðŸ·ï¸ Bob: Setting device name...");
		core.device.set_name(device_name.to_string()).unwrap();

		// Initialize networking
		println!("ðŸŒ Bob: Initializing networking...");
		timeout(Duration::from_secs(10), core.init_networking())
			.await
			.unwrap()
			.unwrap();

		tokio::time::sleep(Duration::from_secs(3)).await;
		println!("âœ… Bob: Networking initialized successfully");

		// Wait for pairing code from Alice
		println!("ðŸ” Bob: Looking for pairing code...");
		let pairing_code = loop {
			if let Ok(code) =
				std::fs::read_to_string("/tmp/spacedrive-persistence-test/pairing_code.txt")
			{
				break code.trim().to_string();
			}
			tokio::time::sleep(Duration::from_millis(500)).await;
		};
		println!("ðŸ“‹ Bob: Found pairing code");

		// Join pairing session
		println!("ðŸ¤ Bob: Joining pairing session...");
		if let Some(networking) = core.networking() {
			timeout(
				Duration::from_secs(15),
				networking.start_pairing_as_joiner(&pairing_code),
			)
			.await
			.unwrap()
			.unwrap();
		} else {
			panic!("Networking not initialized");
		}

		// Wait for connection
		println!("â³ Bob: Waiting for connection to Alice...");
		let mut attempts = 0;
		let max_attempts = 30;

		loop {
			tokio::time::sleep(Duration::from_secs(1)).await;

			let connected_devices = core.get_connected_devices().await.unwrap();
			if !connected_devices.is_empty() {
				println!("ðŸŽ‰ Bob: Initial pairing completed!");
				println!("âœ… Bob: Connected {} devices", connected_devices.len());

				// Verify devices are properly persisted
				if let Some(networking) = core.networking() {
					let registry = networking.device_registry();
					let paired_devices = registry.read().await.get_paired_devices();
					assert!(
						!paired_devices.is_empty(),
						"No paired devices found in registry"
					);
					println!(
						"âœ… Bob: {} devices persisted to registry",
						paired_devices.len()
					);
				}

				// Write success marker
				std::fs::write("/tmp/spacedrive-persistence-test/bob_paired.txt", "success")
					.unwrap();

				// Keep running for a bit to ensure persistence completes
				tokio::time::sleep(Duration::from_secs(3)).await;
				break;
			}

			attempts += 1;
			if attempts >= max_attempts {
				panic!("Bob: Initial pairing timeout");
			}
		}

		// Gracefully shutdown to ensure persistence
		println!("ðŸ›‘ Bob: Shutting down gracefully to ensure persistence...");
		drop(core);
		tokio::time::sleep(Duration::from_secs(2)).await;
		println!("âœ… Bob: Initial phase completed");
	}
}

/// Main test orchestrator - tests device persistence and auto-reconnection
#[tokio::test]
async fn test_device_persistence() {
	println!("ðŸ§ª Testing device persistence and automatic reconnection");

	// Clean up any previous test artifacts
	let _ = std::fs::remove_dir_all("/tmp/spacedrive-persistence-test");
	std::fs::create_dir_all("/tmp/spacedrive-persistence-test").unwrap();

	let mut runner = CargoTestRunner::for_test_file("test_device_persistence")
		.with_timeout(Duration::from_secs(240)) // Longer timeout for restart test
		.add_subprocess("alice", "alice_persistence_scenario")
		.add_subprocess("alice_restart", "alice_persistence_scenario")
		.add_subprocess("bob", "bob_persistence_scenario")
		.add_subprocess("bob_restart", "bob_persistence_scenario");

	// Phase 1: Initial pairing
	println!("\\nðŸ“ PHASE 1: Initial pairing");
	println!("ðŸš€ Starting Alice for initial pairing...");
	runner
		.spawn_single_process("alice")
		.await
		.expect("Failed to spawn Alice");

	// Wait for Alice to initialize
	tokio::time::sleep(Duration::from_secs(8)).await;

	println!("ðŸš€ Starting Bob for initial pairing...");
	runner
		.spawn_single_process("bob")
		.await
		.expect("Failed to spawn Bob");

	// Wait for initial pairing to complete
	let pairing_result = runner
		.wait_for_success(|_| {
			let alice_paired =
				std::fs::read_to_string("/tmp/spacedrive-persistence-test/alice_paired.txt")
					.map(|content| content.trim() == "success")
					.unwrap_or(false);
			let bob_paired =
				std::fs::read_to_string("/tmp/spacedrive-persistence-test/bob_paired.txt")
					.map(|content| content.trim() == "success")
					.unwrap_or(false);
			alice_paired && bob_paired
		})
		.await;

	if pairing_result.is_err() {
		println!("âŒ Initial pairing failed");
		for (name, output) in runner.get_all_outputs() {
			println!("\\n{} output:\\n{}", name, output);
		}
		panic!("Initial pairing failed - cannot proceed with persistence test");
	}

	println!("âœ… Phase 1 complete: Devices paired successfully");

	// Wait a bit to ensure processes have fully shut down
	tokio::time::sleep(Duration::from_secs(5)).await;

	// Phase 2: Restart both devices and verify auto-reconnection
	println!("\\nðŸ“ PHASE 2: Testing automatic reconnection after restart");

	// Clear the pairing code to ensure devices aren't re-pairing
	let _ = std::fs::remove_file("/tmp/spacedrive-persistence-test/pairing_code.txt");

	println!("ðŸ”„ Restarting Alice...");
	runner
		.spawn_single_process("alice_restart")
		.await
		.expect("Failed to spawn Alice restart");

	// Give Alice time to start up
	tokio::time::sleep(Duration::from_secs(5)).await;

	println!("ðŸ”„ Restarting Bob...");
	runner
		.spawn_single_process("bob_restart")
		.await
		.expect("Failed to spawn Bob restart");

	// Wait for auto-reconnection
	let reconnection_result = runner
		.wait_for_success(|_| {
			let alice_reconnected = std::fs::read_to_string(
				"/tmp/spacedrive-persistence-test/alice_restart_success.txt",
			)
			.map(|content| content.trim() == "success")
			.unwrap_or(false);
			let bob_reconnected =
				std::fs::read_to_string("/tmp/spacedrive-persistence-test/bob_restart_success.txt")
					.map(|content| content.trim() == "success")
					.unwrap_or(false);
			alice_reconnected && bob_reconnected
		})
		.await;

	match reconnection_result {
		Ok(_) => {
			println!("\\nðŸŽ‰ Device persistence test successful!");
			println!("âœ… Devices automatically reconnected after restart");
		}
		Err(e) => {
			println!("\\nâŒ Device persistence test failed: {}", e);
			for (name, output) in runner.get_all_outputs() {
				println!("\\n{} output:\\n{}", name, output);
			}
			panic!("Devices did not automatically reconnect after restart");
		}
	}
}
```

## tests/volume_tracking_with_test_volumes.rs

```rust
//! Enhanced volume tracking tests using real test volumes
//!
//! These tests create actual temporary volumes to test volume tracking
//! with different filesystems, capacities, and mount scenarios.

mod helpers;
use helpers::test_volumes::{TestFileSystem, TestVolumeBuilder, TestVolumeManager};

use sd_core_new::{
    Core,
    infrastructure::{
        actions::{Action, output::ActionOutput},
    },
    operations::volumes::{
        track::action::VolumeTrackAction,
        untrack::action::VolumeUntrackAction,
        speed_test::action::VolumeSpeedTestAction,
    },
};
use std::sync::Arc;
use tempfile::tempdir;
use tracing::{info, warn};

/// Check if we have the required privileges to run volume tests
async fn check_test_privileges() -> bool {
    let manager = TestVolumeManager::new();
    manager.check_privileges().await.is_ok()
}

#[tokio::test]
async fn test_real_volume_tracking_lifecycle() {
    let _ = tracing_subscriber::fmt::try_init();
    
    if !check_test_privileges().await {
        warn!("Skipping test - requires elevated privileges");
        return;
    }
    
    // Setup test environment
    let data_dir = tempdir().unwrap();
    let data_path = data_dir.path().to_path_buf();
    
    let core = Arc::new(
        Core::new_with_config(data_path.clone())
            .await
            .expect("Failed to create core"),
    );
    
    // Create test library
    let library = core
        .libraries
        .create_library(
            "Real Volume Test",
            Some(data_path.join("libraries").join("real-volume-test")),
            core.context.clone(),
        )
        .await
        .expect("Failed to create library");
    
    // Create a test volume
    let test_volume = TestVolumeBuilder::new("TestTrackingVol")
        .size_mb(50)
        .filesystem(TestFileSystem::Default)
        .build()
        .await
        .expect("Failed to create test volume");
    
    info!("Created test volume at {:?}", test_volume.path());
    
    // Refresh volumes to detect our new volume
    core.volumes
        .refresh_volumes()
        .await
        .expect("Failed to refresh volumes");
    
    // Find our test volume
    let all_volumes = core.volumes.get_all_volumes().await;
    let our_volume = all_volumes
        .iter()
        .find(|v| v.mount_point == test_volume.mount_point)
        .expect("Test volume should be detected")
        .clone();
    
    info!("Found test volume: {} ({})", our_volume.name, our_volume.fingerprint);
    
    let fingerprint = our_volume.fingerprint.clone();
    let action_manager = core.context.get_action_manager().await
        .expect("Action manager should be initialized");
    
    // Track the volume
    let track_action = Action::VolumeTrack {
        action: VolumeTrackAction {
            fingerprint: fingerprint.clone(),
            library_id: library.id(),
            name: Some("My Custom Test Volume".to_string()),
        },
    };
    
    let result = action_manager.dispatch(track_action).await;
    assert!(result.is_ok(), "Failed to track volume: {:?}", result);
    
    // Verify tracking
    let is_tracked = core.volumes
        .is_volume_tracked(&library, &fingerprint)
        .await
        .expect("Failed to check tracking status");
    assert!(is_tracked, "Volume should be tracked");
    
    // Get tracked volume info
    let tracked_volumes = core.volumes
        .get_tracked_volumes(&library)
        .await
        .expect("Failed to get tracked volumes");
    
    let tracked = tracked_volumes
        .iter()
        .find(|v| v.fingerprint == fingerprint)
        .expect("Our volume should be in tracked list");
    
    assert_eq!(tracked.display_name, Some("My Custom Test Volume".to_string()));
    assert!(tracked.is_online);
    assert_eq!(tracked.total_capacity, Some(50 * 1024 * 1024)); // 50MB
    
    // Untrack the volume
    let untrack_action = Action::VolumeUntrack {
        action: VolumeUntrackAction {
            fingerprint: fingerprint.clone(),
            library_id: library.id(),
        },
    };
    
    let result = action_manager.dispatch(untrack_action).await;
    assert!(result.is_ok(), "Failed to untrack volume");
    
    // Volume cleanup happens automatically via Drop
    info!("Real volume tracking lifecycle test completed");
}

#[tokio::test]
async fn test_different_filesystems() {
    let _ = tracing_subscriber::fmt::try_init();
    
    if !check_test_privileges().await {
        warn!("Skipping test - requires elevated privileges");
        return;
    }
    
    let data_dir = tempdir().unwrap();
    let data_path = data_dir.path().to_path_buf();
    
    let core = Arc::new(
        Core::new_with_config(data_path.clone())
            .await
            .expect("Failed to create core"),
    );
    
    let library = core
        .libraries
        .create_library(
            "Filesystem Test",
            Some(data_path.join("libraries").join("fs-test")),
            core.context.clone(),
        )
        .await
        .expect("Failed to create library");
    
    // Test different filesystems based on platform
    #[cfg(target_os = "macos")]
    let filesystems = vec![
        (TestFileSystem::Apfs, "APFS"),
        (TestFileSystem::HfsPlus, "HFS+"),
        (TestFileSystem::ExFat, "ExFAT"),
    ];
    
    #[cfg(target_os = "windows")]
    let filesystems = vec![
        (TestFileSystem::Ntfs, "NTFS"),
        (TestFileSystem::Fat32, "FAT32"),
        (TestFileSystem::ExFat, "ExFAT"),
    ];
    
    #[cfg(target_os = "linux")]
    let filesystems = vec![
        (TestFileSystem::Ext4, "ext4"),
        (TestFileSystem::Fat32, "FAT32"),
    ];
    
    for (fs_type, fs_name) in filesystems {
        info!("Testing {} filesystem", fs_name);
        
        let volume_name = format!("Test{}", fs_name.replace("+", "Plus"));
        let test_volume = match TestVolumeBuilder::new(&volume_name)
            .size_mb(30)
            .filesystem(fs_type)
            .build()
            .await
        {
            Ok(vol) => vol,
            Err(e) => {
                warn!("Failed to create {} volume: {}", fs_name, e);
                continue;
            }
        };
        
        // Refresh to detect the volume
        core.volumes.refresh_volumes().await.ok();
        
        let all_volumes = core.volumes.get_all_volumes().await;
        if let Some(volume) = all_volumes
            .iter()
            .find(|v| v.mount_point == test_volume.mount_point)
        {
            info!("Detected {} volume: {}", fs_name, volume.name);
            
            // Track the volume
            core.volumes
                .track_volume(&library, &volume.fingerprint, Some(format!("{} Test", fs_name)))
                .await
                .expect("Failed to track volume");
            
            // Verify filesystem info
            let tracked = core.volumes
                .get_tracked_volumes(&library)
                .await
                .expect("Failed to get tracked volumes")
                .into_iter()
                .find(|v| v.fingerprint == volume.fingerprint)
                .expect("Volume should be tracked");
            
            assert_eq!(tracked.file_system, Some(volume.file_system.to_string()));
            info!("Successfully tracked {} volume", fs_name);
            
            // Untrack before next iteration
            core.volumes
                .untrack_volume(&library, &volume.fingerprint)
                .await
                .ok();
        }
    }
    
    info!("Filesystem test completed");
}

#[tokio::test]
async fn test_volume_capacity_scenarios() {
    let _ = tracing_subscriber::fmt::try_init();
    
    if !check_test_privileges().await {
        warn!("Skipping test - requires elevated privileges");
        return;
    }
    
    let data_dir = tempdir().unwrap();
    let data_path = data_dir.path().to_path_buf();
    
    let core = Arc::new(
        Core::new_with_config(data_path.clone())
            .await
            .expect("Failed to create core"),
    );
    
    let library = core
        .libraries
        .create_library(
            "Capacity Test",
            Some(data_path.join("libraries").join("capacity-test")),
            core.context.clone(),
        )
        .await
        .expect("Failed to create library");
    
    // Create volumes with different sizes
    let test_cases = vec![
        ("TinyVol", 10),    // 10 MB
        ("SmallVol", 50),   // 50 MB
        ("MediumVol", 200), // 200 MB
    ];
    
    for (name, size_mb) in test_cases {
        info!("Testing {} MB volume", size_mb);
        
        let test_volume = match TestVolumeBuilder::new(name)
            .size_mb(size_mb)
            .build()
            .await
        {
            Ok(vol) => vol,
            Err(e) => {
                warn!("Failed to create {} MB volume: {}", size_mb, e);
                continue;
            }
        };
        
        // Refresh and find volume
        core.volumes.refresh_volumes().await.ok();
        
        let all_volumes = core.volumes.get_all_volumes().await;
        if let Some(volume) = all_volumes
            .iter()
            .find(|v| v.mount_point == test_volume.mount_point)
        {
            // Track the volume
            core.volumes
                .track_volume(&library, &volume.fingerprint, Some(name.to_string()))
                .await
                .expect("Failed to track volume");
            
            // Verify capacity
            let tracked = core.volumes
                .get_tracked_volumes(&library)
                .await
                .expect("Failed to get tracked volumes")
                .into_iter()
                .find(|v| v.fingerprint == volume.fingerprint)
                .expect("Volume should be tracked");
            
            assert_eq!(
                tracked.total_capacity,
                Some((size_mb as u64) * 1024 * 1024),
                "Volume capacity should match"
            );
            
            // Test speed on different sized volumes
            let action_manager = core.context.get_action_manager().await.unwrap();
            let speed_action = Action::VolumeSpeedTest {
                action: VolumeSpeedTestAction {
                    fingerprint: volume.fingerprint.clone(),
                },
            };
            
            match action_manager.dispatch(speed_action).await {
                Ok(ActionOutput::VolumeSpeedTested { read_speed_mbps, write_speed_mbps, .. }) => {
                    info!("{}: Read {:?} MB/s, Write {:?} MB/s", 
                          name, read_speed_mbps, write_speed_mbps);
                }
                _ => {
                    warn!("Speed test failed for {}", name);
                }
            }
            
            // Cleanup
            core.volumes
                .untrack_volume(&library, &volume.fingerprint)
                .await
                .ok();
        }
    }
    
    info!("Capacity test completed");
}

#[tokio::test]
async fn test_ram_disk_performance() {
    let _ = tracing_subscriber::fmt::try_init();
    
    if !check_test_privileges().await {
        warn!("Skipping test - requires elevated privileges");
        return;
    }
    
    let data_dir = tempdir().unwrap();
    let data_path = data_dir.path().to_path_buf();
    
    let core = Arc::new(
        Core::new_with_config(data_path.clone())
            .await
            .expect("Failed to create core"),
    );
    
    let library = core
        .libraries
        .create_library(
            "RAM Disk Test",
            Some(data_path.join("libraries").join("ramdisk-test")),
            core.context.clone(),
        )
        .await
        .expect("Failed to create library");
    
    // Create RAM disk for performance testing
    let ram_volume = match TestVolumeBuilder::new("RAMDisk")
        .size_mb(100)
        .use_ram_disk()
        .build()
        .await
    {
        Ok(vol) => vol,
        Err(e) => {
            warn!("Failed to create RAM disk: {} - skipping test", e);
            return;
        }
    };
    
    info!("Created RAM disk at {:?}", ram_volume.path());
    
    // Refresh and find the RAM disk
    core.volumes.refresh_volumes().await.ok();
    
    let all_volumes = core.volumes.get_all_volumes().await;
    if let Some(volume) = all_volumes
        .iter()
        .find(|v| v.mount_point == ram_volume.mount_point)
    {
        // Track the RAM disk
        core.volumes
            .track_volume(&library, &volume.fingerprint, Some("Fast RAM Disk".to_string()))
            .await
            .expect("Failed to track RAM disk");
        
        // Run speed test - should be very fast
        let action_manager = core.context.get_action_manager().await.unwrap();
        let speed_action = Action::VolumeSpeedTest {
            action: VolumeSpeedTestAction {
                fingerprint: volume.fingerprint.clone(),
            },
        };
        
        match action_manager.dispatch(speed_action).await {
            Ok(ActionOutput::VolumeSpeedTested { read_speed_mbps, write_speed_mbps, .. }) => {
                info!("RAM Disk speeds - Read: {:?} MB/s, Write: {:?} MB/s", 
                      read_speed_mbps, write_speed_mbps);
                
                // RAM disks should be very fast
                if let (Some(read), Some(write)) = (read_speed_mbps, write_speed_mbps) {
                    assert!(read > 100, "RAM disk read speed should be > 100 MB/s");
                    assert!(write > 100, "RAM disk write speed should be > 100 MB/s");
                }
            }
            _ => {
                warn!("Speed test failed for RAM disk");
            }
        }
    }
    
    info!("RAM disk performance test completed");
}

#[tokio::test]
async fn test_volume_mount_unmount_tracking() {
    let _ = tracing_subscriber::fmt::try_init();
    
    if !check_test_privileges().await {
        warn!("Skipping test - requires elevated privileges");
        return;
    }
    
    let data_dir = tempdir().unwrap();
    let data_path = data_dir.path().to_path_buf();
    
    let core = Arc::new(
        Core::new_with_config(data_path.clone())
            .await
            .expect("Failed to create core"),
    );
    
    let library = core
        .libraries
        .create_library(
            "Mount Test",
            Some(data_path.join("libraries").join("mount-test")),
            core.context.clone(),
        )
        .await
        .expect("Failed to create library");
    
    // Create a test volume that we'll unmount and remount
    let manager = TestVolumeManager::new();
    let config = helpers::test_volumes::TestVolumeConfig {
        name: "RemountTest".to_string(),
        size_bytes: 50 * 1024 * 1024,
        filesystem: TestFileSystem::Default,
        read_only: false,
        use_ram_disk: false,
    };
    
    let test_volume = manager.create_volume(config.clone()).await
        .expect("Failed to create test volume");
    
    // Refresh and find volume
    core.volumes.refresh_volumes().await.ok();
    
    let all_volumes = core.volumes.get_all_volumes().await;
    let volume = all_volumes
        .iter()
        .find(|v| v.mount_point == test_volume.mount_point)
        .expect("Test volume should be detected")
        .clone();
    
    // Track the volume
    core.volumes
        .track_volume(&library, &volume.fingerprint, Some("Remountable Volume".to_string()))
        .await
        .expect("Failed to track volume");
    
    // Verify it's tracked and online
    let tracked = core.volumes
        .get_tracked_volumes(&library)
        .await
        .expect("Failed to get tracked volumes")
        .into_iter()
        .find(|v| v.fingerprint == volume.fingerprint)
        .expect("Volume should be tracked");
    
    assert!(tracked.is_online, "Volume should be online initially");
    
    // Destroy the volume (unmount it)
    manager.destroy_volume(test_volume).await
        .expect("Failed to destroy volume");
    
    // Refresh volumes - the volume should no longer be detected
    core.volumes.refresh_volumes().await.ok();
    
    // Update tracked volume state
    if let Some(current) = core.volumes.get_volume(&volume.fingerprint).await {
        core.volumes
            .update_tracked_volume_state(&library, &volume.fingerprint, &current)
            .await
            .ok();
    }
    
    // Check if volume is now offline in tracking
    let tracked_after = core.volumes
        .get_tracked_volumes(&library)
        .await
        .expect("Failed to get tracked volumes")
        .into_iter()
        .find(|v| v.fingerprint == volume.fingerprint)
        .expect("Volume should still be tracked");
    
    // The volume should still be tracked but might be offline
    assert_eq!(tracked_after.display_name, Some("Remountable Volume".to_string()));
    
    info!("Mount/unmount tracking test completed");
}

#[tokio::test]
async fn test_concurrent_volume_operations() {
    let _ = tracing_subscriber::fmt::try_init();
    
    if !check_test_privileges().await {
        warn!("Skipping test - requires elevated privileges");
        return;
    }
    
    let data_dir = tempdir().unwrap();
    let data_path = data_dir.path().to_path_buf();
    
    let core = Arc::new(
        Core::new_with_config(data_path.clone())
            .await
            .expect("Failed to create core"),
    );
    
    // Create multiple libraries
    let mut libraries = Vec::new();
    for i in 0..3 {
        let lib = core
            .libraries
            .create_library(
                format!("Concurrent Lib {}", i),
                Some(data_path.join("libraries").join(format!("concurrent-{}", i))),
                core.context.clone(),
            )
            .await
            .expect("Failed to create library");
        libraries.push(lib);
    }
    
    // Create a test volume
    let test_volume = TestVolumeBuilder::new("ConcurrentVol")
        .size_mb(100)
        .build()
        .await
        .expect("Failed to create test volume");
    
    // Refresh to detect volume
    core.volumes.refresh_volumes().await.ok();
    
    let all_volumes = core.volumes.get_all_volumes().await;
    let volume = all_volumes
        .iter()
        .find(|v| v.mount_point == test_volume.mount_point)
        .expect("Test volume should be detected")
        .clone();
    
    // Track the same volume in all libraries concurrently
    let mut tasks = Vec::new();
    for (i, library) in libraries.iter().enumerate() {
        let lib = library.clone();
        let vol_manager = core.volumes.clone();
        let fingerprint = volume.fingerprint.clone();
        let name = format!("Concurrent Volume {}", i);
        
        let task = tokio::spawn(async move {
            vol_manager
                .track_volume(&lib, &fingerprint, Some(name))
                .await
        });
        
        tasks.push(task);
    }
    
    // Wait for all tracking operations
    let results: Vec<_> = futures::future::join_all(tasks).await;
    
    // All should succeed
    for (i, result) in results.iter().enumerate() {
        assert!(result.is_ok(), "Task {} failed to join", i);
        assert!(result.as_ref().unwrap().is_ok(), 
                "Library {} failed to track volume", i);
    }
    
    // Verify all libraries have the volume tracked
    for (i, library) in libraries.iter().enumerate() {
        let tracked = core.volumes
            .get_tracked_volumes(library)
            .await
            .expect("Failed to get tracked volumes");
        
        // Find our specific test volume (there might be auto-tracked system volumes)
        let our_volume = tracked
            .iter()
            .find(|v| v.fingerprint == volume.fingerprint)
            .expect(&format!("Library {} should have our test volume tracked", i));
        
        assert_eq!(
            our_volume.display_name,
            Some(format!("Concurrent Volume {}", i)),
            "Library {} should have correct volume name", i
        );
    }
    
    info!("Concurrent volume operations test completed");
}```

## tests/job_shutdown_test.rs

```rust
//! Test for job pausing during shutdown

use sd_core_new::{
    Core,
    infrastructure::{
        database::entities,
        jobs::{types::{JobId, JobStatus}, prelude::*},
    },
    location::{create_location, LocationCreateArgs, IndexMode},
};
use sea_orm::{ActiveModelTrait, EntityTrait};
use std::time::Duration;
use tempfile::TempDir;
use tokio::time::sleep;

#[tokio::test]
async fn test_jobs_paused_on_shutdown() -> Result<(), Box<dyn std::error::Error>> {
    // Setup test environment
    let temp_dir = TempDir::new()?;
    let core_dir = temp_dir.path().join("core");
    tokio::fs::create_dir_all(&core_dir).await?;
    
    let core = Core::new_with_config(core_dir).await?;
    
    // Create library
    let library = core
        .libraries
        .create_library("Test Shutdown Library", None, core.context.clone())
        .await?;
    
    // Create test location with many files to ensure job runs long enough
    let test_location_dir = temp_dir.path().join("test_location");
    tokio::fs::create_dir_all(&test_location_dir).await?;
    
    // Create enough files to ensure indexing takes some time
    for i in 0..200 {
        let file_path = test_location_dir.join(format!("test_file_{}.txt", i));
        tokio::fs::write(&file_path, format!("Test content {}", i)).await?;
        
        // Create some subdirectories with files
        if i % 20 == 0 {
            let subdir = test_location_dir.join(format!("subdir_{}", i));
            tokio::fs::create_dir_all(&subdir).await?;
            for j in 0..10 {
                let subfile = subdir.join(format!("subfile_{}.txt", j));
                tokio::fs::write(&subfile, format!("Subcontent {} {}", i, j)).await?;
            }
        }
    }
    
    // Register device
    let db = library.db();
    let device = core.device.to_device()?;
    let device_model: entities::device::ActiveModel = device.into();
    let device_record = device_model.insert(db.conn()).await?;
    
    // Create location to trigger indexing
    let location_args = LocationCreateArgs {
        path: test_location_dir.clone(),
        name: Some("Test Location".to_string()),
        index_mode: IndexMode::Deep,
    };
    
    create_location(
        library.clone(),
        &core.events,
        location_args,
        device_record.id,
    )
    .await?;
    
    // Wait for indexing to start
    sleep(Duration::from_millis(500)).await;
    
    // Verify we have running jobs
    let job_manager = library.jobs();
    let running_jobs = job_manager.list_jobs(Some(JobStatus::Running)).await?;
    assert!(!running_jobs.is_empty(), "Should have at least one running job");
    
    let job_ids: Vec<JobId> = running_jobs.iter().map(|j| JobId(j.id)).collect();
    println!("Found {} running jobs before shutdown", job_ids.len());
    
    // Shutdown the core, which should pause all jobs
    println!("Shutting down core...");
    core.shutdown().await?;
    
    // Check that jobs were paused
    for job_id in &job_ids {
        let job_info = job_manager.get_job_info(job_id.0).await?;
        if let Some(info) = job_info {
            assert_eq!(
                info.status, 
                JobStatus::Paused, 
                "Job {} should be paused after shutdown",
                job_id.0
            );
            println!("âœ“ Job {} was paused during shutdown", job_id.0);
        }
    }
    
    Ok(())
}


#[tokio::test]
async fn test_shutdown_with_no_running_jobs() -> Result<(), Box<dyn std::error::Error>> {
    // This test ensures shutdown works correctly when no jobs are running
    let temp_dir = TempDir::new()?;
    let core = Core::new_with_config(temp_dir.path().to_path_buf()).await?;
    
    let library = core
        .libraries
        .create_library("Empty Library", None, core.context.clone())
        .await?;
    
    // Verify no running jobs
    let job_manager = library.jobs();
    let running_jobs = job_manager.list_jobs(Some(JobStatus::Running)).await?;
    assert!(running_jobs.is_empty());
    
    // Shutdown should complete without errors
    core.shutdown().await?;
    println!("âœ“ Shutdown completed successfully with no running jobs");
    
    Ok(())
}```

## tests/copy_progress_test.rs

```rust
//! Test for monitoring copy progress with large files
//!
//! This test verifies that copy progress updates smoothly with byte-level
//! granularity rather than jumping in large increments.

use sd_core_new::{
	infrastructure::{
		actions::{manager::ActionManager, Action},
		jobs::types::{JobId, JobStatus},
	},
	operations::files::{
		copy::{action::FileCopyAction, job::CopyOptions},
		input::CopyMethod,
	},
	Core,
};
use std::{
	path::PathBuf,
	sync::{Arc, Mutex},
	time::Duration,
};
use tempfile::TempDir;
use tokio::{fs, time::timeout};
use tracing_subscriber::{layer::SubscriberExt, util::SubscriberInitExt};
use uuid::Uuid;

/// Create a large test file with specified size
async fn create_large_test_file(
	path: &std::path::Path,
	size_mb: usize,
) -> Result<(), std::io::Error> {
	if let Some(parent) = path.parent() {
		fs::create_dir_all(parent).await?;
	}

	// Create file with 1MB chunks to avoid memory issues
	let chunk_size = 1024 * 1024; // 1MB
	let chunk = vec![0u8; chunk_size];

	let mut file = fs::OpenOptions::new()
		.create(true)
		.write(true)
		.truncate(true)
		.open(path)
		.await?;

	use tokio::io::AsyncWriteExt;
	for _ in 0..size_mb {
		file.write_all(&chunk).await?;
	}

	file.sync_all().await?;
	Ok(())
}

#[derive(Debug, Clone)]
struct ProgressSnapshot {
	timestamp: std::time::Instant,
	percentage: f32,
	bytes_copied: u64,
	message: String,
}

#[tokio::test]
async fn test_copy_progress_monitoring_large_file() {
	// Initialize tracing subscriber for debug logs
	let _guard = tracing_subscriber::registry()
		.with(tracing_subscriber::fmt::layer())
		.with(
			tracing_subscriber::EnvFilter::try_from_default_env()
				.unwrap_or_else(|_| tracing_subscriber::EnvFilter::new("info")),
		)
		.set_default();

	// Setup test environment
	let temp_dir = TempDir::new().unwrap();
	let test_root = temp_dir.path();

	// Create source and destination directories
	let source_dir = test_root.join("source");
	let dest_dir = test_root.join("destination");
	fs::create_dir_all(&source_dir).await.unwrap();
	fs::create_dir_all(&dest_dir).await.unwrap();

	// Create a large test file (100MB)
	let source_file = source_dir.join("large_test_file.bin");
	let file_size_mb = 100; // 100MB

	println!("Creating {}MB test file...", file_size_mb);
	create_large_test_file(&source_file, file_size_mb)
		.await
		.unwrap();

	// Verify file size
	let metadata = fs::metadata(&source_file).await.unwrap();
	let expected_size = (file_size_mb * 1024 * 1024) as u64;
	assert_eq!(
		metadata.len(),
		expected_size,
		"Test file should be exactly {}MB",
		file_size_mb
	);

	// Initialize core with custom data directory
	let core_data_dir = test_root.join("core_data");
	let core = Core::new_with_config(core_data_dir).await.unwrap();

	// Create a test library
	let library = core
		.libraries
		.create_library("Progress Test Library", None, core.context.clone())
		.await
		.unwrap();

	let library_id = library.id();

	// Create ActionManager
	let context = core.context.clone();
	let action_manager = ActionManager::new(context);

	// Build the copy action with the exact options from the CLI command
	let copy_action = FileCopyAction {
		sources: vec![source_file.clone()],
		destination: dest_dir.clone(),
		options: CopyOptions {
			overwrite: false,
			verify_checksum: true,     // --verify
			preserve_timestamps: true, // --preserve-timestamps
			delete_after_copy: false,
			move_mode: None,
			copy_method: CopyMethod::StreamingCopy, // --method streaming
		},
	};

	// Create the Action enum with library context
	let action = Action::FileCopy {
		library_id,
		action: copy_action,
	};

	// Setup progress monitoring
	let progress_snapshots = Arc::new(Mutex::new(Vec::new()));
	let progress_snapshots_clone = progress_snapshots.clone();
	let start_time = std::time::Instant::now();

	// Execute the action
	println!("Starting copy operation...");
	let action_output = action_manager
		.dispatch(action)
		.await
		.expect("Action dispatch should succeed");

	// Extract job ID from output
	let job_id = match &action_output {
		sd_core_new::infrastructure::actions::output::ActionOutput::Custom { data, .. } => {
			let job_id_value = data.get("job_id").unwrap();
			let job_id_str = job_id_value.as_str().expect("job_id should be a string");
			Uuid::parse_str(job_id_str).expect("job_id should be valid UUID")
		}
		_ => panic!("Expected Custom ActionOutput variant"),
	};
	println!("Monitoring job ID: {}", job_id);

	// Start monitoring task
	let library_clone = library.clone();
	let expected_size_clone = expected_size;
	let monitor_handle = tokio::spawn(async move {
		let mut last_progress = 0.0;
		let mut consecutive_same_progress = 0;
		let mut poll_count = 0;
		let mut has_seen_progress = false;

		loop {
			poll_count += 1;

			// Get job info from the job manager
			let job_info_result = library_clone.jobs().get_job_info(job_id).await.unwrap();
			if let Some(job_info) = job_info_result {
				let current_progress = job_info.progress * 100.0;

				// Only show debug output every 100 polls to reduce noise
				if poll_count % 100 == 0 && poll_count > 0 {
					println!(
						"Poll #{}: Status={:?}, Progress={:.1}%",
						poll_count, job_info.status, current_progress
					);
				}

				// Debug log when we're near completion
				if current_progress > 99.0 {
					println!(
						"Near completion - Poll #{}: Status={:?}, Progress={:.1}%",
						poll_count, job_info.status, current_progress
					);
				}

				// Record snapshot if progress changed
				if (current_progress - last_progress).abs() > 0.01 {
					consecutive_same_progress = 0;
					has_seen_progress = true;

					let snapshot = ProgressSnapshot {
						timestamp: std::time::Instant::now(),
						percentage: current_progress,
						bytes_copied: (expected_size_clone as f64
							* (current_progress as f64 / 100.0)) as u64,
						message: format!("{:.1}%", current_progress),
					};

					println!(
						"Progress: {:.1}% ({} MB)",
						current_progress,
						snapshot.bytes_copied / (1024 * 1024)
					);

					progress_snapshots_clone.lock().unwrap().push(snapshot);
					last_progress = current_progress;
				}

				// Check if job is complete
				match job_info.status {
					JobStatus::Completed => {
						println!("Job completed! (after {} polls)", poll_count);
						println!("Final progress: {:.1}%", current_progress);
						// Record final progress if we haven't seen any updates
						if !has_seen_progress && current_progress > 0.0 {
							let snapshot = ProgressSnapshot {
								timestamp: std::time::Instant::now(),
								percentage: current_progress,
								bytes_copied: expected_size_clone,
								message: "Final".to_string(),
							};
							progress_snapshots_clone.lock().unwrap().push(snapshot);
						}
						break;
					}
					JobStatus::Failed => {
						println!("Job failed after {} polls", poll_count);
						panic!("Job failed!");
					}
					_ => {
						// Continue monitoring
						consecutive_same_progress += 1;

						// If progress hasn't changed for many iterations, it might be stuck
						if consecutive_same_progress == 100 {
							println!(
								"Warning: Progress appears stuck at {:.1}% after 100 polls",
								current_progress
							);
						}

						// Fail fast if progress is stuck at 0% for too long
						if consecutive_same_progress > 200 && current_progress == 0.0 {
							println!(
								"ERROR: Progress stuck at 0% for {} polls. Aborting test.",
								consecutive_same_progress
							);
							break;
						}
					}
				}
			} else {
				println!(
					"Job info returned None for job {} (poll #{})",
					job_id, poll_count
				);
				// Job might have been removed from running jobs after completion
				// Let's assume it completed successfully
				break;
			}

			// Poll every 50ms to catch fine-grained progress updates
			tokio::time::sleep(Duration::from_millis(50)).await;
		}

		has_seen_progress
	});

	// Wait for job completion with timeout
	let completion_result = timeout(Duration::from_secs(30), monitor_handle).await;

	let has_seen_progress = match completion_result {
		Ok(Ok(has_progress)) => {
			println!("Monitoring completed successfully");
			has_progress
		}
		Ok(Err(e)) => panic!("Monitoring task failed: {}", e),
		Err(_) => panic!("Copy operation timed out after 30 seconds"),
	};

	// Analyze progress snapshots
	let snapshots = progress_snapshots.lock().unwrap();
	println!("\n=== Progress Analysis ===");
	println!("Total snapshots captured: {}", snapshots.len());
	println!("Saw progress updates during copy: {}", has_seen_progress);

	// First check if we got ANY progress updates at all
	if snapshots.is_empty() {
		panic!(
			"No progress updates were captured! Progress stayed at 0% throughout the entire copy operation. \
			This indicates the progress reporting is not working correctly."
		);
	}

	// If we only got one snapshot at the end, that's also a problem
	if snapshots.len() == 1 && !has_seen_progress {
		panic!(
			"Only captured final progress update. Progress reporting did not work during the copy operation."
		);
	}

	if snapshots.len() < 10 {
		panic!(
			"Too few progress updates captured! Only {} snapshots for a {}MB file. \
			Expected smooth byte-level progress updates throughout the operation.",
			snapshots.len(),
			file_size_mb
		);
	}

	// Calculate progress increments
	let mut increments = Vec::new();
	for i in 1..snapshots.len() {
		let increment = snapshots[i].percentage - snapshots[i - 1].percentage;
		if increment > 0.0 {
			increments.push(increment);
		}
	}

	// Calculate statistics
	let avg_increment = increments.iter().sum::<f32>() / increments.len() as f32;
	let max_increment = increments.iter().cloned().fold(0.0f32, f32::max);
	let min_increment = increments.iter().cloned().fold(100.0f32, f32::min);

	println!("Progress increments:");
	println!("  Average: {:.2}%", avg_increment);
	println!("  Maximum: {:.2}%", max_increment);
	println!("  Minimum: {:.2}%", min_increment);
	println!("  Total updates: {}", increments.len());

	// Verify smooth progress (no large jumps)
	// For a 1GB file, we should see many small increments
	// A 25% jump would indicate file-based progress instead of byte-based
	assert!(
		max_increment < 10.0,
		"Progress jumped by {:.1}% - should update smoothly with byte-level granularity",
		max_increment
	);

	// Verify we got reasonable granularity
	assert!(
		snapshots.len() > 20,
		"Expected at least 20 progress updates for a {}MB file, got {}",
		file_size_mb,
		snapshots.len()
	);

	// Verify file was copied successfully
	let dest_file = dest_dir.join("large_test_file.bin");
	assert!(dest_file.exists(), "Destination file should exist");

	let dest_metadata = fs::metadata(&dest_file).await.unwrap();
	assert_eq!(
		dest_metadata.len(),
		expected_size,
		"Copied file size should match source"
	);

	// Calculate effective copy speed
	let total_time = start_time.elapsed();
	let mb_per_second = (file_size_mb as f64) / total_time.as_secs_f64();
	println!("\nCopy performance: {:.1} MB/s", mb_per_second);

	println!("\nâœ… Copy progress monitoring test passed!");
	println!("   - Progress updated smoothly with byte-level granularity");
	println!("   - No large progress jumps detected");
	println!("   - File copied successfully with checksum verification");
}

#[tokio::test]
async fn test_copy_progress_multiple_files() {
	// Initialize tracing subscriber for debug logs
	let _guard = tracing_subscriber::registry()
		.with(tracing_subscriber::fmt::layer())
		.with(
			tracing_subscriber::EnvFilter::try_from_default_env()
				.unwrap_or_else(|_| tracing_subscriber::EnvFilter::new("info")),
		)
		.try_init();

	// This test verifies progress tracking across multiple files
	let temp_dir = TempDir::new().unwrap();
	let test_root = temp_dir.path();

	let source_dir = test_root.join("source");
	let dest_dir = test_root.join("destination");
	fs::create_dir_all(&source_dir).await.unwrap();
	fs::create_dir_all(&dest_dir).await.unwrap();

	// Create 4 files of different sizes
	let files = vec![
		("file1.bin", 100), // 100MB
		("file2.bin", 200), // 200MB
		("file3.bin", 150), // 150MB
		("file4.bin", 50),  // 50MB
	];

	let mut source_files = Vec::new();
	for (name, size_mb) in &files {
		let path = source_dir.join(name);
		println!("Creating {} ({}MB)...", name, size_mb);
		create_large_test_file(&path, *size_mb).await.unwrap();
		source_files.push(path);
	}

	// Initialize core and library
	let core_data_dir = test_root.join("core_data");
	let core = Core::new_with_config(core_data_dir).await.unwrap();
	let library = core
		.libraries
		.create_library("Multi-file Progress Test", None, core.context.clone())
		.await
		.unwrap();
	let library_id = library.id();

	let context = core.context.clone();
	let action_manager = ActionManager::new(context);

	// Build copy action for multiple files
	let copy_action = FileCopyAction {
		sources: source_files,
		destination: dest_dir.clone(),
		options: CopyOptions {
			overwrite: false,
			verify_checksum: true,
			preserve_timestamps: true,
			delete_after_copy: false,
			move_mode: None,
			copy_method: CopyMethod::StreamingCopy,
		},
	};

	let action = Action::FileCopy {
		library_id,
		action: copy_action,
	};

	// Setup progress monitoring
	let progress_snapshots = Arc::new(Mutex::new(Vec::new()));
	let progress_snapshots_clone = progress_snapshots.clone();

	// Execute the action
	println!("\nStarting multi-file copy operation...");
	let action_output = action_manager
		.dispatch(action)
		.await
		.expect("Action dispatch should succeed");

	let job_id = match &action_output {
		sd_core_new::infrastructure::actions::output::ActionOutput::Custom { data, .. } => {
			let job_id_str = data.get("job_id").unwrap().as_str().unwrap();
			Uuid::parse_str(job_id_str).unwrap()
		}
		_ => panic!("Expected Custom ActionOutput variant"),
	};

	// Monitor progress
	let library_clone = library.clone();
	let monitor_handle = tokio::spawn(async move {
		let mut last_progress = 0.0;

		loop {
			let job_info_result = library_clone.jobs().get_job_info(job_id).await.unwrap();
			if let Some(job_info) = job_info_result {
				let current_progress = job_info.progress * 100.0;

				if (current_progress - last_progress).abs() > 0.01 {
					let snapshot = ProgressSnapshot {
						timestamp: std::time::Instant::now(),
						percentage: current_progress,
						bytes_copied: 0, // Would need to calculate from percentage
						message: format!("{:.1}%", current_progress),
					};

					println!("Multi-file progress: {:.1}%", current_progress);
					progress_snapshots_clone.lock().unwrap().push(snapshot);
					last_progress = current_progress;
				}

				if matches!(job_info.status, JobStatus::Completed) {
					break;
				} else if matches!(job_info.status, JobStatus::Failed) {
					panic!("Multi-file job failed!");
				}
			} else {
				println!(
					"Job info returned None for multi-file job {}. Job likely completed.",
					job_id
				);
				break;
			}

			tokio::time::sleep(Duration::from_millis(50)).await;
		}
	});

	timeout(Duration::from_secs(30), monitor_handle)
		.await
		.expect("Multi-file copy should complete within 30 seconds")
		.expect("Monitor task should succeed");

	// Analyze progress
	let snapshots = progress_snapshots.lock().unwrap();
	println!("\n=== Multi-file Progress Analysis ===");
	println!("Total snapshots: {}", snapshots.len());

	// With 4 files totaling 500MB, we should see smooth progress
	// not 4 discrete 25% jumps
	let mut increments = Vec::new();
	for i in 1..snapshots.len() {
		let increment = snapshots[i].percentage - snapshots[i - 1].percentage;
		if increment > 0.0 {
			increments.push(increment);
		}
	}

	let max_increment = increments.iter().cloned().fold(0.0f32, f32::max);
	println!("Maximum progress increment: {:.2}%", max_increment);

	// Should have smooth progress, not 25% jumps
	assert!(
		max_increment < 15.0,
		"Progress should update smoothly across files, not jump by {:.1}%",
		max_increment
	);

	println!("\nâœ… Multi-file progress monitoring test passed!");
}
```

## tests/event_system_test.rs

```rust
//! Event System Integration Test
//!
//! Tests the event bus functionality by performing various operations
//! and verifying that the correct events are emitted. This includes:
//! - Core lifecycle events (CoreShutdown)
//! - Library management events (LibraryCreated, LibraryOpened, LibraryClosed)
//! - Location and indexing events (LocationAdded, IndexingStarted)
//! - Job system events (JobProgress, JobCompleted)
//! - Event filtering capabilities (library-specific filtering)
//! - Multiple concurrent subscribers
//! - Custom event emission and handling
//!
//! Note: These tests should be run with --test-threads=1 to avoid
//! potential conflicts between tests

use sd_core_new::{
    infrastructure::events::{Event, EventFilter},
    location::{create_location, LocationCreateArgs, IndexMode},
    Core,
};
use std::collections::HashSet;
use std::sync::Arc;
use tempfile::TempDir;
use tokio::sync::Mutex;
use tokio::time::{timeout, Duration};

#[tokio::test]
async fn test_core_and_library_events() -> Result<(), Box<dyn std::error::Error>> {
    let temp_dir = TempDir::new()?;
    
    // Set up event collection
    let collected_events = Arc::new(Mutex::new(Vec::new()));
    let events_clone = collected_events.clone();
    
    // Initialize core 
    let core = Core::new_with_config(temp_dir.path().to_path_buf()).await?;
    
    // Note: CoreStarted is emitted during core initialization, so we won't catch it
    // Start collecting events from now on
    let mut event_subscriber = core.events.subscribe();
    let event_collector = tokio::spawn(async move {
        while let Ok(event) = event_subscriber.recv().await {
            events_clone.lock().await.push(event);
        }
    });
    
    // Wait a bit for CoreStarted event
    tokio::time::sleep(Duration::from_millis(100)).await;
    
    // Test 1: Library creation
    let library = core
        .libraries
        .create_library("Test Event Library", None, core.context.clone())
        .await?;
    let library_id = library.id();
    
    // Wait for events to be processed
    tokio::time::sleep(Duration::from_millis(100)).await;
    
    // Test 2: Library operations
    let library_path = library.path().to_path_buf();
    drop(library); // Drop the Arc to release the library
    core.libraries.close_library(library_id).await?;
    tokio::time::sleep(Duration::from_millis(100)).await;
    
    // Open library again by path with context
    let library = core.libraries.open_library_with_context(&library_path, core.context.clone()).await?;
    tokio::time::sleep(Duration::from_millis(100)).await;
    
    // Test 3: Shutdown
    drop(library);
    core.shutdown().await?;
    
    // Wait for shutdown event to be processed
    tokio::time::sleep(Duration::from_millis(100)).await;
    
    // Stop event collector
    event_collector.abort();
    
    // Verify collected events
    let events = collected_events.lock().await;
    
    // Check for expected events
    let event_types: HashSet<String> = events.iter().map(|e| match e {
        Event::CoreStarted => "CoreStarted".to_string(),
        Event::CoreShutdown => "CoreShutdown".to_string(),
        Event::LibraryCreated { .. } => "LibraryCreated".to_string(),
        Event::LibraryOpened { .. } => "LibraryOpened".to_string(),
        Event::LibraryClosed { .. } => "LibraryClosed".to_string(),
        _ => format!("Other({:?})", e),
    }).collect();
    
    println!("Collected events: {:?}", event_types);
    
    // Verify core events (CoreStarted was emitted before we subscribed)
    assert!(event_types.contains("CoreShutdown"), "Should emit CoreShutdown event");
    
    // Verify library events
    assert!(event_types.contains("LibraryCreated"), "Should emit LibraryCreated event");
    assert!(event_types.contains("LibraryClosed"), "Should emit LibraryClosed event");
    assert!(event_types.contains("LibraryOpened"), "Should emit LibraryOpened event");
    
    Ok(())
}

#[tokio::test]
async fn test_location_and_job_events() -> Result<(), Box<dyn std::error::Error>> {
    let temp_dir = TempDir::new()?;
    let core = Core::new_with_config(temp_dir.path().to_path_buf()).await?;
    
    // Create library
    let library = core
        .libraries
        .create_library("Test Location Events", None, core.context.clone())
        .await?;
    
    // Set up filtered event collection - only job and indexing events
    let job_events = Arc::new(Mutex::new(Vec::new()));
    let job_events_clone = job_events.clone();
    
    let mut event_subscriber = core.events.subscribe();
    let event_collector = tokio::spawn(async move {
        while let Ok(event) = event_subscriber.recv().await {
            if event.is_job_event() || matches!(event, 
                Event::IndexingStarted { .. } | 
                Event::IndexingProgress { .. } |
                Event::IndexingCompleted { .. } |
                Event::IndexingFailed { .. } |
                Event::LocationAdded { .. }
            ) {
                job_events_clone.lock().await.push(event);
            }
        }
    });
    
    // Create test location
    let test_location_dir = temp_dir.path().join("test_location");
    tokio::fs::create_dir_all(&test_location_dir).await?;
    tokio::fs::write(test_location_dir.join("test.txt"), "Hello World").await?;
    
    // Register device
    let db = library.db();
    let device = core.device.to_device()?;
    
    use sd_core_new::infrastructure::database::entities;
    use sea_orm::{ActiveModelTrait, ColumnTrait, EntityTrait, QueryFilter};
    
    let device_record = match entities::device::Entity::find()
        .filter(entities::device::Column::Uuid.eq(device.id))
        .one(db.conn())
        .await?
    {
        Some(existing) => existing,
        None => {
            let device_model: entities::device::ActiveModel = device.into();
            device_model.insert(db.conn()).await?
        }
    };
    
    // Add location (triggers indexing job)
    let location_args = LocationCreateArgs {
        path: test_location_dir.clone(),
        name: Some("Test Location".to_string()),
        index_mode: IndexMode::Shallow,
    };
    
    let _location_id = create_location(
        library.clone(),
        &core.events,
        location_args,
        device_record.id,
    )
    .await?;
    
    // Wait for indexing to complete
    tokio::time::sleep(Duration::from_secs(2)).await;
    
    // Stop collector and check events
    event_collector.abort();
    
    let events = job_events.lock().await;
    let event_types: Vec<String> = events.iter().map(|e| match e {
        Event::JobQueued { .. } => "JobQueued".to_string(),
        Event::JobStarted { .. } => "JobStarted".to_string(),
        Event::JobProgress { .. } => "JobProgress".to_string(),
        Event::JobCompleted { .. } => "JobCompleted".to_string(),
        Event::IndexingStarted { .. } => "IndexingStarted".to_string(),
        Event::IndexingProgress { .. } => "IndexingProgress".to_string(),
        Event::IndexingCompleted { .. } => "IndexingCompleted".to_string(),
        Event::LocationAdded { .. } => "LocationAdded".to_string(),
        _ => format!("Other({:?})", e),
    }).collect();
    
    println!("Job-related events: {:?}", event_types);
    
    // Verify job events (Note: JobQueued might not be emitted if job starts immediately)
    assert!(
        event_types.contains(&"JobQueued".to_string()) || 
        event_types.contains(&"JobStarted".to_string()) ||
        event_types.contains(&"JobProgress".to_string()) ||
        event_types.contains(&"JobCompleted".to_string()),
        "Should emit at least one job event"
    );
    
    // Verify location event
    assert!(event_types.contains(&"LocationAdded".to_string()), "Should emit LocationAdded event");
    
    // Cleanup
    let lib_id = library.id();
    core.libraries.close_library(lib_id).await?;
    drop(library);
    core.shutdown().await?;
    
    Ok(())
}

#[tokio::test]
async fn test_event_filtering() -> Result<(), Box<dyn std::error::Error>> {
    let temp_dir = TempDir::new()?;
    let core = Core::new_with_config(temp_dir.path().to_path_buf()).await?;
    
    // Create two libraries
    let library1 = core
        .libraries
        .create_library("Library 1", None, core.context.clone())
        .await?;
    let lib1_id = library1.id();
    
    let library2 = core
        .libraries
        .create_library("Library 2", None, core.context.clone())
        .await?;
    let lib2_id = library2.id();
    
    // Set up filtered event collection - only library1 events
    let lib1_events = Arc::new(Mutex::new(Vec::new()));
    let lib1_events_clone = lib1_events.clone();
    
    let event_subscriber = core.events.subscribe();
    let event_collector = tokio::spawn(async move {
        let mut subscriber = event_subscriber;
        loop {
            match timeout(
                Duration::from_millis(100),
                subscriber.recv_filtered(|e| e.is_for_library(lib1_id))
            ).await {
                Ok(Ok(event)) => {
                    lib1_events_clone.lock().await.push(event);
                }
                _ => break,
            }
        }
    });
    
    // Perform operations on both libraries
    core.libraries.close_library(lib1_id).await?;
    core.libraries.close_library(lib2_id).await?;
    
    // Wait and stop collector
    tokio::time::sleep(Duration::from_millis(500)).await;
    event_collector.abort();
    
    // Check filtered events
    let events = lib1_events.lock().await;
    for event in events.iter() {
        match event {
            Event::LibraryCreated { id, .. } |
            Event::LibraryClosed { id, .. } => {
                assert_eq!(id, &lib1_id, "Should only receive library1 events");
            }
            _ => {}
        }
    }
    
    // Cleanup
    drop(library1);
    drop(library2);
    core.shutdown().await?;
    
    Ok(())
}

#[tokio::test]
async fn test_concurrent_event_subscribers() -> Result<(), Box<dyn std::error::Error>> {
    let temp_dir = TempDir::new()?;
    let core = Core::new_with_config(temp_dir.path().to_path_buf()).await?;
    
    // Create multiple subscribers
    let subscriber1_events = Arc::new(Mutex::new(Vec::new()));
    let subscriber2_events = Arc::new(Mutex::new(Vec::new()));
    let subscriber3_events = Arc::new(Mutex::new(Vec::new()));
    
    let events1 = subscriber1_events.clone();
    let events2 = subscriber2_events.clone();
    let events3 = subscriber3_events.clone();
    
    let mut sub1 = core.events.subscribe();
    let mut sub2 = core.events.subscribe();
    let mut sub3 = core.events.subscribe();
    
    // Start collectors
    let collector1 = tokio::spawn(async move {
        while let Ok(event) = sub1.recv().await {
            if matches!(event, Event::LibraryCreated { .. }) {
                events1.lock().await.push(event);
            }
        }
    });
    
    let collector2 = tokio::spawn(async move {
        while let Ok(event) = sub2.recv().await {
            if matches!(event, Event::LibraryCreated { .. }) {
                events2.lock().await.push(event);
            }
        }
    });
    
    let collector3 = tokio::spawn(async move {
        while let Ok(event) = sub3.recv().await {
            if matches!(event, Event::LibraryCreated { .. }) {
                events3.lock().await.push(event);
            }
        }
    });
    
    // Create a library (should be received by all subscribers)
    let library = core
        .libraries
        .create_library("Broadcast Test", None, core.context.clone())
        .await?;
    
    // Wait for events
    tokio::time::sleep(Duration::from_millis(200)).await;
    
    // Stop collectors
    collector1.abort();
    collector2.abort();
    collector3.abort();
    
    // Verify all subscribers received the event
    assert_eq!(subscriber1_events.lock().await.len(), 1, "Subscriber 1 should receive event");
    assert_eq!(subscriber2_events.lock().await.len(), 1, "Subscriber 2 should receive event");
    assert_eq!(subscriber3_events.lock().await.len(), 1, "Subscriber 3 should receive event");
    
    // Cleanup
    let lib_id = library.id();
    core.libraries.close_library(lib_id).await?;
    drop(library);
    core.shutdown().await?;
    
    Ok(())
}

#[tokio::test]
async fn test_custom_events() -> Result<(), Box<dyn std::error::Error>> {
    let temp_dir = TempDir::new()?;
    let core = Core::new_with_config(temp_dir.path().to_path_buf()).await?;
    
    let collected_events = Arc::new(Mutex::new(Vec::new()));
    let events_clone = collected_events.clone();
    
    let mut event_subscriber = core.events.subscribe();
    let event_collector = tokio::spawn(async move {
        while let Ok(event) = event_subscriber.recv().await {
            if matches!(event, Event::Custom { .. }) {
                events_clone.lock().await.push(event);
            }
        }
    });
    
    // Emit custom events
    let custom_data = serde_json::json!({
        "action": "test_action",
        "value": 42,
        "message": "Custom event test"
    });
    
    core.events.emit(Event::Custom {
        event_type: "test_event".to_string(),
        data: custom_data.clone(),
    });
    
    // Wait for event processing
    tokio::time::sleep(Duration::from_millis(100)).await;
    
    // Stop collector
    event_collector.abort();
    
    // Verify custom event
    let events = collected_events.lock().await;
    assert_eq!(events.len(), 1, "Should receive one custom event");
    
    if let Event::Custom { event_type, data } = &events[0] {
        assert_eq!(event_type, "test_event");
        assert_eq!(data, &custom_data);
    } else {
        panic!("Expected custom event");
    }
    
    // Test subscriber count
    let subscriber_count = core.events.subscriber_count();
    println!("Event subscribers: {}", subscriber_count);
    assert!(subscriber_count > 0, "Should have active subscribers");
    
    core.shutdown().await?;
    
    Ok(())
}```

## tests/database_migration_test.rs

```rust
//! Test database migration functionality

use sd_core_new::infrastructure::database::{Database, migration::Migrator};
use sea_orm_migration::MigratorTrait;
use tempfile::TempDir;
use std::path::Path;

#[tokio::test]
async fn test_database_creation_and_migration() {
    // Create a temporary directory for the test database
    let temp_dir = TempDir::new().unwrap();
    let db_path = temp_dir.path().join("test.db");
    
    println!("Creating database at: {:?}", db_path);
    
    // Create the database
    let db = Database::create(&db_path).await.expect("Failed to create database");
    
    println!("Database created successfully, running migrations...");
    
    // Run migrations with debug info
    println!("Running migrations...");
    let result = db.migrate().await;
    
    match result {
        Ok(()) => {
            println!("âœ… Migrations completed successfully!");
        }
        Err(e) => {
            println!("âŒ Migration failed: {}", e);
            panic!("Migration failed: {}", e);
        }
    }
    
    // Verify the database exists and has tables
    assert!(db_path.exists(), "Database file should exist");
    
    // Try to connect to verify it's a valid database
    let conn = db.conn();
    
    // Try a simple query to verify the database is working
    use sea_orm::{ConnectionTrait, Statement};
    
    let result = conn.execute(Statement::from_string(
        sea_orm::DatabaseBackend::Sqlite,
        "SELECT name FROM sqlite_master WHERE type='table' ORDER BY name;".to_string()
    )).await;
    
    match result {
        Ok(result) => {
            println!("âœ… Database query successful, {} rows affected", result.rows_affected());
        }
        Err(e) => {
            println!("âŒ Database query failed: {}", e);
            panic!("Database query failed: {}", e);
        }
    }
}```

## tests/job_registration_test.rs

```rust
//! Tests for job registration system

use sd_core_new::{
	infrastructure::jobs::{prelude::*, registry::REGISTRY},
	operations::files::copy::job::FileCopyJob,
	shared::types::SdPath,
};
use uuid::Uuid;

#[tokio::test]
async fn test_job_registration() {
	// Test that FileCopyJob is registered
	let job_names = REGISTRY.job_names();
	assert!(
		job_names.contains(&"file_copy"),
		"FileCopyJob should be registered"
	);

	// Test getting schema
	let schema = REGISTRY.get_schema("file_copy");
	assert!(schema.is_some(), "Should be able to get FileCopyJob schema");

	let schema = schema.unwrap();
	assert_eq!(schema.name, "file_copy");
	assert_eq!(schema.resumable, true);
	assert_eq!(schema.description, Some("Copy or move files to a destination"));
}

#[tokio::test]
async fn test_job_creation_from_json() {
	// Create a FileCopyJob using the registry
	let sources = vec![SdPath::new(Uuid::new_v4(), "/test/source")];
	let destination = SdPath::new(Uuid::new_v4(), "/test/dest");
	let job = FileCopyJob::from_paths(sources, destination);

	// Serialize to JSON
	let json_data = serde_json::to_value(&job).expect("Should serialize to JSON");

	// Create job from registry
	let created_job = REGISTRY
		.create_job("file_copy", json_data)
		.expect("Should create job from JSON");

	// Verify it's the right type by serializing state
	let state = created_job
		.serialize_state()
		.expect("Should serialize state");
	assert!(!state.is_empty(), "State should not be empty");
}

#[tokio::test]
async fn test_job_deserialization() {
	// Create a FileCopyJob
	let sources = vec![SdPath::new(Uuid::new_v4(), "/test/source")];
	let destination = SdPath::new(Uuid::new_v4(), "/test/dest");
	let job = FileCopyJob::from_paths(sources, destination);

	// Serialize as MessagePack (how jobs are stored)
	let state = rmp_serde::to_vec(&job).expect("Should serialize as MessagePack");

	// Deserialize using registry
	let deserialized_job = REGISTRY
		.deserialize_job("file_copy", &state)
		.expect("Should deserialize from MessagePack");

	// Verify by re-serializing
	let new_state = deserialized_job
		.serialize_state()
		.expect("Should re-serialize");
	assert_eq!(state, new_state, "States should match after round-trip");
}

#[tokio::test]
async fn test_unregistered_job_error() {
	// Try to create a job that doesn't exist
	let result = REGISTRY.create_job("nonexistent_job", serde_json::json!({}));
	assert!(result.is_err(), "Should fail for unregistered job");

	let error = result.unwrap_err();
	if let JobError::NotFound(msg) = error {
		assert!(
			msg.contains("nonexistent_job"),
			"Error should mention the missing job type"
		);
	} else {
		panic!("Expected NotFound error, got: {:?}", error);
	}
}

#[tokio::test]
async fn test_job_schema_information() {
	let schema = REGISTRY
		.get_schema("file_copy")
		.expect("Should have schema");

	// Verify schema properties
	assert_eq!(schema.name, "file_copy");
	assert!(schema.resumable, "FileCopyJob should be resumable");
	assert_eq!(schema.version, 1, "Should have version 1");
	assert!(schema.description.is_some(), "Should have description");
}

#[test]
fn test_has_job() {
	assert!(REGISTRY.has_job("file_copy"), "Should have file_copy job");
	assert!(
		!REGISTRY.has_job("nonexistent"),
		"Should not have nonexistent job"
	);
}
```

## tests/indexing_test.rs

```rust
//! Indexing Integration Test
//! 
//! Tests the production indexer functionality including:
//! - Location creation and indexing
//! - Smart filtering of system files
//! - Inode tracking for incremental indexing
//! - Event monitoring during indexing
//! - Database persistence of indexed entries
//!
//! Note: These tests should be run with --test-threads=1 to avoid
//! device UUID conflicts when multiple tests run in parallel

use sd_core_new::{
    infrastructure::database::entities,
    location::{create_location, LocationCreateArgs, IndexMode},
    Core,
};
use sea_orm::{
    ActiveModelTrait, ColumnTrait, EntityTrait, PaginatorTrait, QueryFilter,
};
use tempfile::TempDir;
use tokio::time::Duration;

#[tokio::test]
async fn test_location_indexing() -> Result<(), Box<dyn std::error::Error>> {
    // 1. Setup test environment
    let temp_dir = TempDir::new()?;
    let core = Core::new_with_config(temp_dir.path().to_path_buf()).await?;
    
    // 2. Create library
    let library = core
        .libraries
        .create_library("Test Indexing Library", None, core.context.clone())
        .await?;
    
    // 3. Create test location directory with some files
    let test_location_dir = temp_dir.path().join("test_location");
    tokio::fs::create_dir_all(&test_location_dir).await?;
    
    // Create test files
    tokio::fs::write(test_location_dir.join("test1.txt"), "Hello World").await?;
    tokio::fs::write(test_location_dir.join("test2.rs"), "fn main() {}").await?;
    tokio::fs::create_dir_all(test_location_dir.join("subdir")).await?;
    tokio::fs::write(test_location_dir.join("subdir/test3.md"), "# Test").await?;
    
    // Create files that should be filtered
    tokio::fs::write(test_location_dir.join(".DS_Store"), "system file").await?;
    tokio::fs::create_dir_all(test_location_dir.join("node_modules")).await?;
    tokio::fs::write(test_location_dir.join("node_modules/package.json"), "{}").await?;
    
    // 4. Register device in database
    let db = library.db();
    let device = core.device.to_device()?;
    
    let device_record = match entities::device::Entity::find()
        .filter(entities::device::Column::Uuid.eq(device.id))
        .one(db.conn())
        .await?
    {
        Some(existing) => existing,
        None => {
            let device_model: entities::device::ActiveModel = device.into();
            device_model.insert(db.conn()).await?
        }
    };
    
    // 5. Set up to monitor job completion
    // Note: Due to current implementation, IndexingCompleted event may not be emitted
    // So we'll monitor job status directly instead
    
    // 6. Create location and trigger indexing
    let location_args = LocationCreateArgs {
        path: test_location_dir.clone(),
        name: Some("Test Location".to_string()),
        index_mode: IndexMode::Deep,
    };
    
    let location_db_id = create_location(
        library.clone(),
        &core.events,
        location_args,
        device_record.id,
    )
    .await?;
    
    // 7. Wait for indexing to complete by monitoring job status
    let start_time = tokio::time::Instant::now();
    let timeout_duration = Duration::from_secs(30);
    
    let mut job_seen = false;
    let mut last_entry_count = 0;
    let mut stable_count_iterations = 0;
    
    loop {
        // Check all job statuses
        let all_jobs = library.jobs().list_jobs(None).await?;
        let running_jobs = library
            .jobs()
            .list_jobs(Some(
                sd_core_new::infrastructure::jobs::types::JobStatus::Running,
            ))
            .await?;
        
        // If we see a running job, mark that we've seen it
        if !running_jobs.is_empty() {
            job_seen = true;
        }
        
        // Check if any entries have been created (partial progress)
        let current_entries = entities::entry::Entity::find()
            .filter(entities::entry::Column::LocationId.eq(location_db_id))
            .count(db.conn())
            .await?;
        
        println!("Job status - Total: {}, Running: {}, Entries indexed: {}", 
            all_jobs.len(), running_jobs.len(), current_entries);
        
        // If we've seen a job and now there are no jobs, indexing likely completed
        if job_seen && all_jobs.is_empty() && current_entries > 0 {
            // Wait for entries to stabilize
            if current_entries == last_entry_count {
                stable_count_iterations += 1;
                if stable_count_iterations >= 3 {
                    println!("Indexing appears complete (job finished, entries stable)");
                    break;
                }
            } else {
                stable_count_iterations = 0;
            }
            last_entry_count = current_entries;
        }
        
        // Check for failed jobs
        let failed_jobs = library
            .jobs()
            .list_jobs(Some(
                sd_core_new::infrastructure::jobs::types::JobStatus::Failed,
            ))
            .await?;
        
        if !failed_jobs.is_empty() {
            panic!("Indexing job failed");
        }
        
        // Check timeout
        if start_time.elapsed() > timeout_duration {
            panic!("Indexing timed out after {:?}", timeout_duration);
        }
        
        // Wait a bit before checking again
        tokio::time::sleep(Duration::from_millis(500)).await;
    }
    
    // 8. Verify indexed entries in database
    let _entry_count = entities::entry::Entity::find()
        .filter(entities::entry::Column::LocationId.eq(location_db_id))
        .count(db.conn())
        .await?;
    
    let file_count = entities::entry::Entity::find()
        .filter(entities::entry::Column::LocationId.eq(location_db_id))
        .filter(entities::entry::Column::Kind.eq(0)) // Files
        .count(db.conn())
        .await?;
    
    let dir_count = entities::entry::Entity::find()
        .filter(entities::entry::Column::LocationId.eq(location_db_id))
        .filter(entities::entry::Column::Kind.eq(1)) // Directories
        .count(db.conn())
        .await?;
    
    // 9. Verify smart filtering worked
    let all_entries = entities::entry::Entity::find()
        .filter(entities::entry::Column::LocationId.eq(location_db_id))
        .all(db.conn())
        .await?;
    
    // Check that filtered files are not indexed
    for entry in &all_entries {
        assert_ne!(entry.name, ".DS_Store", "System files should be filtered");
        assert_ne!(entry.name, "node_modules", "Dev directories should be filtered");
    }
    
    // 10. Verify expected counts
    assert_eq!(file_count, 3, "Should index 3 files (excluding filtered)");
    assert!(dir_count >= 1, "Should index at least 1 directory (subdir)");
    
    // 11. Verify inode tracking
    let entries_with_inodes = entities::entry::Entity::find()
        .filter(entities::entry::Column::LocationId.eq(location_db_id))
        .filter(entities::entry::Column::Inode.is_not_null())
        .count(db.conn())
        .await?;
    
    assert!(entries_with_inodes > 0, "Entries should have inode tracking");
    
    // 12. Cleanup
    let lib_id = library.id();
    core.libraries.close_library(lib_id).await?;
    drop(library);
    
    core.shutdown().await?;
    
    Ok(())
}

#[tokio::test]
async fn test_incremental_indexing() -> Result<(), Box<dyn std::error::Error>> {
    // 1. Setup
    let temp_dir = TempDir::new()?;
    let core = Core::new_with_config(temp_dir.path().to_path_buf()).await?;
    
    let library = core
        .libraries
        .create_library("Test Incremental Library", None, core.context.clone())
        .await?;
    
    let test_location_dir = temp_dir.path().join("incremental_test");
    tokio::fs::create_dir_all(&test_location_dir).await?;
    
    // Initial files
    tokio::fs::write(test_location_dir.join("file1.txt"), "Initial content").await?;
    tokio::fs::write(test_location_dir.join("file2.txt"), "More content").await?;
    
    // Register device
    let db = library.db();
    let device = core.device.to_device()?;
    
    let device_record = match entities::device::Entity::find()
        .filter(entities::device::Column::Uuid.eq(device.id))
        .one(db.conn())
        .await?
    {
        Some(existing) => existing,
        None => {
            let device_model: entities::device::ActiveModel = device.into();
            device_model.insert(db.conn()).await?
        }
    };
    
    // 2. First indexing run
    let location_args = LocationCreateArgs {
        path: test_location_dir.clone(),
        name: Some("Incremental Test".to_string()),
        index_mode: IndexMode::Deep,
    };
    
    let location_db_id = create_location(
        library.clone(),
        &core.events,
        location_args,
        device_record.id,
    )
    .await?;
    
    // Wait for initial indexing to complete
    let start_time = tokio::time::Instant::now();
    let timeout_duration = Duration::from_secs(10);
    let mut job_seen = false;
    
    loop {
        let running_jobs = library
            .jobs()
            .list_jobs(Some(
                sd_core_new::infrastructure::jobs::types::JobStatus::Running,
            ))
            .await?;
        
        if !running_jobs.is_empty() {
            job_seen = true;
        }
        
        let current_entries = entities::entry::Entity::find()
            .filter(entities::entry::Column::LocationId.eq(location_db_id))
            .count(db.conn())
            .await?;
        
        if job_seen && running_jobs.is_empty() && current_entries > 0 {
            break;
        }
        
        if start_time.elapsed() > timeout_duration {
            break; // Don't fail, just continue
        }
        
        tokio::time::sleep(Duration::from_millis(200)).await;
    }
    
    let initial_file_count = entities::entry::Entity::find()
        .filter(entities::entry::Column::LocationId.eq(location_db_id))
        .filter(entities::entry::Column::Kind.eq(0))
        .count(db.conn())
        .await?;
    
    assert_eq!(initial_file_count, 2, "Should index 2 initial files");
    
    // Cleanup
    let lib_id = library.id();
    core.libraries.close_library(lib_id).await?;
    drop(library);
    
    core.shutdown().await?;
    
    Ok(())
}

#[tokio::test]
async fn test_indexing_error_handling() -> Result<(), Box<dyn std::error::Error>> {
    let temp_dir = TempDir::new()?;
    let core = Core::new_with_config(temp_dir.path().to_path_buf()).await?;
    
    let library = core
        .libraries
        .create_library("Test Error Library", None, core.context.clone())
        .await?;
    
    // Try to index non-existent location
    let non_existent = temp_dir.path().join("does_not_exist");
    
    let db = library.db();
    let device = core.device.to_device()?;
    
    let device_record = match entities::device::Entity::find()
        .filter(entities::device::Column::Uuid.eq(device.id))
        .one(db.conn())
        .await?
    {
        Some(existing) => existing,
        None => {
            let device_model: entities::device::ActiveModel = device.into();
            device_model.insert(db.conn()).await?
        }
    };
    
    let location_args = LocationCreateArgs {
        path: non_existent,
        name: Some("Non-existent".to_string()),
        index_mode: IndexMode::Deep,
    };
    
    // This should handle the error gracefully
    let result = create_location(
        library.clone(),
        &core.events,
        location_args,
        device_record.id,
    )
    .await;
    
    // The location creation should fail for non-existent path
    assert!(result.is_err(), "Should fail to create location for non-existent path");
    
    // Cleanup
    let lib_id = library.id();
    core.libraries.close_library(lib_id).await?;
    drop(library);
    
    core.shutdown().await?;
    
    Ok(())
}```

## tests/job_pause_resume_test.rs

```rust
//! Integration test for job pause/resume functionality

use sd_core_new::{
    infrastructure::{
        database::entities,
        jobs::types::{JobId, JobStatus},
    },
    location::{create_location, LocationCreateArgs, IndexMode},
    Core,
};
use sea_orm::{ActiveModelTrait, ColumnTrait, EntityTrait, PaginatorTrait, QueryFilter};
use std::time::Duration;
use tempfile::TempDir;
use tokio::time::sleep;

#[tokio::test]
async fn test_pause_and_resume_indexing_job() -> Result<(), Box<dyn std::error::Error>> {
    // Setup test environment
    let temp_dir = TempDir::new()?;
    let core = Core::new_with_config(temp_dir.path().to_path_buf()).await?;
    
    // Create library
    let library = core
        .libraries
        .create_library("Test Pause Resume Library", None, core.context.clone())
        .await?;
    
    // Create test location directory with many files
    let test_location_dir = temp_dir.path().join("test_location");
    tokio::fs::create_dir_all(&test_location_dir).await?;
    
    // Create many test files to ensure job runs long enough
    for i in 0..100 {
        let file_path = test_location_dir.join(format!("test_file_{}.txt", i));
        tokio::fs::write(&file_path, format!("Test content {}", i)).await?;
    }
    
    // Register device
    let db = library.db();
    let device = core.device.to_device()?;
    
    let device_record = match entities::device::Entity::find()
        .filter(entities::device::Column::Uuid.eq(device.id))
        .one(db.conn())
        .await?
    {
        Some(existing) => existing,
        None => {
            let device_model: entities::device::ActiveModel = device.into();
            device_model.insert(db.conn()).await?
        }
    };
    
    // Create location to trigger indexing
    let location_args = LocationCreateArgs {
        path: test_location_dir.clone(),
        name: Some("Test Location".to_string()),
        index_mode: IndexMode::Deep,
    };
    
    let _location_db_id = create_location(
        library.clone(),
        &core.events,
        location_args,
        device_record.id,
    )
    .await?;
    
    // Get the indexing job that was created
    let job_manager = library.jobs();
    
    // Wait a bit for job to be created and start
    sleep(Duration::from_millis(200)).await;
    
    // Get running jobs
    let running_jobs = job_manager.list_jobs(Some(JobStatus::Running)).await?;
    assert!(!running_jobs.is_empty(), "Should have a running indexing job");
    
    let job_info = &running_jobs[0];
    let job_id = JobId(job_info.id);
    
    // Wait a bit for job to start processing
    sleep(Duration::from_millis(500)).await;
    
    // Pause the job
    job_manager.pause_job(job_id).await?;
    
    // Wait for pause to take effect
    sleep(Duration::from_millis(200)).await;
    
    // Check job is paused
    let job_info = job_manager.get_job_info(job_id.0).await?.unwrap();
    assert_eq!(job_info.status, JobStatus::Paused, "Job should be paused");
    
    // Record progress when paused
    let paused_progress = job_info.progress;
    assert!(paused_progress > 0.0, "Should have made some progress");
    assert!(paused_progress < 100.0, "Should not be complete");
    
    // Wait a bit to ensure no progress is made while paused
    sleep(Duration::from_millis(500)).await;
    
    // Check progress hasn't changed
    let job_info = job_manager.get_job_info(job_id.0).await?.unwrap();
    assert_eq!(job_info.progress, paused_progress, "Progress should not change while paused");
    
    // Resume the job
    job_manager.resume_job(job_id).await?;
    
    // Wait for job to complete
    let mut retries = 0;
    loop {
        let job_info = job_manager.get_job_info(job_id.0).await?.unwrap();
        match job_info.status {
            JobStatus::Completed => {
                assert!(job_info.progress >= 99.0, "Job should be complete");
                break;
            }
            JobStatus::Failed => {
                panic!("Job failed: {:?}", job_info.error_message);
            }
            _ => {
                if retries > 100 {
                    panic!("Job did not complete in time");
                }
                retries += 1;
                sleep(Duration::from_millis(100)).await;
            }
        }
    }
    
    // Verify files were indexed
    use sea_orm::{PaginatorTrait, QueryFilter, ColumnTrait};
    let indexed_count = entities::entry::Entity::find()
        .filter(entities::entry::Column::LocationId.eq(1))
        .count(db.conn())
        .await?;
    
    assert!(indexed_count > 0, "Files should be indexed");
    
    Ok(())
}

#[tokio::test]
async fn test_pause_paused_job_error() -> Result<(), Box<dyn std::error::Error>> {
    // Setup test environment
    let temp_dir = TempDir::new()?;
    let core = Core::new_with_config(temp_dir.path().to_path_buf()).await?;
    
    // Create library
    let library = core
        .libraries
        .create_library("Test Pause Error Library", None, core.context.clone())
        .await?;
    
    // Create test location
    let test_location_dir = temp_dir.path().join("test_location");
    tokio::fs::create_dir_all(&test_location_dir).await?;
    tokio::fs::write(test_location_dir.join("test.txt"), "content").await?;
    
    // Register device
    let db = library.db();
    let device = core.device.to_device()?;
    let device_model: entities::device::ActiveModel = device.into();
    let device_record = device_model.insert(db.conn()).await?;
    
    // Create location
    let location_args = LocationCreateArgs {
        path: test_location_dir.clone(),
        name: Some("Test Location".to_string()),
        index_mode: IndexMode::Deep,
    };
    
    create_location(
        library.clone(),
        &core.events,
        location_args,
        device_record.id,
    )
    .await?;
    
    // Get the job
    let job_manager = library.jobs();
    sleep(Duration::from_millis(200)).await;
    let running_jobs = job_manager.list_jobs(Some(JobStatus::Running)).await?;
    let job_id = JobId(running_jobs[0].id);
    
    // Pause the job
    job_manager.pause_job(job_id).await?;
    sleep(Duration::from_millis(100)).await;
    
    // Try to pause again - should fail
    let result = job_manager.pause_job(job_id).await;
    assert!(result.is_err(), "Should not be able to pause an already paused job");
    assert!(result.unwrap_err().to_string().contains("Cannot pause job in Paused state"));
    
    Ok(())
}

#[tokio::test]
async fn test_resume_running_job_error() -> Result<(), Box<dyn std::error::Error>> {
    // Setup test environment
    let temp_dir = TempDir::new()?;
    let core = Core::new_with_config(temp_dir.path().to_path_buf()).await?;
    
    // Create library
    let library = core
        .libraries
        .create_library("Test Resume Error Library", None, core.context.clone())
        .await?;
    
    // Create test location with multiple files
    let test_location_dir = temp_dir.path().join("test_location");
    tokio::fs::create_dir_all(&test_location_dir).await?;
    
    for i in 0..10 {
        let file_path = test_location_dir.join(format!("test_file_{}.txt", i));
        tokio::fs::write(&file_path, format!("Test content {}", i)).await?;
    }
    
    // Register device
    let db = library.db();
    let device = core.device.to_device()?;
    let device_model: entities::device::ActiveModel = device.into();
    let device_record = device_model.insert(db.conn()).await?;
    
    // Create location
    let location_args = LocationCreateArgs {
        path: test_location_dir.clone(),
        name: Some("Test Location".to_string()),
        index_mode: IndexMode::Deep,
    };
    
    create_location(
        library.clone(),
        &core.events,
        location_args,
        device_record.id,
    )
    .await?;
    
    // Get the running job
    let job_manager = library.jobs();
    sleep(Duration::from_millis(200)).await;
    let running_jobs = job_manager.list_jobs(Some(JobStatus::Running)).await?;
    let job_id = JobId(running_jobs[0].id);
    
    // Try to resume a running job - should fail
    let result = job_manager.resume_job(job_id).await;
    assert!(result.is_err(), "Should not be able to resume a running job");
    assert!(result.unwrap_err().to_string().contains("Cannot resume job in Running state"));
    
    Ok(())
}```

## tests/volume_tracking_test.rs

```rust
//! Integration tests for volume tracking functionality

use sd_core_new::{
    Core,
    infrastructure::{
        actions::{Action, output::ActionOutput},
    },
    operations::volumes::{
        track::action::VolumeTrackAction,
        untrack::action::VolumeUntrackAction,
        speed_test::action::VolumeSpeedTestAction,
    },
    volume::types::MountType,
};
use std::sync::Arc;
use tempfile::tempdir;
use tracing::info;

#[tokio::test]
async fn test_volume_tracking_lifecycle() {
    // Initialize logging
    let _ = tracing_subscriber::fmt::try_init();

    // Setup test environment
    let data_dir = tempdir().unwrap();
    let data_path = data_dir.path().to_path_buf();
    
    // Initialize core - this handles all the setup automatically
    let core = Arc::new(
        Core::new_with_config(data_path.clone())
            .await
            .expect("Failed to create core"),
    );
    
    // Create a test library
    let library = core
        .libraries
        .create_library(
            "Test Library",
            Some(data_path.join("libraries").join("test-library")),
            core.context.clone(),
        )
        .await
        .expect("Failed to create library");
    
    let library_id = library.id();
    info!("Created test library: {}", library_id);
    
    
    // Get volume manager
    let volume_manager = core.volumes.clone();
    
    // Refresh volumes to ensure we have the latest
    volume_manager
        .refresh_volumes()
        .await
        .expect("Failed to refresh volumes");
    
    // Get all volumes
    let all_volumes = volume_manager.get_all_volumes().await;
    
    info!("Detected {} volumes", all_volumes.len());
    
    // Get first available volume for testing
    let test_volume = all_volumes
        .first()
        .expect("No volumes available for testing")
        .clone();
    
    info!("Using volume '{}' for testing", test_volume.name);
    
    let fingerprint = test_volume.fingerprint.clone();
    
    // Get action manager from core context
    let action_manager = core.context.get_action_manager().await
        .expect("Action manager should be initialized");
    
    // Test 1: Check if volume is already tracked (from auto-tracking)
    info!("Checking initial tracking status...");
    let initial_tracked = volume_manager
        .is_volume_tracked(&library, &fingerprint)
        .await
        .expect("Failed to check tracking status");
    
    if initial_tracked {
        info!("Volume is already tracked (from auto-tracking), untracking first");
        
        // Untrack it first so we can test tracking
        let untrack_action = Action::VolumeUntrack {
            action: VolumeUntrackAction {
                fingerprint: fingerprint.clone(),
                library_id,
            },
        };
        
        let result = action_manager.dispatch(untrack_action).await;
        assert!(result.is_ok(), "Failed to untrack volume: {:?}", result);
    }
    
    // Test 1: Track volume
    info!("Testing volume tracking...");
    {
        let track_action = Action::VolumeTrack {
            action: VolumeTrackAction {
                fingerprint: fingerprint.clone(),
                library_id,
                name: Some("My Test Volume".to_string()),
            },
        };
        
        let result = action_manager.dispatch(track_action).await;
        
        assert!(result.is_ok(), "Failed to track volume: {:?}", result);
        
        if let Ok(ActionOutput::VolumeTracked { volume_name, .. }) = result {
            info!("Volume tracked successfully as '{}'", volume_name);
        }
        
        // Verify volume is tracked
        let is_tracked = volume_manager
            .is_volume_tracked(&library, &fingerprint)
            .await
            .expect("Failed to check tracking status");
        assert!(is_tracked, "Volume should be tracked");
        
        // Get tracked volumes  
        let tracked_volumes = volume_manager
            .get_tracked_volumes(&library)
            .await
            .expect("Failed to get tracked volumes");
        
        // Find our specific volume (there might be others from auto-tracking)
        let our_volume = tracked_volumes.iter()
            .find(|v| v.fingerprint == fingerprint)
            .expect("Our volume should be in tracked volumes");
            
        assert_eq!(
            our_volume.display_name,
            Some("My Test Volume".to_string())
        );
    }
    
    // Test 2: Try to track same volume again (should fail)
    info!("Testing duplicate tracking prevention...");
    {
        let track_action = Action::VolumeTrack {
            action: VolumeTrackAction {
                fingerprint: fingerprint.clone(),
                library_id,
                name: Some("Another Name".to_string()),
            },
        };
        
        let result = action_manager.dispatch(track_action).await;
        
        assert!(result.is_err(), "Should not be able to track volume twice");
        info!("Duplicate tracking correctly prevented");
    }
    
    // Test 3: Untrack volume
    info!("Testing volume untracking...");
    {
        let untrack_action = Action::VolumeUntrack {
            action: VolumeUntrackAction {
                fingerprint: fingerprint.clone(),
                library_id,
            },
        };
        
        let result = action_manager.dispatch(untrack_action).await;
        
        assert!(result.is_ok(), "Failed to untrack volume: {:?}", result);
        
        if let Ok(ActionOutput::VolumeUntracked { .. }) = result {
            info!("Volume untracked successfully");
        }
        
        // Verify volume is no longer tracked
        let is_tracked = volume_manager
            .is_volume_tracked(&library, &fingerprint)
            .await
            .expect("Failed to check tracking status");
        assert!(!is_tracked, "Volume should not be tracked");
        
        // Get tracked volumes and verify our volume is not there
        let tracked_volumes = volume_manager
            .get_tracked_volumes(&library)
            .await
            .expect("Failed to get tracked volumes");
        
        let our_volume_still_tracked = tracked_volumes.iter()
            .any(|v| v.fingerprint == fingerprint);
        assert!(!our_volume_still_tracked, "Our volume should no longer be tracked");
    }
    
    // Test 4: Try to untrack volume that's not tracked (should fail)
    info!("Testing untrack of non-tracked volume...");
    {
        let untrack_action = Action::VolumeUntrack {
            action: VolumeUntrackAction {
                fingerprint: fingerprint.clone(),
                library_id,
            },
        };
        
        let result = action_manager.dispatch(untrack_action).await;
        
        assert!(result.is_err(), "Should not be able to untrack non-tracked volume");
        info!("Untrack of non-tracked volume correctly prevented");
    }
    
    info!("Volume tracking lifecycle test completed successfully");
}

#[tokio::test]
async fn test_volume_tracking_multiple_libraries() {
    // Initialize logging
    let _ = tracing_subscriber::fmt::try_init();

    // Setup test environment
    let data_dir = tempdir().unwrap();
    let data_path = data_dir.path().to_path_buf();
    
    // Initialize core - this handles all the setup automatically
    let core = Arc::new(
        Core::new_with_config(data_path.clone())
            .await
            .expect("Failed to create core"),
    );
    
    // Create two test libraries
    let library1 = core
        .libraries
        .create_library(
            "Library 1",
            Some(data_path.join("libraries").join("library1")),
            core.context.clone(),
        )
        .await
        .expect("Failed to create library 1");
    
    let library2 = core
        .libraries
        .create_library(
            "Library 2",
            Some(data_path.join("libraries").join("library2")),
            core.context.clone(),
        )
        .await
        .expect("Failed to create library 2");
    
    let library1_id = library1.id();
    let library2_id = library2.id();
    
    info!("Created libraries: {} and {}", library1_id, library2_id);
    
    // Get volume manager and refresh
    let volume_manager = core.volumes.clone();
    volume_manager
        .refresh_volumes()
        .await
        .expect("Failed to refresh volumes");
    
    // Get first available volume
    let test_volume = volume_manager
        .get_all_volumes()
        .await
        .first()
        .expect("No volumes available for testing")
        .clone();
    
    let fingerprint = test_volume.fingerprint.clone();
    
    // Get action manager from core context
    let action_manager = core.context.get_action_manager().await
        .expect("Action manager should be initialized");
    
    // Check if volume is already tracked in library 1 (from auto-tracking)
    let is_tracked_lib1 = volume_manager
        .is_volume_tracked(&library1, &fingerprint)
        .await
        .expect("Failed to check tracking status");
        
    if is_tracked_lib1 {
        info!("Volume already tracked in library 1, untracking first");
        let untrack_action = Action::VolumeUntrack {
            action: VolumeUntrackAction {
                fingerprint: fingerprint.clone(),
                library_id: library1_id,
            },
        };
        action_manager.dispatch(untrack_action).await
            .expect("Failed to untrack from library 1");
    }
    
    // Track volume in library 1
    info!("Tracking volume in library 1...");
    {
        let track_action = Action::VolumeTrack {
            action: VolumeTrackAction {
                fingerprint: fingerprint.clone(),
                library_id: library1_id,
                name: Some("Library 1 Volume".to_string()),
            },
        };
        
        let result = action_manager.dispatch(track_action).await;
        assert!(result.is_ok(), "Failed to track volume in library 1");
    }
    
    // Check if volume is already tracked in library 2 (from auto-tracking)
    let is_tracked_lib2 = volume_manager
        .is_volume_tracked(&library2, &fingerprint)
        .await
        .expect("Failed to check tracking status");
        
    if is_tracked_lib2 {
        info!("Volume already tracked in library 2, untracking first");
        let untrack_action = Action::VolumeUntrack {
            action: VolumeUntrackAction {
                fingerprint: fingerprint.clone(),
                library_id: library2_id,
            },
        };
        action_manager.dispatch(untrack_action).await
            .expect("Failed to untrack from library 2");
    }
    
    // Track same volume in library 2 (should succeed)
    info!("Tracking same volume in library 2...");
    {
        let track_action = Action::VolumeTrack {
            action: VolumeTrackAction {
                fingerprint: fingerprint.clone(),
                library_id: library2_id,
                name: Some("Library 2 Volume".to_string()),
            },
        };
        
        let result = action_manager.dispatch(track_action).await;
        assert!(result.is_ok(), "Should be able to track volume in different library");
    }
    
    // Verify both libraries have the volume tracked
    let lib1_volumes = volume_manager
        .get_tracked_volumes(&library1)
        .await
        .expect("Failed to get library 1 volumes");
    
    let lib1_our_volume = lib1_volumes.iter()
        .find(|v| v.fingerprint == fingerprint)
        .expect("Our volume should be in library 1");
    assert_eq!(lib1_our_volume.display_name, Some("Library 1 Volume".to_string()));
    
    let lib2_volumes = volume_manager
        .get_tracked_volumes(&library2)
        .await
        .expect("Failed to get library 2 volumes");
        
    let lib2_our_volume = lib2_volumes.iter()
        .find(|v| v.fingerprint == fingerprint)
        .expect("Our volume should be in library 2");
    assert_eq!(lib2_our_volume.display_name, Some("Library 2 Volume".to_string()));
    
    // Untrack from library 1
    info!("Untracking volume from library 1...");
    {
        let untrack_action = Action::VolumeUntrack {
            action: VolumeUntrackAction {
                fingerprint: fingerprint.clone(),
                library_id: library1_id,
            },
        };
        
        let result = action_manager.dispatch(untrack_action).await;
        assert!(result.is_ok(), "Failed to untrack from library 1");
    }
    
    // Verify library 2 still has it tracked
    let lib2_volumes = volume_manager
        .get_tracked_volumes(&library2)
        .await
        .expect("Failed to get library 2 volumes");
    
    let lib2_still_has_volume = lib2_volumes.iter()
        .any(|v| v.fingerprint == fingerprint);
    assert!(lib2_still_has_volume, "Library 2 should still have volume tracked");
    
    info!("Multiple library volume tracking test completed successfully");
}

#[tokio::test]
async fn test_automatic_system_volume_tracking() {
    let _ = tracing_subscriber::fmt::try_init();
    
    let data_dir = tempdir().unwrap();
    let data_path = data_dir.path().to_path_buf();
    
    let core = Arc::new(
        Core::new_with_config(data_path.clone())
            .await
            .expect("Failed to create core"),
    );
    
    // Create library with default settings (auto_track_system_volumes = true)
    let library = core
        .libraries
        .create_library(
            "Auto Track Test",
            Some(data_path.join("libraries").join("auto-track")),
            core.context.clone(),
        )
        .await
        .expect("Failed to create library");
    
    info!("Created library with auto-tracking enabled");
    
    // Get tracked volumes
    let tracked_volumes = core.volumes
        .get_tracked_volumes(&library)
        .await
        .expect("Failed to get tracked volumes");
    
    // Get system volumes
    let system_volumes = core.volumes.get_system_volumes().await;
    
    info!("Found {} system volumes, {} tracked volumes", 
          system_volumes.len(), tracked_volumes.len());
    
    // Verify all system volumes are tracked
    for sys_vol in &system_volumes {
        let is_tracked = tracked_volumes.iter()
            .any(|tv| tv.fingerprint == sys_vol.fingerprint);
        assert!(is_tracked, 
            "System volume '{}' should be automatically tracked", 
            sys_vol.name);
    }
    
    info!("Automatic system volume tracking test completed");
}

#[tokio::test]
async fn test_auto_tracking_disabled() {
    let _ = tracing_subscriber::fmt::try_init();
    
    // This test verifies manual control over volume tracking
    // Since we can't disable auto-tracking via config after creation,
    // we'll test that we can untrack auto-tracked volumes
    
    let data_dir = tempdir().unwrap();
    let data_path = data_dir.path().to_path_buf();
    
    let core = Arc::new(
        Core::new_with_config(data_path.clone())
            .await
            .expect("Failed to create core"),
    );
    
    let library = core
        .libraries
        .create_library(
            "Manual Track Test",
            Some(data_path.join("libraries").join("manual-track")),
            core.context.clone(),
        )
        .await
        .expect("Failed to create library");
    
    // Get auto-tracked system volumes
    let auto_tracked = core.volumes
        .get_tracked_volumes(&library)
        .await
        .expect("Failed to get tracked volumes");
    
    info!("Found {} auto-tracked volumes", auto_tracked.len());
    
    // Untrack all auto-tracked volumes
    for volume in &auto_tracked {
        core.volumes
            .untrack_volume(&library, &volume.fingerprint)
            .await
            .expect("Failed to untrack volume");
    }
    
    // Verify all volumes are untracked
    let remaining = core.volumes
        .get_tracked_volumes(&library)
        .await
        .expect("Failed to get tracked volumes");
    
    assert_eq!(remaining.len(), 0, 
        "All volumes should be untracked after manual removal");
    
    // Now manually track just one non-system volume if available
    let all_volumes = core.volumes.get_all_volumes().await;
    if let Some(external_volume) = all_volumes.iter()
        .find(|v| !matches!(v.mount_type, MountType::System)) {
        
        core.volumes
            .track_volume(&library, &external_volume.fingerprint, Some("Manual Volume".to_string()))
            .await
            .expect("Failed to manually track volume");
        
        let tracked = core.volumes
            .get_tracked_volumes(&library)
            .await
            .expect("Failed to get tracked volumes");
        
        assert_eq!(tracked.len(), 1, "Should have exactly one manually tracked volume");
        assert_eq!(tracked[0].display_name, Some("Manual Volume".to_string()));
    }
    
    info!("Manual tracking control test completed");
}

#[tokio::test]
async fn test_volume_state_updates() {
    let _ = tracing_subscriber::fmt::try_init();
    
    let data_dir = tempdir().unwrap();
    let data_path = data_dir.path().to_path_buf();
    
    let core = Arc::new(
        Core::new_with_config(data_path.clone())
            .await
            .expect("Failed to create core"),
    );
    
    let library = core
        .libraries
        .create_library(
            "State Update Test",
            Some(data_path.join("libraries").join("state-test")),
            core.context.clone(),
        )
        .await
        .expect("Failed to create library");
    
    // Get a volume to track
    let test_volume = core.volumes
        .get_all_volumes()
        .await
        .first()
        .cloned()
        .expect("No volumes available");
    
    let fingerprint = test_volume.fingerprint.clone();
    
    // Track the volume if not already tracked
    if !core.volumes.is_volume_tracked(&library, &fingerprint).await.unwrap_or(false) {
        core.volumes
            .track_volume(&library, &fingerprint, Some("State Test Volume".to_string()))
            .await
            .expect("Failed to track volume");
    }
    
    // Get initial tracked state
    let initial_tracked = core.volumes
        .get_tracked_volumes(&library)
        .await
        .expect("Failed to get tracked volumes")
        .into_iter()
        .find(|v| v.fingerprint == fingerprint)
        .expect("Volume should be tracked");
    
    info!("Initial volume state - capacity: {:?}, online: {}", 
          initial_tracked.available_capacity, initial_tracked.is_online);
    
    // Update volume state
    core.volumes
        .update_tracked_volume_state(&library, &fingerprint, &test_volume)
        .await
        .expect("Failed to update volume state");
    
    // Get updated state
    let updated_tracked = core.volumes
        .get_tracked_volumes(&library)
        .await
        .expect("Failed to get tracked volumes")
        .into_iter()
        .find(|v| v.fingerprint == fingerprint)
        .expect("Volume should be tracked");
    
    // Verify last_seen_at was updated
    assert!(updated_tracked.last_seen_at >= initial_tracked.last_seen_at,
        "last_seen_at should be updated");
    
    info!("Volume state update test completed");
}

#[tokio::test]
async fn test_volume_speed_test() {
    let _ = tracing_subscriber::fmt::try_init();
    
    let data_dir = tempdir().unwrap();
    let data_path = data_dir.path().to_path_buf();
    
    let core = Arc::new(
        Core::new_with_config(data_path.clone())
            .await
            .expect("Failed to create core"),
    );
    
    // Get first volume for testing
    let test_volume = core.volumes
        .get_all_volumes()
        .await
        .first()
        .cloned()
        .expect("No volumes available");
    
    let fingerprint = test_volume.fingerprint.clone();
    
    info!("Testing speed test on volume '{}'", test_volume.name);
    
    // Create speed test action
    let speed_test_action = Action::VolumeSpeedTest {
        action: VolumeSpeedTestAction {
            fingerprint: fingerprint.clone(),
        },
    };
    
    // Get action manager
    let action_manager = core.context.get_action_manager().await
        .expect("Action manager should be initialized");
    
    // Run speed test
    let result = action_manager.dispatch(speed_test_action).await;
    
    match result {
        Ok(ActionOutput::VolumeSpeedTested { read_speed_mbps, write_speed_mbps, .. }) => {
            info!("Speed test completed: {:?} MB/s read, {:?} MB/s write", 
                  read_speed_mbps, write_speed_mbps);
            if let Some(read_speed) = read_speed_mbps {
                assert!(read_speed > 0, "Read speed should be positive");
            }
            if let Some(write_speed) = write_speed_mbps {
                assert!(write_speed > 0, "Write speed should be positive");
            }
        }
        Ok(_) => panic!("Unexpected action output"),
        Err(e) => {
            // Speed test might fail on some volumes (e.g., read-only)
            info!("Speed test failed (expected for some volumes): {:?}", e);
        }
    }
    
    info!("Volume speed test completed");
}

#[tokio::test]
async fn test_volume_types_and_properties() {
    let _ = tracing_subscriber::fmt::try_init();
    
    let data_dir = tempdir().unwrap();
    let data_path = data_dir.path().to_path_buf();
    
    let core = Arc::new(
        Core::new_with_config(data_path.clone())
            .await
            .expect("Failed to create core"),
    );
    
    // Get all volumes
    let volumes = core.volumes.get_all_volumes().await;
    
    info!("Testing {} volumes for type detection", volumes.len());
    
    // Categorize volumes by type
    let mut system_count = 0;
    let mut external_count = 0;
    let mut network_count = 0;
    
    for volume in &volumes {
        match volume.mount_type {
            MountType::System => {
                system_count += 1;
                // System volumes should be mounted and have valid paths
                assert!(volume.is_mounted, "System volume should be mounted");
                assert!(volume.mount_point.exists(), "System volume mount point should exist");
            }
            MountType::External => {
                external_count += 1;
                // External volumes might or might not be mounted
                info!("External volume '{}' mounted: {}", volume.name, volume.is_mounted);
            }
            MountType::Network => {
                network_count += 1;
                // Network volumes have special properties
                info!("Network volume '{}' detected", volume.name);
            }
            MountType::Virtual => {
                // Virtual volumes (like Docker volumes)
                info!("Virtual volume '{}' detected", volume.name);
            }
        }
        
        // All volumes should have valid fingerprints
        assert!(!volume.fingerprint.0.is_empty(), "Volume fingerprint should not be empty");
        
        // All volumes should have capacity info
        assert!(volume.total_bytes_capacity > 0, "Volume should have capacity");
    }
    
    info!("Volume types - System: {}, External: {}, Network: {}", 
          system_count, external_count, network_count);
    
    // Should have at least one system volume
    assert!(system_count > 0, "Should detect at least one system volume");
}

#[tokio::test]
async fn test_volume_tracking_persistence() {
    let _ = tracing_subscriber::fmt::try_init();
    
    let data_dir = tempdir().unwrap();
    let data_path = data_dir.path().to_path_buf();
    
    // Create core and library
    let core = Arc::new(
        Core::new_with_config(data_path.clone())
            .await
            .expect("Failed to create core"),
    );
    
    let library_path = data_path.join("libraries").join("persist-test.sdlibrary");
    let library = core
        .libraries
        .create_library(
            "Persistence Test",
            Some(library_path.clone()),
            core.context.clone(),
        )
        .await
        .expect("Failed to create library");
    
    let library_id = library.id();
    
    // Get a volume and track it
    let test_volume = core.volumes
        .get_all_volumes()
        .await
        .into_iter()
        .find(|v| !matches!(v.mount_type, MountType::System))
        .unwrap_or_else(|| {
            futures::executor::block_on(core.volumes.get_all_volumes())
                .first()
                .cloned()
                .unwrap()
        });
    
    let fingerprint = test_volume.fingerprint.clone();
    let custom_name = "Persisted Volume".to_string();
    
    // If already tracked (from auto-tracking), untrack first
    if core.volumes.is_volume_tracked(&library, &fingerprint).await.unwrap_or(false) {
        core.volumes
            .untrack_volume(&library, &fingerprint)
            .await
            .expect("Failed to untrack volume");
    }
    
    // Now track with custom name
    core.volumes
        .track_volume(&library, &fingerprint, Some(custom_name.clone()))
        .await
        .expect("Failed to track volume");
    
    // Get tracked volumes before closing
    let tracked_before = core.volumes
        .get_tracked_volumes(&library)
        .await
        .expect("Failed to get tracked volumes");
    
    let volume_count_before = tracked_before.len();
    
    info!("Tracked {} volumes before closing library", volume_count_before);
    
    // Get library path and clone it before closing
    let saved_library_path = library.path().to_path_buf();
    
    // Close the library
    core.libraries
        .close_library(library_id)
        .await
        .expect("Failed to close library");
    
    // Drop the library reference to ensure it's fully released
    drop(library);
    
    // Shutdown core
    drop(core);
    
    // Create new core instance
    let core2 = Arc::new(
        Core::new_with_config(data_path.clone())
            .await
            .expect("Failed to create second core"),
    );
    
    // Reopen the library
    let library2 = core2
        .libraries
        .open_library_with_context(&saved_library_path, core2.context.clone())
        .await
        .expect("Failed to reopen library");
    
    // Get tracked volumes after reopening
    let tracked_after = core2.volumes
        .get_tracked_volumes(&library2)
        .await
        .expect("Failed to get tracked volumes");
    
    // Verify persistence
    assert_eq!(tracked_after.len(), volume_count_before, 
        "Volume tracking should persist across library reopening");
    
    // Find our specific volume
    let persisted_volume = tracked_after.iter()
        .find(|v| v.fingerprint == fingerprint);
    
    if let Some(vol) = persisted_volume {
        assert_eq!(vol.display_name, Some(custom_name),
            "Custom volume name should persist");
    }
    
    info!("Volume tracking persistence test completed");
}

#[tokio::test]
async fn test_volume_tracking_edge_cases() {
    let _ = tracing_subscriber::fmt::try_init();
    
    let data_dir = tempdir().unwrap();
    let data_path = data_dir.path().to_path_buf();
    
    let core = Arc::new(
        Core::new_with_config(data_path.clone())
            .await
            .expect("Failed to create core"),
    );
    
    let library = core
        .libraries
        .create_library(
            "Edge Case Test",
            Some(data_path.join("libraries").join("edge-test")),
            core.context.clone(),
        )
        .await
        .expect("Failed to create library");
    
    let library_id = library.id();
    
    // Get a volume for testing
    let test_volume = core.volumes
        .get_all_volumes()
        .await
        .first()
        .cloned()
        .expect("No volumes available");
    
    let fingerprint = test_volume.fingerprint.clone();
    
    // Get action manager
    let action_manager = core.context.get_action_manager().await
        .expect("Action manager should be initialized");
    
    // Ensure volume is not tracked
    if core.volumes.is_volume_tracked(&library, &fingerprint).await.unwrap_or(false) {
        let untrack_action = Action::VolumeUntrack {
            action: VolumeUntrackAction {
                fingerprint: fingerprint.clone(),
                library_id,
            },
        };
        action_manager.dispatch(untrack_action).await.ok();
    }
    
    // Test 1: Track with empty name
    info!("Testing tracking with empty name...");
    {
        let track_action = Action::VolumeTrack {
            action: VolumeTrackAction {
                fingerprint: fingerprint.clone(),
                library_id,
                name: Some("".to_string()),
            },
        };
        
        let result = action_manager.dispatch(track_action).await;
        assert!(result.is_ok(), "Should handle empty name");
        
        // Untrack for next test
        let untrack_action = Action::VolumeUntrack {
            action: VolumeUntrackAction {
                fingerprint: fingerprint.clone(),
                library_id,
            },
        };
        action_manager.dispatch(untrack_action).await.ok();
    }
    
    // Test 2: Track with None name
    info!("Testing tracking with None name...");
    {
        let track_action = Action::VolumeTrack {
            action: VolumeTrackAction {
                fingerprint: fingerprint.clone(),
                library_id,
                name: None,
            },
        };
        
        let result = action_manager.dispatch(track_action).await;
        assert!(result.is_ok(), "Should handle None name");
        
        // Verify it uses the volume's default name
        let tracked = core.volumes
            .get_tracked_volumes(&library)
            .await
            .expect("Failed to get tracked volumes")
            .into_iter()
            .find(|v| v.fingerprint == fingerprint)
            .expect("Volume should be tracked");
        
        assert!(tracked.display_name.is_none() || tracked.display_name == Some(test_volume.name.clone()),
            "Should use default name when None provided");
    }
    
    info!("Volume edge cases test completed");
}

#[tokio::test]
async fn test_volume_refresh_and_detection() {
    let _ = tracing_subscriber::fmt::try_init();
    
    let data_dir = tempdir().unwrap();
    let data_path = data_dir.path().to_path_buf();
    
    let core = Arc::new(
        Core::new_with_config(data_path.clone())
            .await
            .expect("Failed to create core"),
    );
    
    // Get initial volume count
    let initial_volumes = core.volumes.get_all_volumes().await;
    let initial_count = initial_volumes.len();
    
    info!("Initial volume count: {}", initial_count);
    
    // Refresh volumes
    core.volumes
        .refresh_volumes()
        .await
        .expect("Failed to refresh volumes");
    
    // Get volumes after refresh
    let refreshed_volumes = core.volumes.get_all_volumes().await;
    let refreshed_count = refreshed_volumes.len();
    
    info!("Volume count after refresh: {}", refreshed_count);
    
    // Volume count should remain consistent
    assert_eq!(initial_count, refreshed_count, 
        "Volume count should be consistent after refresh");
    
    // Verify all volumes have valid properties
    for volume in &refreshed_volumes {
        assert!(!volume.fingerprint.0.is_empty(), "Fingerprint should not be empty");
        assert!(!volume.name.is_empty(), "Volume name should not be empty");
        assert!(volume.total_bytes_capacity > 0, "Capacity should be positive");
        
        // Verify mount points exist for mounted volumes
        if volume.is_mounted {
            assert!(volume.mount_point.exists(), 
                "Mount point should exist for mounted volume '{}'", volume.name);
        }
    }
    
    info!("Volume refresh and detection test completed");
}

#[tokio::test]
async fn test_volume_monitor_service() {
    let _ = tracing_subscriber::fmt::try_init();
    
    let data_dir = tempdir().unwrap();
    let data_path = data_dir.path().to_path_buf();
    
    let core = Arc::new(
        Core::new_with_config(data_path.clone())
            .await
            .expect("Failed to create core"),
    );
    
    // Create a library
    let library = core
        .libraries
        .create_library(
            "Monitor Test",
            Some(data_path.join("libraries").join("monitor-test")),
            core.context.clone(),
        )
        .await
        .expect("Failed to create library");
    
    // Get a volume to track
    let test_volume = core.volumes
        .get_all_volumes()
        .await
        .first()
        .cloned()
        .expect("No volumes available");
    
    let fingerprint = test_volume.fingerprint.clone();
    
    // Track the volume
    if !core.volumes.is_volume_tracked(&library, &fingerprint).await.unwrap_or(false) {
        core.volumes
            .track_volume(&library, &fingerprint, Some("Monitored Volume".to_string()))
            .await
            .expect("Failed to track volume");
    }
    
    // Volume monitor service is already initialized by Core
    // Just verify it's working by manually triggering updates
    
    // The volume monitor may already be running from Core initialization
    // We'll just work with the existing state
    
    info!("Volume monitor service started");
    
    // Wait a bit for the monitor to run
    tokio::time::sleep(tokio::time::Duration::from_secs(2)).await;
    
    // Get tracked volume state
    let tracked_before = core.volumes
        .get_tracked_volumes(&library)
        .await
        .expect("Failed to get tracked volumes")
        .into_iter()
        .find(|v| v.fingerprint == fingerprint)
        .expect("Volume should be tracked");
    
    let initial_last_seen = tracked_before.last_seen_at;
    
    // Wait for monitor to update (monitor runs every 30s by default, but we'll trigger a refresh)
    core.volumes
        .refresh_volumes()
        .await
        .expect("Failed to refresh volumes");
    
    // Manually trigger an update to simulate monitor behavior
    core.volumes
        .update_tracked_volume_state(&library, &fingerprint, &test_volume)
        .await
        .expect("Failed to update volume state");
    
    // Get updated state
    let tracked_after = core.volumes
        .get_tracked_volumes(&library)
        .await
        .expect("Failed to get tracked volumes")
        .into_iter()
        .find(|v| v.fingerprint == fingerprint)
        .expect("Volume should be tracked");
    
    // Verify the monitor would update the state
    assert!(tracked_after.last_seen_at >= initial_last_seen,
        "Volume monitor should update last_seen_at");
    
    // Don't stop the monitor as it's managed by Core
    
    info!("Volume monitor service test completed");
}```

## tests/file_transfer_test.rs

```rust
//! Core file transfer test using the new cargo test subprocess framework
//!
//! This test demonstrates cross-device file sharing functionality where Alice
//! (sender) pairs with Bob (receiver) and transfers multiple test files.

use sd_core_new::test_framework::CargoTestRunner;
use sd_core_new::Core;
use std::env;
use std::path::PathBuf;
use std::time::Duration;
use tokio::time::timeout;

/// Alice's file transfer scenario - sender role
#[tokio::test]
#[ignore] // Only run when explicitly called via subprocess
async fn alice_file_transfer_scenario() {
	// Exit early if not running as Alice
	if env::var("TEST_ROLE").unwrap_or_default() != "alice" {
		return;
	}

	// Set test directory for file-based discovery
	env::set_var("SPACEDRIVE_TEST_DIR", "/tmp/spacedrive-file-transfer-test");

	let data_dir = PathBuf::from("/tmp/spacedrive-file-transfer-test/alice");
	let device_name = "Alice's Test Device";

	println!("ðŸŸ¦ Alice: Starting Core file transfer test (sender)");
	println!("ðŸ“ Alice: Data dir: {:?}", data_dir);

	// Initialize Core
	println!("ðŸ”§ Alice: Initializing Core...");
	let mut core = timeout(
		Duration::from_secs(10),
		Core::new_with_config(data_dir.clone()),
	)
	.await
	.unwrap()
	.unwrap();
	println!("âœ… Alice: Core initialized successfully");

	// Set device name
	println!("ðŸ·ï¸ Alice: Setting device name for testing...");
	core.device.set_name(device_name.to_string()).unwrap();

	// Initialize networking
	println!("ðŸŒ Alice: Initializing networking...");
	timeout(Duration::from_secs(10), core.init_networking())
		.await
		.unwrap()
		.unwrap();

	// Wait longer for networking to fully initialize and detect external addresses
	tokio::time::sleep(Duration::from_secs(3)).await;
	println!("âœ… Alice: Networking initialized successfully");

	// Create a library for job dispatch (required for file transfers)
	println!("ðŸ“š Alice: Creating library for file transfer jobs...");
	let _library = core
		.libraries
		.create_library("Alice Transfer Library", None, core.context.clone())
		.await
		.unwrap();
	println!("âœ… Alice: Library created successfully");

	// Start pairing as initiator
	println!("ðŸ”‘ Alice: Starting pairing as initiator for file transfer...");
	let (pairing_code, expires_in) = if let Some(networking) = core.networking() {
		timeout(
			Duration::from_secs(15),
			networking.start_pairing_as_initiator(),
		)
		.await
		.unwrap()
		.unwrap()
	} else {
		panic!("Networking not initialized");
	};

	let short_code = pairing_code
		.split_whitespace()
		.take(3)
		.collect::<Vec<_>>()
		.join(" ");
	println!(
		"âœ… Alice: Pairing code generated: {}... (expires in {}s)",
		short_code, expires_in
	);

	// Write pairing code to shared location for Bob to read
	std::fs::create_dir_all("/tmp/spacedrive-file-transfer-test").unwrap();
	std::fs::write(
		"/tmp/spacedrive-file-transfer-test/pairing_code.txt",
		&pairing_code,
	)
	.unwrap();
	println!(
		"ðŸ“ Alice: Pairing code written to /tmp/spacedrive-file-transfer-test/pairing_code.txt"
	);

	// Wait for pairing completion
	println!("â³ Alice: Waiting for Bob to connect...");
	let mut receiver_device_id = None;
	let mut attempts = 0;
	let max_attempts = 45; // 45 seconds

	loop {
		tokio::time::sleep(Duration::from_secs(1)).await;

		let connected_devices = core.get_connected_devices().await.unwrap();
		if !connected_devices.is_empty() {
			receiver_device_id = Some(connected_devices[0]);
			println!(
				"ðŸŽ‰ Alice: Bob connected! Device ID: {}",
				connected_devices[0]
			);

			// Wait a bit longer to ensure session keys are properly established
			println!("ðŸ”‘ Alice: Allowing extra time for session key establishment...");
			tokio::time::sleep(Duration::from_secs(2)).await;
			break;
		}

		// Also check if there are any paired devices (even if not currently connected)
		if let Some(networking) = core.networking() {
			let device_registry = networking.device_registry();
			let registry = device_registry.read().await;
			let paired_devices = registry.get_paired_devices();
			if !paired_devices.is_empty() {
				println!("ðŸŽ‰ Alice: Found {} paired devices!", paired_devices.len());
				for device in &paired_devices {
					println!(
						"  ðŸ“± Paired: {} (ID: {})",
						device.device_name, device.device_id
					);
				}
				// Use the first paired device as the receiver
				receiver_device_id = Some(paired_devices[0].device_id);
				println!(
					"ðŸ”‘ Alice: Using paired device as receiver: {}",
					paired_devices[0].device_id
				);
				break;
			}
		}

		attempts += 1;
		if attempts >= max_attempts {
			panic!("Alice: Pairing timeout - Bob not connected");
		}

		if attempts % 5 == 0 {
			println!("ðŸ” Alice: Pairing status check {} - waiting", attempts / 5);
		}
	}

	let receiver_id = receiver_device_id.unwrap();

	// Create test files to transfer
	println!("ðŸ“ Alice: Creating test files for transfer...");
	let test_files_dir = data_dir.join("test_files");
	std::fs::create_dir_all(&test_files_dir).unwrap();

	let medium_content = "A".repeat(1024);
	let test_files = vec![
		("small_file.txt", "Hello from Alice's device!"),
		("medium_file.txt", medium_content.as_str()), // 1KB file
		(
			"metadata_test.json",
			r#"{"test": "file", "size": "medium", "purpose": "cross-device-transfer"}"#,
		),
	];

	let mut source_paths = Vec::new();
	for (filename, content) in &test_files {
		let file_path = test_files_dir.join(filename);
		std::fs::write(&file_path, content).unwrap();

		// Generate and display checksum for the file Alice is about to send
		match sd_core_new::domain::content_identity::ContentHashGenerator::generate_content_hash(
			&file_path,
		)
		.await
		{
			Ok(checksum) => {
				println!(
					"  ðŸ“„ Created: {} ({} bytes, checksum: {})",
					filename,
					content.len(),
					&checksum[..32]
				); // Show first 32 chars
			}
			Err(e) => {
				println!(
					"  ðŸ“„ Created: {} ({} bytes, checksum error: {})",
					filename,
					content.len(),
					e
				);
			}
		}

		source_paths.push(file_path);
	}

	// Write file list for Bob to expect
	let file_list: Vec<String> = test_files
		.iter()
		.map(|(name, content)| format!("{}:{}", name, content.len()))
		.collect();
	std::fs::write(
		"/tmp/spacedrive-file-transfer-test/expected_files.txt",
		file_list.join("\n"),
	)
	.unwrap();

	// Debug: Show Alice's view of connected devices
	let alice_devices = core.get_connected_devices_info().await.unwrap();
	println!("ðŸ” Alice: Connected devices before transfer:");
	for device in &alice_devices {
		println!(
			"  ðŸ“± Device: {} (ID: {})",
			device.device_name, device.device_id
		);
	}

	// Initiate cross-device file transfer
	println!("ðŸš€ Alice: Starting cross-device file transfer...");
	println!("ðŸŽ¯ Alice: Sending files to device ID: {}", receiver_id);

	let transfer_results = core
		.services
		.file_sharing
		.share_with_device(
			source_paths,
			receiver_id,
			Some(PathBuf::from("/tmp/received_files")),
		)
		.await;

	match transfer_results {
		Ok(transfer_id) => {
			println!("âœ… Alice: File transfer initiated successfully!");
			println!("ðŸ“‹ Alice: Transfer ID: {:?}", transfer_id);

			// Wait for transfer to complete
			println!("â³ Alice: Waiting for transfer to complete...");
			let mut completed = false;
			for _ in 0..30 {
				// Wait up to 30 seconds
				tokio::time::sleep(Duration::from_secs(1)).await;

				match core
					.services
					.file_sharing
					.get_transfer_status(&transfer_id)
					.await
				{
					Ok(status) => {
						match status.state {
							sd_core_new::services::file_sharing::TransferState::Completed => {
								println!(
									"âœ… Alice: Transfer {:?} completed successfully",
									transfer_id
								);
								completed = true;
								break;
							}
							sd_core_new::services::file_sharing::TransferState::Failed => {
								println!(
									"âŒ Alice: Transfer {:?} failed: {:?}",
									transfer_id, status.error
								);
								completed = false;
								break;
							}
							_ => {
								// Still in progress
								if status.progress.bytes_transferred > 0 {
									println!(
										"ðŸ“Š Alice: Transfer progress: {} / {} bytes",
										status.progress.bytes_transferred,
										status.progress.total_bytes
									);
								}
							}
						}
					}
					Err(e) => {
						println!("âš ï¸ Alice: Could not get transfer status: {}", e);
					}
				}
			}

			if completed {
				println!(
					"âœ… Alice: All transfers completed, now waiting for Bob's confirmation..."
				);

				// Wait for Bob to confirm receipt and verification
				let mut bob_confirmed = false;
				for attempt in 1..=60 {
					// Wait up to 60 seconds for Bob's confirmation
					if std::fs::read_to_string(
						"/tmp/spacedrive-file-transfer-test/bob_received_confirmation.txt",
					)
					.map(|content| content.starts_with("received_and_verified:"))
					.unwrap_or(false)
					{
						println!("âœ… Alice: Bob confirmed file receipt and verification!");
						bob_confirmed = true;
						break;
					}

					if attempt % 10 == 0 {
						println!(
							"ðŸ” Alice: Still waiting for Bob's confirmation... ({}s)",
							attempt
						);
					}
					tokio::time::sleep(Duration::from_secs(1)).await;
				}

				if bob_confirmed {
					println!("FILE_TRANSFER_SUCCESS: Alice completed all file transfers and Bob confirmed receipt");
					// Write success marker for orchestrator to detect
					std::fs::write(
						"/tmp/spacedrive-file-transfer-test/alice_success.txt",
						"success",
					)
					.unwrap();
				} else {
					panic!("Alice: Bob did not confirm file receipt within timeout");
				}
			} else {
				println!(
					"âš ï¸ Alice: Transfer {:?} did not complete in time",
					transfer_id
				);
				panic!("Alice: File transfer did not complete in time");
			}
		}
		Err(e) => {
			println!("âŒ Alice: File transfer failed: {}", e);
			panic!("Alice: File transfer initiation failed: {}", e);
		}
	}

	println!("ðŸ§¹ Alice: File transfer sender test completed");
}

/// Bob's file transfer scenario - receiver role
#[tokio::test]
#[ignore] // Only run when explicitly called via subprocess
async fn bob_file_transfer_scenario() {
	// Exit early if not running as Bob
	if env::var("TEST_ROLE").unwrap_or_default() != "bob" {
		return;
	}

	// Set test directory for file-based discovery
	env::set_var("SPACEDRIVE_TEST_DIR", "/tmp/spacedrive-file-transfer-test");

	let data_dir = PathBuf::from("/tmp/spacedrive-file-transfer-test/bob");
	let device_name = "Bob's Test Device";

	println!("ðŸŸ¦ Bob: Starting Core file transfer test (receiver)");
	println!("ðŸ“ Bob: Data dir: {:?}", data_dir);

	// Initialize Core
	println!("ðŸ”§ Bob: Initializing Core...");
	let mut core = timeout(Duration::from_secs(10), Core::new_with_config(data_dir))
		.await
		.unwrap()
		.unwrap();
	println!("âœ… Bob: Core initialized successfully");

	// Set device name
	println!("ðŸ·ï¸ Bob: Setting device name for testing...");
	core.device.set_name(device_name.to_string()).unwrap();

	// Initialize networking
	println!("ðŸŒ Bob: Initializing networking...");
	timeout(Duration::from_secs(10), core.init_networking())
		.await
		.unwrap()
		.unwrap();

	// Wait longer for networking to fully initialize and detect external addresses
	tokio::time::sleep(Duration::from_secs(3)).await;
	println!("âœ… Bob: Networking initialized successfully");

	// Create a library for job dispatch (required for file transfers)
	println!("ðŸ“š Bob: Creating library for file transfer jobs...");
	let _library = core
		.libraries
		.create_library("Bob Transfer Library", None, core.context.clone())
		.await
		.unwrap();
	println!("âœ… Bob: Library created successfully");

	// Wait for Alice to create pairing code
	println!("ðŸ” Bob: Looking for pairing code from Alice...");
	let pairing_code = loop {
		if let Ok(code) =
			std::fs::read_to_string("/tmp/spacedrive-file-transfer-test/pairing_code.txt")
		{
			break code.trim().to_string();
		}
		tokio::time::sleep(Duration::from_millis(500)).await;
	};
	println!("ðŸ“‹ Bob: Found pairing code");

	// Join pairing session
	println!("ðŸ¤ Bob: Joining pairing with Alice...");
	if let Some(networking) = core.networking() {
		timeout(
			Duration::from_secs(15),
			networking.start_pairing_as_joiner(&pairing_code),
		)
		.await
		.unwrap()
		.unwrap();
	} else {
		panic!("Networking not initialized");
	}
	println!("âœ… Bob: Successfully joined pairing");

	// Wait for pairing completion
	println!("â³ Bob: Waiting for pairing to complete...");
	let mut attempts = 0;
	let max_attempts = 30; // 30 seconds

	loop {
		tokio::time::sleep(Duration::from_secs(1)).await;

		// Check pairing status by looking at connected devices
		let connected_devices = core.get_connected_devices().await.unwrap();
		if !connected_devices.is_empty() {
			println!("ðŸŽ‰ Bob: Pairing completed successfully!");
			println!("âœ… Bob: Connected {} devices", connected_devices.len());

			// Debug: Show Bob's view of connected devices
			let bob_devices = core.get_connected_devices_info().await.unwrap();
			println!("ðŸ” Bob: Connected devices after pairing:");
			for device in &bob_devices {
				println!(
					"  ðŸ“± Device: {} (ID: {})",
					device.device_name, device.device_id
				);
			}

			// Wait a bit longer to ensure session keys are properly established
			println!("ðŸ”‘ Bob: Allowing extra time for session key establishment...");
			tokio::time::sleep(Duration::from_secs(2)).await;
			break;
		}

		// Also check if there are any paired devices (even if not currently connected)
		if let Some(networking) = core.networking() {
			let device_registry = networking.device_registry();
			let registry = device_registry.read().await;
			let paired_devices = registry.get_paired_devices();
			if !paired_devices.is_empty() {
				println!("ðŸŽ‰ Bob: Found {} paired devices!", paired_devices.len());
				for device in &paired_devices {
					println!(
						"  ðŸ“± Paired: {} (ID: {})",
						device.device_name, device.device_id
					);
				}
				// Even if not showing as "connected", we have paired devices, so pairing worked
				break;
			}
		}

		attempts += 1;
		if attempts >= max_attempts {
			panic!("Bob: Pairing timeout - no devices connected");
		}

		if attempts % 5 == 0 {
			println!("ðŸ” Bob: Pairing status check {} - waiting", attempts / 5);
		}
	}

	// Wait for file transfers
	println!("â³ Bob: Waiting for file transfers...");

	// Create directory for received files
	let received_dir = std::path::Path::new("/tmp/received_files");
	std::fs::create_dir_all(received_dir).unwrap();
	println!(
		"ðŸ“ Bob: Created directory for received files: {:?}",
		received_dir
	);

	// Wait for expected files to arrive
	let expected_files = loop {
		if let Ok(content) =
			std::fs::read_to_string("/tmp/spacedrive-file-transfer-test/expected_files.txt")
		{
			break content
				.lines()
				.map(|line| {
					let parts: Vec<&str> = line.split(':').collect();
					(parts[0].to_string(), parts[1].parse::<usize>().unwrap_or(0))
				})
				.collect::<Vec<(String, usize)>>();
		}
		tokio::time::sleep(Duration::from_millis(500)).await;
	};

	println!(
		"ðŸ“‹ Bob: Expecting {} files to be received",
		expected_files.len()
	);
	for (filename, size) in &expected_files {
		println!("  ðŸ“„ Expecting: {} ({} bytes)", filename, size);
	}

	// Monitor for received files
	let mut received_files = Vec::new();
	let start_time = std::time::Instant::now();
	let timeout_duration = Duration::from_secs(60); // 1 minute timeout

	while received_files.len() < expected_files.len() && start_time.elapsed() < timeout_duration {
		tokio::time::sleep(Duration::from_secs(1)).await;

		// Check for new files in received directory
		if let Ok(entries) = std::fs::read_dir(received_dir) {
			for entry in entries {
				if let Ok(entry) = entry {
					let filename = entry.file_name().to_string_lossy().to_string();
					if !received_files.contains(&filename) {
						if let Ok(metadata) = entry.metadata() {
							received_files.push(filename.clone());
							println!(
								"ðŸ“¥ Bob: Received file: {} ({} bytes)",
								filename,
								metadata.len()
							);
						}
					}
				}
			}
		}

		// Debug: Show directory contents periodically
		let elapsed = start_time.elapsed().as_secs();
		if elapsed > 0 && elapsed % 10 == 0 && received_files.is_empty() {
			println!("ðŸ” Bob: Still waiting for files... checking directory:");
			if let Ok(entries) = std::fs::read_dir(received_dir) {
				let file_count = entries.count();
				println!(
					"  ðŸ“ Found {} items in {}",
					file_count,
					received_dir.display()
				);
			}
		}

		if received_files.len() > 0 && received_files.len() % 2 == 0 {
			println!(
				"ðŸ“Š Bob: Progress: {}/{} files received",
				received_files.len(),
				expected_files.len()
			);
		}
	}

	// Verify all expected files were received
	if received_files.len() == expected_files.len() {
		println!("âœ… Bob: All expected files received successfully!");

		// Verify file contents and checksums
		let mut verification_success = true;
		for (expected_name, expected_size) in &expected_files {
			let received_path = received_dir.join(expected_name);
			if received_path.exists() {
				if let Ok(metadata) = std::fs::metadata(&received_path) {
					if metadata.len() == *expected_size as u64 {
						// Generate checksum for received file
						match sd_core_new::domain::content_identity::ContentHashGenerator::generate_content_hash(&received_path).await {
							Ok(checksum) => {
								println!("âœ… Bob: Verified: {} (size: {} bytes, checksum: {})",
									expected_name, metadata.len(), &checksum[..32]); // Show first 32 chars of checksum
							}
							Err(e) => {
								println!("âš ï¸ Bob: Could not generate checksum for {}: {}", expected_name, e);
								println!("âœ… Bob: Verified: {} (size matches)", expected_name);
							}
						}
					} else {
						println!(
							"âŒ Bob: Size mismatch for {}: expected {}, got {}",
							expected_name,
							expected_size,
							metadata.len()
						);
						verification_success = false;
					}
				} else {
					println!("âŒ Bob: Could not read metadata for {}", expected_name);
					verification_success = false;
				}
			} else {
				println!("âŒ Bob: Expected file not found: {}", expected_name);
				verification_success = false;
			}
		}

		if verification_success {
			println!("FILE_TRANSFER_SUCCESS: Bob verified all received files");
			// Write success marker for orchestrator to detect
			std::fs::write(
				"/tmp/spacedrive-file-transfer-test/bob_success.txt",
				"success",
			)
			.unwrap();

			// Also write a timestamped confirmation that Alice can detect
			let timestamp = std::time::SystemTime::now()
				.duration_since(std::time::UNIX_EPOCH)
				.unwrap()
				.as_secs();
			std::fs::write(
				"/tmp/spacedrive-file-transfer-test/bob_received_confirmation.txt",
				format!("received_and_verified:{}", timestamp),
			)
			.unwrap();
			println!("âœ… Bob: Wrote confirmation signal for Alice");
		} else {
			panic!("Bob: File verification failed");
		}
	} else {
		println!(
			"âŒ Bob: Only received {}/{} expected files",
			received_files.len(),
			expected_files.len()
		);
		panic!("Bob: Not all files were received");
	}

	println!("ðŸ§¹ Bob: File transfer receiver test completed");
}

/// Main test orchestrator - spawns cargo test subprocesses for file transfer
#[tokio::test]
async fn test_file_transfer() {
	// Clean up any old test files to avoid race conditions
	let _ = std::fs::remove_dir_all("/tmp/spacedrive-file-transfer-test");
	let _ = std::fs::remove_dir_all("/tmp/received_files");
	std::fs::create_dir_all("/tmp/spacedrive-file-transfer-test").unwrap();

	println!("ðŸ§ª Testing Core file transfer with cargo test subprocess framework");

	let mut runner = CargoTestRunner::for_test_file("file_transfer_test")
		.with_timeout(Duration::from_secs(240)) // 4 minutes for file transfer test
		.add_subprocess("alice", "alice_file_transfer_scenario")
		.add_subprocess("bob", "bob_file_transfer_scenario");

	// Spawn Alice first (sender)
	println!("ðŸš€ Starting Alice as file sender...");
	runner
		.spawn_single_process("alice")
		.await
		.expect("Failed to spawn Alice");

	// Wait for Alice to initialize and generate pairing code
	tokio::time::sleep(Duration::from_secs(8)).await;

	// Start Bob as receiver
	println!("ðŸš€ Starting Bob as file receiver...");
	runner
		.spawn_single_process("bob")
		.await
		.expect("Failed to spawn Bob");

	// Run until both devices successfully complete file transfer using file markers
	let result = runner
		.wait_for_success(|_outputs| {
			let alice_success =
				std::fs::read_to_string("/tmp/spacedrive-file-transfer-test/alice_success.txt")
					.map(|content| content.trim() == "success")
					.unwrap_or(false);
			let bob_success =
				std::fs::read_to_string("/tmp/spacedrive-file-transfer-test/bob_success.txt")
					.map(|content| content.trim() == "success")
					.unwrap_or(false);

			alice_success && bob_success
		})
		.await;

	match result {
		Ok(_) => {
			println!(
				"ðŸŽ‰ Cargo test subprocess file transfer test successful with complete file verification!"
			);
		}
		Err(e) => {
			println!("âŒ Cargo test subprocess file transfer test failed: {}", e);
			for (name, output) in runner.get_all_outputs() {
				println!("\\n{} output:\\n{}", name, output);
			}
			panic!("Cargo test subprocess file transfer test failed - files were not properly transferred and verified");
		}
	}
}
```

## tests/pairing_test.rs

```rust
//! Core pairing test using the new cargo test subprocess framework
//!
//! This test demonstrates the new approach where ALL test logic remains in the test file
//! while still supporting subprocess-based testing for multi-device scenarios.

use sd_core_new::test_framework::CargoTestRunner;
use sd_core_new::Core;
use std::env;
use std::path::PathBuf;
use std::time::Duration;
use tokio::time::timeout;

/// Alice's pairing scenario - ALL logic stays in this test file!
#[tokio::test]
#[ignore] // Only run when explicitly called via subprocess
async fn alice_pairing_scenario() {
	// Exit early if not running as Alice
	if env::var("TEST_ROLE").unwrap_or_default() != "alice" {
		return;
	}

	// Set test directory for file-based discovery
	env::set_var("SPACEDRIVE_TEST_DIR", "/tmp/spacedrive-pairing-test");

	let data_dir = PathBuf::from("/tmp/spacedrive-pairing-test/alice");
	let device_name = "Alice's Test Device";

	println!("ðŸŸ¦ Alice: Starting Core pairing test");
	println!("ðŸ“ Alice: Data dir: {:?}", data_dir);

	// Initialize Core
	println!("ðŸ”§ Alice: Initializing Core...");
	let mut core = timeout(Duration::from_secs(10), Core::new_with_config(data_dir))
		.await
		.unwrap()
		.unwrap();
	println!("âœ… Alice: Core initialized successfully");

	// Set device name
	println!("ðŸ·ï¸ Alice: Setting device name for testing...");
	core.device.set_name(device_name.to_string()).unwrap();

	// Initialize networking
	println!("ðŸŒ Alice: Initializing networking...");
	timeout(Duration::from_secs(10), core.init_networking())
		.await
		.unwrap()
		.unwrap();

	// Wait longer for networking to fully initialize and detect external addresses
	tokio::time::sleep(Duration::from_secs(3)).await;
	println!("âœ… Alice: Networking initialized successfully");

	// Start pairing as initiator
	println!("ðŸ”‘ Alice: Starting pairing as initiator...");
	let (pairing_code, expires_in) = if let Some(networking) = core.networking() {
		timeout(
			Duration::from_secs(15),
			networking.start_pairing_as_initiator(),
		)
		.await
		.unwrap()
		.unwrap()
	} else {
		panic!("Networking not initialized");
	};

	let short_code = pairing_code
		.split_whitespace()
		.take(3)
		.collect::<Vec<_>>()
		.join(" ");
	println!(
		"âœ… Alice: Pairing code generated: {}... (expires in {}s)",
		short_code, expires_in
	);

	// Write pairing code to shared location for Bob to read
	std::fs::create_dir_all("/tmp/spacedrive-pairing-test").unwrap();
	std::fs::write(
		"/tmp/spacedrive-pairing-test/pairing_code.txt",
		&pairing_code,
	)
	.unwrap();
	println!("ðŸ“ Alice: Pairing code written to /tmp/spacedrive-pairing-test/pairing_code.txt");

	// Wait for pairing completion (Alice waits for Bob to connect)
	println!("â³ Alice: Waiting for pairing to complete...");
	let mut attempts = 0;
	let max_attempts = 45; // 45 seconds

	loop {
		tokio::time::sleep(Duration::from_secs(1)).await;

		let connected_devices = core.get_connected_devices().await.unwrap();
		if !connected_devices.is_empty() {
			println!("ðŸŽ‰ Alice: Pairing completed successfully!");
			println!("ðŸ”— Alice: Checking connected devices...");
			println!("âœ… Alice: Connected {} devices", connected_devices.len());

			// Get detailed device info
			let device_info = core.get_connected_devices_info().await.unwrap();
			for device in &device_info {
				println!(
					"ðŸ“± Alice sees: {} (ID: {}, OS: {}, App: {})",
					device.device_name, device.device_id, device.os_version, device.app_version
				);
			}

			println!("PAIRING_SUCCESS: Alice's Test Device connected to Bob successfully");

			// Write success marker for orchestrator to detect
			std::fs::write("/tmp/spacedrive-pairing-test/alice_success.txt", "success").unwrap();

			// Wait a bit longer to give Bob time to detect the connection before Alice exits
			println!("â³ Alice: Waiting for Bob to also detect the connection...");
			tokio::time::sleep(Duration::from_secs(5)).await;
			break;
		}

		attempts += 1;
		if attempts >= max_attempts {
			panic!("Alice: Pairing timeout - no devices connected");
		}

		if attempts % 5 == 0 {
			println!("ðŸ” Alice: Pairing status check {} - waiting", attempts / 5);
		}
	}

	println!("ðŸ§¹ Alice: Test completed");
}

/// Bob's pairing scenario - ALL logic stays in this test file!
#[tokio::test]
#[ignore] // Only run when explicitly called via subprocess
async fn bob_pairing_scenario() {
	// Exit early if not running as Bob
	if env::var("TEST_ROLE").unwrap_or_default() != "bob" {
		return;
	}

	// Set test directory for file-based discovery
	env::set_var("SPACEDRIVE_TEST_DIR", "/tmp/spacedrive-pairing-test");

	let data_dir = PathBuf::from("/tmp/spacedrive-pairing-test/bob");
	let device_name = "Bob's Test Device";

	println!("ðŸŸ¦ Bob: Starting Core pairing test");
	println!("ðŸ“ Bob: Data dir: {:?}", data_dir);

	// Initialize Core
	println!("ðŸ”§ Bob: Initializing Core...");
	let mut core = timeout(Duration::from_secs(10), Core::new_with_config(data_dir))
		.await
		.unwrap()
		.unwrap();
	println!("âœ… Bob: Core initialized successfully");

	// Set device name
	println!("ðŸ·ï¸ Bob: Setting device name for testing...");
	core.device.set_name(device_name.to_string()).unwrap();

	// Initialize networking
	println!("ðŸŒ Bob: Initializing networking...");
	timeout(Duration::from_secs(10), core.init_networking())
		.await
		.unwrap()
		.unwrap();

	// Wait longer for networking to fully initialize and detect external addresses
	tokio::time::sleep(Duration::from_secs(3)).await;
	println!("âœ… Bob: Networking initialized successfully");

	// Wait for initiator to create pairing code
	println!("ðŸ” Bob: Looking for pairing code...");
	let pairing_code = loop {
		if let Ok(code) = std::fs::read_to_string("/tmp/spacedrive-pairing-test/pairing_code.txt") {
			break code.trim().to_string();
		}
		tokio::time::sleep(Duration::from_millis(500)).await;
	};
	println!("ðŸ“‹ Bob: Found pairing code");

	// Join pairing session
	println!("ðŸ¤ Bob: Joining pairing with code...");
	if let Some(networking) = core.networking() {
		timeout(
			Duration::from_secs(15),
			networking.start_pairing_as_joiner(&pairing_code),
		)
		.await
		.unwrap()
		.unwrap();
	} else {
		panic!("Networking not initialized");
	}
	println!("âœ… Bob: Successfully joined pairing");

	// Wait for pairing completion
	println!("â³ Bob: Waiting for pairing to complete...");
	let mut attempts = 0;
	let max_attempts = 30; // 30 seconds

	loop {
		tokio::time::sleep(Duration::from_secs(1)).await;

		// Check pairing status by looking at connected devices
		let connected_devices = core.get_connected_devices().await.unwrap();
		if !connected_devices.is_empty() {
			println!("ðŸŽ‰ Bob: Pairing completed successfully!");
			println!("ðŸ”— Bob: Checking connected devices...");
			println!("âœ… Bob: Connected {} devices", connected_devices.len());

			// Get detailed device info
			let device_info = core.get_connected_devices_info().await.unwrap();
			for device in &device_info {
				println!(
					"ðŸ“± Bob sees: {} (ID: {}, OS: {}, App: {})",
					device.device_name, device.device_id, device.os_version, device.app_version
				);
			}

			println!("PAIRING_SUCCESS: Bob's Test Device connected to Alice successfully");

			// Write success marker for orchestrator to detect
			std::fs::write("/tmp/spacedrive-pairing-test/bob_success.txt", "success").unwrap();
			break;
		}

		attempts += 1;
		if attempts >= max_attempts {
			panic!("Bob: Pairing timeout - no devices connected");
		}

		if attempts % 5 == 0 {
			println!("ðŸ” Bob: Pairing status check {} - waiting", attempts / 5);
		}
	}

	println!("ðŸ§¹ Bob: Test completed");
}

/// Main test orchestrator - spawns cargo test subprocesses
#[tokio::test]
async fn test_core_pairing() {
	const PAIRING_CODE_PATH: &str = "/tmp/spacedrive-pairing-test/pairing_code.txt";

	// Clean up stale pairing code file from previous test runs
	// This prevents Bob from reading old data and fixes the file I/O race condition
	if std::path::Path::new(PAIRING_CODE_PATH).exists() {
		let _ = std::fs::remove_file(PAIRING_CODE_PATH);
		println!("ðŸ§¹ Cleaned up stale pairing code file");
	}
	println!("ðŸ§ª Testing Core pairing with cargo test subprocess framework");

	// Clean up any old pairing files to avoid race conditions
	let _ = std::fs::remove_dir_all("/tmp/spacedrive-pairing-test");
	std::fs::create_dir_all("/tmp/spacedrive-pairing-test").unwrap();

	let mut runner = CargoTestRunner::for_test_file("test_core_pairing")
		.with_timeout(Duration::from_secs(180))
		.add_subprocess("alice", "alice_pairing_scenario")
		.add_subprocess("bob", "bob_pairing_scenario");

	// Spawn Alice first
	println!("ðŸš€ Starting Alice as initiator...");
	runner
		.spawn_single_process("alice")
		.await
		.expect("Failed to spawn Alice");

	// Wait for Alice to initialize and generate pairing code
	tokio::time::sleep(Duration::from_secs(8)).await;

	// Start Bob as joiner
	println!("ðŸš€ Starting Bob as joiner...");
	runner
		.spawn_single_process("bob")
		.await
		.expect("Failed to spawn Bob");

	// Run until both devices successfully pair using file markers
	let result = runner
		.wait_for_success(|_outputs| {
			let alice_success =
				std::fs::read_to_string("/tmp/spacedrive-pairing-test/alice_success.txt")
					.map(|content| content.trim() == "success")
					.unwrap_or(false);
			let bob_success =
				std::fs::read_to_string("/tmp/spacedrive-pairing-test/bob_success.txt")
					.map(|content| content.trim() == "success")
					.unwrap_or(false);

			alice_success && bob_success
		})
		.await;

	match result {
		Ok(_) => {
			println!(
				"ðŸŽ‰ Cargo test subprocess pairing test successful with mutual device recognition!"
			);
		}
		Err(e) => {
			println!("âŒ Cargo test subprocess pairing test failed: {}", e);
			for (name, output) in runner.get_all_outputs() {
				println!("\\n{} output:\\n{}", name, output);
			}
			panic!("Cargo test subprocess pairing test failed - devices did not properly recognize each other");
		}
	}
}
```

## tests/helpers/test_volumes.rs

```rust
//! Cross-platform test volume creation utilities
//!
//! This module provides platform-specific implementations for creating
//! temporary volumes for testing purposes.

use anyhow::{anyhow, Context, Result};
use std::path::PathBuf;
use std::sync::Arc;
use tokio::sync::Mutex;
use tracing::{debug, info};

/// Supported filesystems for test volumes
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum TestFileSystem {
    /// APFS (macOS)
    Apfs,
    /// HFS+ (macOS)
    HfsPlus,
    /// NTFS (Windows)
    Ntfs,
    /// FAT32 (cross-platform)
    Fat32,
    /// ExFAT (cross-platform)
    ExFat,
    /// ext4 (Linux)
    Ext4,
    /// Platform default
    Default,
}

impl TestFileSystem {
    /// Get the filesystem string for the current platform
    pub fn to_platform_string(&self) -> &'static str {
        match self {
            TestFileSystem::Apfs => "APFS",
            TestFileSystem::HfsPlus => "HFS+",
            TestFileSystem::Ntfs => "NTFS",
            TestFileSystem::Fat32 => "FAT32",
            TestFileSystem::ExFat => "ExFAT",
            TestFileSystem::Ext4 => "ext4",
            TestFileSystem::Default => {
                #[cfg(target_os = "macos")]
                return "APFS";
                #[cfg(target_os = "windows")]
                return "NTFS";
                #[cfg(target_os = "linux")]
                return "ext4";
            }
        }
    }
}

/// Configuration for creating a test volume
#[derive(Debug, Clone)]
pub struct TestVolumeConfig {
    /// Volume name
    pub name: String,
    /// Size in bytes
    pub size_bytes: u64,
    /// Filesystem type
    pub filesystem: TestFileSystem,
    /// Whether to create as read-only
    pub read_only: bool,
    /// Use RAM disk if possible
    pub use_ram_disk: bool,
}

impl Default for TestVolumeConfig {
    fn default() -> Self {
        Self {
            name: format!("TestVol_{}", chrono::Utc::now().timestamp()),
            size_bytes: 100 * 1024 * 1024, // 100MB
            filesystem: TestFileSystem::Default,
            read_only: false,
            use_ram_disk: false,
        }
    }
}

/// A test volume that automatically cleans up on drop
pub struct TestVolume {
    /// Mount point of the volume
    pub mount_point: PathBuf,
    /// Volume name
    pub name: String,
    /// Platform-specific identifier
    pub(crate) platform_id: String,
    /// Cleanup function
    pub(crate) cleanup: Option<Box<dyn FnOnce() + Send>>,
}

impl TestVolume {
    /// Get the mount point
    pub fn path(&self) -> &PathBuf {
        &self.mount_point
    }
    
    /// Check if volume is mounted
    pub async fn is_mounted(&self) -> bool {
        self.mount_point.exists()
    }
}

impl Drop for TestVolume {
    fn drop(&mut self) {
        if let Some(cleanup) = self.cleanup.take() {
            cleanup();
        }
    }
}

/// Platform-agnostic test volume manager
pub struct TestVolumeManager {
    #[cfg(target_os = "macos")]
    inner: MacOSTestVolumeManager,
    #[cfg(target_os = "windows")]
    inner: WindowsTestVolumeManager,
    #[cfg(target_os = "linux")]
    inner: LinuxTestVolumeManager,
}

impl TestVolumeManager {
    /// Create a new test volume manager
    pub fn new() -> Self {
        Self {
            #[cfg(target_os = "macos")]
            inner: MacOSTestVolumeManager::new(),
            #[cfg(target_os = "windows")]
            inner: WindowsTestVolumeManager::new(),
            #[cfg(target_os = "linux")]
            inner: LinuxTestVolumeManager::new(),
        }
    }
    
    /// Create a test volume with the given configuration
    pub async fn create_volume(&self, config: TestVolumeConfig) -> Result<TestVolume> {
        self.inner.create_volume(config).await
    }
    
    /// Destroy a test volume
    pub async fn destroy_volume(&self, volume: TestVolume) -> Result<()> {
        self.inner.destroy_volume(volume).await
    }
    
    /// Check if we have required privileges for volume operations
    pub async fn check_privileges(&self) -> Result<()> {
        self.inner.check_privileges().await
    }
}

// macOS implementation
#[cfg(target_os = "macos")]
pub struct MacOSTestVolumeManager {
    temp_dir: PathBuf,
    volumes: Arc<Mutex<Vec<PathBuf>>>,
}

#[cfg(target_os = "macos")]
impl MacOSTestVolumeManager {
    pub fn new() -> Self {
        let temp_dir = std::env::temp_dir().join("spacedrive_test_volumes");
        std::fs::create_dir_all(&temp_dir).ok();
        
        Self {
            temp_dir,
            volumes: Arc::new(Mutex::new(Vec::new())),
        }
    }
    
    pub async fn create_volume(&self, config: TestVolumeConfig) -> Result<TestVolume> {
        info!("Creating test volume '{}' on macOS", config.name);
        
        let _volume_name = config.name.clone();
        let _size_mb = config.size_bytes / (1024 * 1024);
        
        if config.use_ram_disk {
            // Create RAM disk
            self.create_ram_disk(config).await
        } else {
            // Create disk image
            self.create_disk_image(config).await
        }
    }
    
    async fn create_ram_disk(&self, config: TestVolumeConfig) -> Result<TestVolume> {
        let sectors = config.size_bytes / 512;
        
        // Create RAM disk
        let output = tokio::process::Command::new("hdiutil")
            .args(&["attach", "-nomount", &format!("ram://{}", sectors)])
            .output()
            .await
            .context("Failed to create RAM disk")?;
        
        if !output.status.success() {
            return Err(anyhow!("Failed to create RAM disk: {}", 
                String::from_utf8_lossy(&output.stderr)));
        }
        
        let disk_path = String::from_utf8_lossy(&output.stdout)
            .trim()
            .to_string();
        
        debug!("Created RAM disk at {}", disk_path);
        
        // Format the disk
        let fs_type = match config.filesystem {
            TestFileSystem::Apfs => "APFS",
            TestFileSystem::HfsPlus => "HFS+",
            TestFileSystem::ExFat => "ExFAT",
            TestFileSystem::Fat32 => "FAT32",
            _ => "APFS",
        };
        
        let output = tokio::process::Command::new("diskutil")
            .args(&[
                "erasevolume",
                fs_type,
                &config.name,
                &disk_path,
            ])
            .output()
            .await
            .context("Failed to format RAM disk")?;
        
        if !output.status.success() {
            // Clean up RAM disk
            tokio::process::Command::new("hdiutil")
                .args(&["detach", &disk_path])
                .output()
                .await
                .ok();
                
            return Err(anyhow!("Failed to format RAM disk: {}", 
                String::from_utf8_lossy(&output.stderr)));
        }
        
        let mount_point = PathBuf::from(format!("/Volumes/{}", config.name));
        let disk_path_clone = disk_path.clone();
        
        Ok(TestVolume {
            mount_point,
            name: config.name,
            platform_id: disk_path,
            cleanup: Some(Box::new(move || {
                // Detach the RAM disk
                std::process::Command::new("hdiutil")
                    .args(&["detach", &disk_path_clone, "-force"])
                    .output()
                    .ok();
            })),
        })
    }
    
    async fn create_disk_image(&self, config: TestVolumeConfig) -> Result<TestVolume> {
        let dmg_path = self.temp_dir.join(format!("{}.dmg", config.name));
        let size_mb = config.size_bytes / (1024 * 1024);
        
        // Ensure temp directory exists
        tokio::fs::create_dir_all(&self.temp_dir).await?;
        
        // Create disk image
        let fs_type = match config.filesystem {
            TestFileSystem::Apfs => "APFS",
            TestFileSystem::HfsPlus => "HFS+",
            TestFileSystem::ExFat => "ExFAT",
            TestFileSystem::Fat32 => "MS-DOS FAT32",
            _ => "APFS",
        };
        
        let output = tokio::process::Command::new("hdiutil")
            .args(&[
                "create",
                "-size", &format!("{}m", size_mb),
                "-fs", fs_type,
                "-volname", &config.name,
                dmg_path.to_str().unwrap(),
            ])
            .output()
            .await
            .context("Failed to create disk image")?;
        
        if !output.status.success() {
            return Err(anyhow!("Failed to create disk image: {}", 
                String::from_utf8_lossy(&output.stderr)));
        }
        
        // Mount the disk image
        let output = tokio::process::Command::new("hdiutil")
            .args(&["attach", dmg_path.to_str().unwrap()])
            .output()
            .await
            .context("Failed to mount disk image")?;
        
        if !output.status.success() {
            // Clean up disk image
            tokio::fs::remove_file(&dmg_path).await.ok();
            return Err(anyhow!("Failed to mount disk image: {}", 
                String::from_utf8_lossy(&output.stderr)));
        }
        
        // Parse mount info from output
        let output_str = String::from_utf8_lossy(&output.stdout);
        let disk_id = output_str
            .lines()
            .find(|line| line.contains("/dev/disk"))
            .and_then(|line| line.split_whitespace().next())
            .ok_or_else(|| anyhow!("Failed to parse disk identifier"))?
            .to_string();
        
        let mount_point = PathBuf::from(format!("/Volumes/{}", config.name));
        
        // Track the volume
        {
            let mut volumes = self.volumes.lock().await;
            volumes.push(dmg_path.clone());
        }
        
        let dmg_path_clone = dmg_path.clone();
        let disk_id_clone = disk_id.clone();
        let volumes = self.volumes.clone();
        
        Ok(TestVolume {
            mount_point,
            name: config.name,
            platform_id: disk_id,
            cleanup: Some(Box::new(move || {
                // Detach the disk
                std::process::Command::new("hdiutil")
                    .args(&["detach", &disk_id_clone, "-force"])
                    .output()
                    .ok();
                
                // Remove the disk image
                std::fs::remove_file(&dmg_path_clone).ok();
                
                // Remove from tracking
                // Best effort cleanup of tracking
                drop(volumes);
            })),
        })
    }
    
    pub async fn destroy_volume(&self, mut volume: TestVolume) -> Result<()> {
        info!("Destroying test volume '{}'", volume.name);
        
        // The cleanup will be called by Drop
        if let Some(cleanup) = volume.cleanup.take() {
            cleanup();
        }
        
        Ok(())
    }
    
    pub async fn check_privileges(&self) -> Result<()> {
        // On macOS, we don't need special privileges for disk images
        Ok(())
    }
}

// Windows implementation
#[cfg(target_os = "windows")]
pub struct WindowsTestVolumeManager {
    temp_dir: PathBuf,
    volumes: Arc<Mutex<Vec<PathBuf>>>,
}

#[cfg(target_os = "windows")]
impl WindowsTestVolumeManager {
    pub fn new() -> Self {
        let temp_dir = std::env::temp_dir().join("spacedrive_test_volumes");
        std::fs::create_dir_all(&temp_dir).ok();
        
        Self {
            temp_dir,
            volumes: Arc::new(Mutex::new(Vec::new())),
        }
    }
    
    pub async fn create_volume(&self, config: TestVolumeConfig) -> Result<TestVolume> {
        info!("Creating test volume '{}' on Windows", config.name);
        
        // For Windows, we'll use VHD (Virtual Hard Disk)
        self.create_vhd(config).await
    }
    
    async fn create_vhd(&self, config: TestVolumeConfig) -> Result<TestVolume> {
        let vhd_path = self.temp_dir.join(format!("{}.vhdx", config.name));
        let size_mb = config.size_bytes / (1024 * 1024);
        
        // Ensure temp directory exists
        tokio::fs::create_dir_all(&self.temp_dir).await?;
        
        // Create VHD using PowerShell
        let script = format!(
            r#"
            $vhdPath = '{}'
            $sizeBytes = {}
            
            # Create VHD
            New-VHD -Path $vhdPath -SizeBytes $sizeBytes -Dynamic
            
            # Mount VHD
            $vhd = Mount-VHD -Path $vhdPath -PassThru
            
            # Initialize disk
            $disk = Initialize-Disk -Number $vhd.Number -PartitionStyle MBR -PassThru
            
            # Create partition
            $partition = New-Partition -DiskNumber $disk.Number -UseMaximumSize -AssignDriveLetter
            
            # Format volume
            Format-Volume -DriveLetter $partition.DriveLetter -FileSystem {} -NewFileSystemLabel '{}' -Confirm:$false
            
            # Output drive letter
            Write-Output $partition.DriveLetter
            "#,
            vhd_path.to_str().unwrap().replace('\\', "\\\\"),
            config.size_bytes,
            match config.filesystem {
                TestFileSystem::Ntfs => "NTFS",
                TestFileSystem::Fat32 => "FAT32",
                TestFileSystem::ExFat => "exFAT",
                _ => "NTFS",
            },
            config.name
        );
        
        let output = tokio::process::Command::new("powershell")
            .args(&["-NoProfile", "-Command", &script])
            .output()
            .await
            .context("Failed to create VHD")?;
        
        if !output.status.success() {
            return Err(anyhow!("Failed to create VHD: {}", 
                String::from_utf8_lossy(&output.stderr)));
        }
        
        let drive_letter = String::from_utf8_lossy(&output.stdout)
            .trim()
            .to_string();
        
        if drive_letter.is_empty() {
            return Err(anyhow!("Failed to get drive letter for VHD"));
        }
        
        let mount_point = PathBuf::from(format!("{}:\\", drive_letter));
        
        // Track the volume
        {
            let mut volumes = self.volumes.lock().await;
            volumes.push(vhd_path.clone());
        }
        
        let vhd_path_clone = vhd_path.clone();
        let volumes = self.volumes.clone();
        
        Ok(TestVolume {
            mount_point,
            name: config.name,
            platform_id: vhd_path.to_str().unwrap().to_string(),
            cleanup: Some(Box::new(move || {
                // Dismount VHD using PowerShell
                let script = format!(
                    "Dismount-VHD -Path '{}' -Confirm:$false",
                    vhd_path_clone.to_str().unwrap()
                );
                
                std::process::Command::new("powershell")
                    .args(&["-NoProfile", "-Command", &script])
                    .output()
                    .ok();
                
                // Remove the VHD file
                std::fs::remove_file(&vhd_path_clone).ok();
                
                // Remove from tracking
                // Best effort cleanup of tracking
                drop(volumes);
            })),
        })
    }
    
    pub async fn destroy_volume(&self, mut volume: TestVolume) -> Result<()> {
        info!("Destroying test volume '{}'", volume.name);
        
        // The cleanup will be called by Drop
        if let Some(cleanup) = volume.cleanup.take() {
            cleanup();
        }
        
        Ok(())
    }
    
    pub async fn check_privileges(&self) -> Result<()> {
        // Check if we're running as administrator
        let output = tokio::process::Command::new("net")
            .args(&["session"])
            .output()
            .await?;
        
        if !output.status.success() {
            return Err(anyhow!("Administrator privileges required for creating test volumes on Windows"));
        }
        
        Ok(())
    }
}

// Linux implementation
#[cfg(target_os = "linux")]
pub struct LinuxTestVolumeManager {
    temp_dir: PathBuf,
    volumes: Arc<Mutex<Vec<PathBuf>>>,
}

#[cfg(target_os = "linux")]
impl LinuxTestVolumeManager {
    pub fn new() -> Self {
        let temp_dir = std::env::temp_dir().join("spacedrive_test_volumes");
        std::fs::create_dir_all(&temp_dir).ok();
        
        Self {
            temp_dir,
            volumes: Arc::new(Mutex::new(Vec::new())),
        }
    }
    
    pub async fn create_volume(&self, config: TestVolumeConfig) -> Result<TestVolume> {
        info!("Creating test volume '{}' on Linux", config.name);
        
        if config.use_ram_disk {
            // Use tmpfs for RAM disk
            self.create_tmpfs(config).await
        } else {
            // Use loop device with file backing
            self.create_loop_device(config).await
        }
    }
    
    async fn create_tmpfs(&self, config: TestVolumeConfig) -> Result<TestVolume> {
        let mount_point = self.temp_dir.join(&config.name);
        
        // Create mount point
        tokio::fs::create_dir_all(&mount_point).await?;
        
        // Mount tmpfs
        let size_mb = config.size_bytes / (1024 * 1024);
        let output = tokio::process::Command::new("sudo")
            .args(&[
                "mount",
                "-t", "tmpfs",
                "-o", &format!("size={}M", size_mb),
                "tmpfs",
                mount_point.to_str().unwrap(),
            ])
            .output()
            .await
            .context("Failed to mount tmpfs")?;
        
        if !output.status.success() {
            tokio::fs::remove_dir(&mount_point).await.ok();
            return Err(anyhow!("Failed to mount tmpfs: {}", 
                String::from_utf8_lossy(&output.stderr)));
        }
        
        let mount_point_clone = mount_point.clone();
        
        Ok(TestVolume {
            mount_point: mount_point.clone(),
            name: config.name,
            platform_id: mount_point.to_str().unwrap().to_string(),
            cleanup: Some(Box::new(move || {
                // Unmount tmpfs
                std::process::Command::new("sudo")
                    .args(&["umount", mount_point_clone.to_str().unwrap()])
                    .output()
                    .ok();
                
                // Remove mount point
                std::fs::remove_dir(&mount_point_clone).ok();
            })),
        })
    }
    
    async fn create_loop_device(&self, config: TestVolumeConfig) -> Result<TestVolume> {
        let img_path = self.temp_dir.join(format!("{}.img", config.name));
        let mount_point = self.temp_dir.join(&config.name);
        let size_mb = config.size_bytes / (1024 * 1024);
        
        // Ensure directories exist
        tokio::fs::create_dir_all(&self.temp_dir).await?;
        tokio::fs::create_dir_all(&mount_point).await?;
        
        // Create image file
        let output = tokio::process::Command::new("dd")
            .args(&[
                "if=/dev/zero",
                &format!("of={}", img_path.to_str().unwrap()),
                "bs=1M",
                &format!("count={}", size_mb),
            ])
            .output()
            .await
            .context("Failed to create image file")?;
        
        if !output.status.success() {
            return Err(anyhow!("Failed to create image file: {}", 
                String::from_utf8_lossy(&output.stderr)));
        }
        
        // Create loop device
        let output = tokio::process::Command::new("sudo")
            .args(&["losetup", "--find", "--show", img_path.to_str().unwrap()])
            .output()
            .await
            .context("Failed to create loop device")?;
        
        if !output.status.success() {
            tokio::fs::remove_file(&img_path).await.ok();
            return Err(anyhow!("Failed to create loop device: {}", 
                String::from_utf8_lossy(&output.stderr)));
        }
        
        let loop_device = String::from_utf8_lossy(&output.stdout)
            .trim()
            .to_string();
        
        // Format the loop device
        let fs_type = match config.filesystem {
            TestFileSystem::Ext4 => "ext4",
            TestFileSystem::Fat32 => "vfat",
            TestFileSystem::ExFat => "exfat",
            _ => "ext4",
        };
        
        let mkfs_cmd = match fs_type {
            "ext4" => "mkfs.ext4",
            "vfat" => "mkfs.vfat",
            "exfat" => "mkfs.exfat",
            _ => "mkfs.ext4",
        };
        
        let mut args = vec![mkfs_cmd];
        if fs_type == "ext4" {
            args.push("-L");
            args.push(&config.name);
        }
        args.push(&loop_device);
        
        let output = tokio::process::Command::new("sudo")
            .args(&args)
            .output()
            .await
            .context("Failed to format loop device")?;
        
        if !output.status.success() {
            // Clean up loop device
            tokio::process::Command::new("sudo")
                .args(&["losetup", "-d", &loop_device])
                .output()
                .await
                .ok();
            tokio::fs::remove_file(&img_path).await.ok();
            return Err(anyhow!("Failed to format loop device: {}", 
                String::from_utf8_lossy(&output.stderr)));
        }
        
        // Mount the loop device
        let output = tokio::process::Command::new("sudo")
            .args(&[
                "mount",
                &loop_device,
                mount_point.to_str().unwrap(),
            ])
            .output()
            .await
            .context("Failed to mount loop device")?;
        
        if !output.status.success() {
            // Clean up
            tokio::process::Command::new("sudo")
                .args(&["losetup", "-d", &loop_device])
                .output()
                .await
                .ok();
            tokio::fs::remove_file(&img_path).await.ok();
            tokio::fs::remove_dir(&mount_point).await.ok();
            return Err(anyhow!("Failed to mount loop device: {}", 
                String::from_utf8_lossy(&output.stderr)));
        }
        
        // Track the volume
        {
            let mut volumes = self.volumes.lock().await;
            volumes.push(img_path.clone());
        }
        
        let loop_device_clone = loop_device.clone();
        let mount_point_clone = mount_point.clone();
        let img_path_clone = img_path.clone();
        let volumes = self.volumes.clone();
        
        Ok(TestVolume {
            mount_point: mount_point.clone(),
            name: config.name,
            platform_id: loop_device,
            cleanup: Some(Box::new(move || {
                // Unmount
                std::process::Command::new("sudo")
                    .args(&["umount", mount_point_clone.to_str().unwrap()])
                    .output()
                    .ok();
                
                // Detach loop device
                std::process::Command::new("sudo")
                    .args(&["losetup", "-d", &loop_device_clone])
                    .output()
                    .ok();
                
                // Remove files
                std::fs::remove_file(&img_path_clone).ok();
                std::fs::remove_dir(&mount_point_clone).ok();
                
                // Remove from tracking
                // Best effort cleanup of tracking
                drop(volumes);
            })),
        })
    }
    
    pub async fn destroy_volume(&self, mut volume: TestVolume) -> Result<()> {
        info!("Destroying test volume '{}'", volume.name);
        
        // The cleanup will be called by Drop
        if let Some(cleanup) = volume.cleanup.take() {
            cleanup();
        }
        
        Ok(())
    }
    
    pub async fn check_privileges(&self) -> Result<()> {
        // Check if we can use sudo
        let output = tokio::process::Command::new("sudo")
            .args(&["-n", "true"])
            .output()
            .await?;
        
        if !output.status.success() {
            return Err(anyhow!("sudo privileges required for creating test volumes on Linux"));
        }
        
        Ok(())
    }
}

/// Builder for creating test volumes with specific configurations
pub struct TestVolumeBuilder {
    config: TestVolumeConfig,
}

impl TestVolumeBuilder {
    /// Create a new test volume builder
    pub fn new(name: impl Into<String>) -> Self {
        Self {
            config: TestVolumeConfig {
                name: name.into(),
                ..Default::default()
            },
        }
    }
    
    /// Set the volume size in bytes
    pub fn size_bytes(mut self, size: u64) -> Self {
        self.config.size_bytes = size;
        self
    }
    
    /// Set the volume size in megabytes
    pub fn size_mb(self, size_mb: u64) -> Self {
        self.size_bytes(size_mb * 1024 * 1024)
    }
    
    /// Set the volume size in gigabytes
    pub fn size_gb(self, size_gb: u64) -> Self {
        self.size_bytes(size_gb * 1024 * 1024 * 1024)
    }
    
    /// Set the filesystem type
    pub fn filesystem(mut self, fs: TestFileSystem) -> Self {
        self.config.filesystem = fs;
        self
    }
    
    /// Make the volume read-only
    pub fn read_only(mut self) -> Self {
        self.config.read_only = true;
        self
    }
    
    /// Use RAM disk if available
    pub fn use_ram_disk(mut self) -> Self {
        self.config.use_ram_disk = true;
        self
    }
    
    /// Build and create the test volume
    pub async fn build(self) -> Result<TestVolume> {
        let manager = TestVolumeManager::new();
        manager.create_volume(self.config).await
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[tokio::test]
    async fn test_volume_manager_creation() {
        let manager = TestVolumeManager::new();
        
        // Just check that we can create a manager
        // Actual volume creation tests might require privileges
        assert!(manager.check_privileges().await.is_ok() || 
                manager.check_privileges().await.is_err());
    }
    
    #[tokio::test]
    async fn test_volume_builder() {
        let config = TestVolumeBuilder::new("TestVol")
            .size_mb(50)
            .filesystem(TestFileSystem::Default)
            .use_ram_disk()
            .config;
        
        assert_eq!(config.name, "TestVol");
        assert_eq!(config.size_bytes, 50 * 1024 * 1024);
        assert!(config.use_ram_disk);
    }
}```

## tests/helpers/mod.rs

```rust
//! Test helper modules for integration tests

pub mod test_volumes;

pub use test_volumes::*;```

## tests/library_test.rs

```rust
//! Integration tests for the library system

use sd_core_new::Core;
use tempfile::TempDir;

#[tokio::test]
async fn test_library_lifecycle() {
	// Create temporary directory for test
	let temp_dir = TempDir::new().unwrap();

	// Initialize core with custom data directory
	let core = Core::new_with_config(temp_dir.path().to_path_buf())
		.await
		.unwrap();

	// Create library (will be created in the libraries directory)
	let library = core
		.libraries
		.create_library("Test Library", None, core.context.clone())
		.await
		.unwrap();

	assert_eq!(library.name().await, "Test Library");

	// Verify directory structure
	let lib_path = library.path();
	assert!(lib_path.exists());
	assert!(lib_path.join("library.json").exists());
	assert!(lib_path.join("database.db").exists());
	assert!(lib_path.join("thumbnails").exists());
	assert!(lib_path.join("thumbnails/metadata.json").exists());

	// Test thumbnail operations
	// let cas_id = "test123";
	// let thumb_data = b"test thumbnail data";

	// library.save_thumbnail(cas_id, thumb_data).await.unwrap();
	// assert!(library.has_thumbnail(cas_id).await);

	// let retrieved = library.get_thumbnail(cas_id).await.unwrap();
	// assert_eq!(retrieved, thumb_data);

	// Test configuration update
	library
		.update_config(|config| {
			config.description = Some("Test description".to_string());
			config.settings.thumbnail_quality = 90;
		})
		.await
		.unwrap();

	let config = library.config().await;
	assert_eq!(config.description, Some("Test description".to_string()));
	assert_eq!(config.settings.thumbnail_quality, 90);

	// Close library
	let lib_id = library.id();
	let lib_path = library.path().to_path_buf();
	core.libraries.close_library(lib_id).await.unwrap();

	// Drop the library reference to release the lock
	drop(library);

	// Verify can't close again
	assert!(core.libraries.close_library(lib_id).await.is_err());

	// Re-open library
	let reopened = core.libraries.open_library(&lib_path).await.unwrap();
	assert_eq!(reopened.id(), lib_id);
	assert_eq!(reopened.name().await, "Test Library");

	// Verify data persisted
	// assert!(reopened.has_thumbnail(cas_id).await);
	let config = reopened.config().await;
	assert_eq!(config.description, Some("Test description".to_string()));
}

#[tokio::test]
async fn test_library_locking() {
	let temp_dir = TempDir::new().unwrap();
	let core = Core::new_with_config(temp_dir.path().to_path_buf())
		.await
		.unwrap();

	// Create library
	let library = core
		.libraries
		.create_library("Lock Test", None, core.context.clone())
		.await
		.unwrap();

	let lib_path = library.path().to_path_buf();

	// Try to open same library again - should fail
	let result = core.libraries.open_library(&lib_path).await;
	assert!(result.is_err());

	// Close library
	let lib_id = library.id();
	core.libraries.close_library(lib_id).await.unwrap();

	// Drop the library reference to release the lock
	drop(library);

	// Now should be able to open
	let reopened = core.libraries.open_library(&lib_path).await.unwrap();
	assert_eq!(reopened.name().await, "Lock Test");
}

#[tokio::test]
async fn test_library_discovery() {
	let temp_dir = TempDir::new().unwrap();
	let core = Core::new_with_config(temp_dir.path().to_path_buf())
		.await
		.unwrap();

	// Create multiple libraries
	let lib1 = core
		.libraries
		.create_library("Library 1", None, core.context.clone())
		.await
		.unwrap();

	let lib2 = core
		.libraries
		.create_library("Library 2", None, core.context.clone())
		.await
		.unwrap();

	// Close both
	let lib1_id = lib1.id();
	let lib2_id = lib2.id();
	core.libraries.close_library(lib1_id).await.unwrap();
	core.libraries.close_library(lib2_id).await.unwrap();

	// Drop library references to release locks
	drop(lib1);
	drop(lib2);

	// Test auto-loading - reload all libraries
	let loaded_count = core.libraries.load_all().await.unwrap();
	assert!(loaded_count >= 2);

	// Verify libraries were loaded
	let open_libraries = core.libraries.list().await;
	let names: Vec<String> =
		futures::future::join_all(open_libraries.iter().map(|lib| lib.name())).await;

	assert!(names.iter().any(|n| n == "Library 1"));
	assert!(names.iter().any(|n| n == "Library 2"));
}

#[tokio::test]
async fn test_library_name_sanitization() {
	let temp_dir = TempDir::new().unwrap();
	let core = Core::new_with_config(temp_dir.path().to_path_buf())
		.await
		.unwrap();

	// Create library with problematic name
	let library = core
		.libraries
		.create_library("My/Library:Name*", None, core.context.clone())
		.await
		.unwrap();

	// Verify directory name was sanitized
	let dir_name = library.path().file_name().unwrap().to_str().unwrap();
	assert!(dir_name.ends_with(".sdlibrary"));
	assert!(!dir_name.contains('/'));
	assert!(!dir_name.contains(':'));
	assert!(!dir_name.contains('*'));
}
```

## examples/test_migration.rs

```rust
//! Simple migration test to verify the schema works

use sd_core_new::infrastructure::database::migration::Migrator;
use sea_orm::{Database, ConnectionTrait, Statement};
use sea_orm_migration::MigratorTrait;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Initialize logging
    tracing_subscriber::fmt()
        .with_max_level(tracing::Level::DEBUG)
        .init();
    
    println!("=== Migration Test ===\n");
    
    // Create a temporary database for testing
    std::fs::create_dir_all("./data")?;
    let db_path = "./data/test_migration.db";
    if std::path::Path::new(db_path).exists() {
        std::fs::remove_file(db_path)?;
    }
    
    // Connect to database
    let db_url = format!("sqlite://{}?mode=rwc", db_path);
    println!("Connecting to database: {}", db_url);
    let db = Database::connect(&db_url).await?;
    
    // Run migrations
    println!("Running migrations...");
    Migrator::up(&db, None).await?;
    println!("âœ“ Migrations completed successfully!");
    
    // List tables to verify
    println!("\nCreated tables:");
    let result = db
        .query_all(Statement::from_string(
            sea_orm::DatabaseBackend::Sqlite,
            "SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%' ORDER BY name",
        ))
        .await?;
    
    let tables: Vec<String> = result
        .into_iter()
        .filter_map(|row| row.try_get_by::<String, _>("name").ok())
        .collect();
    
    for table in tables {
        println!("  - {}", table);
    }
    
    // Clean up
    drop(db);
    std::fs::remove_file(db_path)?;
    
    Ok(())
}```

## examples/indexing_demo.rs

```rust
//! Desktop Indexing Demo - Production Indexer Showcase
//!
//! This example demonstrates:
//! 1. Starting up Spacedrive Core
//! 2. Creating/opening a library
//! 3. Adding the user's desktop as a location
//! 4. Running the production indexer with all features:
//!    - Smart filtering (skip system files)
//!    - Incremental indexing with change detection
//!    - Performance metrics and reporting
//!    - Multi-phase processing
//! 5. Showing detailed results and metrics

use sd_core_new::{
	infrastructure::{database::entities, events::Event},
	location::{create_location, LocationCreateArgs},
	Core,
};
use sea_orm::{
	ActiveModelTrait, ColumnTrait, EntityTrait, PaginatorTrait, QueryFilter, QuerySelect,
};
use std::path::PathBuf;
use tokio::time::{sleep, Duration};

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
	// Initialize logging with more detail
	tracing_subscriber::fmt()
		.with_env_filter("sd_core_new=debug,desktop_indexing_demo=info")
		.init();

	println!("ðŸš€ === Spacedrive 2 Desktop Indexing Demo ===\n");

	// 1. Initialize Spacedrive Core
	println!("1. ðŸ”§ Initializing Spacedrive Core...");
	let data_dir = PathBuf::from("./data/spacedrive-desktop-demo");
	let core = Core::new_with_config(data_dir.clone()).await?;
	println!("   âœ… Core initialized");
	println!("   ðŸ“± Device ID: {}", core.device.device_id()?);
	println!("   ðŸ’¾ Data directory: {:?}\n", data_dir);

	// 2. Get or create library
	println!("2. ðŸ“š Setting up library...");
	let library = if core.libraries.list().await.is_empty() {
		println!("   Creating new library...");
		let lib = core
			.libraries
			.create_library("Desktop Demo Library", None, core.context.clone())
			.await?;
		println!("   âœ… Created library: {}", lib.name().await);
		lib
	} else {
		let libs = core.libraries.list().await;
		let lib = libs.into_iter().next().unwrap();
		println!("   âœ… Using existing library: {}", lib.name().await);
		lib
	};
	println!("   ðŸ†” Library ID: {}", library.id());
	println!("   ðŸ“‚ Library path: {}\n", library.path().display());

	// 3. Set up desktop location
	println!("3. ðŸ“ Adding Desktop as a location...");
	let desktop_path = dirs::desktop_dir().ok_or("Could not find desktop directory")?;
	println!("   ðŸ–¥ï¸  Desktop path: {}", desktop_path.display());

	// Register device in the database first
	let db = library.db();
	let device = core.device.to_device()?;

	// Check if device already exists, if not create it
	let device_record = match entities::device::Entity::find()
		.filter(entities::device::Column::Uuid.eq(device.id))
		.one(db.conn())
		.await?
	{
		Some(existing) => {
			println!("   âœ… Device already registered");
			existing
		}
		None => {
			println!("   ðŸ“± Registering device...");
			let device_model: entities::device::ActiveModel = device.into();
			let inserted = device_model.insert(db.conn()).await?;
			println!("   âœ… Device registered with ID: {}", inserted.id);
			inserted
		}
	};

	// Use production location management to create location and dispatch indexer job
	println!("   ðŸ“ Creating location with production job dispatch...");
	let location_args = LocationCreateArgs {
		path: desktop_path.clone(),
		name: Some("Desktop".to_string()),
		index_mode: sd_core_new::location::IndexMode::Deep, // Deep indexing with content analysis
	};

	let location_db_id = create_location(
		library.clone(),
		&core.events,
		location_args,
		device_record.id,
	)
	.await?;

	println!("   âœ… Location created with DB ID: {}", location_db_id);
	println!("   ðŸš€ Indexer job dispatched through production job manager!");

	// Add to file watcher (optional - for real-time monitoring)
	// Note: location_id here would need to be retrieved from the database record
	// For simplicity, we'll skip the file watcher for now since the main demo is indexing
	println!("   ðŸ‘ï¸  Production job system is now running indexing...\n");

	// 4. Monitor production indexer with new features
	println!("4. ðŸ” Production Indexer in Action!");
	println!("   âœ¨ New Features Showcase:");
	println!("      ðŸ“ Smart Filtering - Skips system files, caches, node_modules");
	println!("      ðŸ”„ Incremental Indexing - Detects changes via inode tracking");
	println!("      ðŸ“Š Performance Metrics - Detailed timing and throughput");
	println!("      ðŸŽ¯ Multi-phase Processing - Discovery â†’ Processing â†’ Content");
	println!("   ðŸ“‚ Target: {}", desktop_path.display());

	// Set up event monitoring to track job progress
	println!("   ðŸ“¡ Setting up real-time job monitoring...");
	let mut event_subscriber = core.events.subscribe();

	// Spawn event listener to monitor indexing progress
	let events_handle = tokio::spawn(async move {
		while let Ok(event) = event_subscriber.recv().await {
			match event {
				Event::IndexingStarted { location_id } => {
					println!("   ðŸ”„ Indexing started for location: {}", location_id);
				}
				Event::IndexingCompleted {
					location_id,
					total_files,
					total_dirs,
				} => {
					println!("   âœ… Indexing completed for location: {}", location_id);
					println!("      ðŸ“„ Files indexed: {}", total_files);
					println!("      ðŸ“ Directories indexed: {}", total_dirs);
					break; // Exit the event loop when indexing is done
				}
				Event::IndexingFailed { location_id, error } => {
					println!(
						"   âŒ Indexing failed for location: {} - {}",
						location_id, error
					);
					break;
				}
				Event::FilesIndexed { count, .. } => {
					println!("   ðŸ“ˆ Progress: {} files processed", count);
				}
				Event::JobProgress {
					job_id,
					progress,
					message,
				} => {
					// Show production indexer progress details
					if let Some(msg) = message {
						println!(
							"   ðŸ“Š Job {}: {} ({}%)",
							job_id,
							msg,
							(progress * 100.0) as u8
						);
					} else {
						println!("   ðŸ“Š Job {}: {}%", job_id, (progress * 100.0) as u8);
					}
				}
				_ => {} // Ignore other events
			}
		}
	});

	println!("   â³ Waiting for indexing to complete...");
	println!("   ðŸ’¡ Production Indexer Features Active:");
	println!("      ðŸš« Smart Filtering - Automatically skipping:");
	println!("         â€¢ Hidden files (.DS_Store, Thumbs.db)");
	println!("         â€¢ Dev directories (node_modules, .git, target)");
	println!("         â€¢ Cache folders (__pycache__, .cache)");
	println!("         â€¢ Large files (>4GB)");
	println!("      ðŸ”„ Change Detection - Using inode tracking for:");
	println!("         â€¢ Fast incremental updates");
	println!("         â€¢ Move/rename detection");
	println!("         â€¢ Modified file tracking");
	println!("      ðŸ“Š Performance Optimization:");
	println!("         â€¢ Batch processing (1000 items/batch)");
	println!("         â€¢ Path prefix deduplication");
	println!("         â€¢ Parallel content processing");

	// Let's show what files are actually in the desktop
	println!("\n   ðŸ“ Desktop contents preview:");
	let mut file_count = 0;
	let mut dir_count = 0;
	let mut total_size = 0u64;

	if let Ok(entries) = tokio::fs::read_dir(&desktop_path).await {
		let mut entries = entries;
		while let Ok(Some(entry)) = entries.next_entry().await {
			if let Ok(metadata) = entry.metadata().await {
				if metadata.is_file() {
					file_count += 1;
					total_size += metadata.len();
					if file_count <= 5 {
						// Show first 5 files
						println!("      ðŸ“„ {}", entry.file_name().to_string_lossy());
					}
				} else if metadata.is_dir() {
					dir_count += 1;
					if dir_count <= 3 {
						// Show first 3 dirs
						println!("      ðŸ“ {}/", entry.file_name().to_string_lossy());
					}
				}
			}
		}
	}

	if file_count > 5 {
		println!("      ... and {} more files", file_count - 5);
	}
	if dir_count > 3 {
		println!("      ... and {} more directories", dir_count - 3);
	}

	println!("\n   ðŸ“Š Discovery Summary:");
	println!("      ðŸ“„ Files found: {}", file_count);
	println!("      ðŸ“ Directories found: {}", dir_count);
	println!(
		"      ðŸ’¾ Total size: {:.2} MB",
		total_size as f64 / 1024.0 / 1024.0
	);

	// Smart job completion monitoring with checkpoint-based timeout
	println!("\n   â° Monitoring job completion with smart timeout...");
	println!("   ðŸ’¡ Will track checkpoint progress and wait for actual completion");

	let mut last_checkpoint_size = 0u64;
	let mut stall_time = std::time::Instant::now();
	let stall_timeout = Duration::from_secs(120); // Timeout if no progress for 2 minutes
	let poll_interval = Duration::from_secs(5); // Check every 5 seconds

	// Keep the event listener running but don't block on it
	let mut events_completed = false;

	loop {
		// Check if the event listener got completion
		if !events_completed && events_handle.is_finished() {
			events_completed = true;
			println!("   ðŸŽ¯ Event listener detected job completion!");
		}

		// Poll job status from the job manager
		let job_status = library.jobs().list_jobs(None).await?;
		let running_jobs = library
			.jobs()
			.list_jobs(Some(
				sd_core_new::infrastructure::jobs::types::JobStatus::Running,
			))
			.await?;
		let completed_jobs = library
			.jobs()
			.list_jobs(Some(
				sd_core_new::infrastructure::jobs::types::JobStatus::Completed,
			))
			.await?;

		// Also check how many entries have been created so far
		let current_entry_count = entities::entry::Entity::find()
			.count(db.conn())
			.await
			.unwrap_or(0);

		println!(
			"   ðŸ“Š Job Status: {} running, {} completed, {} total",
			running_jobs.len(),
			completed_jobs.len(),
			job_status.len()
		);
		println!("   ðŸ“„ Database entries so far: {}", current_entry_count);

		// Check checkpoint progress by querying actual checkpoint data
		let checkpoint_estimate = {
			// Try to get the latest checkpoint size from the jobs database
			if let Ok(metadata) =
				tokio::fs::metadata("./data/spacedrive-desktop-demo/jobs.db").await
			{
				metadata.len()
			} else {
				0
			}
		};

		if checkpoint_estimate > last_checkpoint_size {
			println!(
				"   ðŸ“ˆ Progress detected: {} bytes checkpoint data",
				checkpoint_estimate
			);
			last_checkpoint_size = checkpoint_estimate;
			stall_time = std::time::Instant::now(); // Reset stall timer
		}

		// Check completion conditions
		if running_jobs.is_empty() && !completed_jobs.is_empty() {
			println!("   âœ… All jobs completed successfully!");
			break;
		} else if running_jobs.is_empty() && events_completed {
			println!("   âœ… No running jobs and events indicate completion!");
			break;
		} else if stall_time.elapsed() > stall_timeout {
			println!(
				"   âš ï¸  Job appears stalled (no progress for {} seconds)",
				stall_timeout.as_secs()
			);
			println!("   ðŸ“Š Final checkpoint size: {} bytes", checkpoint_estimate);
			break;
		}

		// Wait before next poll
		tokio::time::sleep(poll_interval).await;
	}

	// Abort the event listener if it's still running
	if !events_handle.is_finished() {
		events_handle.abort();
	}

	// 5. Show production indexer results
	println!("\n5. ðŸŽ¯ Production Indexer Results:");

	// Check database for our location
	let location_record = entities::location::Entity::find()
		.filter(entities::location::Column::Id.eq(location_db_id))
		.one(db.conn())
		.await?
		.ok_or("Location not found")?;

	// Get entry statistics for this location
	let entry_count = entities::entry::Entity::find()
		.filter(entities::entry::Column::LocationId.eq(location_db_id))
		.count(db.conn())
		.await?;

	let file_count_db = entities::entry::Entity::find()
		.filter(entities::entry::Column::LocationId.eq(location_db_id))
		.filter(entities::entry::Column::Kind.eq(0)) // Files
		.count(db.conn())
		.await?;

	let dir_count_db = entities::entry::Entity::find()
		.filter(entities::entry::Column::LocationId.eq(location_db_id))
		.filter(entities::entry::Column::Kind.eq(1)) // Directories
		.count(db.conn())
		.await?;

	// Check for entries with inodes (change detection feature)
	let entries_with_inodes = entities::entry::Entity::find()
		.filter(entities::entry::Column::LocationId.eq(location_db_id))
		.filter(entities::entry::Column::Inode.is_not_null())
		.count(db.conn())
		.await?;

	// Sample some filtered paths to show filtering worked
	let sample_entries = entities::entry::Entity::find()
		.filter(entities::entry::Column::LocationId.eq(location_db_id))
		.limit(10)
		.all(db.conn())
		.await?;

	let content_identity_count = entities::content_identity::Entity::find()
		.count(db.conn())
		.await?;

	println!("   ðŸ“Š Indexing Statistics:");
	println!("      ðŸ“„ Files indexed: {}", file_count_db);
	println!("      ðŸ“ Directories indexed: {}", dir_count_db);
	println!(
		"      ðŸ”„ Entries with inode tracking: {} ({:.1}%)",
		entries_with_inodes,
		(entries_with_inodes as f64 / entry_count.max(1) as f64) * 100.0
	);
	println!(
		"      ðŸ”— Content identities created: {}",
		content_identity_count
	);

	println!("\n   ðŸš« Smart Filtering Validation:");
	println!("      Checking indexed files don't include filtered patterns...");

	let mut filtered_correctly = true;
	for entry in &sample_entries {
		// Check if any system files got through
		if entry.name == ".DS_Store" || entry.name == "Thumbs.db" {
			println!(
				"      âŒ Found system file that should be filtered: {}",
				entry.name
			);
			filtered_correctly = false;
		}
		if entry.name == "node_modules" || entry.name == ".git" || entry.name == "__pycache__" {
			println!(
				"      âŒ Found dev directory that should be filtered: {}",
				entry.name
			);
			filtered_correctly = false;
		}
	}

	if filtered_correctly {
		println!("      âœ… All sampled entries passed filtering validation!");
	}

	println!("\n   ðŸ“ Sample Indexed Entries:");
	for (i, entry) in sample_entries.iter().take(5).enumerate() {
		let kind = match entry.kind {
			0 => "ðŸ“„",
			1 => "ðŸ“",
			2 => "ðŸ”—",
			_ => "â“",
		};
		println!(
			"      {} {} {} ({})",
			i + 1,
			kind,
			entry.name,
			if entry.extension.is_some() {
				entry.extension.as_ref().unwrap()
			} else {
				"no ext"
			}
		);
	}

	// Check job status
	let running_jobs = library
		.jobs()
		.list_jobs(Some(
			sd_core_new::infrastructure::jobs::types::JobStatus::Running,
		))
		.await?;
	let completed_jobs = library
		.jobs()
		.list_jobs(Some(
			sd_core_new::infrastructure::jobs::types::JobStatus::Completed,
		))
		.await?;

	println!("\n   ðŸ’¼ Job System Status:");
	println!("      ðŸ”„ Running jobs: {}", running_jobs.len());
	println!("      âœ… Completed jobs: {}", completed_jobs.len());

	println!("\n   âœ¨ Production Indexer Features Demonstrated:");
	println!("      ðŸš« Smart Filtering - Automatically skipped system/cache files");
	println!(
		"      ðŸ”„ Incremental Ready - {} entries have inode tracking",
		entries_with_inodes
	);
	println!("      ðŸ“Š Batch Processing - Efficient memory usage");
	println!("      ðŸŽ¯ Multi-phase - Discovery â†’ Processing â†’ Content");
	println!(
		"      ðŸ” Content Deduplication - {} unique content IDs",
		content_identity_count
	);

	// 6. Show volume integration
	println!("\n6. ðŸ’¾ Volume Management:");
	println!("   ðŸ” Volume detection: âœ… Active");
	println!("   ðŸ“Š Volume tracking: âœ… Ready");
	println!("   âš¡ Speed testing: âœ… Available");
	println!("   ðŸ”„ Mount monitoring: âœ… Active");

	// 7. Event system demo
	println!("\n7. ðŸ“¡ Event System:");
	println!(
		"   ðŸŽ¯ Event subscribers: {}",
		core.events.subscriber_count()
	);
	println!("   ðŸ“¨ Events ready for:");
	println!("      - File operations (copy, move, delete)");
	println!("      - Library changes");
	println!("      - Volume events");
	println!("      - Indexing progress");
	println!("      - Job status updates");

	// 8. Production indexer achievements
	println!("\n8. ðŸŽ¯ Production Indexer Achievements:");
	println!("   This demo showcased the new production indexer:");
	println!("   âœ… Smart filtering skipped system files automatically");
	println!("   âœ… Inode tracking enabled incremental indexing");
	println!("   âœ… Multi-phase processing with detailed progress");
	println!("   âœ… Performance metrics and batch optimization");
	println!("   âœ… Path prefix deduplication for storage efficiency");
	println!("   âœ… Content identity generation for deduplication");
	println!("   âœ… Full resumability with checkpoint support");
	println!("   âœ… Non-critical error collection and reporting");

	// Show example of what would happen on re-index
	println!("\n   ðŸ”„ Incremental Indexing Preview:");
	println!("   Next run would:");
	println!("   â€¢ Use inode tracking to detect moved/renamed files");
	println!("   â€¢ Only process modified files (compare timestamps)");
	println!("   â€¢ Skip unchanged files entirely");
	println!("   â€¢ Detect and remove deleted entries");

	// Final job status check
	let final_running = library
		.jobs()
		.list_jobs(Some(
			sd_core_new::infrastructure::jobs::types::JobStatus::Running,
		))
		.await?;
	let final_completed = library
		.jobs()
		.list_jobs(Some(
			sd_core_new::infrastructure::jobs::types::JobStatus::Completed,
		))
		.await?;

	println!("\n   ðŸ“‹ Final Job Summary:");
	println!("      ðŸ”„ Still running: {}", final_running.len());
	println!("      âœ… Completed: {}", final_completed.len());

	if !final_running.is_empty() {
		println!("   ðŸ’¡ Remaining jobs will continue in background");
		println!("   ðŸ”„ Run the demo again to see persisted results!");
	}

	// Brief pause to see final status
	sleep(Duration::from_secs(2)).await;

	// 9. Graceful shutdown
	println!("\n9. ðŸ›‘ Shutting down gracefully...");
	core.shutdown().await?;

	println!("\nâœ… === Desktop Indexing Demo Complete! ===");
	println!("ðŸŽ‰ Spacedrive 2 Production Job System Working!");
	println!();
	println!("ðŸ“ Demo data stored at: {:?}", data_dir);
	println!("ðŸ”„ Run again to see library auto-loading and job persistence!");
	println!();
	println!("ðŸš€ Production system achievements:");
	println!("  âœ¨ Full core lifecycle with real job dispatch");
	println!("  ðŸ—„ï¸  Database integration with actual file indexing");
	println!("  ðŸ“‚ Production job manager dispatching real jobs");
	println!("  ðŸ’¾ Real-time progress monitoring via events");
	println!("  ðŸ“¡ Event system with live job status updates");
	println!("  ðŸ‘ï¸  File watching integration ready");
	println!("  ðŸ·ï¸  User metadata innovation (every file taggable)");
	println!("  ðŸ”„ Content deduplication with CAS IDs");
	println!("  ðŸ—‚ï¸  Path optimization for efficient storage");
	println!("  ðŸ”§ Production-ready architecture patterns");

	Ok(())
}
```

## examples/library_demo.rs

```rust
//! Library demo using full core lifecycle

use sd_core_new::infrastructure::database::entities;
use sd_core_new::Core;
use sea_orm::{ActiveModelTrait, ActiveValue::NotSet, EntityTrait, PaginatorTrait, Set};
use std::path::PathBuf;
use uuid::Uuid;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
	// Initialize logging
	tracing_subscriber::fmt()
		.with_env_filter("sd_core_new=debug")
		.init();

	println!("=== Spacedrive Core Lifecycle Demo ===\n");

	// 1. Initialize core with custom data directory
	println!("1. Initializing Spacedrive Core...");
	let data_dir = PathBuf::from("./data/spacedrive-demo-data");
	let core = Core::new_with_config(data_dir.clone()).await?;
	println!("   âœ“ Core initialized with data directory: {:?}", data_dir);
	println!("   âœ“ Device UUID: {}", core.device.device_id()?);

	// 2. Check application config
	{
		let config = core.config();
		let app_config = config.read().await;
		println!("\n2. Application Configuration:");
		println!("   - Data directory: {:?}", app_config.data_dir);
		println!("   - Log level: {}", app_config.log_level);
		println!("   - P2P enabled: {}", app_config.p2p.enabled);
		println!("   - Theme: {}", app_config.preferences.theme);
	}

	// 3. Subscribe to events
	println!("\n3. Setting up event listener...");
	let mut events = core.events.subscribe();
	tokio::spawn(async move {
		while let Ok(event) = events.recv().await {
			println!("   [EVENT] {:?}", event);
		}
	});

	// 4. Check for existing libraries
	println!("\n4. Checking for existing libraries...");
	let libraries = core.libraries.list().await;
	println!("   Found {} open libraries", libraries.len());

	if libraries.is_empty() {
		// 5. Create a new library
		println!("\n5. Creating new library...");
		let library = core
			.libraries
			.create_library("Lifecycle Demo Library", None, core.context.clone())
			.await?;
		println!("   âœ“ Library created: {}", library.name().await);
		println!("   âœ“ ID: {}", library.id());
		println!("   âœ“ Path: {}", library.path().display());

		// 6. Add some test data
		println!("\n6. Adding test data...");
		let db = library.db();
		let device = core.device.to_device()?;

		// Register device
		let device_model = entities::device::ActiveModel {
			id: NotSet,
			uuid: Set(device.id),
			name: Set(device.name.clone()),
			os: Set(device.os.to_string()),
			os_version: Set(None),
			hardware_model: Set(device.hardware_model),
			network_addresses: Set(serde_json::json!([])),
			is_online: Set(true),
			last_seen_at: Set(chrono::Utc::now()),
			capabilities: Set(serde_json::json!({
				"indexing": true,
				"p2p": true,
				"cloud": false
			})),
			sync_leadership: Set(serde_json::json!(device.sync_leadership)),
			created_at: Set(device.created_at),
			updated_at: Set(device.updated_at),
		};
		let inserted_device = device_model.insert(db.conn()).await?;
		println!("   âœ“ Device registered");

		// Add location
		let location = entities::location::ActiveModel {
			id: NotSet,
			uuid: Set(Uuid::new_v4()),
			device_id: Set(inserted_device.id),
			path: Set(std::env::current_dir()?.to_string_lossy().to_string()),
			name: Set(Some("Current Directory".to_string())),
			index_mode: Set("shallow".to_string()),
			scan_state: Set("pending".to_string()),
			last_scan_at: Set(None),
			error_message: Set(None),
			total_file_count: Set(0),
			total_byte_size: Set(0),
			created_at: Set(chrono::Utc::now()),
			updated_at: Set(chrono::Utc::now()),
		};
		location.insert(db.conn()).await?;
		println!("   âœ“ Location added");
	} else {
		// Show existing libraries
		println!("\n5. Existing libraries:");
		for library in &libraries {
			println!("   - {} ({})", library.name().await, library.id());

			// Show some stats
			let db = library.db();
			let entry_count = entities::entry::Entity::find().count(db.conn()).await?;
			let location_count = entities::location::Entity::find().count(db.conn()).await?;
			println!(
				"     Entries: {}, Locations: {}",
				entry_count, location_count
			);
		}
	}

	// 7. Demonstrate graceful shutdown
	println!("\n7. Press Ctrl+C to trigger graceful shutdown...");
	tokio::signal::ctrl_c().await?;

	println!("\n8. Shutting down...");
	core.shutdown().await?;
	println!("   âœ“ Core shutdown complete");

	println!("\nâœ… Lifecycle demo completed!");
	println!("\nðŸ“ Data stored at: {:?}", data_dir);
	println!("   Run again to see library auto-loading in action!");

	Ok(())
}
```

## examples/file_type_demo.rs

```rust
//! Example demonstrating the new file type identification system

use sd_core_new::domain::ContentKind;
use sd_core_new::file_type::FileTypeRegistry;
use std::path::Path;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
	// Create registry with built-in types
	let registry = FileTypeRegistry::new();

	println!("=== File Type Identification Demo ===\n");

	// Example 1: Simple extension matching
	println!("1. Extension matching:");
	let jpg_types = registry.get_by_extension("jpg");
	for ft in jpg_types {
		println!("  Found: {} ({})", ft.name, ft.id);
		println!("  MIME: {:?}", ft.mime_types);
		println!("  Category: {:?}", ft.category);
	}

	// Example 2: MIME type lookup
	println!("\n2. MIME type lookup:");
	if let Some(ft) = registry.get_by_mime("image/png") {
		println!("  image/png -> {} ({})", ft.name, ft.id);
	}

	// Example 3: Extension conflicts
	println!("\n3. Extension conflicts:");
	let ts_types = registry.get_by_extension("ts");
	println!("  '.ts' matches {} file types:", ts_types.len());
	for ft in ts_types {
		println!(
			"    - {} ({}) priority={}",
			ft.name, ft.category as u8, ft.priority
		);
	}

	// Example 4: File identification (would use magic bytes)
	println!("\n4. File identification simulation:");

	// Simulate identifying a TypeScript file
	println!("  Identifying 'app.ts':");
	let ts_candidates = registry.get_by_extension("ts");
	for (i, ft) in ts_candidates.iter().enumerate() {
		println!(
			"    Candidate {}: {} (priority={})",
			i + 1,
			ft.name,
			ft.priority
		);
		if !ft.magic_bytes.is_empty() {
			println!("      Has {} magic byte patterns", ft.magic_bytes.len());
		} else {
			println!("      No magic bytes (text file)");
		}
	}

	// In real usage with a file:
	// let result = registry.identify(Path::new("video.ts")).await?;
	// println!("Identified as: {} with {}% confidence", result.file_type.name, result.confidence);

	// Example 5: Rich metadata
	println!("\n5. File type metadata:");
	if let Some(jpeg) = registry.get("image/jpeg") {
		println!(
			"  JPEG metadata: {}",
			serde_json::to_string_pretty(&jpeg.metadata)?
		);
	}

	// Example 6: Integration with domain model
	println!("\n6. Domain model integration:");
	if let Some(png) = registry.get("image/png") {
		println!("  PNG uses ContentKind::{:?} directly", png.category);
		println!("  File type category maps to domain ContentKind enum");
	}

	Ok(())
}
```

## examples/shutdown_demo.rs

```rust
//! Demonstration of jobs being paused during shutdown

use sd_core_new::{Core, infrastructure::jobs::types::JobStatus};
use std::time::Duration;
use tokio::time::sleep;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Initialize logging
    tracing_subscriber::fmt::init();
    
    println!("=== Job Shutdown Demo ===\n");
    
    // Create Core instance
    let core = Core::new().await?;
    
    // Get open libraries
    let libraries = core.libraries.get_open_libraries().await;
    if libraries.is_empty() {
        println!("No open libraries found.");
        println!("\nTo test shutdown behavior:");
        println!("1. Create a library: spacedrive library create \"Test Library\"");
        println!("2. Start an indexing job: spacedrive location add /path/to/large/folder");
        println!("3. Run this demo while indexing is in progress");
        return Ok(());
    }
    
    // Check for running jobs across all libraries
    let mut total_running = 0;
    for library in &libraries {
        let job_manager = library.jobs();
        let running_jobs = job_manager.list_jobs(Some(JobStatus::Running)).await?;
        
        if !running_jobs.is_empty() {
            println!("Library {} has {} running jobs:", library.id(), running_jobs.len());
            for job in &running_jobs {
                println!("  - {} ({}): {:.1}% complete", job.name, job.id, job.progress);
            }
            total_running += running_jobs.len();
        }
    }
    
    if total_running == 0 {
        println!("No running jobs found. Start some jobs first to test shutdown behavior.");
        return Ok(());
    }
    
    println!("\n{} total running jobs found.", total_running);
    println!("\nShutting down in 3 seconds...");
    println!("All running jobs will be paused and can be resumed later.");
    
    for i in (1..=3).rev() {
        println!("{}...", i);
        sleep(Duration::from_secs(1)).await;
    }
    
    println!("\nInitiating shutdown...");
    let start = std::time::Instant::now();
    
    // Shutdown the core - this will pause all running jobs
    core.shutdown().await?;
    
    let elapsed = start.elapsed();
    println!("\nâœ“ Shutdown completed in {:.2} seconds", elapsed.as_secs_f32());
    println!("âœ“ All running jobs have been paused and their state saved");
    println!("\nThese jobs will automatically resume when Spacedrive restarts.");
    
    Ok(())
}```

## examples/location_watcher_demo.rs

```rust
//! Location Watcher Demo
//!
//! This example demonstrates how to use the location watcher to monitor
//! file system changes in real-time.

use sd_core_new::{infrastructure::events::Event, Core};
use std::path::PathBuf;
use tokio::time::{sleep, Duration};
use tracing::info;
use uuid::Uuid;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
	// Initialize logging
	tracing_subscriber::fmt::init();

	info!("Starting Location Watcher Demo");

	// Initialize core
	let core = Core::new().await?;
	info!("Core initialized successfully");

	// Create a test library
	let library = core
		.libraries
		.create_library("Watcher Demo Library", None, core.context.clone())
		.await?;

	let library_id = library.id();
	info!("Created demo library: {}", library_id);

	// Add a location to watch
	let watch_dir = PathBuf::from("./data/spacedrive_watcher_demo");
	tokio::fs::create_dir_all(&watch_dir).await?;

	let location_id = Uuid::new_v4();
	core.add_watched_location(location_id, library_id, watch_dir.clone(), true)
		.await?;
	info!("Added watched location: {}", watch_dir.display());

	// Subscribe to events
	let mut event_subscriber = core.events.subscribe();

	// Spawn event listener
	let events_handle = tokio::spawn(async move {
		info!("Event listener started");

		while let Ok(event) = event_subscriber.recv().await {
			match event {
				Event::EntryCreated {
					library_id,
					entry_id,
				} => {
					info!(
						"ðŸ“ File created - Library: {}, Entry: {}",
						library_id, entry_id
					);
				}
				Event::EntryModified {
					library_id,
					entry_id,
				} => {
					info!(
						"âœï¸  File modified - Library: {}, Entry: {}",
						library_id, entry_id
					);
				}
				Event::EntryDeleted {
					library_id,
					entry_id,
				} => {
					info!(
						"ðŸ—‘ï¸  File deleted - Library: {}, Entry: {}",
						library_id, entry_id
					);
				}
				Event::EntryMoved {
					library_id,
					entry_id,
					old_path,
					new_path,
				} => {
					info!(
						"ðŸ“¦ File moved - Library: {}, Entry: {}, {} -> {}",
						library_id, entry_id, old_path, new_path
					);
				}
				_ => {} // Ignore other events for this demo
			}
		}
	});

	// Simulate file operations
	info!("Starting file operations simulation...");

	// Create a test file
	let test_file = watch_dir.join("test_file.txt");
	tokio::fs::write(&test_file, "Hello, Spacedrive!").await?;
	info!("Created test file: {}", test_file.display());
	sleep(Duration::from_millis(200)).await;

	// Modify the file
	tokio::fs::write(&test_file, "Hello, Spacedrive! Modified content.").await?;
	info!("Modified test file");
	sleep(Duration::from_millis(200)).await;

	// Create a directory
	let test_dir = watch_dir.join("test_directory");
	tokio::fs::create_dir(&test_dir).await?;
	info!("Created test directory: {}", test_dir.display());
	sleep(Duration::from_millis(200)).await;

	// Create a file in the directory
	let nested_file = test_dir.join("nested_file.txt");
	tokio::fs::write(&nested_file, "Nested file content").await?;
	info!("Created nested file: {}", nested_file.display());
	sleep(Duration::from_millis(200)).await;

	// Rename the file
	let renamed_file = test_dir.join("renamed_file.txt");
	tokio::fs::rename(&nested_file, &renamed_file).await?;
	info!(
		"Renamed file: {} -> {}",
		nested_file.display(),
		renamed_file.display()
	);
	sleep(Duration::from_millis(200)).await;

	// Delete the file
	tokio::fs::remove_file(&renamed_file).await?;
	info!("Deleted file: {}", renamed_file.display());
	sleep(Duration::from_millis(200)).await;

	// Delete the directory
	tokio::fs::remove_dir(&test_dir).await?;
	info!("Deleted directory: {}", test_dir.display());
	sleep(Duration::from_millis(200)).await;

	// Delete the original test file
	tokio::fs::remove_file(&test_file).await?;
	info!("Deleted test file: {}", test_file.display());

	// Give some time for all events to be processed
	sleep(Duration::from_secs(2)).await;

	// Display current watched locations
	let watched_locations = core.get_watched_locations().await;
	info!("Currently watching {} locations:", watched_locations.len());
	for location in watched_locations {
		info!(
			"  - {} ({}): {} [{}]",
			location.id,
			location.library_id,
			location.path.display(),
			if location.enabled {
				"enabled"
			} else {
				"disabled"
			}
		);
	}

	// Clean up
	core.remove_watched_location(location_id).await?;
	info!("Removed watched location");

	// Clean up directory
	if watch_dir.exists() {
		tokio::fs::remove_dir_all(&watch_dir).await?;
		info!("Cleaned up demo directory");
	}

	// Stop event listener
	events_handle.abort();

	// Shutdown core
	core.shutdown().await?;
	info!("Demo completed successfully");

	Ok(())
}
```

## examples/volume_demo.rs

```rust
//! Volume system demonstration

use sd_core_new::{
    Core,
    volume::{
        types::{VolumeDetectionConfig, DiskType, FileSystem, MountType},
        VolumeExt,
    },
    infrastructure::events::{EventFilter, EventSubscriber},
};
use std::time::Duration;
use tokio::time::timeout;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Initialize logging
    tracing_subscriber::fmt()
        .with_env_filter("sd_core_new=info")
        .init();
    
    println!("=== Spacedrive Volume System Demo ===\n");
    
    // Initialize core (which includes volume manager)
    println!("1. Initializing Spacedrive Core with volume detection...");
    let core = Core::new().await?;
    println!("   âœ“ Core initialized");
    
    // Get volume statistics
    println!("\n2. Volume Statistics:");
    let stats = core.volumes.get_statistics().await;
    println!("   â€¢ Total volumes: {}", stats.total_volumes);
    println!("   â€¢ Mounted volumes: {}", stats.mounted_volumes);
    println!("   â€¢ Total capacity: {:.2} GB", 
        stats.total_capacity as f64 / 1024.0 / 1024.0 / 1024.0);
    println!("   â€¢ Total available: {:.2} GB", 
        stats.total_available as f64 / 1024.0 / 1024.0 / 1024.0);
    
    // Show disk type breakdown
    if !stats.by_type.is_empty() {
        println!("   â€¢ By disk type:");
        for (disk_type, count) in &stats.by_type {
            println!("     - {:?}: {}", disk_type, count);
        }
    }
    
    // Show filesystem breakdown
    if !stats.by_filesystem.is_empty() {
        println!("   â€¢ By filesystem:");
        for (fs, count) in &stats.by_filesystem {
            println!("     - {}: {}", fs, count);
        }
    }
    
    // List all detected volumes
    println!("\n3. Detected Volumes:");
    let volumes = core.volumes.get_all_volumes().await;
    
    if volumes.is_empty() {
        println!("   No volumes detected (possibly running in restricted environment)");
    } else {
        for (i, volume) in volumes.iter().enumerate() {
            println!("   {}. {} ({})", i + 1, volume.name, volume.fingerprint);
            println!("      Path: {}", volume.mount_point.display());
            println!("      Type: {} | Filesystem: {} | Disk: {:?}", 
                volume.mount_type, volume.file_system, volume.disk_type);
            println!("      Capacity: {:.2} GB | Available: {:.2} GB", 
                volume.total_bytes_capacity as f64 / 1024.0 / 1024.0 / 1024.0,
                volume.total_bytes_available as f64 / 1024.0 / 1024.0 / 1024.0);
            println!("      Mounted: {} | Read-only: {}", 
                volume.is_mounted, volume.read_only);
            
            // Show capabilities
            let supports_fast_copy = volume.supports_fast_copy();
            let optimal_chunk = volume.optimal_chunk_size();
            println!("      Fast copy support: {} | Optimal chunk: {}KB", 
                supports_fast_copy, optimal_chunk / 1024);
            
            if let (Some(read), Some(write)) = (volume.read_speed_mbps, volume.write_speed_mbps) {
                println!("      Speed: {}MB/s read, {}MB/s write", read, write);
            }
            
            println!();
        }
    }
    
    // Test path lookup
    println!("4. Testing Path-to-Volume Lookup:");
    let test_paths = [
        std::env::temp_dir(),
        std::env::current_dir().unwrap_or_default(),
        std::path::PathBuf::from("/"),
        std::path::PathBuf::from("/tmp"),
        std::path::PathBuf::from("/Users"),
        std::path::PathBuf::from("/home"),
        std::path::PathBuf::from("C:\\"),
        std::path::PathBuf::from("C:\\Windows"),
    ];
    
    for path in &test_paths {
        if path.exists() {
            if let Some(volume) = core.volumes.volume_for_path(path).await {
                println!("   {} â†’ {} ({})", 
                    path.display(), volume.name, volume.file_system);
            } else {
                println!("   {} â†’ No volume found", path.display());
            }
        }
    }
    
    // Test same volume detection
    println!("\n5. Testing Same-Volume Detection:");
    let temp_dir = std::env::temp_dir();
    let current_dir = std::env::current_dir().unwrap_or_default();
    
    if temp_dir.exists() && current_dir.exists() {
        let same_volume = core.volumes.same_volume(&temp_dir, &current_dir).await;
        println!("   Temp directory and current directory on same volume: {}", same_volume);
    }
    
    // Test volume space queries
    println!("\n6. Testing Volume Space Queries:");
    let space_requirements = [
        (1024 * 1024 * 1024, "1 GB"),          // 1GB
        (10 * 1024 * 1024 * 1024u64, "10 GB"), // 10GB
        (100 * 1024 * 1024 * 1024u64, "100 GB"), // 100GB
    ];
    
    for (bytes, description) in &space_requirements {
        let volumes_with_space = core.volumes.volumes_with_space(*bytes).await;
        println!("   Volumes with at least {}: {}", description, volumes_with_space.len());
    }
    
    // Test volume monitoring events (if we have volumes)
    if !volumes.is_empty() {
        println!("\n7. Testing Volume Events (5 second window):");
        let mut subscriber = core.events.subscribe();
        
        // Force a volume refresh to generate events
        let _ = core.volumes.refresh_volumes().await;
        
        let event_timeout = timeout(Duration::from_secs(5), async {
            let mut event_count = 0;
            loop {
                match subscriber.recv().await {
                    Ok(event) => {
                        if event.is_volume_event() {
                            event_count += 1;
                            println!("   Volume event received: {:?}", event);
                            
                            if event_count >= 3 {
                                break; // Don't wait for too many events
                            }
                        }
                    }
                    Err(_) => break,
                }
            }
            event_count
        }).await;
        
        match event_timeout {
            Ok(count) => println!("   Received {} volume events", count),
            Err(_) => println!("   No volume events received in timeout window"),
        }
    }
    
    // Run speed test on a suitable volume (if any)
    println!("\n8. Testing Volume Speed (optional):");
    let writable_volumes: Vec<_> = volumes.iter()
        .filter(|v| v.is_mounted && !v.read_only && v.mount_type != MountType::Network)
        .collect();
    
    if let Some(volume) = writable_volumes.first() {
        println!("   Running speed test on: {}", volume.name);
        
        match core.volumes.run_speed_test(&volume.fingerprint).await {
            Ok(()) => {
                if let Some(updated_volume) = core.volumes.get_volume(&volume.fingerprint).await {
                    if let (Some(read), Some(write)) = 
                        (updated_volume.read_speed_mbps, updated_volume.write_speed_mbps) {
                        println!("   âœ“ Speed test completed: {}MB/s read, {}MB/s write", read, write);
                    }
                }
            }
            Err(e) => {
                println!("   âš  Speed test failed: {}", e);
            }
        }
    } else {
        println!("   No suitable volumes found for speed testing");
    }
    
    // Show platform-specific information
    println!("\n9. Platform Information:");
    println!("   Operating System: {}", std::env::consts::OS);
    println!("   Architecture: {}", std::env::consts::ARCH);
    
    // Platform-specific notes
    match std::env::consts::OS {
        "macos" => {
            println!("   Note: macOS APFS volumes support instant cloning (copy-on-write)");
            let apfs_volumes = volumes.iter()
                .filter(|v| v.file_system == FileSystem::APFS)
                .count();
            if apfs_volumes > 0 {
                println!("   Found {} APFS volumes with fast copy support", apfs_volumes);
            }
        }
        "linux" => {
            println!("   Note: Btrfs and ZFS support instant copying via reflinks");
            let cow_volumes = volumes.iter()
                .filter(|v| matches!(v.file_system, FileSystem::Btrfs | FileSystem::ZFS))
                .count();
            if cow_volumes > 0 {
                println!("   Found {} CoW filesystem volumes", cow_volumes);
            }
        }
        "windows" => {
            println!("   Note: ReFS supports block cloning for fast copies");
            let refs_volumes = volumes.iter()
                .filter(|v| v.file_system == FileSystem::ReFS)
                .count();
            if refs_volumes > 0 {
                println!("   Found {} ReFS volumes with fast copy support", refs_volumes);
            }
        }
        _ => {
            println!("   Volume detection may be limited on this platform");
        }
    }
    
    println!("\n10. Integration with Copy Operations:");
    println!("   The volume system enables:");
    println!("   â€¢ Automatic copy strategy selection (instant vs streaming)");
    println!("   â€¢ Optimal chunk size determination based on disk type");
    println!("   â€¢ Cross-volume operation detection");
    println!("   â€¢ Performance-aware routing");
    
    if !volumes.is_empty() {
        // Show copy strategy examples
        let first_volume = &volumes[0];
        if volumes.len() > 1 {
            let second_volume = &volumes[1];
            println!("\n   Example copy strategies:");
            println!("   {} â†’ {} (same volume): {}", 
                first_volume.name, first_volume.name,
                if first_volume.supports_fast_copy() { "Instant clone" } else { "Optimized copy" }
            );
            println!("   {} â†’ {} (cross-volume): Streaming copy with {}KB chunks", 
                first_volume.name, second_volume.name,
                first_volume.optimal_chunk_size() / 1024
            );
        }
    }
    
    println!("\nâœ… Volume system demo completed!");
    println!("\nThe volume system provides:");
    println!("â€¢ Cross-platform volume detection");
    println!("â€¢ Real-time monitoring and event emission");
    println!("â€¢ Performance testing and optimization");
    println!("â€¢ Integration with Core event bus");
    println!("â€¢ Foundation for intelligent copy operations");
    
    Ok(())
}```

## examples/indexing_showcase.rs

```rust
//! Showcase of the production-ready indexer implementation
//! 
//! This example demonstrates the sophisticated features of our new indexer:
//! - Multi-phase processing (Discovery â†’ Processing â†’ Content)
//! - Hardcoded filtering with should_skip_path
//! - Incremental indexing with inode tracking
//! - Performance metrics and reporting
//! - Full resumability with checkpoints

use std::path::Path;

fn main() {
    println!("ðŸš€ Spacedrive Production Indexer Showcase\n");
    
    // Demonstrate the filtering system
    showcase_filtering();
    
    // Show the modular architecture
    showcase_architecture();
    
    // Display sample metrics output
    showcase_metrics();
}

fn showcase_filtering() {
    println!("ðŸ“ Smart Filtering System");
    println!("========================\n");
    
    // Import the actual function from our implementation
    use sd_core_new::operations::indexing::filters::should_skip_path;
    
    let test_paths = vec![
        // Files that should be skipped
        (".DS_Store", true, "macOS system file"),
        ("Thumbs.db", true, "Windows thumbnail cache"),
        ("node_modules", true, "npm packages directory"),
        (".git", true, "Git repository data"),
        ("target", true, "Rust build directory"),
        ("__pycache__", true, "Python cache"),
        (".mypy_cache", true, "Python type checker cache"),
        
        // Files that should NOT be skipped
        ("document.pdf", false, "Regular document"),
        ("photo.jpg", false, "Image file"),
        ("src", false, "Source code directory"),
        (".config", false, "User config directory (allowed)"),
        ("project.rs", false, "Rust source file"),
    ];
    
    println!("Testing path filtering:");
    for (path_str, should_skip, description) in test_paths {
        let path = Path::new(path_str);
        let skipped = should_skip_path(path);
        let result = if skipped == should_skip { "âœ…" } else { "âŒ" };
        println!("  {} {:20} -> {:8} ({})", 
            result, 
            path_str, 
            if skipped { "SKIP" } else { "INDEX" },
            description
        );
    }
    
    println!("\nðŸ’¡ Note: This is where the future IndexerRuleEngine will integrate!");
    println!("   The should_skip_path function has a clear TODO marker for rules system.\n");
}

fn showcase_architecture() {
    println!("ðŸ—ï¸  Modular Architecture");
    println!("=======================\n");
    
    println!("core-new/src/operations/indexing/");
    println!("â”œâ”€â”€ mod.rs                 # Module exports and documentation");
    println!("â”œâ”€â”€ job.rs                 # Main IndexerJob with state machine");
    println!("â”œâ”€â”€ state.rs               # Resumable state management");
    println!("â”œâ”€â”€ entry.rs               # Entry processing with inode support");
    println!("â”œâ”€â”€ filters.rs             # Hardcoded filtering (â†’ future rules)");
    println!("â”œâ”€â”€ metrics.rs             # Performance tracking");
    println!("â”œâ”€â”€ change_detection/      # Incremental indexing");
    println!("â”‚   â””â”€â”€ mod.rs            # Inode-based change detection");
    println!("â””â”€â”€ phases/                # Multi-phase processing");
    println!("    â”œâ”€â”€ discovery.rs       # Directory walking");
    println!("    â”œâ”€â”€ processing.rs      # Database operations");
    println!("    â””â”€â”€ content.rs         # CAS ID generation\n");
    
    println!("Key Features:");
    println!("âœ… Full resumability with checkpoint system");
    println!("âœ… Inode tracking for move/rename detection");
    println!("âœ… Batch processing (1000 items per batch)");
    println!("âœ… Non-critical error collection");
    println!("âœ… Path prefix optimization");
    println!("âœ… Content deduplication ready\n");
}

fn showcase_metrics() {
    println!("ðŸ“Š Performance Metrics");
    println!("=====================\n");
    
    // Show what metrics output looks like
    let sample_output = r#"Indexing completed in 12.5s:
- Files: 10,234 (818.7/s)
- Directories: 1,523 (121.8/s)  
- Total size: 2.34 GB (191.23 MB/s)
- Database writes: 10,234 in 11 batches (avg 930.4 items/batch)
- Errors: 5 (skipped 1,523 paths)
- Phase timing: discovery 5.2s, processing 6.1s, content 1.2s"#;
    
    println!("Sample metrics output:");
    println!("{}\n", sample_output);
    
    // Show the indexer progress phases
    println!("Progress Tracking Phases:");
    println!("1ï¸âƒ£  Discovery:   'Found 245 entries in /Users/demo/Documents'");
    println!("2ï¸âƒ£  Processing:  'Batch 3/11' (database operations)");
    println!("3ï¸âƒ£  Content:     'Generating content identities (456/1234)'");
    println!("4ï¸âƒ£  Finalizing:  'Cleaning up and saving final state'\n");
    
    // Show change detection in action
    println!("ðŸ”„ Incremental Indexing Example:");
    println!("First run:  Indexed 5,000 files");
    println!("Second run: Detected 3 new, 5 modified, 2 moved files");
    println!("            Only processed 10 files instead of 5,000!");
    println!("            Used inode tracking to detect moves efficiently\n");
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_showcase_runs() {
        // Just verify our showcase compiles and runs
        showcase_filtering();
        showcase_architecture();
        showcase_metrics();
    }
}```

## examples/simple_pause_resume.rs

```rust
//! Simple demonstration of pause/resume functionality

use sd_core_new::Core;
use std::time::Duration;
use tokio::time::sleep;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Initialize logging
    tracing_subscriber::fmt::init();
    
    println!("=== Simple Pause/Resume Demo ===\n");
    
    // Create Core instance
    let core = Core::new().await?;
    
    // Get open libraries
    let libraries = core.libraries.get_open_libraries().await;
    if libraries.is_empty() {
        println!("No open libraries found. Please create and open a library first.");
        return Ok(());
    }
    
    // Use the first library
    let library = libraries[0].clone();
    println!("Using library: {}", library.id());
    
    // Get job manager
    let job_manager = library.jobs();
    
    // List running jobs
    println!("\nChecking for running jobs...");
    let running_jobs = job_manager.list_jobs(Some(sd_core_new::infrastructure::jobs::types::JobStatus::Running)).await?;
    
    if running_jobs.is_empty() {
        println!("No running jobs found.");
        println!("\nTo test pause/resume:");
        println!("1. Start an indexing job: spacedrive location add /path/to/folder");
        println!("2. Run this demo again while indexing is in progress");
        return Ok(());
    }
    
    // Get the first running job
    let job_info = &running_jobs[0];
    let job_id = sd_core_new::infrastructure::jobs::types::JobId(job_info.id);
    
    println!("\nFound running job:");
    println!("  ID: {}", job_info.id);
    println!("  Name: {}", job_info.name);
    println!("  Progress: {:.1}%", job_info.progress);
    
    // Pause the job
    println!("\nPausing job...");
    match job_manager.pause_job(job_id).await {
        Ok(_) => println!("âœ“ Job paused successfully"),
        Err(e) => {
            println!("âœ— Failed to pause job: {}", e);
            return Ok(());
        }
    }
    
    // Check status
    sleep(Duration::from_millis(500)).await;
    let job_info = job_manager.get_job_info(job_id.0).await?.unwrap();
    println!("\nJob status after pause:");
    println!("  Status: {:?}", job_info.status);
    println!("  Progress: {:.1}%", job_info.progress);
    
    // Wait a bit
    println!("\nWaiting 3 seconds while paused...");
    sleep(Duration::from_secs(3)).await;
    
    // Check progress hasn't changed
    let job_info_after = job_manager.get_job_info(job_id.0).await?.unwrap();
    println!("\nProgress after waiting: {:.1}% (should be same)", job_info_after.progress);
    
    // Resume the job
    println!("\nResuming job...");
    match job_manager.resume_job(job_id).await {
        Ok(_) => println!("âœ“ Job resumed successfully"),
        Err(e) => {
            println!("âœ— Failed to resume job: {}", e);
            return Ok(());
        }
    }
    
    // Monitor progress
    println!("\nMonitoring progress for 5 seconds...");
    for i in 0..5 {
        sleep(Duration::from_secs(1)).await;
        let job_info = job_manager.get_job_info(job_id.0).await?.unwrap();
        println!("  Progress: {:.1}% - Status: {:?}", job_info.progress, job_info.status);
        
        if matches!(job_info.status, sd_core_new::infrastructure::jobs::types::JobStatus::Completed) {
            println!("\nâœ“ Job completed!");
            break;
        }
    }
    
    println!("\nâœ¨ Demo completed!");
    
    Ok(())
}```

## examples/pause_resume_demo.rs

```rust
//! Demonstration of job pause/resume functionality

use sd_core_new::{
    Core,
    location::{create_location, LocationCreateArgs, IndexMode},
    infrastructure::{
        database::entities,
        jobs::types::{JobId, JobStatus},
    },
};
use sea_orm::{ActiveModelTrait, EntityTrait};
use std::time::Duration;
use tempfile::TempDir;
use tokio::time::sleep;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Initialize logging
    tracing_subscriber::fmt::init();
    
    println!("=== Job Pause/Resume Demo ===\n");
    
    // Setup test environment
    let temp_dir = TempDir::new()?;
    let core = Core::new_with_config(temp_dir.path().to_path_buf()).await?;
    
    // Create library
    println!("1. Creating library...");
    let library = core
        .libraries
        .create_library("Demo Library", None, core.context.clone())
        .await?;
    
    // Create test location with files
    let test_location = temp_dir.path().join("test_location");
    tokio::fs::create_dir_all(&test_location).await?;
    
    println!("2. Creating test files...");
    for i in 0..50 {
        let file_path = test_location.join(format!("test_file_{}.txt", i));
        tokio::fs::write(&file_path, format!("Test content {}", i)).await?;
    }
    
    // Register device
    let db = library.db();
    let device = core.device.to_device()?;
    let device_model: entities::device::ActiveModel = device.into();
    let device_record = device_model.insert(db.conn()).await?;
    
    // Create location to trigger indexing
    println!("3. Creating location and starting indexing job...");
    let location_args = LocationCreateArgs {
        path: test_location.clone(),
        name: Some("Demo Location".to_string()),
        index_mode: IndexMode::Deep,
    };
    
    create_location(
        library.clone(),
        &core.events,
        location_args,
        device_record.id,
    )
    .await?;
    
    // Get the indexing job
    let job_manager = library.jobs();
    sleep(Duration::from_millis(200)).await;
    
    let running_jobs = job_manager.list_jobs(Some(JobStatus::Running)).await?;
    if running_jobs.is_empty() {
        println!("No running jobs found!");
        return Ok(());
    }
    
    let job_info = &running_jobs[0];
    let job_id = JobId(job_info.id);
    println!("   Found indexing job: {} ({})", job_info.name, job_id.0);
    
    // Let it run for a bit
    println!("\n4. Letting job run for 1 second...");
    sleep(Duration::from_secs(1)).await;
    
    // Check progress
    let job_info = job_manager.get_job_info(job_id.0).await?.unwrap();
    println!("   Progress: {:.1}%", job_info.progress);
    
    // Pause the job
    println!("\n5. Pausing the job...");
    job_manager.pause_job(job_id).await?;
    sleep(Duration::from_millis(200)).await;
    
    let job_info = job_manager.get_job_info(job_id.0).await?.unwrap();
    println!("   Job status: {:?}", job_info.status);
    println!("   Progress when paused: {:.1}%", job_info.progress);
    
    // Wait while paused
    println!("\n6. Waiting 2 seconds while paused...");
    sleep(Duration::from_secs(2)).await;
    
    let job_info_after_wait = job_manager.get_job_info(job_id.0).await?.unwrap();
    println!("   Progress after waiting: {:.1}% (should be same)", job_info_after_wait.progress);
    assert_eq!(job_info.progress, job_info_after_wait.progress, "Progress should not change while paused");
    
    // Resume the job
    println!("\n7. Resuming the job...");
    job_manager.resume_job(job_id).await?;
    
    // Monitor until completion
    println!("\n8. Waiting for job to complete...");
    let mut last_progress = job_info_after_wait.progress;
    loop {
        sleep(Duration::from_millis(500)).await;
        let job_info = job_manager.get_job_info(job_id.0).await?.unwrap();
        
        if job_info.progress != last_progress {
            println!("   Progress: {:.1}%", job_info.progress);
            last_progress = job_info.progress;
        }
        
        match job_info.status {
            JobStatus::Completed => {
                println!("\nâœ… Job completed successfully!");
                break;
            }
            JobStatus::Failed => {
                println!("\nâŒ Job failed: {:?}", job_info.error_message);
                break;
            }
            _ => continue,
        }
    }
    
    // Check results
    use sea_orm::{PaginatorTrait, QueryFilter, ColumnTrait};
    let indexed_count = entities::entry::Entity::find()
        .filter(entities::entry::Column::LocationId.eq(1))
        .count(db.conn())
        .await?;
    
    println!("\n9. Results:");
    println!("   Files indexed: {}", indexed_count);
    println!("   Expected: 50");
    
    println!("\nâœ¨ Demo completed successfully!");
    
    Ok(())
}```

## spacedrive-jobs-derive/src/lib.rs

```rust
//! Derive macros for automatic job registration

use proc_macro::TokenStream;
use quote::quote;
use syn::{parse_macro_input, DeriveInput, Data, DataStruct};

/// Derive macro for automatic job registration
/// 
/// This macro generates the necessary code to automatically register a job type
/// with the job registry using the `inventory` crate.
/// 
/// Usage:
/// ```rust
/// use spacedrive_jobs_derive::Job;
/// 
/// #[derive(Job, Serialize, Deserialize)]
/// pub struct MyJob {
///     // job fields
/// }
/// 
/// impl JobHandler for MyJob {
///     // implementation
/// }
/// ```
#[proc_macro_derive(Job)]
pub fn derive_job(input: TokenStream) -> TokenStream {
    let input = parse_macro_input!(input as DeriveInput);
    let name = &input.ident;
    
    // Ensure this is a struct
    let _data = match &input.data {
        Data::Struct(DataStruct { .. }) => {},
        _ => {
            return syn::Error::new_spanned(
                &input.ident,
                "Job can only be derived for structs"
            ).to_compile_error().into();
        }
    };

    let expanded = quote! {
        // Auto-register the job using inventory
        inventory::submit! {
            crate::infrastructure::jobs::types::JobRegistration {
                name: <#name as crate::infrastructure::jobs::traits::Job>::NAME,
                schema_fn: <#name as crate::infrastructure::jobs::traits::Job>::schema,
                create_fn: |data| {
                    let job: #name = serde_json::from_value(data)?;
                    Ok(Box::new(job))
                },
                deserialize_fn: |data| {
                    let job: #name = rmp_serde::from_slice(data)?;
                    Ok(Box::new(job))
                },
            }
        }
        
        // Implement ErasedJob for the job type
        impl crate::infrastructure::jobs::types::ErasedJob for #name {
            fn create_executor(
                self: Box<Self>,
                job_id: crate::infrastructure::jobs::types::JobId,
                library: std::sync::Arc<crate::library::Library>,
                job_db: std::sync::Arc<crate::infrastructure::jobs::database::JobDb>,
                status_tx: tokio::sync::watch::Sender<crate::infrastructure::jobs::types::JobStatus>,
                progress_tx: tokio::sync::mpsc::UnboundedSender<crate::infrastructure::jobs::progress::Progress>,
                broadcast_tx: tokio::sync::broadcast::Sender<crate::infrastructure::jobs::progress::Progress>,
                checkpoint_handler: std::sync::Arc<dyn crate::infrastructure::jobs::context::CheckpointHandler>,
                networking: Option<std::sync::Arc<crate::services::networking::NetworkingService>>,
                volume_manager: Option<std::sync::Arc<crate::volume::VolumeManager>>,
            ) -> Box<dyn sd_task_system::Task<crate::infrastructure::jobs::error::JobError>> {
                Box::new(crate::infrastructure::jobs::executor::JobExecutor::new(
                    *self,
                    job_id,
                    library,
                    job_db,
                    status_tx,
                    progress_tx,
                    broadcast_tx,
                    checkpoint_handler,
                    networking,
                    volume_manager,
                ))
            }
            
            fn serialize_state(&self) -> Result<Vec<u8>, crate::infrastructure::jobs::error::JobError> {
                rmp_serde::to_vec(self)
                    .map_err(|e| crate::infrastructure::jobs::error::JobError::serialization(format!("{}", e)))
            }
        }
    };

    TokenStream::from(expanded)
}```

## src/file_type/registry.rs

```rust
//! File type registry - the main API for file type identification

use super::{FileType, FileTypeError, IdentificationMethod, IdentificationResult, Result};
use crate::domain::ContentKind;
use crate::file_type::magic::MagicBytePattern;
use serde::Deserialize;
use std::collections::HashMap;
use std::path::Path;
use tokio::fs::File;
use tokio::io::{AsyncReadExt, AsyncSeekExt};

/// Maximum bytes to read for magic byte identification
const MAX_MAGIC_BYTES: usize = 8192;

/// Maximum bytes to read for content analysis
const MAX_CONTENT_BYTES: usize = 4096;

/// TOML structure for file type definitions
#[derive(Debug, Deserialize)]
struct FileTypeDefinitions {
    file_types: Vec<FileTypeDefinition>,
}

/// TOML structure for a single file type
#[derive(Debug, Deserialize)]
struct FileTypeDefinition {
    id: String,
    name: String,
    extensions: Vec<String>,
    mime_types: Vec<String>,
    #[serde(default)]
    uti: Option<String>,
    category: String,
    priority: u8,
    #[serde(default)]
    magic_bytes: Vec<MagicByteDefinition>,
    #[serde(default)]
    metadata: serde_json::Value,
}

/// TOML structure for magic bytes
#[derive(Debug, Deserialize)]
struct MagicByteDefinition {
    pattern: String,
    offset: usize,
    priority: u8,
}

/// Registry of all known file types
pub struct FileTypeRegistry {
    /// All registered file types by ID
    types: HashMap<String, FileType>,
    
    /// Extension to type IDs mapping
    extension_map: HashMap<String, Vec<String>>,
    
    /// MIME type to type ID mapping
    mime_map: HashMap<String, String>,
}

impl FileTypeRegistry {
    /// Create a new registry with built-in types
    pub fn new() -> Self {
        let mut registry = Self {
            types: HashMap::new(),
            extension_map: HashMap::new(),
            mime_map: HashMap::new(),
        };
        
        // Load built-in types
        registry.load_builtin_types();
        
        registry
    }
    
    /// Load built-in file type definitions
    fn load_builtin_types(&mut self) {
        // Load all TOML definitions from the builtin module
        let toml_definitions = super::builtin::get_builtin_toml_definitions();
        
        for toml_content in toml_definitions {
            // Use the loader to parse TOML
            if let Err(e) = self.load_from_toml(toml_content) {
                eprintln!("Failed to load builtin definitions: {}", e);
            }
        }
    }
    
    /// Register a file type
    pub fn register(&mut self, file_type: FileType) -> Result<()> {
        // Add to main registry
        let id = file_type.id.clone();
        
        // Update extension map
        for ext in &file_type.extensions {
            self.extension_map
                .entry(ext.to_lowercase())
                .or_insert_with(Vec::new)
                .push(id.clone());
        }
        
        // Update MIME map
        for mime in &file_type.mime_types {
            self.mime_map.insert(mime.clone(), id.clone());
        }
        
        self.types.insert(id, file_type);
        
        Ok(())
    }
    
    /// Get a file type by ID
    pub fn get(&self, id: &str) -> Option<&FileType> {
        self.types.get(id)
    }
    
    /// Get file types by extension
    pub fn get_by_extension(&self, ext: &str) -> Vec<&FileType> {
        let ext = ext.trim_start_matches('.').to_lowercase();
        
        self.extension_map
            .get(&ext)
            .map(|ids| {
                ids.iter()
                    .filter_map(|id| self.types.get(id))
                    .collect()
            })
            .unwrap_or_default()
    }
    
    /// Get file type by MIME type
    pub fn get_by_mime(&self, mime: &str) -> Option<&FileType> {
        self.mime_map
            .get(mime)
            .and_then(|id| self.types.get(id))
    }
    
    /// Identify a file type from a path
    pub async fn identify(&self, path: &Path) -> Result<IdentificationResult> {
        // Get extension
        let extension = path
            .extension()
            .and_then(|s| s.to_str())
            .unwrap_or("");
        
        // Get possible types by extension
        let candidates = self.get_by_extension(extension);
        
        match candidates.len() {
            0 => {
                // No extension match, try magic bytes on all types
                self.identify_by_magic_bytes(path, &self.types.values().collect::<Vec<_>>())
                    .await
            }
            1 => {
                // Single match, verify with magic bytes if available
                let file_type = candidates[0];
                if file_type.magic_bytes.is_empty() {
                    Ok(IdentificationResult {
                        file_type: file_type.clone(),
                        confidence: 90,
                        method: IdentificationMethod::Extension,
                    })
                } else {
                    // Verify with magic bytes
                    match self.check_magic_bytes(path, file_type).await {
                        Ok(true) => Ok(IdentificationResult {
                            file_type: file_type.clone(),
                            confidence: 100,
                            method: IdentificationMethod::Combined,
                        }),
                        _ => Ok(IdentificationResult {
                            file_type: file_type.clone(),
                            confidence: 70,
                            method: IdentificationMethod::Extension,
                        }),
                    }
                }
            }
            _ => {
                // Multiple candidates, use magic bytes to resolve
                self.identify_by_magic_bytes(path, &candidates).await
            }
        }
    }
    
    /// Identify by magic bytes from a set of candidates
    async fn identify_by_magic_bytes(
        &self,
        path: &Path,
        candidates: &[&FileType],
    ) -> Result<IdentificationResult> {
        // Read file header
        let mut file = File::open(path).await?;
        let mut buffer = vec![0u8; MAX_MAGIC_BYTES];
        let bytes_read = file.read(&mut buffer).await?;
        buffer.truncate(bytes_read);
        
        // Check each candidate
        let mut matches: Vec<(&FileType, u8)> = Vec::new();
        
        for candidate in candidates {
            for pattern in &candidate.magic_bytes {
                if pattern.matches(&buffer) {
                    matches.push((candidate, pattern.priority));
                    break;
                }
            }
        }
        
        // Sort by priority (highest first)
        matches.sort_by_key(|(_, priority)| std::cmp::Reverse(*priority));
        
        if let Some((file_type, _)) = matches.first() {
            Ok(IdentificationResult {
                file_type: (*file_type).clone(),
                confidence: 95,
                method: IdentificationMethod::MagicBytes,
            })
        } else {
            // No magic byte match, try content analysis for text files
            if candidates.iter().any(|ft| {
                matches!(ft.category, ContentKind::Text | ContentKind::Code)
            }) {
                self.identify_by_content(path, candidates).await
            } else {
                Err(FileTypeError::UnknownType)
            }
        }
    }
    
    /// Check if a specific file type's magic bytes match
    async fn check_magic_bytes(&self, path: &Path, file_type: &FileType) -> Result<bool> {
        if file_type.magic_bytes.is_empty() {
            return Ok(true);
        }
        
        let mut file = File::open(path).await?;
        let mut buffer = vec![0u8; MAX_MAGIC_BYTES];
        let bytes_read = file.read(&mut buffer).await?;
        buffer.truncate(bytes_read);
        
        Ok(file_type.magic_bytes.iter().any(|pattern| pattern.matches(&buffer)))
    }
    
    /// Identify by content analysis (for text files)
    async fn identify_by_content(
        &self,
        path: &Path,
        candidates: &[&FileType],
    ) -> Result<IdentificationResult> {
        // Read first part of file
        let mut file = File::open(path).await?;
        let mut buffer = vec![0u8; MAX_CONTENT_BYTES];
        let bytes_read = file.read(&mut buffer).await?;
        buffer.truncate(bytes_read);
        
        // Try to convert to string
        if let Ok(content) = String::from_utf8(buffer) {
            // Simple heuristics for now
            if content.contains("import") || content.contains("export") || content.contains("interface") {
                // Likely TypeScript
                if let Some(ts) = candidates.iter().find(|ft| ft.id == "text/typescript") {
                    return Ok(IdentificationResult {
                        file_type: (*ts).clone(),
                        confidence: 85,
                        method: IdentificationMethod::ContentAnalysis,
                    });
                }
            }
        }
        
        // Default to first text candidate
        if let Some(text_type) = candidates.iter().find(|ft| {
            matches!(ft.category, ContentKind::Text | ContentKind::Code)
        }) {
            Ok(IdentificationResult {
                file_type: (*text_type).clone(),
                confidence: 60,
                method: IdentificationMethod::Extension,
            })
        } else {
            Err(FileTypeError::UnknownType)
        }
    }
    
    /// Load definitions from a TOML string
    pub fn load_from_toml(&mut self, content: &str) -> Result<()> {
        let defs: FileTypeDefinitions = toml::from_str(content)
            .map_err(|e| FileTypeError::InvalidConfig(format!("TOML parse error: {}", e)))?;
        
        for def in defs.file_types {
            let file_type = self.definition_to_file_type(def)?;
            self.register(file_type)?;
        }
        
        Ok(())
    }
    
    /// Convert a definition to a FileType
    fn definition_to_file_type(&self, def: FileTypeDefinition) -> Result<FileType> {
        // Parse category
        let category = match def.category.as_str() {
            "document" => ContentKind::Document,
            "video" => ContentKind::Video,
            "image" => ContentKind::Image,
            "audio" => ContentKind::Audio,
            "archive" => ContentKind::Archive,
            "executable" => ContentKind::Executable,
            "text" => ContentKind::Text,
            "code" => ContentKind::Code,
            "database" => ContentKind::Database,
            "book" => ContentKind::Book,
            "font" => ContentKind::Font,
            "mesh" => ContentKind::Mesh,
            "config" => ContentKind::Config,
            "encrypted" => ContentKind::Encrypted,
            "key" => ContentKind::Key,
            _ => ContentKind::Unknown,
        };
        
        // Parse magic bytes
        let mut magic_bytes = Vec::new();
        for mb_def in def.magic_bytes {
            let pattern = MagicBytePattern::from_hex_string(
                &mb_def.pattern,
                mb_def.offset,
                mb_def.priority,
            ).map_err(|e| FileTypeError::InvalidConfig(format!("Invalid magic bytes: {}", e)))?;
            magic_bytes.push(pattern);
        }
        
        Ok(FileType {
            id: def.id,
            name: def.name,
            extensions: def.extensions,
            mime_types: def.mime_types,
            uti: def.uti,
            magic_bytes,
            category,
            priority: def.priority,
            metadata: def.metadata,
        })
    }
}

impl Default for FileTypeRegistry {
    fn default() -> Self {
        Self::new()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[tokio::test]
    async fn test_registry_basic() {
        let registry = FileTypeRegistry::new();
        
        // Test getting by extension
        let jpeg_types = registry.get_by_extension("jpg");
        assert_eq!(jpeg_types.len(), 1);
        assert_eq!(jpeg_types[0].id, "image/jpeg");
        
        // Test getting by MIME
        let png_type = registry.get_by_mime("image/png");
        assert!(png_type.is_some());
        assert_eq!(png_type.unwrap().id, "image/png");
        
        // Test extension conflict
        let ts_types = registry.get_by_extension("ts");
        assert_eq!(ts_types.len(), 2); // TypeScript and MPEG-TS
    }
}```

## src/file_type/magic.rs

```rust
//! Magic byte pattern matching

use serde::{Deserialize, Serialize};
use std::fmt;

/// A pattern of magic bytes for file identification
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MagicBytePattern {
    /// The byte pattern
    pub bytes: Vec<MagicByte>,
    
    /// Offset from start of file
    pub offset: usize,
    
    /// Priority for conflict resolution (higher = more specific)
    pub priority: u8,
}

/// A single byte in a magic pattern
#[derive(Debug, Clone, Copy, Serialize, Deserialize)]
#[serde(untagged)]
pub enum MagicByte {
    /// Exact byte value
    Exact(u8),
    
    /// Any byte (wildcard)
    Any,
    
    /// Range of values
    Range { min: u8, max: u8 },
}

impl MagicBytePattern {
    /// Create a pattern from hex string (e.g., "FF D8 FF ?? 00-FF")
    pub fn from_hex_string(s: &str, offset: usize, priority: u8) -> Result<Self, String> {
        let bytes = s
            .split_whitespace()
            .map(|part| {
                if part == "??" || part == "?" {
                    Ok(MagicByte::Any)
                } else if part.contains('-') {
                    let parts: Vec<&str> = part.split('-').collect();
                    if parts.len() != 2 {
                        return Err(format!("Invalid range: {}", part));
                    }
                    let min = u8::from_str_radix(parts[0], 16)
                        .map_err(|_| format!("Invalid hex: {}", parts[0]))?;
                    let max = u8::from_str_radix(parts[1], 16)
                        .map_err(|_| format!("Invalid hex: {}", parts[1]))?;
                    Ok(MagicByte::Range { min, max })
                } else {
                    u8::from_str_radix(part, 16)
                        .map(MagicByte::Exact)
                        .map_err(|_| format!("Invalid hex: {}", part))
                }
            })
            .collect::<Result<Vec<_>, _>>()?;
        
        Ok(Self {
            bytes,
            offset,
            priority,
        })
    }
    
    /// Check if this pattern matches the given buffer
    pub fn matches(&self, buf: &[u8]) -> bool {
        let start = self.offset;
        let end = start + self.bytes.len();
        
        if buf.len() < end {
            return false;
        }
        
        let slice = &buf[start..end];
        
        for (i, byte_pattern) in self.bytes.iter().enumerate() {
            if !byte_pattern.matches(slice[i]) {
                return false;
            }
        }
        
        true
    }
    
    /// Get the minimum buffer size needed to check this pattern
    pub fn required_size(&self) -> usize {
        self.offset + self.bytes.len()
    }
}

impl MagicByte {
    /// Check if this pattern matches a byte
    pub fn matches(&self, byte: u8) -> bool {
        match self {
            MagicByte::Exact(b) => *b == byte,
            MagicByte::Any => true,
            MagicByte::Range { min, max } => byte >= *min && byte <= *max,
        }
    }
}

impl fmt::Display for MagicByte {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        match self {
            MagicByte::Exact(b) => write!(f, "{:02X}", b),
            MagicByte::Any => write!(f, "??"),
            MagicByte::Range { min, max } => write!(f, "{:02X}-{:02X}", min, max),
        }
    }
}

impl fmt::Display for MagicBytePattern {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(f, "offset={}: ", self.offset)?;
        for (i, byte) in self.bytes.iter().enumerate() {
            if i > 0 {
                write!(f, " ")?;
            }
            write!(f, "{}", byte)?;
        }
        Ok(())
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_magic_byte_pattern_from_hex() {
        let pattern = MagicBytePattern::from_hex_string("FF D8 FF", 0, 100).unwrap();
        assert_eq!(pattern.bytes.len(), 3);
        assert!(matches!(pattern.bytes[0], MagicByte::Exact(0xFF)));
        assert!(matches!(pattern.bytes[1], MagicByte::Exact(0xD8)));
        assert!(matches!(pattern.bytes[2], MagicByte::Exact(0xFF)));
    }
    
    #[test]
    fn test_magic_byte_pattern_with_wildcards() {
        let pattern = MagicBytePattern::from_hex_string("47 ?? ?? 47", 0, 90).unwrap();
        assert_eq!(pattern.bytes.len(), 4);
        assert!(matches!(pattern.bytes[0], MagicByte::Exact(0x47)));
        assert!(matches!(pattern.bytes[1], MagicByte::Any));
        assert!(matches!(pattern.bytes[2], MagicByte::Any));
        assert!(matches!(pattern.bytes[3], MagicByte::Exact(0x47)));
    }
    
    #[test]
    fn test_pattern_matching() {
        let pattern = MagicBytePattern::from_hex_string("FF D8", 0, 100).unwrap();
        assert!(pattern.matches(&[0xFF, 0xD8, 0xFF]));
        assert!(!pattern.matches(&[0xFF, 0xD7]));
        assert!(!pattern.matches(&[0xFF])); // Too short
        
        // Test with offset
        let pattern = MagicBytePattern::from_hex_string("50 4B", 2, 100).unwrap();
        assert!(pattern.matches(&[0x00, 0x00, 0x50, 0x4B]));
        assert!(!pattern.matches(&[0x50, 0x4B, 0x00, 0x00]));
    }
}```

## src/file_type/mod.rs

```rust
//! File type identification system
//! 
//! A modern, extensible file type identification system that combines
//! extension matching, magic bytes, and content analysis.

use crate::domain::ContentKind;
use serde::{Deserialize, Serialize};
use serde_json::Value as JsonValue;
use std::collections::HashMap;
use std::path::Path;
use thiserror::Error;
use uuid::Uuid;

pub mod registry;
pub mod magic;
pub mod builtin;

pub use registry::FileTypeRegistry;
pub use magic::{MagicBytePattern, MagicByte};

/// A file type definition
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct FileType {
    /// Unique identifier (e.g., "image/jpeg")
    pub id: String,
    
    /// Human-readable name
    pub name: String,
    
    /// File extensions (without dots)
    pub extensions: Vec<String>,
    
    /// MIME types
    pub mime_types: Vec<String>,
    
    /// Uniform Type Identifier (macOS)
    pub uti: Option<String>,
    
    /// Magic byte patterns for identification
    pub magic_bytes: Vec<MagicBytePattern>,
    
    /// Category for grouping
    pub category: ContentKind,
    
    /// Priority for conflict resolution (higher = preferred)
    pub priority: u8,
    
    /// Extensible metadata
    pub metadata: JsonValue,
}


/// Result of file type identification
#[derive(Debug, Clone)]
pub struct IdentificationResult {
    /// The identified file type
    pub file_type: FileType,
    
    /// Confidence level (0-100)
    pub confidence: u8,
    
    /// How it was identified
    pub method: IdentificationMethod,
}

/// How a file was identified
#[derive(Debug, Clone, Copy)]
pub enum IdentificationMethod {
    /// Identified by file extension only
    Extension,
    
    /// Identified by magic bytes
    MagicBytes,
    
    /// Identified by content analysis
    ContentAnalysis,
    
    /// Identified by multiple methods
    Combined,
}

/// Errors that can occur during file type identification
#[derive(Error, Debug)]
pub enum FileTypeError {
    #[error("Unknown file type")]
    UnknownType,
    
    #[error("Ambiguous file type: {0}")]
    AmbiguousType(String),
    
    #[error("IO error: {0}")]
    Io(#[from] std::io::Error),
    
    #[error("Invalid configuration: {0}")]
    InvalidConfig(String),
}

pub type Result<T> = std::result::Result<T, FileTypeError>;

impl FileType {
    /// Check if this file type matches an extension
    pub fn matches_extension(&self, ext: &str) -> bool {
        self.extensions.iter().any(|e| e.eq_ignore_ascii_case(ext))
    }
    
    /// Get the primary MIME type
    pub fn primary_mime_type(&self) -> Option<&str> {
        self.mime_types.first().map(|s| s.as_str())
    }
    
    /// Get the primary extension
    pub fn primary_extension(&self) -> Option<&str> {
        self.extensions.first().map(|s| s.as_str())
    }
}```

## src/file_type/builtin.rs

```rust
//! Built-in file type definitions
//! 
//! Loads the built-in file type definitions from embedded TOML files.

use once_cell::sync::Lazy;

/// Embedded TOML definitions
pub static BUILTIN_DEFINITIONS: Lazy<Vec<&'static str>> = Lazy::new(|| {
    vec![
        include_str!("definitions/images.toml"),
        include_str!("definitions/video.toml"),
        include_str!("definitions/audio.toml"),
        include_str!("definitions/documents.toml"),
        include_str!("definitions/code.toml"),
        include_str!("definitions/archives.toml"),
        include_str!("definitions/misc.toml"),
    ]
});

/// Get all built-in TOML definitions
pub fn get_builtin_toml_definitions() -> &'static [&'static str] {
    &BUILTIN_DEFINITIONS
}```

## src/bin/cli.rs

```rust
//! Spacedrive Core CLI
//! 
//! The main CLI binary for Spacedrive Core operations.
//! 
//! Usage:
//!   spacedrive-cli --help
//!   spacedrive-cli library create "My Library"
//!   spacedrive-cli location add /path/to/folder
//!   spacedrive-cli tui

use sd_core_new::infrastructure::cli;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    cli::run().await
}```

## src/config/mod.rs

```rust
//! Application configuration management

use serde::{Deserialize, Serialize};
use std::path::PathBuf;
use chrono::{DateTime, Utc};
use anyhow::{anyhow, Result};
use std::fs;

pub mod app_config;
pub mod migration;

pub use app_config::AppConfig;
pub use migration::Migrate;

/// Platform-specific data directory resolution
pub fn default_data_dir() -> Result<PathBuf> {
    #[cfg(target_os = "macos")]
    let dir = dirs::data_dir()
        .ok_or_else(|| anyhow!("Could not determine data directory"))?
        .join("spacedrive");
    
    #[cfg(target_os = "windows")]
    let dir = dirs::data_dir()
        .ok_or_else(|| anyhow!("Could not determine data directory"))?
        .join("Spacedrive");
    
    #[cfg(target_os = "linux")]
    let dir = dirs::data_local_dir()
        .ok_or_else(|| anyhow!("Could not determine data directory"))?
        .join("spacedrive");
    
    // Create directory if it doesn't exist
    fs::create_dir_all(&dir)?;
    
    Ok(dir)
}

/// P2P configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct P2PConfig {
    pub enabled: bool,
    pub discovery: String, // "local", "global", "disabled"
}

impl Default for P2PConfig {
    fn default() -> Self {
        Self {
            enabled: true,
            discovery: "local".to_string(),
        }
    }
}

/// User preferences
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Preferences {
    pub theme: String,     // "light", "dark", "system"
    pub language: String,  // ISO 639-1 code
}

impl Default for Preferences {
    fn default() -> Self {
        Self {
            theme: "system".to_string(),
            language: "en".to_string(),
        }
    }
}```

## src/config/app_config.rs

```rust
//! Application configuration

use super::{P2PConfig, Preferences, default_data_dir};
use crate::config::migration::Migrate;
use serde::{Deserialize, Serialize};
use std::path::PathBuf;
use anyhow::{anyhow, Result};
use std::fs;
use tracing::{info, warn};

/// Main application configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AppConfig {
    /// Config schema version
    pub version: u32,
    
    /// Data directory path
    pub data_dir: PathBuf,
    
    /// Logging level
    pub log_level: String,
    
    /// Whether telemetry is enabled
    pub telemetry_enabled: bool,
    
    /// P2P configuration
    pub p2p: P2PConfig,
    
    /// User preferences
    pub preferences: Preferences,
}

impl AppConfig {
    /// Load configuration from the default location
    pub fn load() -> Result<Self> {
        let data_dir = default_data_dir()?;
        Self::load_from(&data_dir)
    }
    
    /// Load configuration from a specific data directory
    pub fn load_from(data_dir: &PathBuf) -> Result<Self> {
        let config_path = data_dir.join("spacedrive.json");
        
        if config_path.exists() {
            info!("Loading config from {:?}", config_path);
            let json = fs::read_to_string(&config_path)?;
            let mut config: AppConfig = serde_json::from_str(&json)?;
            
            // Apply migrations if needed
            if config.version < Self::target_version() {
                info!("Migrating config from v{} to v{}", config.version, Self::target_version());
                config.migrate()?;
                config.save()?;
            }
            
            Ok(config)
        } else {
            warn!("No config found, creating default at {:?}", config_path);
            let config = Self::default_with_dir(data_dir.clone());
            config.save()?;
            Ok(config)
        }
    }
    
    /// Load or create configuration
    pub fn load_or_create(data_dir: &PathBuf) -> Result<Self> {
        Self::load_from(data_dir).or_else(|_| {
            let config = Self::default_with_dir(data_dir.clone());
            config.save()?;
            Ok(config)
        })
    }
    
    /// Create default configuration with specific data directory
    pub fn default_with_dir(data_dir: PathBuf) -> Self {
        Self {
            version: Self::target_version(),
            data_dir,
            log_level: "info".to_string(),
            telemetry_enabled: true,
            p2p: P2PConfig::default(),
            preferences: Preferences::default(),
        }
    }
    
    /// Save configuration to disk
    pub fn save(&self) -> Result<()> {
        // Ensure directory exists
        fs::create_dir_all(&self.data_dir)?;
        
        let config_path = self.data_dir.join("spacedrive.json");
        let json = serde_json::to_string_pretty(self)?;
        fs::write(&config_path, json)?;
        info!("Saved config to {:?}", config_path);
        Ok(())
    }
    
    /// Get the path for logs directory
    pub fn logs_dir(&self) -> PathBuf {
        self.data_dir.join("logs")
    }
    
    /// Get the path for libraries directory
    pub fn libraries_dir(&self) -> PathBuf {
        self.data_dir.join("libraries")
    }
    
    /// Ensure all required directories exist
    pub fn ensure_directories(&self) -> Result<()> {
        fs::create_dir_all(&self.data_dir)?;
        fs::create_dir_all(self.logs_dir())?;
        fs::create_dir_all(self.libraries_dir())?;
        Ok(())
    }
}

impl Default for AppConfig {
    fn default() -> Self {
        let data_dir = default_data_dir().unwrap_or_else(|_| PathBuf::from("."));
        Self::default_with_dir(data_dir)
    }
}

impl Migrate for AppConfig {
    fn current_version(&self) -> u32 {
        self.version
    }
    
    fn target_version() -> u32 {
        1 // Current schema version
    }
    
    fn migrate(&mut self) -> Result<()> {
        match self.version {
            0 => {
                // Future migration from v0 to v1 would go here
                self.version = 1;
                Ok(())
            }
            1 => Ok(()), // Already at target version
            v => Err(anyhow!("Unknown config version: {}", v)),
        }
    }
}```

## src/config/migration.rs

```rust
//! Configuration migration system

use anyhow::Result;

/// Trait for versioned configuration migration
pub trait Migrate {
    /// Get the current version of this configuration
    fn current_version(&self) -> u32;
    
    /// Get the target version this configuration should be migrated to
    fn target_version() -> u32;
    
    /// Apply migrations to bring configuration to target version
    fn migrate(&mut self) -> Result<()>;
    
    /// Check if migration is needed
    fn needs_migration(&self) -> bool {
        self.current_version() < Self::target_version()
    }
}```

## src/location/manager.rs

```rust
//! Location Manager - Orchestrates location lifecycle and indexing

use super::{LocationError, LocationResult, ManagedLocation, IndexMode};
use crate::{
    infrastructure::{
        database::entities,
        events::{Event, EventBus},
        jobs::{manager::JobManager, traits::Job},
    },
    library::Library,
    operations::indexing::job::{IndexerJob, IndexerJobConfig},
    shared::types::SdPath,
};
use sea_orm::{
    ActiveModelTrait, ActiveValue::Set, ColumnTrait, EntityTrait, PaginatorTrait, QueryFilter,
};
use std::{path::PathBuf, sync::Arc};
use tokio::fs;
use tracing::{debug, error, info, warn};
use uuid::Uuid;

/// Manages locations and their lifecycle
#[derive(Clone)]
pub struct LocationManager {
    events: EventBus,
}

impl LocationManager {
    pub fn new(events: EventBus) -> Self {
        Self { events }
    }

    /// Add a new location to the library
    pub async fn add_location(
        &self,
        library: Arc<Library>,
        path: PathBuf,
        name: Option<String>,
        device_id: i32,
        index_mode: IndexMode,
    ) -> LocationResult<(Uuid, String)> {
        info!("Adding location: {}", path.display());

        // Validate the path
        self.validate_path(&path).await?;

        // Check if location already exists
        let existing = entities::location::Entity::find()
            .filter(entities::location::Column::Path.eq(path.to_string_lossy().to_string()))
            .one(library.db().conn())
            .await?;

        if existing.is_some() {
            return Err(LocationError::LocationExists { path });
        }

        // Create the location record
        let location_id = Uuid::new_v4();
        let display_name = name.unwrap_or_else(|| {
            path.file_name()
                .and_then(|n| n.to_str())
                .unwrap_or("Unknown")
                .to_string()
        });

        let location_model = entities::location::ActiveModel {
            id: sea_orm::ActiveValue::NotSet,
            uuid: Set(location_id),
            device_id: Set(device_id),
            path: Set(path.to_string_lossy().to_string()),
            name: Set(Some(display_name.clone())),
            index_mode: Set(index_mode.to_string()),
            scan_state: Set("pending".to_string()),
            last_scan_at: Set(None),
            error_message: Set(None),
            total_file_count: Set(0),
            total_byte_size: Set(0),
            created_at: Set(chrono::Utc::now()),
            updated_at: Set(chrono::Utc::now()),
        };

        let location_record = location_model.insert(library.db().conn()).await?;
        info!("Created location record with ID: {}", location_record.id);

        // Create managed location
        let managed_location = ManagedLocation {
            id: location_id,
            name: display_name.clone(),
            path: path.clone(),
            device_id,
            library_id: library.id(),
            indexing_enabled: true,
            index_mode,
            watch_enabled: true,
        };

        // Emit location added event
        self.events.emit(Event::LocationAdded {
            library_id: library.id(),
            location_id,
            path: path.clone(),
        });

        // Also emit indexing started event
        self.events.emit(Event::IndexingStarted { location_id });

        // Start indexing job
        let job_id = match self.start_indexing(library, &managed_location).await {
            Ok(job_id) => {
                info!("Started indexing job {} for location '{}'", job_id, path.display());

                // Emit job started event
                self.events.emit(Event::JobStarted {
                    job_id: job_id.clone(),
                    job_type: "Indexing".to_string(),
                });

                job_id
            }
            Err(e) => {
                error!("Failed to start indexing for location '{}': {}", path.display(), e);
                // Return empty job ID if indexing fails
                String::new()
            }
        };

        info!("Successfully added location '{}'", path.display());
        Ok((location_id, job_id))
    }

    /// Start indexing for a location
    pub async fn start_indexing(
        &self,
        library: Arc<Library>,
        location: &ManagedLocation,
    ) -> LocationResult<String> {
        info!(
            "Starting indexing for location '{}' in mode {:?}",
            location.path.display(),
            location.index_mode
        );

        // Update scan state to "scanning"
        self.update_scan_state(&library, location.id, "scanning", None).await?;

        // Create SdPath for the location
        let device_uuid = self.get_device_uuid(&library, location.device_id).await?;
        let location_sd_path = SdPath::new(device_uuid, location.path.clone());

        // Create indexer job using new configuration pattern
        let config = IndexerJobConfig::new(location.id, location_sd_path, location.index_mode.into());
        let indexer_job = IndexerJob::new(config);

        // Submit to job manager
        let job_manager = library.jobs();
        let job_handle = job_manager.dispatch(indexer_job).await?;
        let job_id = job_handle.id();

        info!("Started indexing job {} for location '{}'", job_id, location.path.display());

        // The job system will handle:
        // - Progress updates via the event bus
        // - Updating scan state when complete/failed
        // - Emitting appropriate events

        Ok(job_id.to_string())
    }

    /// Update scan state for a location
    async fn update_scan_state(
        &self,
        library: &Library,
        location_id: Uuid,
        scan_state: &str,
        error_message: Option<String>,
    ) -> LocationResult<()> {
        use sea_orm::ActiveValue::Set;

        let location = entities::location::Entity::find()
            .filter(entities::location::Column::Uuid.eq(location_id))
            .one(library.db().conn())
            .await?
            .ok_or_else(|| LocationError::LocationNotFound { id: location_id })?;

        let mut active_location: entities::location::ActiveModel = location.into();
        active_location.scan_state = Set(scan_state.to_string());
        active_location.error_message = Set(error_message);
        if scan_state == "completed" {
            active_location.last_scan_at = Set(Some(chrono::Utc::now()));
        }
        active_location.updated_at = Set(chrono::Utc::now());

        active_location.update(library.db().conn()).await?;
        Ok(())
    }

    /// Update location statistics
    pub async fn update_location_stats(
        &self,
        library: &Library,
        location_id: Uuid,
        file_count: i32,
        total_size: i64,
    ) -> LocationResult<()> {
        use sea_orm::ActiveValue::Set;

        let location = entities::location::Entity::find()
            .filter(entities::location::Column::Uuid.eq(location_id))
            .one(library.db().conn())
            .await?
            .ok_or_else(|| LocationError::LocationNotFound { id: location_id })?;

        let mut active_location: entities::location::ActiveModel = location.into();
        active_location.total_file_count = Set(file_count as i64);
        active_location.total_byte_size = Set(total_size);
        active_location.updated_at = Set(chrono::Utc::now());

        active_location.update(library.db().conn()).await?;
        Ok(())
    }

    /// Get device UUID from device ID
    async fn get_device_uuid(&self, library: &Library, device_id: i32) -> LocationResult<Uuid> {
        let device = entities::device::Entity::find_by_id(device_id)
            .one(library.db().conn())
            .await?
            .ok_or_else(|| LocationError::Other(format!("Device {} not found", device_id)))?;

        Ok(device.uuid)
    }

    /// Validate a path before creating a location
    async fn validate_path(&self, path: &PathBuf) -> LocationResult<()> {
        // Check if path exists
        if !path.exists() {
            return Err(LocationError::PathNotFound {
                path: path.clone()
            });
        }

        // Check if it's a directory
        let metadata = fs::metadata(path).await?;
        if !metadata.is_dir() {
            return Err(LocationError::InvalidPath(
                "Path must be a directory".to_string()
            ));
        }

        // Check if we have read permissions
        match fs::read_dir(path).await {
            Ok(_) => Ok(()),
            Err(e) => match e.kind() {
                std::io::ErrorKind::PermissionDenied => {
                    Err(LocationError::PathNotAccessible {
                        path: path.clone()
                    })
                }
                _ => Err(LocationError::Io(e)),
            }
        }
    }

    /// Remove a location
    pub async fn remove_location(
        &self,
        library: &Library,
        location_id: Uuid,
    ) -> LocationResult<()> {
        info!("Removing location {}", location_id);

        // Find the location
        let location = entities::location::Entity::find()
            .filter(entities::location::Column::Uuid.eq(location_id))
            .one(library.db().conn())
            .await?
            .ok_or_else(|| LocationError::LocationNotFound { id: location_id })?;

        // Delete the location (cascades to entries)
        entities::location::Entity::delete_by_id(location.id)
            .exec(library.db().conn())
            .await?;

        // Emit event
        self.events.emit(Event::LocationRemoved {
            library_id: library.id(),
            location_id,
        });

        info!("Successfully removed location {}", location_id);
        Ok(())
    }

    /// List all locations for a library
    pub async fn list_locations(
        &self,
        library: &Library,
    ) -> LocationResult<Vec<ManagedLocation>> {
        let locations = entities::location::Entity::find()
            .all(library.db().conn())
            .await?;

        let mut managed_locations = Vec::new();
        for loc in locations {
            managed_locations.push(ManagedLocation {
                id: loc.uuid,
                name: loc.name.unwrap_or_else(|| "Unknown".to_string()),
                path: PathBuf::from(&loc.path),
                device_id: loc.device_id,
                library_id: library.id(),
                indexing_enabled: true,
                index_mode: loc.index_mode.parse().unwrap_or(IndexMode::Content),
                watch_enabled: true,
            });
        }

        Ok(managed_locations)
    }

    /// Rescan a location
    pub async fn rescan_location(
        &self,
        library: Arc<Library>,
        location_id: Uuid,
        force: bool,
    ) -> LocationResult<String> {
        info!("Rescanning location {} (force: {})", location_id, force);

        // Get the location
        let location = entities::location::Entity::find()
            .filter(entities::location::Column::Uuid.eq(location_id))
            .one(library.db().conn())
            .await?
            .ok_or_else(|| LocationError::LocationNotFound { id: location_id })?;

        let managed_location = ManagedLocation {
            id: location.uuid,
            name: location.name.unwrap_or_else(|| "Unknown".to_string()),
            path: PathBuf::from(&location.path),
            device_id: location.device_id,
            library_id: library.id(),
            indexing_enabled: true,
            index_mode: location.index_mode.parse().unwrap_or(IndexMode::Content),
            watch_enabled: true,
        };

        // Start indexing (the indexer will handle incremental updates unless force is true)
        self.start_indexing(library, &managed_location).await
    }
}


impl std::str::FromStr for IndexMode {
    type Err = String;

    fn from_str(s: &str) -> Result<Self, Self::Err> {
        match s {
            "shallow" => Ok(IndexMode::Shallow),
            "quick" => Ok(IndexMode::Quick),
            "content" => Ok(IndexMode::Content),
            "deep" => Ok(IndexMode::Deep),
            "full" => Ok(IndexMode::Full),
            _ => Err(format!("Unknown index mode: {}", s)),
        }
    }
}```

## src/location/mod.rs

```rust
//! Location management - simplified implementation matching core patterns

pub mod manager;

use crate::{
	infrastructure::{
		database::entities,
		events::{Event, EventBus},
		jobs::{handle::JobHandle, output::IndexedOutput, types::JobStatus},
	},
	library::Library,
	operations::indexing::{IndexMode as JobIndexMode, IndexerJob, IndexerJobConfig},
	shared::types::SdPath,
};

use sea_orm::{ActiveModelTrait, ActiveValue::Set, ColumnTrait, EntityTrait, QueryFilter};
use serde::{Deserialize, Serialize};
use std::{path::PathBuf, sync::Arc};
use tokio::fs;
use tracing::{error, info, warn};
use uuid::Uuid;

pub use manager::LocationManager;

/// Location creation arguments (simplified from production version)
#[derive(Debug, Serialize, Deserialize)]
pub struct LocationCreateArgs {
	pub path: PathBuf,
	pub name: Option<String>,
	pub index_mode: IndexMode,
}

/// Location indexing mode
#[derive(Debug, Clone, Copy, Serialize, Deserialize, PartialEq, Eq)]
pub enum IndexMode {
	/// Only scan file/directory structure
	Shallow,
	/// Quick scan (metadata only)
	Quick,
	/// Include content hashing for deduplication
	Content,
	/// Full indexing with content analysis and metadata
	Deep,
	/// Full indexing with all features
	Full,
}

impl From<IndexMode> for JobIndexMode {
	fn from(mode: IndexMode) -> Self {
		match mode {
			IndexMode::Shallow => JobIndexMode::Shallow,
			IndexMode::Quick => JobIndexMode::Content,
			IndexMode::Content => JobIndexMode::Content,
			IndexMode::Deep => JobIndexMode::Deep,
			IndexMode::Full => JobIndexMode::Deep,
		}
	}
}

impl From<&str> for IndexMode {
	fn from(s: &str) -> Self {
		match s.to_lowercase().as_str() {
			"shallow" => IndexMode::Shallow,
			"quick" => IndexMode::Quick,
			"content" => IndexMode::Content,
			"deep" => IndexMode::Deep,
			"full" => IndexMode::Full,
			_ => IndexMode::Full,
		}
	}
}

impl std::fmt::Display for IndexMode {
	fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
		match self {
			IndexMode::Shallow => write!(f, "shallow"),
			IndexMode::Quick => write!(f, "quick"),
			IndexMode::Content => write!(f, "content"),
			IndexMode::Deep => write!(f, "deep"),
			IndexMode::Full => write!(f, "full"),
		}
	}
}

/// Managed location representation
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ManagedLocation {
	pub id: Uuid,
	pub name: String,
	pub path: PathBuf,
	pub device_id: i32,
	pub library_id: Uuid,
	pub indexing_enabled: bool,
	pub index_mode: IndexMode,
	pub watch_enabled: bool,
}

/// Location management errors
#[derive(Debug, thiserror::Error)]
pub enum LocationError {
	#[error("Database error: {0}")]
	Database(#[from] sea_orm::DbErr),
	#[error("Path does not exist: {path}")]
	PathNotFound { path: PathBuf },
	#[error("Path not accessible: {path}")]
	PathNotAccessible { path: PathBuf },
	#[error("Location already exists: {path}")]
	LocationExists { path: PathBuf },
	#[error("Location not found: {id}")]
	LocationNotFound { id: Uuid },
	#[error("IO error: {0}")]
	Io(#[from] std::io::Error),
	#[error("Invalid path: {0}")]
	InvalidPath(String),
	#[error("Job error: {0}")]
	Job(#[from] crate::infrastructure::jobs::error::JobError),
	#[error("Other error: {0}")]
	Other(String),
}

pub type LocationResult<T> = Result<T, LocationError>;

/// Create a new location and start indexing (production pattern)
pub async fn create_location(
	library: Arc<Library>,
	events: &EventBus,
	args: LocationCreateArgs,
	device_id: i32,
) -> LocationResult<i32> {
	let path_str = args
		.path
		.to_str()
		.ok_or_else(|| LocationError::InvalidPath("Non-UTF8 path".to_string()))?;

	// Validate path exists
	if !args.path.exists() {
		return Err(LocationError::PathNotFound { path: args.path });
	}

	if !args.path.is_dir() {
		return Err(LocationError::InvalidPath(
			"Path must be a directory".to_string(),
		));
	}

	// Check if location already exists
	let existing = entities::location::Entity::find()
		.filter(entities::location::Column::Path.eq(path_str))
		.one(library.db().conn())
		.await?;

	if existing.is_some() {
		return Err(LocationError::LocationExists { path: args.path });
	}

	// Create location record
	let location_id = Uuid::new_v4();
	let name = args.name.unwrap_or_else(|| {
		args.path
			.file_name()
			.and_then(|n| n.to_str())
			.unwrap_or("Unknown")
			.to_string()
	});

	let location_model = entities::location::ActiveModel {
		id: Set(0), // Auto-increment
		uuid: Set(location_id),
		device_id: Set(device_id),
		path: Set(path_str.to_string()),
		name: Set(Some(name.clone())),
		index_mode: Set(args.index_mode.to_string()),
		scan_state: Set("pending".to_string()),
		last_scan_at: Set(None),
		error_message: Set(None),
		total_file_count: Set(0),
		total_byte_size: Set(0),
		created_at: Set(chrono::Utc::now()),
		updated_at: Set(chrono::Utc::now()),
	};

	let location_record = location_model.insert(library.db().conn()).await?;
	let location_db_id = location_record.id;

	info!("Created location '{}' with ID: {}", name, location_db_id);

	// Emit location added event
	events.emit(Event::LocationAdded {
		library_id: library.id(),
		location_id,
		path: args.path.clone(),
	});

	// Start indexing (simplified - in production this goes through proper job manager)
	start_location_indexing(
		library.clone(),
		events,
		location_db_id,
		location_id,
		args.path,
		args.index_mode,
	)
	.await?;

	Ok(location_db_id)
}

/// Start indexing for a location (production implementation)
async fn start_location_indexing(
	library: Arc<Library>,
	events: &EventBus,
	location_db_id: i32,
	location_uuid: Uuid,
	path: PathBuf,
	index_mode: IndexMode,
) -> LocationResult<()> {
	info!("Starting indexing for location: {}", path.display());

	// Update scan state to "running"
	update_location_scan_state(library.clone(), location_db_id, "running", None).await?;

	// Emit indexing started event
	events.emit(Event::IndexingStarted {
		location_id: location_uuid,
	});

	// Get device UUID for SdPath
	let device_uuid = get_device_uuid(library.clone()).await?;
	let location_sd_path = SdPath::new(device_uuid, path.clone());

	// Create and dispatch indexer job through the proper job manager
	let config = IndexerJobConfig::new(location_uuid, location_sd_path, index_mode.into());
	let indexer_job = IndexerJob::new(config);

	match library.jobs().dispatch(indexer_job).await {
		Ok(job_handle) => {
			info!(
				"Successfully dispatched indexer job {} for location: {}",
				job_handle.id(),
				path.display()
			);

			// Monitor job progress asynchronously
			let events_clone = events.clone();
			let library_clone = library.clone();
			let handle_clone = job_handle.clone();

			tokio::spawn(async move {
				monitor_indexing_job(
					handle_clone,
					events_clone,
					library_clone,
					location_db_id,
					location_uuid,
					path,
				)
				.await;
			});
		}
		Err(e) => {
			error!(
				"Failed to dispatch indexer job for {}: {}",
				path.display(),
				e
			);

			// Update scan state to failed
			if let Err(update_err) = update_location_scan_state(
				library.clone(),
				location_db_id,
				"failed",
				Some(e.to_string()),
			)
			.await
			{
				error!("Failed to update scan state: {}", update_err);
			}

			events.emit(Event::IndexingFailed {
				location_id: location_uuid,
				error: e.to_string(),
			});

			return Err(LocationError::Other(format!(
				"Failed to start indexing: {}",
				e
			)));
		}
	}

	Ok(())
}

/// Monitor indexing job progress and update location state accordingly
async fn monitor_indexing_job(
	job_handle: JobHandle,
	events: EventBus,
	library: Arc<Library>,
	location_db_id: i32,
	location_uuid: Uuid,
	path: PathBuf,
) {
	info!(
		"Monitoring indexer job {} for location: {}",
		job_handle.id(),
		path.display()
	);

	// Wait for job completion
	let job_result = job_handle.wait().await;

	match job_result {
		Ok(output) => {
			info!(
				"Indexing completed successfully for location: {}",
				path.display()
			);

			// Parse output to get statistics
			if let Some(indexer_output) = output.as_indexed() {
				// Update location stats
				if let Err(e) = update_location_stats(
					library.clone(),
					location_db_id,
					indexer_output.total_files,
					indexer_output.total_bytes,
				)
				.await
				{
					error!("Failed to update location stats: {}", e);
				}

				// Update scan state to completed
				if let Err(e) =
					update_location_scan_state(library.clone(), location_db_id, "completed", None)
						.await
				{
					error!("Failed to update scan state: {}", e);
				}

				// Emit completion events
				events.emit(Event::IndexingCompleted {
					location_id: location_uuid,
					total_files: indexer_output.total_files,
					total_dirs: indexer_output.total_dirs,
				});

				events.emit(Event::FilesIndexed {
					library_id: library.id(),
					location_id: location_uuid,
					count: indexer_output.total_files as usize,
				});

				info!(
					"Location indexing completed: {} ({} files, {} dirs, {} bytes)",
					path.display(),
					indexer_output.total_files,
					indexer_output.total_dirs,
					indexer_output.total_bytes
				);
			} else {
				warn!("Job completed but output format was unexpected");

				// Update scan state to completed anyway
				if let Err(e) =
					update_location_scan_state(library.clone(), location_db_id, "completed", None)
						.await
				{
					error!("Failed to update scan state: {}", e);
				}
			}
		}
		Err(e) => {
			error!("Indexing failed for {}: {}", path.display(), e);

			// Update scan state to failed
			if let Err(update_err) = update_location_scan_state(
				library.clone(),
				location_db_id,
				"failed",
				Some(e.to_string()),
			)
			.await
			{
				error!("Failed to update scan state: {}", update_err);
			}

			events.emit(Event::IndexingFailed {
				location_id: location_uuid,
				error: e.to_string(),
			});
		}
	}
}

/// Scan directory to get basic stats
async fn scan_directory_stats(path: &PathBuf) -> Result<(u64, u64), std::io::Error> {
	let mut file_count = 0u64;
	let mut total_size = 0u64;

	let mut stack = vec![path.clone()];

	while let Some(current_path) = stack.pop() {
		if let Ok(mut entries) = fs::read_dir(&current_path).await {
			while let Ok(Some(entry)) = entries.next_entry().await {
				if let Ok(metadata) = entry.metadata().await {
					if metadata.is_file() {
						file_count += 1;
						total_size += metadata.len();
					} else if metadata.is_dir() {
						stack.push(entry.path());
					}
				}
			}
		}
	}

	Ok((file_count, total_size))
}

/// Update location scan state
async fn update_location_scan_state(
	library: Arc<Library>,
	location_id: i32,
	state: &str,
	error_message: Option<String>,
) -> LocationResult<()> {
	let location = entities::location::Entity::find_by_id(location_id)
		.one(library.db().conn())
		.await?
		.ok_or_else(|| LocationError::LocationNotFound { id: Uuid::nil() })?;

	let mut active_location: entities::location::ActiveModel = location.into();
	active_location.scan_state = Set(state.to_string());
	active_location.error_message = Set(error_message);
	active_location.updated_at = Set(chrono::Utc::now());

	if state == "running" {
		active_location.last_scan_at = Set(Some(chrono::Utc::now()));
	}

	active_location.update(library.db().conn()).await?;
	Ok(())
}

/// Update location statistics
async fn update_location_stats(
	library: Arc<Library>,
	location_id: i32,
	file_count: u64,
	total_size: u64,
) -> LocationResult<()> {
	let location = entities::location::Entity::find_by_id(location_id)
		.one(library.db().conn())
		.await?
		.ok_or_else(|| LocationError::LocationNotFound { id: Uuid::nil() })?;

	let mut active_location: entities::location::ActiveModel = location.into();
	active_location.total_file_count = Set(file_count as i64);
	active_location.total_byte_size = Set(total_size as i64);
	active_location.updated_at = Set(chrono::Utc::now());

	active_location.update(library.db().conn()).await?;
	Ok(())
}

/// Get device UUID for current device
async fn get_device_uuid(library: Arc<Library>) -> LocationResult<Uuid> {
	let device = entities::device::Entity::find()
		.one(library.db().conn())
		.await?
		.ok_or_else(|| LocationError::InvalidPath("No device found".to_string()))?;

	Ok(device.uuid)
}

/// List all locations for a library
pub async fn list_locations(
	library: Arc<Library>,
) -> LocationResult<Vec<entities::location::Model>> {
	Ok(entities::location::Entity::find()
		.all(library.db().conn())
		.await?)
}
```

## src/lib.rs

```rust
#![allow(warnings)]
//! Spacedrive Core v2
//!
//! A unified, simplified architecture for cross-platform file management.

pub mod config;
pub mod context;
pub mod device;
pub mod domain;
pub mod file_type;
pub mod infrastructure;
pub mod keys;
pub mod library;
pub mod location;
pub mod operations;
pub mod services;
pub mod shared;
pub mod test_framework;
pub mod volume;

use services::networking::protocols::PairingProtocolHandler;

// Compatibility module for legacy networking references
pub mod networking {
	pub use crate::services::networking::*;
}

use crate::config::AppConfig;
use crate::context::CoreContext;
use crate::device::DeviceManager;
use crate::infrastructure::actions::manager::ActionManager;
use crate::infrastructure::events::{Event, EventBus};
use crate::library::LibraryManager;
use crate::services::Services;
use crate::volume::{VolumeDetectionConfig, VolumeManager};
use std::path::PathBuf;
use std::sync::Arc;
use tokio::sync::{mpsc, RwLock};
use tracing::{error, info};

/// Pending pairing request information
#[derive(Debug, Clone)]
pub struct PendingPairingRequest {
	pub request_id: uuid::Uuid,
	pub device_id: uuid::Uuid,
	pub device_name: String,
	pub received_at: chrono::DateTime<chrono::Utc>,
}

/// Spacedrop request message
#[derive(serde::Serialize, serde::Deserialize)]
struct SpacedropRequest {
	transfer_id: uuid::Uuid,
	file_path: String,
	sender_name: String,
	message: Option<String>,
	file_size: u64,
}

// NOTE: SimplePairingUI has been moved to CLI infrastructure
// See: src/infrastructure/cli/pairing_ui.rs for CLI-specific implementations

/// Bridge between networking events and core events
pub struct NetworkEventBridge {
	network_events: mpsc::UnboundedReceiver<networking::NetworkEvent>,
	core_events: Arc<EventBus>,
}

impl NetworkEventBridge {
	pub fn new(
		network_events: mpsc::UnboundedReceiver<networking::NetworkEvent>,
		core_events: Arc<EventBus>,
	) -> Self {
		Self {
			network_events,
			core_events,
		}
	}

	pub async fn run(mut self) {
		while let Some(event) = self.network_events.recv().await {
			if let Some(core_event) = self.translate_event(event) {
				self.core_events.emit(core_event);
			}
		}
	}

	fn translate_event(&self, event: networking::NetworkEvent) -> Option<Event> {
		match event {
			networking::NetworkEvent::ConnectionEstablished { device_id, .. } => {
				Some(Event::DeviceConnected {
					device_id,
					device_name: "Connected Device".to_string(),
				})
			}
			networking::NetworkEvent::ConnectionLost { device_id, .. } => {
				Some(Event::DeviceDisconnected { device_id })
			}
			networking::NetworkEvent::PairingCompleted {
				device_id,
				device_info,
			} => Some(Event::DeviceConnected {
				device_id,
				device_name: device_info.device_name,
			}),
			_ => None, // Some events don't map to core events
		}
	}
}

/// The main context for all core operations
pub struct Core {
	/// Application configuration
	pub config: Arc<RwLock<AppConfig>>,

	/// Device manager
	pub device: Arc<DeviceManager>,

	/// Library manager
	pub libraries: Arc<LibraryManager>,

	/// Volume manager
	pub volumes: Arc<VolumeManager>,

	/// Event bus for state changes
	pub events: Arc<EventBus>,

	/// Container for high-level services
	pub services: Services,

	/// Shared context for core components
	pub context: Arc<CoreContext>,
}

impl Core {
	/// Initialize a new Core instance with default data directory
	pub async fn new() -> Result<Self, Box<dyn std::error::Error>> {
		let data_dir = crate::config::default_data_dir()?;
		Self::new_with_config(data_dir).await
	}

	/// Initialize a new Core instance with custom data directory
	pub async fn new_with_config(data_dir: PathBuf) -> Result<Self, Box<dyn std::error::Error>> {
		info!("Initializing Spacedrive Core at {:?}", data_dir);

		// 1. Load or create app config
		let config = AppConfig::load_or_create(&data_dir)?;
		config.ensure_directories()?;
		let config = Arc::new(RwLock::new(config));

		// 2. Initialize device manager
		let device = Arc::new(DeviceManager::init_with_path(&data_dir)?);
		// Set the global device ID for legacy compatibility
		shared::types::set_current_device_id(device.device_id()?);

		// 3. Create event bus
		let events = Arc::new(EventBus::default());

		// 4. Initialize volume manager
		let volume_config = VolumeDetectionConfig::default();
		let device_id = device.device_id()?;
		let volumes = Arc::new(VolumeManager::new(device_id, volume_config, events.clone()));

		// 5. Initialize volume detection
		info!("Initializing volume detection...");
		match volumes.initialize().await {
			Ok(()) => info!("Volume manager initialized"),
			Err(e) => error!("Failed to initialize volume manager: {}", e),
		}

		// 6. Initialize library manager with libraries directory
		let libraries_dir = config.read().await.libraries_dir();
		let libraries = Arc::new(LibraryManager::new_with_dir(libraries_dir, events.clone()));

		// 7. Initialize library key manager
		let library_key_manager =
			Arc::new(crate::keys::library_key_manager::LibraryKeyManager::new()?);

		// 8. Register all job types
		info!("Registering job types...");
		crate::operations::register_all_jobs();
		info!("Job types registered");

		// 9. Create the context that will be shared with services
		let context = Arc::new(CoreContext::new(
			events.clone(),
			device.clone(),
			libraries.clone(),
			volumes.clone(),
			library_key_manager.clone(),
		));

		// 10. Auto-load all libraries with context for job manager initialization
		info!("Loading existing libraries...");
		match libraries.load_all_with_context(context.clone()).await {
			Ok(count) => info!("Loaded {} libraries", count),
			Err(e) => error!("Failed to load libraries: {}", e),
		}

		// 11. Initialize services, passing them the context
		let services = Services::new(context.clone());

		info!("Starting background services...");
		match services.start_all().await {
			Ok(()) => info!("Background services started"),
			Err(e) => error!("Failed to start services: {}", e),
		}

		// 12. Initialize ActionManager and set it in context
		let action_manager = Arc::new(crate::infrastructure::actions::manager::ActionManager::new(
			context.clone(),
		));
		context.set_action_manager(action_manager).await;

		// 13. Emit startup event
		events.emit(Event::CoreStarted);

		Ok(Self {
			config,
			device,
			libraries,
			volumes,
			events,
			services,
			context,
		})
	}

	/// Get the application configuration
	pub fn config(&self) -> Arc<RwLock<AppConfig>> {
		self.config.clone()
	}

	/// Initialize networking using master key
	pub async fn init_networking(&mut self) -> Result<(), Box<dyn std::error::Error>> {
		self.init_networking_with_logger(Arc::new(networking::SilentLogger))
			.await
	}

	/// Initialize networking with custom logger
	pub async fn init_networking_with_logger(
		&mut self,
		logger: Arc<dyn networking::NetworkLogger>,
	) -> Result<(), Box<dyn std::error::Error>> {
		logger.info("Initializing networking...").await;

		// Initialize networking service through the services container
		let data_dir = self.config.read().await.data_dir.clone();
		self.services
			.init_networking(
				self.device.clone(),
				self.services.library_key_manager.clone(),
				data_dir,
			)
			.await?;

		// Start the networking service
		self.services.start_networking().await?;

		// Get the networking service for protocol registration
		if let Some(networking_service) = self.services.networking() {
			// Register default protocol handlers
			self.register_default_protocols(&networking_service).await?;

			// Set up event bridge to integrate with core event system
			let event_bridge = NetworkEventBridge::new(
				networking_service
					.subscribe_events()
					.await
					.unwrap_or_else(|| {
						let (_, rx) = tokio::sync::mpsc::unbounded_channel();
						rx
					}),
				self.events.clone(),
			);
			tokio::spawn(event_bridge.run());

			// Make networking service available to the context for other services
			self.context.set_networking(networking_service).await;
		}

		logger.info("Networking initialized successfully").await;
		Ok(())
	}

	/// Register default protocol handlers
	async fn register_default_protocols(
		&self,
		networking: &networking::NetworkingService,
	) -> Result<(), Box<dyn std::error::Error>> {
		let logger = std::sync::Arc::new(networking::utils::logging::ConsoleLogger);

		// Get command sender for the pairing handler's state machine
		let command_sender = networking
			.command_sender()
			.ok_or("NetworkingEventLoop command sender not available")?
			.clone();

		let pairing_handler = Arc::new(networking::protocols::PairingProtocolHandler::new(
			networking.identity().clone(),
			networking.device_registry(),
			logger.clone(),
			command_sender,
		));

		// Start the state machine task for pairing
		networking::protocols::PairingProtocolHandler::start_state_machine_task(
			pairing_handler.clone(),
		);

		// Start cleanup task for expired sessions
		networking::protocols::PairingProtocolHandler::start_cleanup_task(pairing_handler.clone());

		let messaging_handler = networking::protocols::MessagingProtocolHandler::new();
		let mut file_transfer_handler =
			networking::protocols::FileTransferProtocolHandler::new_default(logger.clone());

		// Inject device registry into file transfer handler for encryption
		file_transfer_handler.set_device_registry(networking.device_registry());

		let protocol_registry = networking.protocol_registry();
		{
			let mut registry = protocol_registry.write().await;
			registry.register_handler(pairing_handler)?;
			registry.register_handler(Arc::new(messaging_handler))?;
			registry.register_handler(Arc::new(file_transfer_handler))?;
		}

		Ok(())
	}

	/// Initialize networking from Arc<Core> - for daemon use
	pub async fn init_networking_shared(
		core: Arc<Core>,
	) -> Result<Arc<Core>, Box<dyn std::error::Error>> {
		info!("Initializing networking for shared core...");

		// Create a new Core with networking enabled
		let mut new_core =
			Core::new_with_config(core.config().read().await.data_dir.clone()).await?;

		// Initialize networking on the new core
		new_core.init_networking().await?;

		info!("Networking initialized successfully for shared core");
		Ok(Arc::new(new_core))
	}

	/// Start the networking service (must be called after init_networking)
	pub async fn start_networking(&self) -> Result<(), Box<dyn std::error::Error>> {
		if let Some(_networking) = self.services.networking() {
			// Networking is already started in init_networking
			info!("Networking system is active and ready");
			Ok(())
		} else {
			Err("Networking not initialized. Call init_networking() first.".into())
		}
	}

	/// Get the networking service (if initialized)
	pub fn networking(&self) -> Option<Arc<networking::NetworkingService>> {
		self.services.networking()
	}

	/// Get list of connected devices
	pub async fn get_connected_devices(
		&self,
	) -> Result<Vec<uuid::Uuid>, Box<dyn std::error::Error>> {
		Ok(self.services.device.get_connected_devices().await?)
	}

	/// Get detailed information about connected devices
	pub async fn get_connected_devices_info(
		&self,
	) -> Result<Vec<networking::DeviceInfo>, Box<dyn std::error::Error>> {
		Ok(self.services.device.get_connected_devices_info().await?)
	}

	/// Add a location to the file system watcher
	pub async fn add_watched_location(
		&self,
		location_id: uuid::Uuid,
		library_id: uuid::Uuid,
		path: std::path::PathBuf,
		enabled: bool,
	) -> Result<(), Box<dyn std::error::Error>> {
		use crate::services::location_watcher::WatchedLocation;

		let watched_location = WatchedLocation {
			id: location_id,
			library_id,
			path,
			enabled,
		};

		Ok(self
			.services
			.location_watcher
			.add_location(watched_location)
			.await?)
	}

	/// Remove a location from the file system watcher
	pub async fn remove_watched_location(
		&self,
		location_id: uuid::Uuid,
	) -> Result<(), Box<dyn std::error::Error>> {
		Ok(self
			.services
			.location_watcher
			.remove_location(location_id)
			.await?)
	}

	/// Update file watching settings for a location
	pub async fn update_watched_location(
		&self,
		location_id: uuid::Uuid,
		enabled: bool,
	) -> Result<(), Box<dyn std::error::Error>> {
		Ok(self
			.services
			.location_watcher
			.update_location(location_id, enabled)
			.await?)
	}

	/// Get all currently watched locations
	pub async fn get_watched_locations(
		&self,
	) -> Vec<crate::services::location_watcher::WatchedLocation> {
		self.services.location_watcher.get_watched_locations().await
	}

	/// Shutdown the core gracefully
	pub async fn shutdown(&self) -> Result<(), Box<dyn std::error::Error>> {
		info!("Shutting down Spacedrive Core...");

		// Networking service is stopped by services.stop_all()

		// Stop all services
		self.services.stop_all().await?;

		// Stop volume monitoring
		self.volumes.stop_monitoring().await;

		// Close all libraries
		self.libraries.close_all().await?;

		// Save configuration
		self.config.write().await.save()?;

		// Emit shutdown event
		self.events.emit(Event::CoreShutdown);

		info!("Spacedrive Core shutdown complete");
		Ok(())
	}
}
```

## src/library/error.rs

```rust
//! Library-specific error types

use std::path::PathBuf;
use thiserror::Error;
use uuid::Uuid;

/// Library operation errors
#[derive(Error, Debug)]
pub enum LibraryError {
    /// Library is already open
    #[error("Library {0} is already open")]
    AlreadyOpen(Uuid),
    
    /// Library is already in use by another process
    #[error("Library is already in use by another process")]
    AlreadyInUse,
    
    /// Stale lock file detected
    #[error("Stale lock file detected - library may have crashed previously")]
    StaleLock,
    
    /// Not a valid library directory
    #[error("Not a valid library directory: {0}")]
    NotALibrary(PathBuf),
    
    /// Library not found
    #[error("Library not found: {0}")]
    NotFound(String),
    
    /// Invalid library name
    #[error("Invalid library name: {0}")]
    InvalidName(String),
    
    /// Library already exists
    #[error("Library already exists at: {0}")]
    AlreadyExists(PathBuf),
    
    /// Configuration error
    #[error("Configuration error: {0}")]
    ConfigError(String),
    
    /// Database error
    #[error("Database error: {0}")]
    DatabaseError(#[from] sea_orm::DbErr),
    
    /// IO error
    #[error("IO error: {0}")]
    IoError(#[from] std::io::Error),
    
    /// JSON error
    #[error("JSON error: {0}")]
    JsonError(#[from] serde_json::Error),
    
    /// Job system error
    #[error("Job system error: {0}")]
    JobError(#[from] crate::infrastructure::jobs::error::JobError),
    
    /// Generic error
    #[error("{0}")]
    Other(String),
}

/// Result type for library operations
pub type Result<T> = std::result::Result<T, LibraryError>;```

## src/library/config.rs

```rust
//! Library configuration types

use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use uuid::Uuid;

/// Library configuration stored in library.json
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LibraryConfig {
    /// Version of the configuration format
    pub version: u32,
    
    /// Unique identifier for this library
    pub id: Uuid,
    
    /// Human-readable name
    pub name: String,
    
    /// Optional description
    pub description: Option<String>,
    
    /// When the library was created
    pub created_at: DateTime<Utc>,
    
    /// When the library was last modified
    pub updated_at: DateTime<Utc>,
    
    /// Library-specific settings
    pub settings: LibrarySettings,
    
    /// Library statistics
    pub statistics: LibraryStatistics,
}

/// Library-specific settings
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LibrarySettings {
    /// Whether to generate thumbnails for media files
    pub generate_thumbnails: bool,
    
    /// Thumbnail quality (0-100)
    pub thumbnail_quality: u8,
    
    /// Whether to enable AI-powered tagging
    pub enable_ai_tagging: bool,
    
    /// Whether sync is enabled for this library
    pub sync_enabled: bool,
    
    /// Whether the library is encrypted at rest
    pub encryption_enabled: bool,
    
    /// Custom thumbnail sizes to generate
    pub thumbnail_sizes: Vec<u32>,
    
    /// File extensions to ignore during indexing
    pub ignored_extensions: Vec<String>,
    
    /// Maximum file size to index (in bytes)
    pub max_file_size: Option<u64>,
    
    /// Whether to automatically track system volumes
    pub auto_track_system_volumes: bool,
    
    /// Whether to automatically track external volumes when connected
    pub auto_track_external_volumes: bool,
}

impl LibraryConfig {
    /// Load library configuration from a JSON file
    pub async fn load(path: &std::path::Path) -> Result<Self, super::error::LibraryError> {
        let config_data = tokio::fs::read_to_string(path).await
            .map_err(|e| super::error::LibraryError::IoError(e))?;
        let config: LibraryConfig = serde_json::from_str(&config_data)
            .map_err(|e| super::error::LibraryError::JsonError(e))?;
        Ok(config)
    }
}

impl Default for LibrarySettings {
    fn default() -> Self {
        Self {
            generate_thumbnails: true,
            thumbnail_quality: 85,
            enable_ai_tagging: false,
            sync_enabled: false,
            encryption_enabled: false,
            thumbnail_sizes: vec![128, 256, 512],
            ignored_extensions: vec![
                ".tmp".to_string(),
                ".temp".to_string(),
                ".cache".to_string(),
                ".part".to_string(),
            ],
            max_file_size: Some(100 * 1024 * 1024 * 1024), // 100GB
            auto_track_system_volumes: true, // Default to true for user convenience
            auto_track_external_volumes: false, // Default to false for privacy
        }
    }
}

/// Library statistics
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LibraryStatistics {
    /// Total number of files indexed
    pub total_files: u64,
    
    /// Total size of all files in bytes
    pub total_size: u64,
    
    /// Number of locations in this library
    pub location_count: u32,
    
    /// Number of tags created
    pub tag_count: u32,
    
    /// Number of thumbnails generated
    pub thumbnail_count: u64,
    
    /// Last time the library was fully indexed
    pub last_indexed: Option<DateTime<Utc>>,
    
    /// When these statistics were last updated
    pub updated_at: DateTime<Utc>,
}

impl Default for LibraryStatistics {
    fn default() -> Self {
        Self {
            total_files: 0,
            total_size: 0,
            location_count: 0,
            tag_count: 0,
            thumbnail_count: 0,
            last_indexed: None,
            updated_at: Utc::now(),
        }
    }
}

/// Thumbnail generation metadata
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ThumbnailMetadata {
    /// Version of the thumbnail format
    pub version: u32,
    
    /// Quality setting used for generation
    pub quality: u8,
    
    /// Sizes that were generated
    pub sizes: Vec<u32>,
    
    /// When this metadata was created
    pub created_at: DateTime<Utc>,
}

impl Default for ThumbnailMetadata {
    fn default() -> Self {
        Self {
            version: 1,
            quality: 85,
            sizes: vec![128, 256, 512],
            created_at: Utc::now(),
        }
    }
}```

## src/library/manager.rs

```rust
//! Library manager - handles creation, opening, and discovery of libraries

use super::{
	config::{LibraryConfig, LibrarySettings, LibraryStatistics, ThumbnailMetadata},
	error::{LibraryError, Result},
	lock::LibraryLock,
	Library, LIBRARY_CONFIG_VERSION, LIBRARY_EXTENSION,
};
use crate::{
	context::CoreContext,
	infrastructure::{
		database::{entities, Database},
		events::{Event, EventBus},
		jobs::manager::JobManager,
	},
};
use chrono::Utc;
use sea_orm::{ActiveModelTrait, ColumnTrait, EntityTrait, QueryFilter};
use std::collections::HashMap;
use std::path::{Path, PathBuf};
use std::sync::Arc;
use tokio::sync::RwLock;
use tracing::{error, info, warn};
use uuid::Uuid;

/// Information about a discovered library
#[derive(Debug, Clone)]
pub struct DiscoveredLibrary {
	/// Path to the library directory
	pub path: PathBuf,

	/// Library configuration
	pub config: LibraryConfig,

	/// Whether the library is currently locked
	pub is_locked: bool,
}

/// Manages all Spacedrive libraries
pub struct LibraryManager {
	/// Currently open libraries
	libraries: Arc<RwLock<HashMap<Uuid, Arc<Library>>>>,

	/// Paths to search for libraries
	search_paths: Vec<PathBuf>,

	/// Event bus for library events
	event_bus: Arc<EventBus>,
}

impl LibraryManager {
	/// Create a new library manager
	pub fn new(event_bus: Arc<EventBus>) -> Self {
		// Default search paths
		let mut search_paths = vec![];

		// Add user's home directory
		if let Some(home) = dirs::home_dir() {
			search_paths.push(home.join("Spacedrive").join("Libraries"));
		}

		Self {
			libraries: Arc::new(RwLock::new(HashMap::new())),
			search_paths,
			event_bus,
		}
	}

	/// Create a new library manager with a specific libraries directory
	pub fn new_with_dir(libraries_dir: PathBuf, event_bus: Arc<EventBus>) -> Self {
		let search_paths = vec![libraries_dir];

		Self {
			libraries: Arc::new(RwLock::new(HashMap::new())),
			search_paths,
			event_bus,
		}
	}

	/// Load all discovered libraries with context for job manager initialization
	pub async fn load_all_with_context(&self, context: Arc<CoreContext>) -> Result<usize> {
		let discovered = self.scan_for_libraries().await?;
		let mut loaded_count = 0;

		for disc in discovered {
			if !disc.is_locked {
				match self
					.open_library_with_context(&disc.path, context.clone())
					.await
				{
					Ok(_) => loaded_count += 1,
					Err(e) => {
						warn!("Failed to load library at {:?}: {}", disc.path, e);
					}
				}
			}
		}

		Ok(loaded_count)
	}

	/// Add a search path for libraries
	pub fn add_search_path(&mut self, path: PathBuf) {
		if !self.search_paths.contains(&path) {
			self.search_paths.push(path);
		}
	}

	/// Create a new library
	pub async fn create_library(
		&self,
		name: impl Into<String>,
		location: Option<PathBuf>,
		context: Arc<CoreContext>,
	) -> Result<Arc<Library>> {
		let name = name.into();

		// Validate name
		if name.is_empty() {
			return Err(LibraryError::InvalidName(
				"Name cannot be empty".to_string(),
			));
		}

		// Sanitize name for filesystem
		let safe_name = sanitize_filename(&name);

		// Determine base path
		let base_path = location.unwrap_or_else(|| {
			self.search_paths.first().cloned().unwrap_or_else(|| {
				dirs::home_dir()
					.unwrap_or_else(|| PathBuf::from("."))
					.join("Spacedrive")
					.join("Libraries")
			})
		});

		// Ensure base path exists
		tokio::fs::create_dir_all(&base_path).await?;

		// Find unique library path
		let library_path = find_unique_library_path(&base_path, &safe_name).await?;

		// Create library directory
		tokio::fs::create_dir_all(&library_path).await?;

		// Initialize library
		self.initialize_library(&library_path, name).await?;

		// Open the newly created library with context
		let library = self
			.open_library_with_context(&library_path, context.clone())
			.await?;

		// Emit event
		self.event_bus.emit(Event::LibraryCreated {
			id: library.id(),
			name: library.name().await,
			path: library_path,
		});

		Ok(library)
	}

	/// Open a library from a path
	pub async fn open_library(&self, path: impl AsRef<Path>) -> Result<Arc<Library>> {
		let path = path.as_ref();

		// Validate it's a library directory
		if !is_library_directory(path) {
			return Err(LibraryError::NotALibrary(path.to_path_buf()));
		}

		// Acquire lock
		let lock = LibraryLock::acquire(path)?;

		// Load configuration
		let config_path = path.join("library.json");
		let config_data = tokio::fs::read_to_string(&config_path).await?;
		let config: LibraryConfig = serde_json::from_str(&config_data)?;

		// Check if already open
		{
			let libraries = self.libraries.read().await;
			if libraries.contains_key(&config.id) {
				return Err(LibraryError::AlreadyOpen(config.id));
			}
		}

		// Open database
		let db_path = path.join("database.db");
		let db = Arc::new(Database::open(&db_path).await?);

		// This method now requires context - redirect to open_library_with_context
		return Err(LibraryError::Other(
			"Context required for library creation".to_string(),
		));
	}

	/// Open a library with CoreContext for job manager initialization
	pub async fn open_library_with_context(
		&self,
		path: &Path,
		context: Arc<CoreContext>,
	) -> Result<Arc<Library>> {
		info!("Opening library at {:?}", path);

		// Acquire lock
		let lock = LibraryLock::acquire(path)?;

		// Load config
		let config_path = path.join("library.json");
		let config = LibraryConfig::load(&config_path).await?;

		// Ensure library ID is set
		if config.id.is_nil() {
			return Err(LibraryError::Other("Library config has nil ID".to_string()));
		}

		// Open database
		let db_path = path.join("database.db");
		let db = Arc::new(Database::open(&db_path).await?);

		// Create job manager with context
		let job_manager =
			Arc::new(JobManager::new(path.to_path_buf(), context.clone(), config.id).await?);
		job_manager.initialize().await?;

		// Create library instance
		let library = Arc::new(Library {
			path: path.to_path_buf(),
			config: RwLock::new(config.clone()),
			db,
			jobs: job_manager,
			_lock: lock,
		});

		// Ensure device is registered in this library
		if let Err(e) = self.ensure_device_registered(&library, &context).await {
			warn!("Failed to register device in library {}: {}", config.id, e);
		}

		// Register library
		{
			let mut libraries = self.libraries.write().await;
			libraries.insert(config.id, library.clone());
		}

		// Auto-track user-relevant volumes for this library
		info!(
			"Auto-tracking user-relevant volumes for library {}",
			config.name
		);
		if let Err(e) = context
			.volume_manager
			.auto_track_user_volumes(&library)
			.await
		{
			warn!("Failed to auto-track user-relevant volumes: {}", e);
		}

		// Emit event
		self.event_bus.emit(Event::LibraryOpened {
			id: config.id,
			name: config.name,
			path: path.to_path_buf(),
		});

		info!("Opened library {} at {:?}", library.id(), path);

		Ok(library)
	}

	/// Close a library
	pub async fn close_library(&self, id: Uuid) -> Result<()> {
		let library = {
			let mut libraries = self.libraries.write().await;
			libraries.remove(&id)
		};

		if let Some(library) = library {
			let name = library.name().await;

			// Shutdown the library gracefully
			if let Err(e) = library.shutdown().await {
				error!("Error during library shutdown: {}", e);
				// Continue with close even if shutdown has errors
			}

			// Emit event
			self.event_bus.emit(Event::LibraryClosed { id, name });

			info!("Closed library {}", id);
			Ok(())
		} else {
			Err(LibraryError::NotFound(id.to_string()))
		}
	}

	/// Get an open library by ID
	pub async fn get_library(&self, id: Uuid) -> Option<Arc<Library>> {
		self.libraries.read().await.get(&id).cloned()
	}

	/// Get all open libraries
	pub async fn get_open_libraries(&self) -> Vec<Arc<Library>> {
		self.libraries.read().await.values().cloned().collect()
	}

	/// Get the primary library (first available library)
	pub async fn get_primary_library(&self) -> Option<Arc<Library>> {
		self.get_open_libraries().await.into_iter().next()
	}

	/// List all open libraries
	pub async fn list(&self) -> Vec<Arc<Library>> {
		self.get_open_libraries().await
	}

	/// Load all libraries from the search paths
	pub async fn load_all(&self) -> Result<usize> {
		let mut loaded_count = 0;

		for search_path in &self.search_paths.clone() {
			if !search_path.exists() {
				info!("Search path {:?} does not exist, skipping", search_path);
				continue;
			}

			match tokio::fs::read_dir(search_path).await {
				Ok(mut entries) => {
					while let Some(entry) = entries.next_entry().await? {
						let path = entry.path();

						if is_library_directory(&path) {
							match self.open_library(&path).await {
								Ok(_) => {
									loaded_count += 1;
									info!("Auto-loaded library from {:?}", path);
								}
								Err(LibraryError::AlreadyOpen(_)) => {
									// Library is already open, skip
								}
								Err(e) => {
									warn!("Failed to auto-load library from {:?}: {}", path, e);
								}
							}
						}
					}
				}
				Err(e) => {
					warn!("Failed to read directory {:?}: {}", search_path, e);
				}
			}
		}

		Ok(loaded_count)
	}

	/// Close all open libraries
	pub async fn close_all(&self) -> Result<()> {
		let library_ids: Vec<Uuid> = self.libraries.read().await.keys().cloned().collect();

		for id in library_ids {
			if let Err(e) = self.close_library(id).await {
				error!("Failed to close library {}: {}", id, e);
			}
		}

		Ok(())
	}

	/// Scan search paths for libraries
	pub async fn scan_for_libraries(&self) -> Result<Vec<DiscoveredLibrary>> {
		let mut discovered = Vec::new();

		for search_path in &self.search_paths {
			if !search_path.exists() {
				continue;
			}

			let mut entries = tokio::fs::read_dir(search_path).await?;

			while let Some(entry) = entries.next_entry().await? {
				let path = entry.path();

				if is_library_directory(&path) {
					match self.read_library_info(&path).await {
						Ok(info) => discovered.push(info),
						Err(e) => {
							error!("Failed to read library at {:?}: {}", path, e);
						}
					}
				}
			}
		}

		Ok(discovered)
	}

	/// Initialize a new library directory
	async fn initialize_library(&self, path: &Path, name: String) -> Result<()> {
		// Create subdirectories
		tokio::fs::create_dir_all(path.join("thumbnails")).await?;
		tokio::fs::create_dir_all(path.join("previews")).await?;
		tokio::fs::create_dir_all(path.join("indexes")).await?;
		tokio::fs::create_dir_all(path.join("exports")).await?;

		// Create configuration
		let config = LibraryConfig {
			version: LIBRARY_CONFIG_VERSION,
			id: Uuid::new_v4(),
			name,
			description: None,
			created_at: Utc::now(),
			updated_at: Utc::now(),
			settings: LibrarySettings::default(),
			statistics: LibraryStatistics::default(),
		};

		// Save configuration
		let config_path = path.join("library.json");
		let json = serde_json::to_string_pretty(&config)?;
		tokio::fs::write(config_path, json).await?;

		// Initialize database
		let db_path = path.join("database.db");
		let db = Database::create(&db_path).await?;

		// Run initial migrations
		db.migrate().await?;

		// Create thumbnail metadata
		let thumb_meta = ThumbnailMetadata::default();
		let thumb_meta_path = path.join("thumbnails").join("metadata.json");
		let json = serde_json::to_string_pretty(&thumb_meta)?;
		tokio::fs::write(thumb_meta_path, json).await?;

		info!("Initialized new library '{}' at {:?}", config.name, path);

		Ok(())
	}

	/// Read library information without opening it
	async fn read_library_info(&self, path: &Path) -> Result<DiscoveredLibrary> {
		let config_path = path.join("library.json");
		let config_data = tokio::fs::read_to_string(&config_path).await?;
		let config: LibraryConfig = serde_json::from_str(&config_data)?;

		// Check if locked (but ignore stale locks)
		let lock_path = path.join(".sdlibrary.lock");
		let is_locked = if lock_path.exists() {
			// Use the LibraryLock's stale detection logic
			!LibraryLock::is_lock_stale(&lock_path).unwrap_or(true)
		} else {
			false
		};

		Ok(DiscoveredLibrary {
			path: path.to_path_buf(),
			config,
			is_locked,
		})
	}

	/// Ensure the current device is registered in the library
	async fn ensure_device_registered(
		&self,
		library: &Arc<Library>,
		context: &Arc<CoreContext>,
	) -> Result<()> {
		let db = library.db();
		let device = context
			.device_manager
			.to_device()
			.map_err(|e| LibraryError::Other(format!("Failed to get device info: {}", e)))?;

		// Check if device exists
		let existing = entities::device::Entity::find()
			.filter(entities::device::Column::Uuid.eq(device.id))
			.one(db.conn())
			.await
			.map_err(LibraryError::DatabaseError)?;

		if existing.is_none() {
			// Register the device
			use sea_orm::ActiveValue::Set;
			let device_model = entities::device::ActiveModel {
				id: sea_orm::ActiveValue::NotSet,
				uuid: Set(device.id),
				name: Set(device.name.clone()),
				os: Set(device.os.to_string()),
				os_version: Set(None),
				hardware_model: Set(device.hardware_model),
				network_addresses: Set(serde_json::json!(device.network_addresses)),
				is_online: Set(true),
				last_seen_at: Set(device.last_seen_at),
				capabilities: Set(serde_json::json!({
					"indexing": true,
					"p2p": true,
					"volume_detection": true
				})),
				sync_leadership: Set(serde_json::json!(device.sync_leadership)),
				created_at: Set(device.created_at),
				updated_at: Set(device.updated_at),
			};

			device_model
				.insert(db.conn())
				.await
				.map_err(LibraryError::DatabaseError)?;

			info!(
				"Registered device {} in library {}",
				device.id,
				library.id()
			);
		}

		Ok(())
	}
}

/// Check if a path is a library directory
fn is_library_directory(path: &Path) -> bool {
	path.extension()
		.and_then(|ext| ext.to_str())
		.map(|ext| ext == LIBRARY_EXTENSION)
		.unwrap_or(false)
}

/// Sanitize a filename for safe filesystem usage
fn sanitize_filename(name: &str) -> String {
	// Replace problematic characters
	name.chars()
		.map(|c| match c {
			'/' | '\\' | ':' | '*' | '?' | '"' | '<' | '>' | '|' => '-',
			c if c.is_control() => '-',
			c => c,
		})
		.collect::<String>()
		.trim()
		.to_string()
}

/// Find a unique library path by adding numbers if needed
async fn find_unique_library_path(base_path: &Path, name: &str) -> Result<PathBuf> {
	let mut path = base_path.join(format!("{}.{}", name, LIBRARY_EXTENSION));
	let mut counter = 1;

	while path.exists() {
		path = base_path.join(format!("{} {}.{}", name, counter, LIBRARY_EXTENSION));
		counter += 1;

		if counter > 1000 {
			return Err(LibraryError::Other(
				"Could not find unique library name".to_string(),
			));
		}
	}

	Ok(path)
}

#[cfg(test)]
mod tests {
	use super::*;
	use tempfile::TempDir;

	#[tokio::test]
	async fn test_sanitize_filename() {
		assert_eq!(sanitize_filename("My Library"), "My Library");
		assert_eq!(sanitize_filename("My/Library"), "My-Library");
		assert_eq!(sanitize_filename("My\\Library"), "My-Library");
		assert_eq!(sanitize_filename("My:Library"), "My-Library");
		assert_eq!(sanitize_filename("My*Library?"), "My-Library-");
	}

	#[tokio::test]
	async fn test_is_library_directory() {
		assert!(is_library_directory(Path::new(
			"/path/to/My Library.sdlibrary"
		)));
		assert!(!is_library_directory(Path::new("/path/to/My Library")));
		assert!(!is_library_directory(Path::new("/path/to/My Library.txt")));
	}
}
```

## src/library/mod.rs

```rust
//! Library management system
//! 
//! This module provides the core library functionality for Spacedrive.
//! Each library is a self-contained directory with its own database,
//! thumbnails, and other data.

mod config;
mod error;
mod lock;
mod manager;

pub use config::{LibraryConfig, LibrarySettings, LibraryStatistics};
pub use error::{LibraryError, Result};
pub use lock::LibraryLock;
pub use manager::{LibraryManager, DiscoveredLibrary};

use crate::infrastructure::{
    database::Database,
    jobs::manager::JobManager,
};
use std::path::{Path, PathBuf};
use std::sync::Arc;
use tokio::sync::RwLock;
use uuid::Uuid;

/// Represents an open Spacedrive library
pub struct Library {
    /// Root directory of the library (the .sdlibrary folder)
    path: PathBuf,
    
    /// Library configuration
    config: RwLock<LibraryConfig>,
    
    /// Database connection
    db: Arc<Database>,
    
    /// Job manager for this library
    jobs: Arc<JobManager>,
    
    /// Lock preventing concurrent access
    _lock: LibraryLock,
}

impl Library {
    /// Get the library ID
    pub fn id(&self) -> Uuid {
        // Config is immutable for ID, so we can use try_read
        self.config.try_read().map(|c| c.id).unwrap_or_else(|_| {
            // This should never happen in practice
            panic!("Failed to read library config for ID")
        })
    }
    
    /// Get the library name
    pub async fn name(&self) -> String {
        self.config.read().await.name.clone()
    }
    
    /// Get the library path
    pub fn path(&self) -> &Path {
        &self.path
    }
    
    /// Get the database
    pub fn db(&self) -> &Arc<Database> {
        &self.db
    }
    
    /// Get the job manager
    pub fn jobs(&self) -> &Arc<JobManager> {
        &self.jobs
    }
    
    /// Get a copy of the current configuration
    pub async fn config(&self) -> LibraryConfig {
        self.config.read().await.clone()
    }
    
    /// Update library configuration
    pub async fn update_config<F>(&self, f: F) -> Result<()>
    where
        F: FnOnce(&mut LibraryConfig),
    {
        let mut config = self.config.write().await;
        f(&mut config);
        config.updated_at = chrono::Utc::now();
        
        // Save to disk
        let config_path = self.path.join("library.json");
        let json = serde_json::to_string_pretty(&*config)?;
        tokio::fs::write(config_path, json).await?;
        
        Ok(())
    }
    
    /// Save library configuration to disk
    pub async fn save_config(&self, config: &LibraryConfig) -> Result<()> {
        let config_path = self.path.join("library.json");
        let json = serde_json::to_string_pretty(config)?;
        tokio::fs::write(config_path, json).await?;
        Ok(())
    }
    
    /// Get the thumbnail directory for this library
    pub fn thumbnails_dir(&self) -> PathBuf {
        self.path.join("thumbnails")
    }
    
    /// Get the path for a specific thumbnail with size
    pub fn thumbnail_path(&self, cas_id: &str, size: u32) -> PathBuf {
        if cas_id.len() < 4 {
            // Fallback for short IDs
            return self.thumbnails_dir().join(format!("{}_{}.webp", cas_id, size));
        }
        
        // Two-level sharding based on first four characters
        let shard1 = &cas_id[0..2];
        let shard2 = &cas_id[2..4];
        
        self.thumbnails_dir()
            .join(shard1)
            .join(shard2)
            .join(format!("{}_{}.webp", cas_id, size))
    }
    
    /// Get the path for any thumbnail size (legacy compatibility)
    pub fn thumbnail_path_legacy(&self, cas_id: &str) -> PathBuf {
        self.thumbnail_path(cas_id, 256) // Default to 256px
    }
    
    /// Save a thumbnail with specific size
    pub async fn save_thumbnail(&self, cas_id: &str, size: u32, data: &[u8]) -> Result<()> {
        let path = self.thumbnail_path(cas_id, size);
        
        // Ensure parent directory exists
        if let Some(parent) = path.parent() {
            tokio::fs::create_dir_all(parent).await?;
        }
        
        // Write thumbnail
        tokio::fs::write(path, data).await?;
        
        Ok(())
    }
    
    /// Check if a thumbnail exists for a specific size
    pub async fn has_thumbnail(&self, cas_id: &str, size: u32) -> bool {
        tokio::fs::metadata(self.thumbnail_path(cas_id, size))
            .await
            .is_ok()
    }
    
    /// Shutdown the library, gracefully stopping all jobs
    pub async fn shutdown(&self) -> Result<()> {
        // Shutdown the job manager, which will pause all running jobs
        self.jobs.shutdown().await?;
        
        // Save config to ensure any updates are persisted
        let config = self.config.read().await;
        self.save_config(&*config).await?;
        
        Ok(())
    }
    
    /// Check if thumbnails exist for all specified sizes
    pub async fn has_all_thumbnails(&self, cas_id: &str, sizes: &[u32]) -> bool {
        for &size in sizes {
            if !self.has_thumbnail(cas_id, size).await {
                return false;
            }
        }
        true
    }
    
    /// Get thumbnail data for specific size
    pub async fn get_thumbnail(&self, cas_id: &str, size: u32) -> Result<Vec<u8>> {
        let path = self.thumbnail_path(cas_id, size);
        Ok(tokio::fs::read(path).await?)
    }
    
    /// Get the best available thumbnail (largest size available)
    pub async fn get_best_thumbnail(&self, cas_id: &str, preferred_sizes: &[u32]) -> Result<Option<(u32, Vec<u8>)>> {
        // Try sizes in descending order
        let mut sizes = preferred_sizes.to_vec();
        sizes.sort_by(|a, b| b.cmp(a));
        
        for &size in &sizes {
            if self.has_thumbnail(cas_id, size).await {
                let data = self.get_thumbnail(cas_id, size).await?;
                return Ok(Some((size, data)));
            }
        }
        
        Ok(None)
    }
    
    /// Start thumbnail generation job
    pub async fn generate_thumbnails(&self, entry_ids: Option<Vec<Uuid>>) -> Result<crate::infrastructure::jobs::handle::JobHandle> {
        use crate::operations::media::thumbnail::{ThumbnailJob, ThumbnailJobConfig};
        
        let config = ThumbnailJobConfig {
            sizes: self.config().await.settings.thumbnail_sizes.clone(),
            quality: self.config().await.settings.thumbnail_quality,
            regenerate: false,
            batch_size: 50,
            max_concurrent: 4,
        };
        
        let job = if let Some(ids) = entry_ids {
            ThumbnailJob::for_entries(ids, config)
        } else {
            ThumbnailJob::new(config)
        };
        
        self.jobs().dispatch(job).await
            .map_err(|e| LibraryError::JobError(e))
    }
    
    /// Update library statistics
    pub async fn update_statistics<F>(&self, f: F) -> Result<()>
    where
        F: FnOnce(&mut LibraryStatistics),
    {
        self.update_config(|config| {
            f(&mut config.statistics);
            config.statistics.updated_at = chrono::Utc::now();
        }).await
    }
}

// Note: Library does not implement Clone due to the exclusive lock
// Use Arc<Library> when you need shared access

/// Current library configuration version
pub const LIBRARY_CONFIG_VERSION: u32 = 2;

/// Library directory extension
pub const LIBRARY_EXTENSION: &str = "sdlibrary";```

## src/library/lock.rs

```rust
//! Library lock implementation to prevent concurrent access

use super::error::{LibraryError, Result};
use crate::shared::types::get_current_device_id;
use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use std::fs::{File, OpenOptions};
use std::io::Write;
use std::path::{Path, PathBuf};
use std::time::{Duration, SystemTime};
use uuid::Uuid;

/// Information stored in the lock file
#[derive(Debug, Serialize, Deserialize)]
pub struct LockInfo {
    /// ID of the device holding the lock
    pub device_id: Uuid,
    
    /// Process ID
    pub process_id: u32,
    
    /// When the lock was acquired
    pub acquired_at: DateTime<Utc>,
    
    /// Optional description (e.g., "indexing", "backup")
    pub description: Option<String>,
}

/// A lock that prevents concurrent access to a library
pub struct LibraryLock {
    /// Path to the lock file
    path: PathBuf,
    
    /// The open file handle (keeps the lock active)
    _file: File,
}

impl LibraryLock {
    /// Attempt to acquire a lock on the library
    pub fn acquire(library_path: &Path) -> Result<Self> {
        let lock_path = library_path.join(".sdlibrary.lock");
        
        // Try to create the lock file exclusively
        match OpenOptions::new()
            .write(true)
            .create_new(true)
            .open(&lock_path)
        {
            Ok(mut file) => {
                // Write lock information
                let lock_info = LockInfo {
                    device_id: get_current_device_id(),
                    process_id: std::process::id(),
                    acquired_at: Utc::now(),
                    description: None,
                };
                
                let json = serde_json::to_string_pretty(&lock_info)?;
                file.write_all(json.as_bytes())?;
                file.sync_all()?;
                
                Ok(Self {
                    path: lock_path,
                    _file: file,
                })
            }
            Err(e) if e.kind() == std::io::ErrorKind::AlreadyExists => {
                // Lock file exists, check if it's stale
                if Self::is_lock_stale(&lock_path)? {
                    // Remove stale lock and try again
                    std::fs::remove_file(&lock_path)?;
                    
                    // Recursive call to try again
                    Self::acquire(library_path)
                } else {
                    Err(LibraryError::AlreadyInUse)
                }
            }
            Err(e) => Err(e.into()),
        }
    }
    
    /// Check if a lock file is stale (older than 1 hour or process no longer running)
    pub fn is_lock_stale(lock_path: &Path) -> Result<bool> {
        let metadata = std::fs::metadata(lock_path)?;
        let modified = metadata.modified()?;
        let age = SystemTime::now()
            .duration_since(modified)
            .unwrap_or(Duration::ZERO);
        
        // Consider lock stale if older than 1 hour
        if age > Duration::from_secs(3600) {
            return Ok(true);
        }
        
        // Also check if the process is still running
        if let Ok(contents) = std::fs::read_to_string(lock_path) {
            if let Ok(lock_info) = serde_json::from_str::<LockInfo>(&contents) {
                // Check if process is still running
                if !is_process_running(lock_info.process_id) {
                    return Ok(true);
                }
            }
        }
        
        Ok(false)
    }
    
    /// Try to read lock information (for debugging)
    pub fn read_lock_info(library_path: &Path) -> Result<Option<LockInfo>> {
        let lock_path = library_path.join(".sdlibrary.lock");
        
        if !lock_path.exists() {
            return Ok(None);
        }
        
        let contents = std::fs::read_to_string(lock_path)?;
        let info: LockInfo = serde_json::from_str(&contents)?;
        
        Ok(Some(info))
    }
}

/// Check if a process is still running (Unix-specific implementation)
#[cfg(unix)]
fn is_process_running(pid: u32) -> bool {
    use std::process::Command;
    
    match Command::new("ps")
        .arg("-p")
        .arg(pid.to_string())
        .output()
    {
        Ok(output) => output.status.success(),
        Err(_) => false,
    }
}

/// Check if a process is still running (Windows implementation)
#[cfg(windows)]
fn is_process_running(pid: u32) -> bool {
    use std::process::Command;
    
    match Command::new("tasklist")
        .arg("/fi")
        .arg(&format!("pid eq {}", pid))
        .arg("/fo")
        .arg("csv")
        .output()
    {
        Ok(output) => {
            let output_str = String::from_utf8_lossy(&output.stdout);
            output_str.lines().count() > 1 // Header + process line if exists
        }
        Err(_) => false,
    }
}

impl Drop for LibraryLock {
    fn drop(&mut self) {
        // Clean up the lock file when the lock is dropped
        let _ = std::fs::remove_file(&self.path);
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use tempfile::TempDir;
    
    #[test]
    fn test_library_lock() {
        let temp_dir = TempDir::new().unwrap();
        let library_path = temp_dir.path().join("test.sdlibrary");
        std::fs::create_dir_all(&library_path).unwrap();
        
        // First lock should succeed
        let _lock1 = LibraryLock::acquire(&library_path).unwrap();
        
        // Second lock should fail
        match LibraryLock::acquire(&library_path) {
            Err(LibraryError::AlreadyInUse) => {}
            _ => panic!("Expected AlreadyInUse error"),
        }
        
        // Lock file should exist
        assert!(library_path.join(".sdlibrary.lock").exists());
    }
    
    #[test]
    fn test_lock_cleanup() {
        let temp_dir = TempDir::new().unwrap();
        let library_path = temp_dir.path().join("test.sdlibrary");
        std::fs::create_dir_all(&library_path).unwrap();
        
        {
            let _lock = LibraryLock::acquire(&library_path).unwrap();
            assert!(library_path.join(".sdlibrary.lock").exists());
        }
        
        // Lock file should be cleaned up after drop
        assert!(!library_path.join(".sdlibrary.lock").exists());
    }
}```

## src/shared/types.rs

```rust
//! Core type definitions

use std::path::{Path, PathBuf};
use std::fmt;
use uuid::Uuid;
use serde::{Serialize, Deserialize};

/// A path within the Spacedrive Virtual Distributed File System
///
/// This is the core abstraction that enables cross-device operations.
/// An SdPath can represent:
/// - A local file on this device
/// - A file on another device in the same library
/// - A cloud-synced file
/// - A file that exists in multiple locations
#[derive(Debug, Clone, PartialEq, Eq, Hash, Serialize, Deserialize)]
pub struct SdPath {
    /// The device where this file exists
    pub device_id: Uuid,

    /// The local path on that device
    pub path: PathBuf,
}

impl SdPath {
    /// Create a new SdPath
    pub fn new(device_id: Uuid, path: impl Into<PathBuf>) -> Self {
        Self {
            device_id,
            path: path.into(),
        }
    }


    /// Create an SdPath for a local file on this device
    pub fn local(path: impl Into<PathBuf>) -> Self {
        Self {
            device_id: get_current_device_id(), // Get the current device ID
            path: path.into(),
        }
    }

    /// Check if this path is on the current device
    pub fn is_local(&self) -> bool {
        self.device_id == get_current_device_id()
    }

    /// Get the local PathBuf if this is a local path
    pub fn as_local_path(&self) -> Option<&Path> {
        if self.is_local() {
            Some(&self.path)
        } else {
            None
        }
    }

    /// Convert to a display string
    pub fn display(&self) -> String {
        if self.is_local() {
            self.path.display().to_string()
        } else {
            format!("{}:{}", self.device_id, self.path.display())
        }
    }

    /// Get just the file name
    pub fn file_name(&self) -> Option<&str> {
        self.path.file_name()?.to_str()
    }

    /// Get the parent directory as an SdPath
    pub fn parent(&self) -> Option<SdPath> {
        self.path.parent().map(|p| SdPath {
            device_id: self.device_id,
            path: p.to_path_buf(),
        })
    }

    /// Join with another path component
    pub fn join(&self, path: impl AsRef<Path>) -> SdPath {
        SdPath {
            device_id: self.device_id,
            path: self.path.join(path),
        }
    }

    /// Get the volume that contains this path (if local and volume manager available)
    pub async fn get_volume(&self, volume_manager: &crate::volume::VolumeManager) -> Option<crate::volume::Volume> {
        if let Some(local_path) = self.as_local_path() {
            volume_manager.volume_for_path(local_path).await
        } else {
            None
        }
    }
    
    /// Check if this path is on the same volume as another path
    pub async fn same_volume(&self, other: &SdPath, volume_manager: &crate::volume::VolumeManager) -> bool {
        if !self.is_local() || !other.is_local() {
            return false;
        }
        
        if let (Some(self_path), Some(other_path)) = (self.as_local_path(), other.as_local_path()) {
            volume_manager.same_volume(self_path, other_path).await
        } else {
            false
        }
    }
}

impl fmt::Display for SdPath {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(f, "{}", self.display())
    }
}

use std::sync::RwLock;

/// Global reference to current device ID
/// This is set during Core initialization
pub static CURRENT_DEVICE_ID: once_cell::sync::Lazy<RwLock<Uuid>> =
    once_cell::sync::Lazy::new(|| RwLock::new(Uuid::nil()));

/// Initialize the current device ID
pub fn set_current_device_id(id: Uuid) {
    if let Ok(mut device_id) = CURRENT_DEVICE_ID.write() {
        *device_id = id;
    }
}

/// Get the current device ID
pub fn get_current_device_id() -> Uuid {
    match CURRENT_DEVICE_ID.read() {
        Ok(guard) => *guard,
        Err(_) => Uuid::nil(),
    }
}

/// A batch of SdPaths, useful for operations on multiple files
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SdPathBatch {
    pub paths: Vec<SdPath>,
}

impl SdPathBatch {
    /// Create a new batch
    pub fn new(paths: Vec<SdPath>) -> Self {
        Self { paths }
    }

    /// Filter to only local paths
    pub fn local_only(&self) -> Vec<&Path> {
        self.paths.iter()
            .filter_map(|p| p.as_local_path())
            .collect()
    }

    /// Group by device
    pub fn by_device(&self) -> std::collections::HashMap<Uuid, Vec<&SdPath>> {
        let mut map = std::collections::HashMap::new();
        for path in &self.paths {
            map.entry(path.device_id).or_insert_with(Vec::new).push(path);
        }
        map
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_sdpath_creation() {
        let device_id = Uuid::new_v4();
        let path = SdPath::new(device_id, "/home/user/file.txt");

        assert_eq!(path.device_id, device_id);
        assert_eq!(path.path, PathBuf::from("/home/user/file.txt"));
    }

    #[test]
    fn test_sdpath_display() {
        let device_id = Uuid::new_v4();
        let path = SdPath::new(device_id, "/home/user/file.txt");

        let display = path.display();
        assert!(display.contains(&device_id.to_string()));
        assert!(display.contains("/home/user/file.txt"));
    }
}```

## src/shared/mod.rs

```rust
//! Shared types and utilities

pub mod errors;
pub mod types;
pub mod utils;```

## src/shared/errors.rs

```rust
//! Unified error handling for the core

use thiserror::Error;

/// Main error type for core operations
#[derive(Error, Debug)]
pub enum CoreError {
    #[error("Database error: {0}")]
    Database(#[from] sea_orm::DbErr),
    
    #[error("IO error: {0}")]
    Io(#[from] std::io::Error),
    
    #[error("File operation error: {0}")]
    FileOp(#[from] FileOpError),
    
    #[error("Not found: {0}")]
    NotFound(String),
    
    #[error("Invalid operation: {0}")]
    InvalidOperation(String),
    
    #[error("Other error: {0}")]
    Other(#[from] anyhow::Error),
}

/// Errors specific to file operations
#[derive(Error, Debug)]
pub enum FileOpError {
    #[error("Source not found: {0}")]
    SourceNotFound(String),
    
    #[error("Destination not found: {0}")]
    DestinationNotFound(String),
    
    #[error("Permission denied: {0}")]
    PermissionDenied(String),
    
    #[error("File exists: {0}")]
    FileExists(String),
    
    #[error("Not a directory: {0}")]
    NotADirectory(String),
    
    #[error("IO error: {0}")]
    Io(#[from] std::io::Error),
    
    #[error("Other: {0}")]
    Other(String),
}

impl From<&str> for FileOpError {
    fn from(s: &str) -> Self {
        FileOpError::Other(s.to_string())
    }
}

impl From<String> for FileOpError {
    fn from(s: String) -> Self {
        FileOpError::Other(s)
    }
}

/// Result type alias for core operations
pub type Result<T> = std::result::Result<T, CoreError>;```

## src/shared/utils.rs

```rust
```

## src/operations/libraries/delete/action.rs

```rust
//! Library deletion action handler

use crate::{
    context::CoreContext,
    infrastructure::actions::{
        Action, error::{ActionError, ActionResult}, handler::ActionHandler, output::ActionOutput,
    },
    register_action_handler,
};
use async_trait::async_trait;
use serde::{Deserialize, Serialize};
use std::sync::Arc;
use uuid::Uuid;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LibraryDeleteAction {
    // Library deletion doesn't need additional fields beyond library_id
}

pub struct LibraryDeleteHandler;

impl LibraryDeleteHandler {
    pub fn new() -> Self {
        Self
    }
}

#[async_trait]
impl ActionHandler for LibraryDeleteHandler {
    async fn execute(
        &self,
        context: Arc<CoreContext>,
        action: Action,
    ) -> ActionResult<ActionOutput> {
        if let Action::LibraryDelete(action) = action {
            // For now, library deletion is not implemented in the library manager
            // This would need to be implemented as a proper method
            Err(ActionError::Internal("Library deletion not yet implemented".to_string()))
        } else {
            Err(crate::infrastructure::actions::error::ActionError::InvalidActionType)
        }
    }

    fn can_handle(&self, action: &Action) -> bool {
        matches!(action, Action::LibraryDelete(_))
    }

    fn supported_actions() -> &'static [&'static str] {
        &["library.delete"]
    }
}

// Register this handler
register_action_handler!(LibraryDeleteHandler, "library.delete");```

## src/operations/libraries/delete/mod.rs

```rust
//! Library delete operations

pub mod action;
pub mod output;

pub use output::LibraryDeleteOutput;```

## src/operations/libraries/delete/output.rs

```rust
//! Library delete operation output types

use crate::infrastructure::actions::output::ActionOutputTrait;
use serde::{Deserialize, Serialize};
use uuid::Uuid;

/// Output from library delete action dispatch
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LibraryDeleteOutput {
    pub library_id: Uuid,
    pub name: String,
}

impl LibraryDeleteOutput {
    pub fn new(library_id: Uuid, name: String) -> Self {
        Self {
            library_id,
            name,
        }
    }
}

impl ActionOutputTrait for LibraryDeleteOutput {
    fn to_json(&self) -> serde_json::Value {
        serde_json::to_value(self).unwrap_or(serde_json::Value::Null)
    }
    
    fn display_message(&self) -> String {
        format!(
            "Deleted library '{}' with ID {}",
            self.name, self.library_id
        )
    }
    
    fn output_type(&self) -> &'static str {
        "library.delete.completed"
    }
}```

## src/operations/libraries/mod.rs

```rust
//! Library operations

pub mod create;
pub mod delete;
pub mod export;
pub mod rename;

pub use create::*;
pub use delete::*;
pub use export::*;
pub use rename::*;```

## src/operations/libraries/rename/action.rs

```rust
//! Library rename action handler

use crate::{
    context::CoreContext,
    infrastructure::actions::{
        error::{ActionError, ActionResult},
        handler::ActionHandler,
        output::ActionOutput,
        Action,
    },
    library::LibraryConfig,
    register_action_handler,
};
use async_trait::async_trait;
use serde::{Deserialize, Serialize};
use std::sync::Arc;
use uuid::Uuid;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LibraryRenameAction {
    pub library_id: Uuid,
    pub new_name: String,
}

pub struct LibraryRenameHandler;

impl LibraryRenameHandler {
    pub fn new() -> Self {
        Self
    }
}

#[async_trait]
impl ActionHandler for LibraryRenameHandler {
    async fn validate(
        &self,
        _context: Arc<CoreContext>,
        action: &Action,
    ) -> ActionResult<()> {
        if let Action::LibraryRename { action, .. } = action {
            if action.new_name.is_empty() {
                return Err(ActionError::Validation {
                    field: "new_name".to_string(),
                    message: "Library name cannot be empty".to_string(),
                });
            }
            Ok(())
        } else {
            Err(ActionError::InvalidActionType)
        }
    }

    async fn execute(
        &self,
        context: Arc<CoreContext>,
        action: Action,
    ) -> ActionResult<ActionOutput> {
        if let Action::LibraryRename { library_id, action } = action {
            let library_manager = &context.library_manager;
            
            // Get the specific library
            let library = library_manager
                .get_library(library_id)
                .await
                .ok_or(ActionError::LibraryNotFound(library_id))?;

            // Get current config
            let old_config = library.config().await;
            let old_name = old_config.name.clone();
            
            // Update the library name using update_config
            library.update_config(|config| {
                config.name = action.new_name.clone();
            }).await
                .map_err(|e| ActionError::Internal(format!("Failed to save config: {}", e)))?;

            let output = super::output::LibraryRenameOutput {
                library_id,
                old_name,
                new_name: action.new_name,
            };

            Ok(ActionOutput::from_trait(output))
        } else {
            Err(ActionError::InvalidActionType)
        }
    }

    fn can_handle(&self, action: &Action) -> bool {
        matches!(action, Action::LibraryRename { .. })
    }

    fn supported_actions() -> &'static [&'static str] {
        &["library.rename"]
    }
}

register_action_handler!(LibraryRenameHandler, "library.rename");```

## src/operations/libraries/rename/mod.rs

```rust
//! Library rename operation

pub mod action;
pub mod output;```

## src/operations/libraries/rename/output.rs

```rust
//! Library rename operation output

use crate::infrastructure::actions::output::ActionOutputTrait;
use serde::{Deserialize, Serialize};
use uuid::Uuid;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LibraryRenameOutput {
    pub library_id: Uuid,
    pub old_name: String,
    pub new_name: String,
}

impl ActionOutputTrait for LibraryRenameOutput {
    fn to_json(&self) -> serde_json::Value {
        serde_json::to_value(self).unwrap_or(serde_json::Value::Null)
    }

    fn display_message(&self) -> String {
        format!(
            "Renamed library '{}' to '{}'",
            self.old_name, self.new_name
        )
    }

    fn output_type(&self) -> &'static str {
        "library.rename.output"
    }
}```

## src/operations/libraries/export/action.rs

```rust
//! Library export action handler

use crate::{
    context::CoreContext,
    infrastructure::actions::{
        error::{ActionError, ActionResult},
        handler::ActionHandler,
        output::ActionOutput,
        Action,
    },
    register_action_handler,
};
use async_trait::async_trait;
use serde::{Deserialize, Serialize};
use std::{path::PathBuf, sync::Arc};
use uuid::Uuid;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LibraryExportAction {
    pub library_id: Uuid,
    pub export_path: PathBuf,
    pub include_thumbnails: bool,
    pub include_previews: bool,
}

pub struct LibraryExportHandler;

impl LibraryExportHandler {
    pub fn new() -> Self {
        Self
    }
}

#[async_trait]
impl ActionHandler for LibraryExportHandler {
    async fn validate(
        &self,
        _context: Arc<CoreContext>,
        action: &Action,
    ) -> ActionResult<()> {
        if let Action::LibraryExport { action, .. } = action {
            // Ensure parent directory exists
            if let Some(parent) = action.export_path.parent() {
                if !parent.exists() {
                    return Err(ActionError::Validation {
                        field: "export_path".to_string(),
                        message: "Export directory does not exist".to_string(),
                    });
                }
            }
            Ok(())
        } else {
            Err(ActionError::InvalidActionType)
        }
    }

    async fn execute(
        &self,
        context: Arc<CoreContext>,
        action: Action,
    ) -> ActionResult<ActionOutput> {
        if let Action::LibraryExport { library_id, action } = action {
            let library_manager = &context.library_manager;
            
            // Get the specific library
            let library = library_manager
                .get_library(library_id)
                .await
                .ok_or(ActionError::LibraryNotFound(library_id))?;

            // Create export directory
            let export_dir = &action.export_path;
            tokio::fs::create_dir_all(&export_dir).await
                .map_err(|e| ActionError::Internal(format!("Failed to create export directory: {}", e)))?;

            // Export library config
            let config = library.config().await;
            let config_path = export_dir.join("library.json");
            let config_json = serde_json::to_string_pretty(&config)
                .map_err(|e| ActionError::Internal(format!("Failed to serialize config: {}", e)))?;
            tokio::fs::write(&config_path, config_json).await
                .map_err(|e| ActionError::Internal(format!("Failed to write config: {}", e)))?;

            // Export database (as SQL dump for portability)
            // TODO: Implement actual database export
            let db_export_path = export_dir.join("database.sql");
            tokio::fs::write(&db_export_path, "-- Database export not yet implemented").await
                .map_err(|e| ActionError::Internal(format!("Failed to write database export: {}", e)))?;

            let mut exported_files = vec![
                config_path.to_string_lossy().to_string(),
                db_export_path.to_string_lossy().to_string(),
            ];

            // Optionally export thumbnails
            if action.include_thumbnails {
                let thumbnails_src = library.path().join("thumbnails");
                let thumbnails_dst = export_dir.join("thumbnails");
                if thumbnails_src.exists() {
                    // TODO: Copy thumbnails directory
                    exported_files.push("thumbnails/".to_string());
                }
            }

            // Optionally export previews
            if action.include_previews {
                let previews_src = library.path().join("previews");
                let previews_dst = export_dir.join("previews");
                if previews_src.exists() {
                    // TODO: Copy previews directory
                    exported_files.push("previews/".to_string());
                }
            }

            let output = super::output::LibraryExportOutput {
                library_id,
                library_name: config.name.clone(),
                export_path: action.export_path,
                exported_files,
            };

            Ok(ActionOutput::from_trait(output))
        } else {
            Err(ActionError::InvalidActionType)
        }
    }

    fn can_handle(&self, action: &Action) -> bool {
        matches!(action, Action::LibraryExport { .. })
    }

    fn supported_actions() -> &'static [&'static str] {
        &["library.export"]
    }
}

register_action_handler!(LibraryExportHandler, "library.export");```

## src/operations/libraries/export/mod.rs

```rust
//! Library export operation

pub mod action;
pub mod output;```

## src/operations/libraries/export/output.rs

```rust
//! Library export operation output

use crate::infrastructure::actions::output::ActionOutputTrait;
use serde::{Deserialize, Serialize};
use std::path::PathBuf;
use uuid::Uuid;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LibraryExportOutput {
    pub library_id: Uuid,
    pub library_name: String,
    pub export_path: PathBuf,
    pub exported_files: Vec<String>,
}

impl ActionOutputTrait for LibraryExportOutput {
    fn to_json(&self) -> serde_json::Value {
        serde_json::to_value(self).unwrap_or(serde_json::Value::Null)
    }

    fn display_message(&self) -> String {
        format!(
            "Exported library '{}' to {} ({} files)",
            self.library_name,
            self.export_path.display(),
            self.exported_files.len()
        )
    }

    fn output_type(&self) -> &'static str {
        "library.export.output"
    }
}```

## src/operations/libraries/create/action.rs

```rust
//! Library creation action handler

use crate::{
    context::CoreContext,
    infrastructure::actions::{
        error::{ActionError, ActionResult}, 
        handler::ActionHandler, 
        output::ActionOutput,
    },
    register_action_handler,
};
use super::output::LibraryCreateOutput;
use async_trait::async_trait;
use std::sync::Arc;
use std::path::PathBuf;
use uuid::Uuid;

#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct LibraryCreateAction {
    pub name: String,
    pub path: Option<PathBuf>,
}

pub struct LibraryCreateHandler;

impl LibraryCreateHandler {
    pub fn new() -> Self {
        Self
    }
}

#[async_trait]
impl ActionHandler for LibraryCreateHandler {
    async fn validate(
        &self,
        _context: Arc<CoreContext>,
        action: &crate::infrastructure::actions::Action,
    ) -> ActionResult<()> {
        if let crate::infrastructure::actions::Action::LibraryCreate(action) = action {
            if action.name.trim().is_empty() {
                return Err(ActionError::Validation {
                    field: "name".to_string(),
                    message: "Library name cannot be empty".to_string(),
                });
            }
            Ok(())
        } else {
            Err(ActionError::InvalidActionType)
        }
    }

    async fn execute(
        &self,
        context: Arc<CoreContext>,
        action: crate::infrastructure::actions::Action,
    ) -> ActionResult<ActionOutput> {
        if let crate::infrastructure::actions::Action::LibraryCreate(action) = action {
            let library_manager = &context.library_manager;
            let new_library = library_manager.create_library(action.name.clone(), action.path.clone(), context.clone()).await?;

            let library_name = new_library.name().await;
            let output = LibraryCreateOutput::new(
                new_library.id(),
                library_name,
                new_library.path().to_path_buf(),
            );
            Ok(ActionOutput::from_trait(output))
        } else {
            Err(ActionError::InvalidActionType)
        }
    }

    fn can_handle(&self, action: &crate::infrastructure::actions::Action) -> bool {
        matches!(action, crate::infrastructure::actions::Action::LibraryCreate(_))
    }

    fn supported_actions() -> &'static [&'static str] {
        &["library.create"]
    }
}

// Register this handler
register_action_handler!(LibraryCreateHandler, "library.create");```

## src/operations/libraries/create/mod.rs

```rust
//! Library create operations

pub mod action;
pub mod output;

pub use output::LibraryCreateOutput;```

## src/operations/libraries/create/output.rs

```rust
//! Library create operation output types

use crate::infrastructure::actions::output::ActionOutputTrait;
use serde::{Deserialize, Serialize};
use std::path::PathBuf;
use uuid::Uuid;

/// Output from library create action dispatch
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LibraryCreateOutput {
    pub library_id: Uuid,
    pub name: String,
    pub path: PathBuf,
}

impl LibraryCreateOutput {
    pub fn new(library_id: Uuid, name: String, path: PathBuf) -> Self {
        Self {
            library_id,
            name,
            path,
        }
    }
}

impl ActionOutputTrait for LibraryCreateOutput {
    fn to_json(&self) -> serde_json::Value {
        serde_json::to_value(self).unwrap_or(serde_json::Value::Null)
    }
    
    fn display_message(&self) -> String {
        format!(
            "Created library '{}' with ID {} at {}",
            self.name, self.library_id, self.path.display()
        )
    }
    
    fn output_type(&self) -> &'static str {
        "library.create.completed"
    }
}```

## src/operations/content/action.rs

```rust
//! Content analysis action handler

use crate::{
    context::CoreContext,
    infrastructure::actions::{
        error::{ActionError, ActionResult}, 
        handler::ActionHandler, 
        output::ActionOutput,
    },
    register_action_handler,
};
use async_trait::async_trait;
use std::sync::Arc;
use uuid::Uuid;

#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct ContentAction {
    pub paths: Vec<std::path::PathBuf>,
    pub analyze_content: bool,
    pub extract_metadata: bool,
}

pub struct ContentHandler;

impl ContentHandler {
    pub fn new() -> Self {
        Self
    }
}

#[async_trait]
impl ActionHandler for ContentHandler {
    async fn validate(
        &self,
        _context: Arc<CoreContext>,
        action: &crate::infrastructure::actions::Action,
    ) -> ActionResult<()> {
        // TODO: Re-enable when ContentAnalysis variant is added back
        Err(ActionError::Internal("ContentAnalysis action not yet implemented".to_string()))
    }

    async fn execute(
        &self,
        context: Arc<CoreContext>,
        action: crate::infrastructure::actions::Action,
    ) -> ActionResult<ActionOutput> {
        // TODO: Re-enable when ContentAnalysis variant is added back
        Err(ActionError::Internal("ContentAnalysis action not yet implemented".to_string()))
    }

    fn can_handle(&self, action: &crate::infrastructure::actions::Action) -> bool {
        // TODO: Re-enable when ContentAnalysis variant is added back
        false
    }

    fn supported_actions() -> &'static [&'static str] {
        &["content.analyze"]
    }
}

register_action_handler!(ContentHandler, "content.analyze");```

## src/operations/content/mod.rs

```rust
//! Content operations for library-scoped content management

pub mod action;

use chrono::{DateTime, Utc};
use sea_orm::{ColumnTrait, DatabaseConnection, EntityTrait, QueryFilter};
use serde::{Deserialize, Serialize};
use std::sync::Arc;
use uuid::Uuid;

use crate::infrastructure::database::entities::{
	content_identity::{self, Entity as ContentIdentity, Model as ContentIdentityModel},
	entry::{self, Entity as Entry, Model as EntryModel},
};

pub use action::ContentAction;
use crate::shared::errors::Result;

#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct ContentInstance {
	pub entry_uuid: Option<Uuid>,
	pub path: String, // TODO: Replace with SdPath when available
	pub device_uuid: Uuid,
	pub size: i64,
	pub modified_at: Option<DateTime<Utc>>,
}

#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct LibraryContentStats {
	pub entry_count: i32,
	pub total_size: i64,    // Size of one instance
	pub combined_size: i64, // Calculated on-demand (entry_count * total_size)
	pub integrity_hash: Option<String>,
	pub content_hash: String,
	pub mime_type_id: Option<i32>,
	pub kind_id: i32,
	pub has_media_data: bool,
	pub first_seen: DateTime<Utc>,
	pub last_verified: DateTime<Utc>,
}

pub struct ContentService {
	library_db: Arc<DatabaseConnection>,
}

impl ContentService {
	pub fn new(library_db: Arc<DatabaseConnection>) -> Self {
		Self { library_db }
	}

	/// Find all instances of content within this library only
	pub async fn find_content_instances(
		&self,
		content_identity_uuid: Uuid,
	) -> Result<Vec<ContentInstance>> {
		// First find the content identity by UUID
		let content_identity = ContentIdentity::find()
			.filter(content_identity::Column::Uuid.eq(content_identity_uuid))
			.one(&*self.library_db)
			.await?
			.ok_or_else(|| {
				crate::shared::errors::CoreError::NotFound("Content identity not found".to_string())
			})?;

		// Find all entries that reference this content identity
		let entries = Entry::find()
			.filter(entry::Column::ContentId.eq(content_identity.id))
			.all(&*self.library_db)
			.await?;

		let mut instances = Vec::new();
		for entry in entries {
			// TODO: Replace with proper SdPath materialization once available
			let path = format!("{}/{}", entry.relative_path, entry.name);

			// TODO: Get device UUID from location when that relationship is available
			let device_uuid = Uuid::new_v4(); // Placeholder

			instances.push(ContentInstance {
				entry_uuid: entry.uuid,
				path,
				device_uuid,
				size: entry.size,
				modified_at: Some(entry.modified_at),
			});
		}

		Ok(instances)
	}

	/// Get content statistics within this library
	pub async fn get_content_stats(
		&self,
		content_identity_uuid: Uuid,
	) -> Result<LibraryContentStats> {
		let content_identity = ContentIdentity::find()
			.filter(content_identity::Column::Uuid.eq(content_identity_uuid))
			.one(&*self.library_db)
			.await?
			.ok_or_else(|| {
				crate::shared::errors::CoreError::NotFound("Content identity not found".to_string())
			})?;

		Ok(LibraryContentStats {
			entry_count: content_identity.entry_count,
			total_size: content_identity.total_size,
			combined_size: content_identity.combined_size(),
			integrity_hash: content_identity.integrity_hash,
			content_hash: content_identity.content_hash,
			mime_type_id: content_identity.mime_type_id,
			kind_id: content_identity.kind_id,
			has_media_data: content_identity.media_data.is_some(),
			first_seen: content_identity.first_seen_at,
			last_verified: content_identity.last_verified_at,
		})
	}

	/// Find content identity by content hash
	pub async fn find_by_content_hash(
		&self,
		content_hash: &str,
	) -> Result<Option<ContentIdentityModel>> {
		let content_identity = ContentIdentity::find()
			.filter(content_identity::Column::ContentHash.eq(content_hash))
			.one(&*self.library_db)
			.await?;

		Ok(content_identity)
	}

	/// Get all content identities with entry counts above threshold
	pub async fn find_duplicated_content(
		&self,
		min_instances: i32,
	) -> Result<Vec<ContentIdentityModel>> {
		let content_identities = ContentIdentity::find()
			.filter(content_identity::Column::EntryCount.gte(min_instances))
			.all(&*self.library_db)
			.await?;

		Ok(content_identities)
	}

	/// Calculate total library deduplication savings
	pub async fn calculate_deduplication_savings(&self) -> Result<i64> {
		let content_identities = ContentIdentity::find()
			.filter(content_identity::Column::EntryCount.gt(1)) // Only duplicated content
			.all(&*self.library_db)
			.await?;

		let total_savings: i64 = content_identities
			.iter()
			.map(|content| {
				// Savings = (instances - 1) * size_per_instance
				(content.entry_count - 1) as i64 * content.total_size
			})
			.sum();

		Ok(total_savings)
	}
}
```

## src/operations/mod.rs

```rust
//! Operations module - contains all business operations and use cases
//!
//! This module organizes all business operations for Spacedrive:
//! - File operations (copy, move, delete, validate, duplicate detection)
//! - Indexing operations
//! - Media processing (thumbnails, etc.)
//! - Content operations (deduplication, statistics)
//! - Metadata operations (hierarchical tagging)

pub mod content;
pub mod devices;
pub mod files;
pub mod indexing;
pub mod libraries;
pub mod locations;
pub mod media;
pub mod metadata;
pub mod volumes;

/// Register all jobs with the job system
///
/// This should be called during core initialization to register all available job types
pub fn register_all_jobs() {
	// File operation jobs
	register_job::<files::copy::FileCopyJob>();
	register_job::<files::copy::MoveJob>();
	register_job::<files::delete::DeleteJob>();
	register_job::<files::duplicate_detection::DuplicateDetectionJob>();
	register_job::<files::validation::ValidationJob>();

	// Indexing jobs
	register_job::<indexing::IndexerJob>();

	// Media processing jobs
	register_job::<media::ThumbnailJob>();
}

/// Register a single job type with the job system
///
/// This function would be called automatically by a derive macro in a real implementation,
/// but for now we call it manually for each job type.
fn register_job<T>()
where
	T: crate::infrastructure::jobs::traits::Job + 'static,
{
	// In a real implementation with inventory, this would automatically register the job
	// For now, this serves as documentation of which jobs should be registered

	// The actual registration would happen via:
	// inventory::submit! {
	//     crate::infrastructure::jobs::registration::JobRegistration::new::<T>()
	// }

	// For now we'll just log that the job type exists
}
```

## src/operations/locations/remove/action.rs

```rust
//! Location remove action handler

use crate::{
    context::CoreContext,
    location::manager::LocationManager,
    infrastructure::actions::{
        Action, error::{ActionError, ActionResult}, handler::ActionHandler, output::ActionOutput,
    },
    register_action_handler,
};
use super::output::LocationRemoveOutput;
use async_trait::async_trait;
use serde::{Deserialize, Serialize};
use std::sync::Arc;
use uuid::Uuid;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LocationRemoveAction {
    pub location_id: Uuid,
}

pub struct LocationRemoveHandler;

impl LocationRemoveHandler {
    pub fn new() -> Self {
        Self
    }
}

#[async_trait]
impl ActionHandler for LocationRemoveHandler {
    async fn execute(
        &self,
        context: Arc<CoreContext>,
        action: Action,
    ) -> ActionResult<ActionOutput> {
        if let Action::LocationRemove { library_id, action } = action {
            let library_manager = &context.library_manager;
            
            // Get the specific library
            let library = library_manager
                .get_library(library_id)
                .await
                .ok_or(ActionError::LibraryNotFound(library_id))?;

            // Remove the location
            let location_manager = LocationManager::new(context.events.as_ref().clone());
            location_manager
                .remove_location(&library, action.location_id)
                .await
                .map_err(|e| ActionError::Internal(e.to_string()))?;

            let output = LocationRemoveOutput::new(action.location_id, None);
            Ok(ActionOutput::from_trait(output))
        } else {
            Err(ActionError::InvalidActionType)
        }
    }

    fn can_handle(&self, action: &Action) -> bool {
        matches!(action, Action::LocationRemove { .. })
    }

    fn supported_actions() -> &'static [&'static str] {
        &["location.remove"]
    }
}

// Register this handler
register_action_handler!(LocationRemoveHandler, "location.remove");```

## src/operations/locations/remove/mod.rs

```rust
//! Location remove operations

pub mod action;
pub mod output;

pub use output::LocationRemoveOutput;```

## src/operations/locations/remove/output.rs

```rust
//! Location remove operation output types

use crate::infrastructure::actions::output::ActionOutputTrait;
use serde::{Deserialize, Serialize};
use uuid::Uuid;

/// Output from location remove action dispatch
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LocationRemoveOutput {
    pub location_id: Uuid,
    pub path: Option<String>,
}

impl LocationRemoveOutput {
    pub fn new(location_id: Uuid, path: Option<String>) -> Self {
        Self {
            location_id,
            path,
        }
    }
}

impl ActionOutputTrait for LocationRemoveOutput {
    fn to_json(&self) -> serde_json::Value {
        serde_json::to_value(self).unwrap_or(serde_json::Value::Null)
    }
    
    fn display_message(&self) -> String {
        match &self.path {
            Some(path) => format!(
                "Removed location with ID {} at {}",
                self.location_id, path
            ),
            None => format!(
                "Removed location with ID {}",
                self.location_id
            ),
        }
    }
    
    fn output_type(&self) -> &'static str {
        "location.remove.completed"
    }
}```

## src/operations/locations/rescan/action.rs

```rust
//! Location rescan action handler

use crate::{
    context::CoreContext,
    infrastructure::actions::{
        error::{ActionError, ActionResult},
        handler::ActionHandler,
        output::ActionOutput,
        Action,
    },
    operations::indexing::{IndexMode, job::IndexerJob},
    register_action_handler,
    shared::types::SdPath,
};
use async_trait::async_trait;
use serde::{Deserialize, Serialize};
use std::sync::Arc;
use uuid::Uuid;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LocationRescanAction {
    pub location_id: Uuid,
    pub full_rescan: bool,
}

pub struct LocationRescanHandler;

impl LocationRescanHandler {
    pub fn new() -> Self {
        Self
    }
}

#[async_trait]
impl ActionHandler for LocationRescanHandler {
    async fn execute(
        &self,
        context: Arc<CoreContext>,
        action: Action,
    ) -> ActionResult<ActionOutput> {
        if let Action::LocationRescan { library_id, action } = action {
            let library_manager = &context.library_manager;
            
            // Get the specific library
            let library = library_manager
                .get_library(library_id)
                .await
                .ok_or(ActionError::LibraryNotFound(library_id))?;

            // Get location details from database
            use crate::infrastructure::database::entities;
            use sea_orm::{ColumnTrait, EntityTrait, QueryFilter};
            
            let location = entities::location::Entity::find()
                .filter(entities::location::Column::Uuid.eq(action.location_id))
                .one(library.db().conn())
                .await
                .map_err(|e| ActionError::Internal(format!("Database error: {}", e)))?
                .ok_or_else(|| ActionError::Internal(format!("Location not found: {}", action.location_id)))?;

            let location_path = SdPath::local(&location.path);
            
            // Determine index mode based on full_rescan flag
            let mode = if action.full_rescan {
                IndexMode::Deep
            } else {
                // Convert from string to IndexMode
                match location.index_mode.as_str() {
                    "shallow" => IndexMode::Shallow,
                    "content" => IndexMode::Content,
                    "deep" => IndexMode::Deep,
                    _ => IndexMode::Content,
                }
            };

            // Create indexer job for rescan
            let job = IndexerJob::from_location(action.location_id, location_path, mode);

            // Dispatch the job
            let job_handle = library
                .jobs()
                .dispatch(job)
                .await
                .map_err(ActionError::Job)?;

            let output = super::output::LocationRescanOutput {
                location_id: action.location_id,
                location_path: location.path,
                job_id: job_handle.id().into(),
                full_rescan: action.full_rescan,
            };

            Ok(ActionOutput::from_trait(output))
        } else {
            Err(ActionError::InvalidActionType)
        }
    }

    fn can_handle(&self, action: &Action) -> bool {
        matches!(action, Action::LocationRescan { .. })
    }

    fn supported_actions() -> &'static [&'static str] {
        &["location.rescan"]
    }
}

register_action_handler!(LocationRescanHandler, "location.rescan");```

## src/operations/locations/rescan/mod.rs

```rust
//! Location rescan operation

pub mod action;
pub mod output;```

## src/operations/locations/rescan/output.rs

```rust
//! Location rescan operation output

use crate::infrastructure::actions::output::ActionOutputTrait;
use serde::{Deserialize, Serialize};
use uuid::Uuid;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LocationRescanOutput {
    pub location_id: Uuid,
    pub location_path: String,
    pub job_id: Uuid,
    pub full_rescan: bool,
}

impl ActionOutputTrait for LocationRescanOutput {
    fn to_json(&self) -> serde_json::Value {
        serde_json::to_value(self).unwrap_or(serde_json::Value::Null)
    }

    fn display_message(&self) -> String {
        let scan_type = if self.full_rescan { "Full" } else { "Quick" };
        format!(
            "{} rescan started for location {} (job: {})",
            scan_type, self.location_path, self.job_id
        )
    }

    fn output_type(&self) -> &'static str {
        "location.rescan.output"
    }
}```

## src/operations/locations/mod.rs

```rust
//! Location operations

pub mod add;
pub mod index;
pub mod remove;
pub mod rescan;

pub use add::*;
pub use index::*;
pub use remove::*;
pub use rescan::*;```

## src/operations/locations/add/action.rs

```rust
//! Location add action handler

use super::output::LocationAddOutput;
use crate::{
	context::CoreContext,
	infrastructure::actions::{
		error::{ActionError, ActionResult},
		handler::ActionHandler,
		output::ActionOutput,
		Action,
	},
	infrastructure::database::entities,
	location::manager::LocationManager,
	operations::indexing::IndexMode,
	register_action_handler,
};
use async_trait::async_trait;
use sea_orm::{ColumnTrait, EntityTrait, QueryFilter};
use serde::{Deserialize, Serialize};
use std::{path::PathBuf, sync::Arc};
use uuid::Uuid;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LocationAddAction {
	pub path: PathBuf,
	pub name: Option<String>,
	pub mode: IndexMode,
}

pub struct LocationAddHandler;

impl LocationAddHandler {
	pub fn new() -> Self {
		Self
	}
}

#[async_trait]
impl ActionHandler for LocationAddHandler {
	async fn validate(&self, _context: Arc<CoreContext>, action: &Action) -> ActionResult<()> {
		if let Action::LocationAdd {
			library_id: _,
			action,
		} = action
		{
			if !action.path.exists() {
				return Err(ActionError::Validation {
					field: "path".to_string(),
					message: "Path does not exist".to_string(),
				});
			}
			if !action.path.is_dir() {
				return Err(ActionError::Validation {
					field: "path".to_string(),
					message: "Path must be a directory".to_string(),
				});
			}
			Ok(())
		} else {
			Err(ActionError::InvalidActionType)
		}
	}

	async fn execute(
		&self,
		context: Arc<CoreContext>,
		action: Action,
	) -> ActionResult<ActionOutput> {
		if let Action::LocationAdd { library_id, action } = action {
			let library_manager = &context.library_manager;

			// Get the specific library
			let library = library_manager
				.get_library(library_id)
				.await
				.ok_or(ActionError::LibraryNotFound(library_id))?;

			// Get the device UUID from the device manager
			let device_uuid = context
				.device_manager
				.device_id()
				.map_err(ActionError::device_manager_error)?;

			// Get device record from database to get the integer ID
			let db = library.db().conn();
			let device_record = entities::device::Entity::find()
				.filter(entities::device::Column::Uuid.eq(device_uuid))
				.one(db)
				.await
				.map_err(ActionError::SeaOrm)?
				.ok_or_else(|| ActionError::DeviceNotFound(device_uuid))?;

			// Add the location using LocationManager
			let location_manager = LocationManager::new(context.events.as_ref().clone());

			let location_mode = match action.mode {
				IndexMode::Shallow => crate::location::IndexMode::Shallow,
				IndexMode::Content => crate::location::IndexMode::Content,
				IndexMode::Deep => crate::location::IndexMode::Deep,
			};

			// Store the name to use for output since we're moving it
			let name_for_output = action.name.clone();

			let (location_id, job_id_string) = location_manager
				.add_location(
					library.clone(),
					action.path.clone(),
					action.name,
					device_record.id,
					location_mode,
				)
				.await
				.map_err(|e| ActionError::Internal(e.to_string()))?;

			// Parse the job ID from the string returned by add_location
			let job_id = if !job_id_string.is_empty() {
				Uuid::parse_str(&job_id_string)
					.map_err(|e| ActionError::Internal(format!("Failed to parse job ID: {}", e)))?
			} else {
				// If no job was created by add_location, we should handle this case
				return Err(ActionError::Internal("Location added but indexing failed to start".to_string()));
			};

			let output = LocationAddOutput::new(location_id, action.path.clone(), name_for_output)
				.with_job_id(job_id.into());
			Ok(ActionOutput::from_trait(output))
		} else {
			Err(ActionError::InvalidActionType)
		}
	}

	fn can_handle(&self, action: &Action) -> bool {
		matches!(action, Action::LocationAdd { .. })
	}

	fn supported_actions() -> &'static [&'static str] {
		&["location.add"]
	}
}

// Register this handler
register_action_handler!(LocationAddHandler, "location.add");
```

## src/operations/locations/add/mod.rs

```rust
//! Location add operations

pub mod action;
pub mod output;

pub use output::LocationAddOutput;```

## src/operations/locations/add/output.rs

```rust
//! Location add operation output types

use crate::infrastructure::actions::output::ActionOutputTrait;
use serde::{Deserialize, Serialize};
use std::path::PathBuf;
use uuid::Uuid;

/// Output from location add action dispatch
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LocationAddOutput {
    pub location_id: Uuid,
    pub path: PathBuf,
    pub name: Option<String>,
    pub job_id: Option<Uuid>,
}

impl LocationAddOutput {
    pub fn new(location_id: Uuid, path: PathBuf, name: Option<String>) -> Self {
        Self {
            location_id,
            path,
            name,
            job_id: None,
        }
    }

    pub fn with_job_id(mut self, job_id: Uuid) -> Self {
        self.job_id = Some(job_id);
        self
    }
}

impl ActionOutputTrait for LocationAddOutput {
    fn to_json(&self) -> serde_json::Value {
        serde_json::to_value(self).unwrap_or(serde_json::Value::Null)
    }
    
    fn display_message(&self) -> String {
        match &self.name {
            Some(name) => format!(
                "Added location '{}' with ID {} at {}",
                name, self.location_id, self.path.display()
            ),
            None => format!(
                "Added location with ID {} at {}",
                self.location_id, self.path.display()
            ),
        }
    }
    
    fn output_type(&self) -> &'static str {
        "location.add.completed"
    }
}```

## src/operations/locations/index/action.rs

```rust
//! Location index action handler

use crate::{
    context::CoreContext,
    infrastructure::{
        actions::{
            Action, error::{ActionError, ActionResult}, handler::ActionHandler, output::ActionOutput,
        },
    },
    operations::{
        indexing::{IndexMode, job::IndexerJob},
    },
    register_action_handler,
    shared::types::SdPath,
};
use async_trait::async_trait;
use serde::{Deserialize, Serialize};
use std::sync::Arc;
use uuid::Uuid;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LocationIndexAction {
    pub location_id: Uuid,
    pub mode: IndexMode,
}

pub struct LocationIndexHandler;

impl LocationIndexHandler {
    pub fn new() -> Self {
        Self
    }
}

#[async_trait]
impl ActionHandler for LocationIndexHandler {
    async fn execute(
        &self,
        context: Arc<CoreContext>,
        action: Action,
    ) -> ActionResult<ActionOutput> {
        if let Action::LocationIndex { library_id, action } = action {
            let library_manager = &context.library_manager;
            
            // Get the specific library
            let library = library_manager
                .get_library(library_id)
                .await
                .ok_or(ActionError::LibraryNotFound(library_id))?;

            // TODO: In a real implementation, we'd query the database to get the location's actual path
            // For now, let's create a placeholder SdPath  
            let location_path = SdPath::local("/placeholder"); // This should be the actual location path
            
            // Create indexer job directly
            let job = IndexerJob::from_location(action.location_id, location_path, action.mode);

            // Dispatch the job directly
            let job_handle = library
                .jobs()
                .dispatch(job)
                .await
                .map_err(ActionError::Job)?;

            Ok(ActionOutput::success("Location indexed successfully"))
        } else {
            Err(ActionError::InvalidActionType)
        }
    }

    fn can_handle(&self, action: &Action) -> bool {
        matches!(action, Action::LocationIndex { .. })
    }

    fn supported_actions() -> &'static [&'static str] {
        &["location.index"]
    }
}

// Register this handler
register_action_handler!(LocationIndexHandler, "location.index");```

## src/operations/locations/index/mod.rs

```rust
//! Location index operations

pub mod action;```

## src/operations/files/delete/action.rs

```rust
//! File delete action handler

use super::job::{DeleteJob, DeleteMode, DeleteOptions};
use super::output::FileDeleteOutput;
use crate::{
	context::CoreContext,
	infrastructure::actions::{
		error::{ActionError, ActionResult},
		handler::ActionHandler,
		output::ActionOutput,
		Action,
	},
	register_action_handler,
	shared::types::{SdPath, SdPathBatch},
};
use async_trait::async_trait;
use serde::{Deserialize, Serialize};
use std::{path::PathBuf, sync::Arc};
use uuid::Uuid;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct FileDeleteAction {
	pub targets: Vec<PathBuf>,
	pub options: DeleteOptions,
}

pub struct FileDeleteHandler;

impl FileDeleteHandler {
	pub fn new() -> Self {
		Self
	}
}

#[async_trait]
impl ActionHandler for FileDeleteHandler {
	async fn validate(&self, _context: Arc<CoreContext>, action: &Action) -> ActionResult<()> {
		if let Action::FileDelete {
			library_id: _,
			action,
		} = action
		{
			if action.targets.is_empty() {
				return Err(ActionError::Validation {
					field: "targets".to_string(),
					message: "At least one target file must be specified".to_string(),
				});
			}
			Ok(())
		} else {
			Err(ActionError::InvalidActionType)
		}
	}

	async fn execute(
		&self,
		context: Arc<CoreContext>,
		action: Action,
	) -> ActionResult<ActionOutput> {
		if let Action::FileDelete { library_id, action } = action {
			let library_manager = &context.library_manager;

			// Get the specific library
			let library = library_manager
				.get_library(library_id)
				.await
				.ok_or(ActionError::LibraryNotFound(library_id))?;

			// Create job instance directly (no JSON roundtrip)
			let targets_count = action.targets.len();
			let targets = action
				.targets
				.into_iter()
				.map(|path| SdPath::local(path))
				.collect();

			let mode = if action.options.permanent {
				DeleteMode::Permanent
			} else {
				DeleteMode::Trash
			};

			let job = DeleteJob::new(SdPathBatch::new(targets), mode);

			// Dispatch the job directly
			let job_handle = library
				.jobs()
				.dispatch(job)
				.await
				.map_err(ActionError::Job)?;

			// Return action output instead of receipt
			let output = FileDeleteOutput::new(job_handle.id().into(), targets_count);
			Ok(ActionOutput::from_trait(output))
		} else {
			Err(ActionError::InvalidActionType)
		}
	}

	fn can_handle(&self, action: &Action) -> bool {
		matches!(action, Action::FileDelete { .. })
	}

	fn supported_actions() -> &'static [&'static str] {
		&["file.delete"]
	}
}

// Register this handler
register_action_handler!(FileDeleteHandler, "file.delete");
```

## src/operations/files/delete/job.rs

```rust
//! Delete job implementation

use crate::{
    infrastructure::jobs::prelude::*,
    shared::types::SdPathBatch,
};
use serde::{Deserialize, Serialize};
use std::{
    path::PathBuf,
    time::{Duration, Instant},
};
use tokio::fs;

/// Delete operation modes
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum DeleteMode {
    /// Move to trash/recycle bin
    Trash,
    /// Permanent deletion (cannot be undone)
    Permanent,
    /// Secure deletion (overwrite data)
    Secure,
}

/// Options for file delete operations
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DeleteOptions {
    pub permanent: bool,
    pub recursive: bool,
}

impl Default for DeleteOptions {
    fn default() -> Self {
        Self {
            permanent: false,
            recursive: false,
        }
    }
}

/// Delete job for removing files and directories
#[derive(Debug, Serialize, Deserialize)]
pub struct DeleteJob {
    pub targets: SdPathBatch,
    pub mode: DeleteMode,
    pub confirm_permanent: bool,
    
    // Internal state for resumption
    #[serde(skip)]
    completed_deletions: Vec<usize>,
    #[serde(skip, default = "Instant::now")]
    started_at: Instant,
}

/// Delete progress information
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DeleteProgress {
    pub current_file: String,
    pub files_deleted: usize,
    pub total_files: usize,
    pub bytes_deleted: u64,
    pub total_bytes: u64,
    pub current_operation: String,
    pub estimated_remaining: Option<Duration>,
}

impl JobProgress for DeleteProgress {}

impl Job for DeleteJob {
    const NAME: &'static str = "delete_files";
    const RESUMABLE: bool = true;
    const DESCRIPTION: Option<&'static str> = Some("Delete files and directories");
}

#[async_trait::async_trait]
impl JobHandler for DeleteJob {
    type Output = DeleteOutput;
    
    async fn run(&mut self, ctx: JobContext<'_>) -> JobResult<Self::Output> {
        ctx.log(format!("Starting {} deletion of {} files", 
            match self.mode {
                DeleteMode::Trash => "trash",
                DeleteMode::Permanent => "permanent",
                DeleteMode::Secure => "secure",
            },
            self.targets.paths.len()
        ));
        
        // Safety check for permanent deletion
        if matches!(self.mode, DeleteMode::Permanent | DeleteMode::Secure) && !self.confirm_permanent {
            return Err(JobError::execution(
                "Permanent deletion requires explicit confirmation"
            ));
        }
        
        // Validate targets exist
        self.validate_targets(&ctx).await?;
        
        let total_files = self.targets.paths.len();
        let mut deleted_count = 0;
        let mut total_bytes = 0u64;
        let mut failed_deletions = Vec::new();
        
        // Calculate total size for progress
        let estimated_total_bytes = self.calculate_total_size(&ctx).await?;
        
        // Process deletions
        for (index, target) in self.targets.paths.iter().enumerate() {
            ctx.check_interrupt().await?;
            
            // Skip if already processed (for resumption)
            if self.completed_deletions.contains(&index) {
                continue;
            }
            
            if let Some(local_path) = target.as_local_path() {
                ctx.progress(Progress::structured(DeleteProgress {
                    current_file: local_path.display().to_string(),
                    files_deleted: deleted_count,
                    total_files,
                    bytes_deleted: total_bytes,
                    total_bytes: estimated_total_bytes,
                    current_operation: self.get_operation_name(),
                    estimated_remaining: None,
                }));
                
                match self.delete_path(local_path, &ctx).await {
                    Ok(bytes) => {
                        deleted_count += 1;
                        total_bytes += bytes;
                        self.completed_deletions.push(index);
                        
                        ctx.log(format!("Deleted: {}", local_path.display()));
                        
                        // Checkpoint every 10 files
                        if deleted_count % 10 == 0 {
                            ctx.checkpoint().await?;
                        }
                    }
                    Err(e) => {
                        failed_deletions.push(DeleteError {
                            path: local_path.to_path_buf(),
                            error: e.to_string(),
                        });
                        ctx.add_non_critical_error(format!(
                            "Failed to delete {}: {}", local_path.display(), e
                        ));
                    }
                }
            }
        }
        
        ctx.log(format!("Delete operation completed: {} deleted, {} failed", 
            deleted_count, failed_deletions.len()));
        
        Ok(DeleteOutput {
            deleted_count,
            failed_count: failed_deletions.len(),
            total_bytes,
            duration: self.started_at.elapsed(),
            failed_deletions,
            mode: self.mode.clone(),
        })
    }
}

impl DeleteJob {
    /// Create a new delete job
    pub fn new(targets: SdPathBatch, mode: DeleteMode) -> Self {
        Self {
            targets,
            mode,
            confirm_permanent: false,
            completed_deletions: Vec::new(),
            started_at: Instant::now(),
        }
    }
    
    /// Create a trash operation
    pub fn trash(targets: SdPathBatch) -> Self {
        Self::new(targets, DeleteMode::Trash)
    }
    
    /// Create a permanent delete operation (requires confirmation)
    pub fn permanent(targets: SdPathBatch, confirmed: bool) -> Self {
        let mut job = Self::new(targets, DeleteMode::Permanent);
        job.confirm_permanent = confirmed;
        job
    }
    
    /// Create a secure delete operation (requires confirmation)
    pub fn secure(targets: SdPathBatch, confirmed: bool) -> Self {
        let mut job = Self::new(targets, DeleteMode::Secure);
        job.confirm_permanent = confirmed;
        job
    }
    
    /// Validate that all targets exist
    async fn validate_targets(&self, ctx: &JobContext<'_>) -> JobResult<()> {
        for target in &self.targets.paths {
            if let Some(local_path) = target.as_local_path() {
                if !fs::try_exists(local_path).await.unwrap_or(false) {
                    return Err(JobError::execution(format!(
                        "Target does not exist: {}", 
                        local_path.display()
                    )));
                }
            }
        }
        Ok(())
    }
    
    /// Calculate total size for progress reporting
    async fn calculate_total_size(&self, ctx: &JobContext<'_>) -> JobResult<u64> {
        let mut total = 0u64;
        
        for target in &self.targets.paths {
            if let Some(local_path) = target.as_local_path() {
                total += self.get_path_size(local_path).await.unwrap_or(0);
            }
        }
        
        Ok(total)
    }
    
    /// Get size of a path (file or directory) using iterative approach
    async fn get_path_size(&self, path: &std::path::Path) -> Result<u64, std::io::Error> {
        let mut total = 0u64;
        let mut stack = vec![path.to_path_buf()];
        
        while let Some(current_path) = stack.pop() {
            let metadata = fs::metadata(&current_path).await?;
            
            if metadata.is_file() {
                total += metadata.len();
            } else if metadata.is_dir() {
                let mut dir = fs::read_dir(&current_path).await?;
                while let Some(entry) = dir.next_entry().await? {
                    stack.push(entry.path());
                }
            }
        }
        
        Ok(total)
    }
    
    /// Delete a single path according to the specified mode
    async fn delete_path(
        &self,
        path: &std::path::Path,
        ctx: &JobContext<'_>
    ) -> Result<u64, std::io::Error> {
        let size = self.get_path_size(path).await.unwrap_or(0);
        
        match self.mode {
            DeleteMode::Trash => {
                self.move_to_trash(path).await?;
            }
            DeleteMode::Permanent => {
                self.permanent_delete(path).await?;
            }
            DeleteMode::Secure => {
                self.secure_delete(path).await?;
            }
        }
        
        Ok(size)
    }
    
    /// Move file to system trash/recycle bin
    async fn move_to_trash(&self, path: &std::path::Path) -> Result<(), std::io::Error> {
        // On Unix systems, we typically move to ~/.local/share/Trash/files/
        // On Windows, we'd use the Recycle Bin API
        // On macOS, we'd move to ~/.Trash/
        
        #[cfg(unix)]
        {
            self.move_to_trash_unix(path).await?;
        }
        
        #[cfg(windows)]
        {
            self.move_to_trash_windows(path).await?;
        }
        
        #[cfg(target_os = "macos")]
        {
            self.move_to_trash_macos(path).await?;
        }
        
        Ok(())
    }
    
    #[cfg(unix)]
    async fn move_to_trash_unix(&self, path: &std::path::Path) -> Result<(), std::io::Error> {
        // Follow XDG Trash specification
        let home = std::env::var("HOME").map_err(|_| {
            std::io::Error::new(std::io::ErrorKind::NotFound, "HOME not set")
        })?;
        
        let trash_dir = std::path::Path::new(&home).join(".local/share/Trash/files");
        fs::create_dir_all(&trash_dir).await?;
        
        let filename = path.file_name().ok_or_else(|| {
            std::io::Error::new(std::io::ErrorKind::InvalidInput, "Invalid filename")
        })?;
        
        let trash_path = trash_dir.join(filename);
        
        // Find unique name if file already exists in trash
        let final_trash_path = self.find_unique_trash_name(&trash_path).await?;
        
        // Move to trash
        fs::rename(path, final_trash_path).await?;
        
        Ok(())
    }
    
    #[cfg(windows)]
    async fn move_to_trash_windows(&self, path: &std::path::Path) -> Result<(), std::io::Error> {
        // On Windows, we'd typically use the SHFileOperation API
        // For now, we'll use a simple implementation that moves to a temp trash folder
        let temp_dir = std::env::temp_dir().join("spacedrive_trash");
        fs::create_dir_all(&temp_dir).await?;
        
        let filename = path.file_name().ok_or_else(|| {
            std::io::Error::new(std::io::ErrorKind::InvalidInput, "Invalid filename")
        })?;
        
        let trash_path = temp_dir.join(filename);
        let final_trash_path = self.find_unique_trash_name(&trash_path).await?;
        
        fs::rename(path, final_trash_path).await?;
        
        Ok(())
    }
    
    #[cfg(target_os = "macos")]
    async fn move_to_trash_macos(&self, path: &std::path::Path) -> Result<(), std::io::Error> {
        let home = std::env::var("HOME").map_err(|_| {
            std::io::Error::new(std::io::ErrorKind::NotFound, "HOME not set")
        })?;
        
        let trash_dir = std::path::Path::new(&home).join(".Trash");
        
        let filename = path.file_name().ok_or_else(|| {
            std::io::Error::new(std::io::ErrorKind::InvalidInput, "Invalid filename")
        })?;
        
        let trash_path = trash_dir.join(filename);
        let final_trash_path = self.find_unique_trash_name(&trash_path).await?;
        
        fs::rename(path, final_trash_path).await?;
        
        Ok(())
    }
    
    /// Find a unique name in the trash directory
    async fn find_unique_trash_name(&self, base_path: &std::path::Path) -> Result<PathBuf, std::io::Error> {
        let mut candidate = base_path.to_path_buf();
        let mut counter = 1;
        
        while fs::try_exists(&candidate).await? {
            let stem = base_path.file_stem().unwrap_or_default();
            let extension = base_path.extension();
            
            let new_name = if let Some(ext) = extension {
                format!("{} ({})", stem.to_string_lossy(), counter)
            } else {
                format!("{} ({})", stem.to_string_lossy(), counter)
            };
            
            candidate = base_path.with_file_name(new_name);
            if let Some(ext) = extension {
                candidate.set_extension(ext);
            }
            
            counter += 1;
        }
        
        Ok(candidate)
    }
    
    /// Permanently delete file or directory
    async fn permanent_delete(&self, path: &std::path::Path) -> Result<(), std::io::Error> {
        let metadata = fs::metadata(path).await?;
        
        if metadata.is_file() {
            fs::remove_file(path).await?;
        } else if metadata.is_dir() {
            fs::remove_dir_all(path).await?;
        }
        
        Ok(())
    }
    
    /// Securely delete file by overwriting with random data
    async fn secure_delete(&self, path: &std::path::Path) -> Result<(), std::io::Error> {
        let metadata = fs::metadata(path).await?;
        
        if metadata.is_file() {
            // Overwrite file with random data multiple times
            self.secure_overwrite_file(path, metadata.len()).await?;
            fs::remove_file(path).await?;
        } else if metadata.is_dir() {
            // Recursively secure delete directory contents
            self.secure_delete_directory(path).await?;
            fs::remove_dir_all(path).await?;
        }
        
        Ok(())
    }
    
    /// Securely overwrite a file with random data
    async fn secure_overwrite_file(&self, path: &std::path::Path, size: u64) -> Result<(), std::io::Error> {
        use rand::RngCore;
        use tokio::io::{AsyncWriteExt, AsyncSeekExt};
        
        // Open file for writing
        let mut file = fs::OpenOptions::new()
            .write(true)
            .truncate(false)
            .open(path)
            .await?;
        
        // Overwrite with random data (3 passes)
        for _ in 0..3 {
            file.seek(std::io::SeekFrom::Start(0)).await?;
            
            // Write random data in chunks
            let mut remaining = size;
            
            while remaining > 0 {
                let chunk_size = std::cmp::min(remaining, 64 * 1024) as usize; // 64KB chunks
                
                // Generate random data synchronously to avoid Send issues
                let buffer = {
                    let mut rng = rand::thread_rng();
                    let mut buf = vec![0u8; chunk_size];
                    rng.fill_bytes(&mut buf);
                    buf
                };
                
                file.write_all(&buffer).await?;
                remaining -= chunk_size as u64;
            }
            
            file.flush().await?;
            file.sync_all().await?;
        }
        
        Ok(())
    }
    
    /// Secure delete directory using iterative approach
    async fn secure_delete_directory(&self, path: &std::path::Path) -> Result<(), std::io::Error> {
        let mut stack = vec![path.to_path_buf()];
        
        while let Some(current_path) = stack.pop() {
            let mut dir = fs::read_dir(&current_path).await?;
            
            while let Some(entry) = dir.next_entry().await? {
                let entry_path = entry.path();
                
                if entry_path.is_file() {
                    let metadata = fs::metadata(&entry_path).await?;
                    self.secure_overwrite_file(&entry_path, metadata.len()).await?;
                    fs::remove_file(&entry_path).await?;
                } else if entry_path.is_dir() {
                    stack.push(entry_path);
                }
            }
        }
        
        Ok(())
    }
    
    /// Get operation name for progress display
    fn get_operation_name(&self) -> String {
        match self.mode {
            DeleteMode::Trash => "Moving to trash".to_string(),
            DeleteMode::Permanent => "Permanently deleting".to_string(),
            DeleteMode::Secure => "Securely deleting".to_string(),
        }
    }
}

/// Error information for failed deletions
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DeleteError {
    pub path: PathBuf,
    pub error: String,
}

/// Job output for delete operations
#[derive(Debug, Serialize, Deserialize)]
pub struct DeleteOutput {
    pub deleted_count: usize,
    pub failed_count: usize,
    pub total_bytes: u64,
    pub duration: Duration,
    pub failed_deletions: Vec<DeleteError>,
    pub mode: DeleteMode,
}

impl From<DeleteOutput> for JobOutput {
    fn from(output: DeleteOutput) -> Self {
        JobOutput::FileDelete {
            deleted_count: output.deleted_count,
            failed_count: output.failed_count,
            total_bytes: output.total_bytes,
        }
    }
}```

## src/operations/files/delete/mod.rs

```rust
//! File delete operations

pub mod action;
pub mod job;
pub mod output;

pub use job::*;
pub use output::FileDeleteOutput;```

## src/operations/files/delete/output.rs

```rust
//! File delete operation output types

use crate::infrastructure::actions::output::ActionOutputTrait;
use serde::{Deserialize, Serialize};
use uuid::Uuid;

/// Output from file delete action dispatch
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct FileDeleteOutput {
    pub job_id: Uuid,
    pub targets_count: usize,
}

impl FileDeleteOutput {
    pub fn new(job_id: Uuid, targets_count: usize) -> Self {
        Self {
            job_id,
            targets_count,
        }
    }
}

impl ActionOutputTrait for FileDeleteOutput {
    fn to_json(&self) -> serde_json::Value {
        serde_json::to_value(self).unwrap_or(serde_json::Value::Null)
    }
    
    fn display_message(&self) -> String {
        format!(
            "Dispatched file delete job {} for {} file(s)",
            self.job_id, self.targets_count
        )
    }
    
    fn output_type(&self) -> &'static str {
        "file.delete.dispatched"
    }
}```

## src/operations/files/mod.rs

```rust
//! File operations

pub mod copy;
pub mod delete;
pub mod validation;
pub mod duplicate_detection;

pub use copy::*;
pub use delete::*;
pub use validation::*;
pub use duplicate_detection::*;```

## src/operations/files/duplicate_detection/action.rs

```rust
//! File duplicate detection action handler

use crate::{
    context::CoreContext,
    infrastructure::actions::{
        error::{ActionError, ActionResult}, 
        handler::ActionHandler, 
        output::ActionOutput,
    },
    register_action_handler,
    shared::types::{SdPath, SdPathBatch},
};
use super::job::{DuplicateDetectionJob, DetectionMode};
use async_trait::async_trait;
use std::sync::Arc;
use uuid::Uuid;

#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct DuplicateDetectionAction {
    pub paths: Vec<std::path::PathBuf>,
    pub algorithm: String,
    pub threshold: f64,
}

pub struct DuplicateDetectionHandler;

impl DuplicateDetectionHandler {
    pub fn new() -> Self {
        Self
    }
}

#[async_trait]
impl ActionHandler for DuplicateDetectionHandler {
    async fn validate(
        &self,
        _context: Arc<CoreContext>,
        action: &crate::infrastructure::actions::Action,
    ) -> ActionResult<()> {
        if let crate::infrastructure::actions::Action::DetectDuplicates { action, .. } = action {
            if action.paths.is_empty() {
                return Err(ActionError::Validation {
                    field: "paths".to_string(),
                    message: "At least one path must be specified".to_string(),
                });
            }
            Ok(())
        } else {
            Err(ActionError::InvalidActionType)
        }
    }

    async fn execute(
        &self,
        context: Arc<CoreContext>,
        action: crate::infrastructure::actions::Action,
    ) -> ActionResult<ActionOutput> {
        if let crate::infrastructure::actions::Action::DetectDuplicates { library_id, action } = action {
            let library_manager = &context.library_manager;
            
            let library = library_manager.get_library(library_id).await
                .ok_or(ActionError::Internal(format!("Library not found: {}", library_id)))?;

            // Convert paths to SdPath and create job
            let search_paths = action.paths
                .into_iter()
                .map(|path| SdPath::local(path))
                .collect();

            // Parse algorithm to detection mode
            let mode = match action.algorithm.as_str() {
                "content_hash" => DetectionMode::ContentHash,
                "size_only" => DetectionMode::SizeOnly,
                "name_and_size" => DetectionMode::NameAndSize,
                "deep_scan" => DetectionMode::DeepScan,
                _ => DetectionMode::ContentHash, // default
            };

            let job = DuplicateDetectionJob::new(SdPathBatch::new(search_paths), mode);

            // Dispatch the job directly
            let job_handle = library
                .jobs()
                .dispatch(job)
                .await
                .map_err(ActionError::Job)?;

            Ok(ActionOutput::success("Duplicate detection job dispatched successfully"))
        } else {
            Err(ActionError::InvalidActionType)
        }
    }

    fn can_handle(&self, action: &crate::infrastructure::actions::Action) -> bool {
        matches!(action, crate::infrastructure::actions::Action::DetectDuplicates { .. })
    }

    fn supported_actions() -> &'static [&'static str] {
        &["file.detect_duplicates"]
    }
}

register_action_handler!(DuplicateDetectionHandler, "file.detect_duplicates");```

## src/operations/files/duplicate_detection/job.rs

```rust
//! Duplicate detection job implementation

use crate::{
	domain::content_identity::ContentHashGenerator,
	infrastructure::jobs::prelude::*,
	shared::types::{SdPath, SdPathBatch},
};
use serde::{Deserialize, Serialize};
use std::{
	collections::{HashMap, HashSet},
	path::PathBuf,
	time::{Duration, Instant},
};
use tokio::fs;
use uuid::Uuid;

/// Duplicate detection modes
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum DetectionMode {
	/// Compare only file sizes
	SizeOnly,
	/// Compare file sizes and CAS IDs (content hash)
	ContentHash,
	/// Compare file names and sizes
	NameAndSize,
	/// Deep comparison with full content verification
	DeepScan,
}

/// Duplicate detection job for finding duplicate files
#[derive(Debug, Serialize, Deserialize)]
pub struct DuplicateDetectionJob {
	pub search_paths: SdPathBatch,
	pub mode: DetectionMode,
	pub min_file_size: u64,
	pub max_file_size: Option<u64>,
	pub file_extensions: Option<HashSet<String>>,

	// Internal state for resumption
	#[serde(skip)]
	processed_files: HashSet<PathBuf>,
	#[serde(skip)]
	size_groups: HashMap<u64, Vec<FileInfo>>,
	#[serde(skip, default = "Instant::now")]
	started_at: Instant,
}

/// File information for duplicate detection
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct FileInfo {
	pub path: SdPath,
	pub size: u64,
	pub content_hash: Option<String>,
	pub modified: Option<std::time::SystemTime>,
}

/// Duplicate group containing files with same content
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DuplicateGroup {
	pub files: Vec<FileInfo>,
	pub total_size: u64,
	pub wasted_space: u64, // Size that could be saved by keeping only one copy
	pub detection_method: String,
}

/// Duplicate detection progress information
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DuplicateProgress {
	pub current_file: String,
	pub files_scanned: usize,
	pub total_files: usize,
	pub duplicates_found: usize,
	pub potential_savings: u64,
	pub current_operation: String,
}

impl JobProgress for DuplicateProgress {}

impl Job for DuplicateDetectionJob {
	const NAME: &'static str = "duplicate_detection";
	const RESUMABLE: bool = true;
	const DESCRIPTION: Option<&'static str> = Some("Find duplicate files");
}

#[async_trait::async_trait]
impl JobHandler for DuplicateDetectionJob {
	type Output = DuplicateDetectionOutput;

	async fn run(&mut self, ctx: JobContext<'_>) -> JobResult<Self::Output> {
		ctx.log(format!(
			"Starting duplicate detection with mode: {:?}",
			self.mode
		));

		// Collect all files to scan
		let files_to_scan = self.collect_files(&ctx).await?;
		let total_files = files_to_scan.len();

		ctx.log(format!(
			"Found {} files to scan for duplicates",
			total_files
		));

		// Phase 1: Group by size
		self.group_by_size(&files_to_scan, &ctx, total_files)
			.await?;

		// Phase 2: Further analysis based on mode
		let duplicate_groups = match self.mode {
			DetectionMode::SizeOnly => self.find_size_duplicates(&ctx).await?,
			DetectionMode::ContentHash => self.find_content_duplicates(&ctx).await?,
			DetectionMode::NameAndSize => self.find_name_size_duplicates(&ctx).await?,
			DetectionMode::DeepScan => self.find_deep_scan_duplicates(&ctx).await?,
		};

		let total_duplicates = duplicate_groups.iter().map(|g| g.files.len() - 1).sum();
		let potential_savings: u64 = duplicate_groups.iter().map(|g| g.wasted_space).sum();

		ctx.log(format!("Duplicate detection completed: {} groups found, {} duplicates, {} bytes potential savings",
            duplicate_groups.len(), total_duplicates, potential_savings));

		Ok(DuplicateDetectionOutput {
			duplicate_groups,
			total_files_scanned: total_files,
			total_duplicates,
			potential_savings,
			duration: self.started_at.elapsed(),
			detection_mode: self.mode.clone(),
		})
	}
}

impl DuplicateDetectionJob {
	/// Create a new duplicate detection job
	pub fn new(search_paths: SdPathBatch, mode: DetectionMode) -> Self {
		Self {
			search_paths,
			mode,
			min_file_size: 1024, // 1KB minimum
			max_file_size: None,
			file_extensions: None,
			processed_files: HashSet::new(),
			size_groups: HashMap::new(),
			started_at: Instant::now(),
		}
	}

	/// Set minimum file size filter
	pub fn with_min_size(mut self, min_size: u64) -> Self {
		self.min_file_size = min_size;
		self
	}

	/// Set maximum file size filter
	pub fn with_max_size(mut self, max_size: u64) -> Self {
		self.max_file_size = Some(max_size);
		self
	}

	/// Set file extension filter
	pub fn with_extensions(mut self, extensions: Vec<String>) -> Self {
		self.file_extensions = Some(extensions.into_iter().collect());
		self
	}

	/// Collect all files to scan
	async fn collect_files(&self, ctx: &JobContext<'_>) -> JobResult<Vec<FileInfo>> {
		let mut files = Vec::new();

		for search_path in &self.search_paths.paths {
			ctx.check_interrupt().await?;

			if let Some(local_path) = search_path.as_local_path() {
				self.collect_files_recursive(local_path, search_path, &mut files, ctx)
					.await?;
			}
		}

		Ok(files)
	}

	/// Collect files from a directory using iterative approach
	async fn collect_files_recursive(
		&self,
		path: &std::path::Path,
		sd_path: &SdPath,
		files: &mut Vec<FileInfo>,
		ctx: &JobContext<'_>,
	) -> JobResult<()> {
		let mut stack = vec![(path.to_path_buf(), sd_path.clone())];

		while let Some((current_path, current_sd_path)) = stack.pop() {
			ctx.check_interrupt().await?;

			let metadata = fs::metadata(&current_path).await?;

			if metadata.is_file() {
				if self.should_include_file(&current_path, metadata.len()) {
					files.push(FileInfo {
						path: current_sd_path,
						size: metadata.len(),
						content_hash: None,
						modified: metadata.modified().ok(),
					});
				}
			} else if metadata.is_dir() {
				let mut dir = fs::read_dir(&current_path).await?;

				while let Some(entry) = dir.next_entry().await? {
					let entry_path = entry.path();
					let entry_sd_path = current_sd_path.join(entry.file_name());
					stack.push((entry_path, entry_sd_path));
				}
			}
		}

		Ok(())
	}

	/// Check if a file should be included based on filters
	fn should_include_file(&self, path: &std::path::Path, size: u64) -> bool {
		// Size filters
		if size < self.min_file_size {
			return false;
		}

		if let Some(max_size) = self.max_file_size {
			if size > max_size {
				return false;
			}
		}

		// Extension filter
		if let Some(ref extensions) = self.file_extensions {
			if let Some(ext) = path.extension().and_then(|e| e.to_str()) {
				if !extensions.contains(&ext.to_lowercase()) {
					return false;
				}
			} else {
				// No extension - exclude if we have extension filter
				return false;
			}
		}

		true
	}

	/// Group files by size for initial duplicate detection
	async fn group_by_size(
		&mut self,
		files: &[FileInfo],
		ctx: &JobContext<'_>,
		total_files: usize,
	) -> JobResult<()> {
		for (index, file) in files.iter().enumerate() {
			ctx.check_interrupt().await?;

			ctx.progress(Progress::structured(DuplicateProgress {
				current_file: file.path.display(),
				files_scanned: index + 1,
				total_files,
				duplicates_found: 0,
				potential_savings: 0,
				current_operation: "Grouping by size".to_string(),
			}));

			self.size_groups
				.entry(file.size)
				.or_insert_with(Vec::new)
				.push(file.clone());

			// Checkpoint every 100 files
			if index % 100 == 0 {
				ctx.checkpoint().await?;
			}
		}

		Ok(())
	}

	/// Find duplicates based on size only
	async fn find_size_duplicates(&self, ctx: &JobContext<'_>) -> JobResult<Vec<DuplicateGroup>> {
		let mut groups = Vec::new();

		for (size, files) in &self.size_groups {
			if files.len() > 1 {
				let wasted_space = *size * (files.len() as u64 - 1);
				groups.push(DuplicateGroup {
					files: files.clone(),
					total_size: *size * files.len() as u64,
					wasted_space,
					detection_method: "Size comparison".to_string(),
				});
			}
		}

		Ok(groups)
	}

	/// Find duplicates based on content hash
	async fn find_content_duplicates(
		&self,
		ctx: &JobContext<'_>,
	) -> JobResult<Vec<DuplicateGroup>> {
		let mut groups = Vec::new();
		let mut processed = 0;
		let total_candidates: usize = self
			.size_groups
			.values()
			.map(|files| if files.len() > 1 { files.len() } else { 0 })
			.sum();

		for (size, files) in &self.size_groups {
			if files.len() > 1 {
				ctx.check_interrupt().await?;

				// Generate content hashes for files with same size
				let mut hash_groups: HashMap<String, Vec<FileInfo>> = HashMap::new();

				for file in files {
					ctx.progress(Progress::structured(DuplicateProgress {
						current_file: file.path.display(),
						files_scanned: processed + 1,
						total_files: total_candidates,
						duplicates_found: groups.len(),
						potential_savings: groups
							.iter()
							.map(|g: &DuplicateGroup| g.wasted_space)
							.sum(),
						current_operation: "Computing content hashes".to_string(),
					}));

					if let Some(local_path) = file.path.as_local_path() {
						match ContentHashGenerator::generate_content_hash(local_path).await {
							Ok(content_hash) => {
								let mut file_with_cas = file.clone();
								file_with_cas.content_hash = Some(content_hash.clone());
								hash_groups
									.entry(content_hash)
									.or_insert_with(Vec::new)
									.push(file_with_cas);
							}
							Err(e) => {
								ctx.add_non_critical_error(format!(
									"Failed to generate CAS ID for {}: {}",
									file.path.display(),
									e
								));
							}
						}
					}

					processed += 1;
				}

				// Create groups for files with same hash
				for (hash, hash_files) in hash_groups {
					if hash_files.len() > 1 {
						let wasted_space = *size * (hash_files.len() as u64 - 1);
						groups.push(DuplicateGroup {
							files: hash_files,
							total_size: *size * files.len() as u64,
							wasted_space,
							detection_method: format!("Content hash: {}", &hash[..8]),
						});
					}
				}
			}
		}

		Ok(groups)
	}

	/// Find duplicates based on name and size
	async fn find_name_size_duplicates(
		&self,
		ctx: &JobContext<'_>,
	) -> JobResult<Vec<DuplicateGroup>> {
		let mut groups = Vec::new();
		let mut name_size_groups: HashMap<(String, u64), Vec<FileInfo>> = HashMap::new();

		for (size, files) in &self.size_groups {
			if files.len() > 1 {
				for file in files {
					if let Some(filename) = file.path.file_name() {
						let key = (filename.to_string(), *size);
						name_size_groups
							.entry(key)
							.or_insert_with(Vec::new)
							.push(file.clone());
					}
				}
			}
		}

		for ((name, size), files) in name_size_groups {
			if files.len() > 1 {
				let file_count = files.len() as u64;
				let wasted_space = size * (file_count - 1);
				groups.push(DuplicateGroup {
					files,
					total_size: size * file_count,
					wasted_space,
					detection_method: format!("Name + size: {}", name),
				});
			}
		}

		Ok(groups)
	}

	/// Find duplicates with deep scanning (byte-by-byte comparison)
	async fn find_deep_scan_duplicates(
		&self,
		ctx: &JobContext<'_>,
	) -> JobResult<Vec<DuplicateGroup>> {
		// For deep scan, we first use content hash, then verify with byte comparison
		let mut hash_groups = self.find_content_duplicates(ctx).await?;

		// Additional verification for critical duplicates could go here
		// For now, content hash is sufficient for deep scanning

		for group in &mut hash_groups {
			group.detection_method = "Deep scan with content verification".to_string();
		}

		Ok(hash_groups)
	}
}

/// Job output for duplicate detection
#[derive(Debug, Serialize, Deserialize)]
pub struct DuplicateDetectionOutput {
	pub duplicate_groups: Vec<DuplicateGroup>,
	pub total_files_scanned: usize,
	pub total_duplicates: usize,
	pub potential_savings: u64,
	pub duration: Duration,
	pub detection_mode: DetectionMode,
}

impl From<DuplicateDetectionOutput> for JobOutput {
	fn from(output: DuplicateDetectionOutput) -> Self {
		JobOutput::DuplicateDetection {
			duplicate_groups: output.duplicate_groups.len(),
			total_duplicates: output.total_duplicates,
			potential_savings: output.potential_savings,
		}
	}
}
```

## src/operations/files/duplicate_detection/mod.rs

```rust
//! File duplicate detection operations

pub mod action;
pub mod job;

pub use job::*;
pub use action::DuplicateDetectionAction;```

## src/operations/files/copy/strategy.rs

```rust
//! Copy strategy implementations for different file operation scenarios

use crate::{
    infrastructure::jobs::prelude::*,
    shared::types::SdPath,
    volume::VolumeManager,
    operations::files::copy::job::CopyPhase,
};
use anyhow::Result;
use async_trait::async_trait;
use std::path::Path;
use tokio::fs;
use tokio::io::{AsyncReadExt, AsyncWriteExt};

/// Progress callback for strategy implementations to report granular progress
/// Parameters: bytes_copied_for_current_file, total_bytes_for_current_file
pub type ProgressCallback<'a> = Box<dyn Fn(u64, u64) + Send + Sync + 'a>;

/// Defines a method for performing a file copy operation
#[async_trait]
pub trait CopyStrategy: Send + Sync {
    /// Executes the copy strategy for a single source path
    async fn execute<'a>(&self, ctx: &JobContext<'a>, source: &SdPath, destination: &SdPath, verify_checksum: bool, progress_callback: Option<&ProgressCallback<'a>>) -> Result<u64>;
}

/// Strategy for an atomic move on the same volume
pub struct LocalMoveStrategy;

#[async_trait]
impl CopyStrategy for LocalMoveStrategy {
    async fn execute<'a>(&self, ctx: &JobContext<'a>, source: &SdPath, destination: &SdPath, verify_checksum: bool, progress_callback: Option<&ProgressCallback<'a>>) -> Result<u64> {
        let source_path = source.as_local_path()
            .ok_or_else(|| anyhow::anyhow!("Source path is not local"))?;
        let dest_path = destination.as_local_path()
            .ok_or_else(|| anyhow::anyhow!("Destination path is not local"))?;

        // Get file size before moving
        let metadata = fs::metadata(source_path).await?;
        let size = if metadata.is_file() {
            metadata.len()
        } else {
            get_path_size(source_path).await?
        };

        // Report progress at current offset before starting
        if let Some(callback) = progress_callback {
            callback(0, size);
        }

        // Create destination directory if needed
        if let Some(parent) = dest_path.parent() {
            fs::create_dir_all(parent).await?;
        }

        // Use atomic rename for same-volume moves
        fs::rename(source_path, dest_path).await?;

        // Report progress at 100% after completion
        if let Some(callback) = progress_callback {
            callback(size, size);
        }

        ctx.log(format!(
            "Atomic move: {} -> {}",
            source_path.display(),
            dest_path.display()
        ));

        Ok(size)
    }
}

/// Strategy for streaming a copy between different local volumes
pub struct LocalStreamCopyStrategy;

#[async_trait]
impl CopyStrategy for LocalStreamCopyStrategy {
    async fn execute<'a>(&self, ctx: &JobContext<'a>, source: &SdPath, destination: &SdPath, verify_checksum: bool, progress_callback: Option<&ProgressCallback<'a>>) -> Result<u64> {
        let source_path = source.as_local_path()
            .ok_or_else(|| anyhow::anyhow!("Source path is not local"))?;
        let dest_path = destination.as_local_path()
            .ok_or_else(|| anyhow::anyhow!("Destination path is not local"))?;

        // Get volume information for optimization
        let (source_vol, dest_vol) = if let Some(volume_manager) = ctx.volume_manager() {
            let source_vol = volume_manager.volume_for_path(source_path).await;
            let dest_vol = volume_manager.volume_for_path(dest_path).await;
            (source_vol, dest_vol)
        } else {
            (None, None)
        };

        let volume_info = match (&source_vol, &dest_vol) {
            (Some(s), Some(d)) => Some((s, d)),
            _ => None,
        };

        let bytes_copied = copy_file_streaming(source_path, dest_path, volume_info, ctx, verify_checksum, progress_callback).await?;

        ctx.log(format!(
            "Cross-volume streaming copy: {} -> {} ({} bytes)",
            source_path.display(),
            dest_path.display(),
            bytes_copied
        ));

        Ok(bytes_copied)
    }
}

/// Strategy for transferring a file to another device
pub struct RemoteTransferStrategy;

#[async_trait]
impl CopyStrategy for RemoteTransferStrategy {
    async fn execute<'a>(&self, ctx: &JobContext<'a>, source: &SdPath, destination: &SdPath, verify_checksum: bool, progress_callback: Option<&ProgressCallback<'a>>) -> Result<u64> {
        // Get networking service
        let networking = ctx.networking_service()
            .ok_or_else(|| anyhow::anyhow!("Networking service not available"))?;

        // Get local path
        let local_path = source.as_local_path()
            .ok_or_else(|| anyhow::anyhow!("Source must be local path"))?;

        // Read file metadata
        let metadata = tokio::fs::metadata(local_path).await?;
        let file_size = metadata.len();

        ctx.log(format!(
            "Initiating cross-device transfer: {} ({} bytes) -> device:{}",
            local_path.display(),
            file_size,
            destination.device_id
        ));

        // Create file metadata for transfer
        let file_metadata = crate::services::networking::protocols::FileMetadata {
            name: local_path.file_name()
                .unwrap_or_default()
                .to_string_lossy()
                .to_string(),
            size: file_size,
            modified: metadata.modified().ok(),
            is_directory: metadata.is_dir(),
            checksum: Some(calculate_file_checksum(local_path).await?),
            mime_type: None,
        };

        // Get file transfer protocol handler
        let networking_guard = &*networking;
        let protocol_registry = networking_guard.protocol_registry();
        let registry_guard = protocol_registry.read().await;

        let file_transfer_handler = registry_guard.get_handler("file_transfer")
            .ok_or_else(|| anyhow::anyhow!("File transfer protocol not registered"))?;

        let file_transfer_protocol = file_transfer_handler.as_any()
            .downcast_ref::<crate::services::networking::protocols::FileTransferProtocolHandler>()
            .ok_or_else(|| anyhow::anyhow!("Invalid file transfer protocol handler"))?;

        // Initiate transfer
        let transfer_id = file_transfer_protocol.initiate_transfer(
            destination.device_id,
            local_path.to_path_buf(),
            crate::services::networking::protocols::TransferMode::TrustedCopy,
        ).await?;

        ctx.log(format!("Transfer initiated with ID: {}", transfer_id));

        // Send transfer request to remote device
        let chunk_size = 64 * 1024u32;
        let total_chunks = ((file_size + chunk_size as u64 - 1) / chunk_size as u64) as u32;

        let transfer_request = crate::services::networking::protocols::file_transfer::FileTransferMessage::TransferRequest {
            transfer_id,
            file_metadata: file_metadata.clone(),
            transfer_mode: crate::services::networking::protocols::TransferMode::TrustedCopy,
            chunk_size,
            total_chunks,
            destination_path: destination.path.to_string_lossy().to_string(),
        };

        let request_data = rmp_serde::to_vec(&transfer_request)?;

        // Send transfer request over network
        networking_guard.send_message(
            destination.device_id,
            "file_transfer",
            request_data,
        ).await?;

        ctx.log(format!("Transfer request sent to device {}", destination.device_id));

        // Stream file data
        drop(networking_guard);
        drop(registry_guard);

        stream_file_data(
            local_path,
            transfer_id,
            file_transfer_protocol,
            file_size,
            destination.device_id,
            ctx,
            progress_callback,
        ).await?;

        ctx.log(format!("Cross-device transfer completed: {} bytes", file_size));
        Ok(file_size)
    }
}

/// Helper function to get size of a path (file or directory)
async fn get_path_size(path: &Path) -> Result<u64, std::io::Error> {
    let mut total = 0u64;
    let mut stack = vec![path.to_path_buf()];

    while let Some(current_path) = stack.pop() {
        let metadata = fs::metadata(&current_path).await?;

        if metadata.is_file() {
            total += metadata.len();
        } else if metadata.is_dir() {
            let mut dir = fs::read_dir(&current_path).await?;
            while let Some(entry) = dir.next_entry().await? {
                stack.push(entry.path());
            }
        }
    }

    Ok(total)
}

/// Copy a single file with streaming and progress tracking
async fn copy_single_file<'a>(
    source: &Path,
    destination: &Path,
    volume_info: Option<(&crate::volume::Volume, &crate::volume::Volume)>,
    ctx: &JobContext<'a>,
    verify_checksum: bool,
    file_size: u64,
    progress_callback: Option<&ProgressCallback<'a>>,
) -> Result<u64, std::io::Error> {
    let result = copy_single_file_with_offset(source, destination, volume_info, ctx, verify_checksum, file_size, progress_callback, 0).await?;
    
    // For single file copies, send completion signal
    if let Some(callback) = progress_callback {
        callback(result, u64::MAX);
    }
    
    Ok(result)
}

/// Copy a single file with streaming and progress tracking, with byte offset for cumulative progress
async fn copy_single_file_with_offset<'a>(
    source: &Path,
    destination: &Path,
    volume_info: Option<(&crate::volume::Volume, &crate::volume::Volume)>,
    ctx: &JobContext<'a>,
    verify_checksum: bool,
    file_size: u64,
    progress_callback: Option<&ProgressCallback<'a>>,
    byte_offset: u64,
) -> Result<u64, std::io::Error> {
    // Create destination directory if needed
    if let Some(parent) = destination.parent() {
        fs::create_dir_all(parent).await?;
    }

    let mut source_file = fs::File::open(source).await?;
    let mut dest_file = fs::File::create(destination).await?;

    // Determine optimal chunk size based on volume characteristics
    let chunk_size = if let Some((source_vol, dest_vol)) = volume_info {
        source_vol.optimal_chunk_size().min(dest_vol.optimal_chunk_size())
    } else {
        64 * 1024 // Default 64KB chunks
    };

    let mut buffer = vec![0u8; chunk_size];
    let mut total_copied = 0u64;
    let mut last_progress_update = std::time::Instant::now();

    // Initialize checksums if verification is enabled
    let mut source_hasher = if verify_checksum {
        Some(blake3::Hasher::new())
    } else {
        None
    };
    
    let mut dest_hasher = if verify_checksum {
        Some(blake3::Hasher::new())
    } else {
        None
    };

    loop {
        // Check for cancellation
        if let Err(_) = ctx.check_interrupt().await {
            // Clean up partial file on cancellation
            let _ = fs::remove_file(destination).await;
            return Err(std::io::Error::new(
                std::io::ErrorKind::Interrupted,
                "Operation cancelled"
            ));
        }

        let bytes_read = source_file.read(&mut buffer).await?;
        if bytes_read == 0 {
            break; // EOF
        }

        let chunk = &buffer[..bytes_read];
        dest_file.write_all(chunk).await?;
        total_copied += bytes_read as u64;

        // Update checksums if verification is enabled
        if let Some(hasher) = &mut source_hasher {
            hasher.update(chunk);
        }
        if let Some(hasher) = &mut dest_hasher {
            hasher.update(chunk);
        }

        // Update progress every 50ms for smoother updates
        if last_progress_update.elapsed() >= std::time::Duration::from_millis(50) {
            if let Some(callback) = progress_callback {
                // Send bytes copied within current file
                // The aggregator will add this to the bytes_completed_before_current
                callback(total_copied, file_size);
                
                // Debug log every 100MB
                if total_copied % (100 * 1024 * 1024) < bytes_read as u64 {
                    ctx.log(format!("Strategy progress callback: {} / {} bytes", total_copied, file_size));
                }
            }
            last_progress_update = std::time::Instant::now();
            
            // Explicitly yield to the scheduler to allow other tasks (like progress reporting) to run
            tokio::task::yield_now().await;
        }
    }

    // Ensure all data is written to disk
    dest_file.flush().await?;
    dest_file.sync_all().await?;

    // Final progress update to ensure we show 100%
    if let Some(callback) = progress_callback {
        callback(total_copied, file_size);
        ctx.log(format!("Strategy final progress: {} / {} bytes (100%)", total_copied, file_size));
    }

    // Verify checksums if enabled
    if verify_checksum {
        if let (Some(source_hasher), Some(dest_hasher)) = (source_hasher, dest_hasher) {
            let source_hash = source_hasher.finalize();
            let dest_hash = dest_hasher.finalize();
            
            if source_hash != dest_hash {
                // Clean up corrupted file
                let _ = fs::remove_file(destination).await;
                return Err(std::io::Error::new(
                    std::io::ErrorKind::InvalidData,
                    format!("Checksum verification failed: source={}, dest={}", source_hash.to_hex(), dest_hash.to_hex())
                ));
            }
            
            ctx.log(format!(
                "Checksum verification passed for {}: {}",
                destination.display(),
                source_hash.to_hex()
            ));
        }
    }

    // Copy file permissions and timestamps if requested
    let source_metadata = fs::metadata(source).await?;
    let dest_file = fs::File::open(destination).await?;
    
    #[cfg(unix)]
    {
        use std::os::unix::fs::PermissionsExt;
        let permissions = std::fs::Permissions::from_mode(source_metadata.permissions().mode());
        dest_file.set_permissions(permissions).await?;
    }

    Ok(total_copied)
}


/// Copy file with streaming and progress tracking for cross-volume operations
async fn copy_file_streaming<'a>(
    source: &Path,
    destination: &Path,
    volume_info: Option<(&crate::volume::Volume, &crate::volume::Volume)>,
    ctx: &JobContext<'a>,
    verify_checksum: bool,
    progress_callback: Option<&ProgressCallback<'a>>,
) -> Result<u64, std::io::Error> {
    // Create destination directory if needed
    if let Some(parent) = destination.parent() {
        fs::create_dir_all(parent).await?;
    }

    let metadata = fs::metadata(source).await?;
    if metadata.is_dir() {
        // For directories, we need to accumulate progress across multiple files
        fs::create_dir_all(destination).await?;
        let mut total_size = 0u64;
        
        // First, collect all files to copy
        let mut files_to_copy = Vec::new();
        let mut stack = vec![(source.to_path_buf(), destination.to_path_buf())];
        
        while let Some((src_path, dest_path)) = stack.pop() {
            if src_path.is_file() {
                files_to_copy.push((src_path, dest_path));
            } else if src_path.is_dir() {
                fs::create_dir_all(&dest_path).await?;
                let mut dir = fs::read_dir(&src_path).await?;
                while let Some(entry) = dir.next_entry().await? {
                    let entry_src = entry.path();
                    let entry_dest = dest_path.join(entry.file_name());
                    stack.push((entry_src, entry_dest));
                }
            }
        }
        
        // Now copy all files, tracking cumulative progress
        let mut cumulative_bytes = 0u64;
        for (src_path, dest_path) in files_to_copy {
            // Check for cancellation
            if let Err(_) = ctx.check_interrupt().await {
                return Err(std::io::Error::new(
                    std::io::ErrorKind::Interrupted,
                    "Operation cancelled"
                ));
            }
            
            let file_metadata = fs::metadata(&src_path).await?;
            let file_size = file_metadata.len();
            
            
            // Copy the file (offset no longer needed as aggregator tracks it)
            let bytes_copied = copy_single_file_with_offset(&src_path, &dest_path, volume_info, ctx, verify_checksum, file_size, progress_callback, 0).await?;
            cumulative_bytes += bytes_copied;
            total_size += bytes_copied;
            
            // Signal completion of this file, passing its total size
            if let Some(callback) = progress_callback {
                callback(bytes_copied, u64::MAX); // Send file size and MAX signal
            }
        }
        
        return Ok(total_size);
    }

    let file_size = metadata.len();
    // Use the copy_single_file helper function
    copy_single_file(source, destination, volume_info, ctx, verify_checksum, file_size, progress_callback).await
}

/// Calculate file checksum for integrity verification
async fn calculate_file_checksum(path: &Path) -> Result<String> {
    crate::domain::content_identity::ContentHashGenerator::generate_content_hash(path)
        .await
        .map_err(|e| anyhow::anyhow!("Failed to generate content hash: {}", e))
}

/// Stream file data in chunks to the remote device
async fn stream_file_data<'a>(
    file_path: &Path,
    transfer_id: uuid::Uuid,
    file_transfer_protocol: &crate::services::networking::protocols::FileTransferProtocolHandler,
    total_size: u64,
    destination_device_id: uuid::Uuid,
    ctx: &JobContext<'a>,
    progress_callback: Option<&ProgressCallback<'a>>,
) -> Result<()> {
    use tokio::io::AsyncReadExt;
    use blake3::Hasher;

    // Get networking service for sending chunks
    let networking = ctx.networking_service()
        .ok_or_else(|| anyhow::anyhow!("Networking service not available"))?;

    let mut file = tokio::fs::File::open(file_path).await?;

    let chunk_size = 64 * 1024; // 64KB chunks
    let total_chunks = (total_size + chunk_size - 1) / chunk_size;
    let mut buffer = vec![0u8; chunk_size as usize];
    let mut chunk_index = 0u32;
    let mut bytes_transferred = 0u64;

    ctx.log(format!("Starting to stream {} chunks to device {}", total_chunks, destination_device_id));

    loop {
        ctx.check_interrupt().await?;

        let bytes_read = file.read(&mut buffer).await?;
        if bytes_read == 0 {
            break; // End of file
        }

        // Calculate chunk checksum
        let chunk_data = &buffer[..bytes_read];
        let chunk_checksum = blake3::hash(chunk_data);

        // Update progress
        bytes_transferred += bytes_read as u64;
        if let Some(callback) = progress_callback {
            callback(bytes_transferred, total_size);
        }

        // Get session keys for encryption
        let session_keys = file_transfer_protocol.get_session_keys_for_device(destination_device_id).await?;

        // Encrypt chunk using file transfer protocol
        let (encrypted_data, nonce) = file_transfer_protocol.encrypt_chunk(
            &session_keys.send_key,
            &transfer_id,
            chunk_index,
            chunk_data,
        )?;

        // Create encrypted file chunk message
        let chunk_message = crate::services::networking::protocols::file_transfer::FileTransferMessage::FileChunk {
            transfer_id,
            chunk_index,
            data: encrypted_data,
            nonce,
            chunk_checksum: *chunk_checksum.as_bytes(), // Checksum of original unencrypted data
        };

        // Serialize and send chunk over network
        let chunk_data = rmp_serde::to_vec(&chunk_message)?;

        let networking_guard = &*networking;
        networking_guard.send_message(
            destination_device_id,
            "file_transfer",
            chunk_data,
        ).await?;

        // Record chunk in local protocol handler for tracking
        file_transfer_protocol.record_chunk_received(
            &transfer_id,
            chunk_index,
            bytes_read as u64,
        )?;

        chunk_index += 1;

        ctx.log(format!("Sent chunk {}/{} ({} bytes)", chunk_index, total_chunks, bytes_read));

        // Small delay to prevent overwhelming the network
        tokio::time::sleep(std::time::Duration::from_millis(10)).await;
    }

    // Send transfer completion message
    let final_checksum = calculate_file_checksum(file_path).await?;
    let completion_message = crate::services::networking::protocols::file_transfer::FileTransferMessage::TransferComplete {
        transfer_id,
        final_checksum,
        total_bytes: bytes_transferred,
    };

    let completion_data = rmp_serde::to_vec(&completion_message)?;

    let networking_guard = &*networking;
    networking_guard.send_message(
        destination_device_id,
        "file_transfer",
        completion_data,
    ).await?;

    // Mark transfer as completed locally
    file_transfer_protocol.update_session_state(
        &transfer_id,
        crate::services::networking::protocols::file_transfer::TransferState::Completed,
    )?;

    ctx.log(format!("File streaming completed: {} chunks, {} bytes sent to device {}",
        chunk_index, bytes_transferred, destination_device_id));
    Ok(())
}```

## src/operations/files/copy/action.rs

```rust
//! File copy action handler

use super::{
	input::FileCopyInput,
	job::{CopyOptions, FileCopyJob},
	output::FileCopyActionOutput,
};
use crate::{
	context::CoreContext,
	infrastructure::{
		actions::{
			builder::{ActionBuildError, ActionBuilder},
			error::{ActionError, ActionResult},
			handler::ActionHandler,
			output::ActionOutput,
			Action,
		},
		cli::adapters::FileCopyCliArgs,
	},
	register_action_handler,
	shared::types::{SdPath, SdPathBatch},
};
use async_trait::async_trait;
use clap::Parser;
use serde::{Deserialize, Serialize};
use std::{path::PathBuf, sync::Arc};
use uuid::Uuid;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct FileCopyAction {
	pub sources: Vec<PathBuf>,
	pub destination: PathBuf,
	pub options: CopyOptions,
}

/// Builder for creating FileCopyAction instances with fluent API
#[derive(Debug, Clone)]
pub struct FileCopyActionBuilder {
	input: FileCopyInput,
	errors: Vec<String>,
}

impl FileCopyActionBuilder {
	/// Create a new builder
	pub fn new() -> Self {
		Self {
			input: FileCopyInput::default(),
			errors: Vec::new(),
		}
	}

	/// Create builder from core input type (primary interface)
	pub fn from_input(input: FileCopyInput) -> Self {
		Self {
			input,
			errors: Vec::new(),
		}
	}

	/// Add multiple source files
	pub fn sources<I, P>(mut self, sources: I) -> Self
	where
		I: IntoIterator<Item = P>,
		P: Into<PathBuf>,
	{
		self.input
			.sources
			.extend(sources.into_iter().map(|p| p.into()));
		self
	}

	/// Add a single source file
	pub fn source<P: Into<PathBuf>>(mut self, source: P) -> Self {
		self.input.sources.push(source.into());
		self
	}

	/// Set the destination path
	pub fn destination<P: Into<PathBuf>>(mut self, dest: P) -> Self {
		self.input.destination = dest.into();
		self
	}

	/// Set whether to overwrite existing files
	pub fn overwrite(mut self, overwrite: bool) -> Self {
		self.input.overwrite = overwrite;
		self
	}

	/// Set whether to verify checksums during copy
	pub fn verify_checksum(mut self, verify: bool) -> Self {
		self.input.verify_checksum = verify;
		self
	}

	/// Set whether to preserve file timestamps
	pub fn preserve_timestamps(mut self, preserve: bool) -> Self {
		self.input.preserve_timestamps = preserve;
		self
	}

	/// Enable move mode (delete source after copy)
	pub fn move_files(mut self, enable: bool) -> Self {
		self.input.move_files = enable;
		self
	}

	/// Validate sources exist and are readable
	fn validate_sources(&mut self) {
		// First do basic validation from input
		if let Err(basic_errors) = self.input.validate() {
			self.errors.extend(basic_errors);
			return;
		}

		// Then do filesystem validation
		for source in &self.input.sources {
			if !source.exists() {
				self.errors
					.push(format!("Source file does not exist: {}", source.display()));
			} else if source.is_dir() && !source.read_dir().is_ok() {
				self.errors
					.push(format!("Cannot read directory: {}", source.display()));
			} else if source.is_file() && std::fs::metadata(source).is_err() {
				self.errors
					.push(format!("Cannot access file: {}", source.display()));
			}
		}
	}

	/// Validate destination is valid
	fn validate_destination(&mut self) {
		if let Some(parent) = self.input.destination.parent() {
			if !parent.exists() {
				self.errors.push(format!(
					"Destination directory does not exist: {}",
					parent.display()
				));
			}
		}
	}
}

impl ActionBuilder for FileCopyActionBuilder {
	type Action = FileCopyAction;
	type Error = ActionBuildError;

	fn validate(&self) -> Result<(), Self::Error> {
		let mut builder = self.clone();
		builder.validate_sources();
		builder.validate_destination();

		if !builder.errors.is_empty() {
			return Err(ActionBuildError::validations(builder.errors));
		}

		Ok(())
	}

	fn build(self) -> Result<Self::Action, Self::Error> {
		self.validate()?;

		let options = self.input.to_copy_options();
		Ok(FileCopyAction {
			sources: self.input.sources,
			destination: self.input.destination,
			options,
		})
	}
}

impl FileCopyActionBuilder {
	/// Create builder from CLI args (interface-specific convenience method)
	pub fn from_cli_args(args: FileCopyCliArgs) -> Self {
		Self::from_input(args.into())
	}
}

/// Convenience methods on FileCopyAction
impl FileCopyAction {
	/// Create a new builder
	pub fn builder() -> FileCopyActionBuilder {
		FileCopyActionBuilder::new()
	}

	/// Quick builder for copying a single file
	pub fn copy_file<S: Into<PathBuf>, D: Into<PathBuf>>(
		source: S,
		dest: D,
	) -> FileCopyActionBuilder {
		FileCopyActionBuilder::new()
			.source(source)
			.destination(dest)
	}

	/// Quick builder for copying multiple files
	pub fn copy_files<I, P, D>(sources: I, dest: D) -> FileCopyActionBuilder
	where
		I: IntoIterator<Item = P>,
		P: Into<PathBuf>,
		D: Into<PathBuf>,
	{
		FileCopyActionBuilder::new()
			.sources(sources)
			.destination(dest)
	}
}

pub struct FileCopyHandler;

impl FileCopyHandler {
	pub fn new() -> Self {
		Self
	}
}

#[async_trait]
impl ActionHandler for FileCopyHandler {
	async fn validate(&self, _context: Arc<CoreContext>, action: &Action) -> ActionResult<()> {
		if let Action::FileCopy {
			library_id: _,
			action,
		} = action
		{
			if action.sources.is_empty() {
				return Err(ActionError::Validation {
					field: "sources".to_string(),
					message: "At least one source file must be specified".to_string(),
				});
			}

			// Additional validation could include:
			// - Check if source files exist
			// - Check permissions
			// - Check if destination is valid
			// - Check if it would be a cross-device operation

			Ok(())
		} else {
			Err(ActionError::InvalidActionType)
		}
	}

	async fn execute(
		&self,
		context: Arc<CoreContext>,
		action: Action,
	) -> ActionResult<ActionOutput> {
		if let Action::FileCopy { library_id, action } = action {
			let library_manager = &context.library_manager;

			// Get the specific library
			let library = library_manager
				.get_library(library_id)
				.await
				.ok_or(ActionError::LibraryNotFound(library_id))?;

			// Create job instance directly (no JSON roundtrip)
			let sources_count = action.sources.len();
			let destination_display = action.destination.display().to_string();
			let sources = action
				.sources
				.into_iter()
				.map(|path| SdPath::local(path))
				.collect();

			let job =
				FileCopyJob::new(SdPathBatch::new(sources), SdPath::local(action.destination))
					.with_options(action.options);

			// Dispatch job directly
			let job_handle = library
				.jobs()
				.dispatch(job)
				.await
				.map_err(ActionError::Job)?;

			// Return domain-specific output
			let output = FileCopyActionOutput::new(
				job_handle.id().into(),
				sources_count,
				destination_display,
			);
			Ok(ActionOutput::from_trait(output))
		} else {
			Err(ActionError::InvalidActionType)
		}
	}

	fn can_handle(&self, action: &Action) -> bool {
		matches!(action, Action::FileCopy { .. })
	}

	fn supported_actions() -> &'static [&'static str] {
		&["file.copy"]
	}
}

// Register this handler
register_action_handler!(FileCopyHandler, "file.copy");

#[cfg(test)]
mod tests {
	use super::*;
	use crate::{
		infrastructure::cli::adapters::{copy::CopyMethodCli, FileCopyCliArgs},
		operations::files::input::CopyMethod,
	};
	use std::path::PathBuf;

	#[test]
	fn test_builder_fluent_api() {
		let action = FileCopyAction::builder()
			.sources(["/src/file1.txt", "/src/file2.txt"])
			.destination("/dest/")
			.overwrite(true)
			.verify_checksum(true)
			.preserve_timestamps(false)
			.move_files(true)
			.build();

		// Note: This will fail validation because files don't exist, but it tests the API
		assert!(action.is_err());
		match action.unwrap_err() {
			ActionBuildError::Validation(errors) => {
				assert!(!errors.is_empty());
				assert!(errors.iter().any(|e| e.contains("does not exist")));
			}
			_ => panic!("Expected validation error"),
		}
	}

	#[test]
	fn test_builder_validation_empty_sources() {
		let result = FileCopyAction::builder().destination("/dest/").build();

		assert!(result.is_err());
		match result.unwrap_err() {
			ActionBuildError::Validation(errors) => {
				assert!(errors.iter().any(|e| e.contains("At least one source")));
			}
			_ => panic!("Expected validation error"),
		}
	}

	#[test]
	fn test_builder_from_input() {
		let input = FileCopyInput::new(vec!["/file1.txt".into(), "/file2.txt".into()], "/dest/")
			.with_overwrite(true)
			.with_verification(true)
			.with_move(false);

		let builder = FileCopyActionBuilder::from_input(input.clone());

		// Test that builder has correct values from input
		assert_eq!(
			builder.input.sources,
			vec![PathBuf::from("/file1.txt"), PathBuf::from("/file2.txt")]
		);
		assert_eq!(builder.input.destination, PathBuf::from("/dest/"));
		assert!(builder.input.overwrite);
		assert!(builder.input.verify_checksum);
		assert!(!builder.input.move_files);
	}

	#[test]
	fn test_cli_integration() {
		let args = FileCopyCliArgs {
			sources: vec!["/src/file.txt".into()],
			destination: "/dest/".into(),
			method: CopyMethodCli::Auto,
			overwrite: true,
			verify: false,
			preserve_timestamps: true,
			move_files: false,
		};

		let builder = FileCopyActionBuilder::from_cli_args(args);

		// Test that builder has correct values set from CLI args
		assert_eq!(builder.input.sources, vec![PathBuf::from("/src/file.txt")]);
		assert_eq!(builder.input.destination, PathBuf::from("/dest/"));
		assert!(builder.input.overwrite);
		assert!(!builder.input.verify_checksum);
		assert!(builder.input.preserve_timestamps);
		assert!(!builder.input.move_files);
	}

	#[test]
	fn test_convenience_methods() {
		// Test single file copy
		let builder = FileCopyAction::copy_file("/src/file.txt", "/dest/file.txt");
		assert_eq!(builder.input.sources, vec![PathBuf::from("/src/file.txt")]);
		assert_eq!(builder.input.destination, PathBuf::from("/dest/file.txt"));

		// Test multiple files copy
		let sources = vec!["/src/file1.txt", "/src/file2.txt"];
		let builder = FileCopyAction::copy_files(sources.clone(), "/dest/");
		assert_eq!(
			builder.input.sources,
			sources.into_iter().map(PathBuf::from).collect::<Vec<_>>()
		);
		assert_eq!(builder.input.destination, PathBuf::from("/dest/"));
	}

	#[test]
	fn test_builder_chaining() {
		let builder = FileCopyAction::builder()
			.source("/file1.txt")
			.source("/file2.txt")
			.source("/file3.txt")
			.destination("/dest/")
			.overwrite(true)
			.verify_checksum(false)
			.preserve_timestamps(true)
			.move_files(false);

		assert_eq!(builder.input.sources.len(), 3);
		assert!(builder.input.overwrite);
		assert!(!builder.input.verify_checksum);
		assert!(builder.input.preserve_timestamps);
		assert!(!builder.input.move_files);
	}

	#[test]
	fn test_input_abstraction_flow() {
		// Test the full flow: CLI args -> Input -> Builder -> Action
		let cli_args = FileCopyCliArgs {
			sources: vec!["/source.txt".into()],
			destination: "/dest.txt".into(),
			method: CopyMethodCli::Auto,
			overwrite: false,
			verify: true,
			preserve_timestamps: false,
			move_files: true,
		};

		// Convert CLI args to input
		let input: FileCopyInput = cli_args.into();
		assert_eq!(input.sources, vec![PathBuf::from("/source.txt")]);
		assert!(input.verify_checksum);
		assert!(!input.preserve_timestamps);
		assert!(input.move_files);

		// Create builder from input
		let builder = FileCopyActionBuilder::from_input(input);

		// Verify the copy options are correct
		let copy_options = builder.input.to_copy_options();
		assert!(!copy_options.overwrite);
		assert!(copy_options.verify_checksum);
		assert!(!copy_options.preserve_timestamps);
		assert!(copy_options.delete_after_copy);
	}
}
```

## src/operations/files/copy/job.rs

```rust
//! Simplified FileCopyJob using the Strategy Pattern

use super::{input::CopyMethod, routing::CopyStrategyRouter, database::CopyDatabaseQuery};
use crate::{
    infrastructure::jobs::prelude::*,
    infrastructure::jobs::generic_progress::{GenericProgress, ToGenericProgress},
    shared::types::{SdPath, SdPathBatch},
};
use serde::{Deserialize, Serialize};
use std::{
    collections::HashMap,
    path::PathBuf,
    sync::{Arc, Mutex},
    time::{Duration, Instant},
};
use uuid::Uuid;

/// Move operation modes for UI context
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum MoveMode {
    /// Standard move operation
    Move,
    /// Rename a single file/directory
    Rename,
    /// Cut and paste operation (same as move but different UX context)
    Cut,
}

/// Options for file copy operations
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CopyOptions {
    pub overwrite: bool,
    pub verify_checksum: bool,
    pub preserve_timestamps: bool,
    pub delete_after_copy: bool,
    pub move_mode: Option<MoveMode>,
    pub copy_method: CopyMethod,
}

impl Default for CopyOptions {
    fn default() -> Self {
        Self {
            overwrite: false,
            verify_checksum: false,
            preserve_timestamps: true,
            delete_after_copy: false,
            move_mode: None,
            copy_method: CopyMethod::Auto,
        }
    }
}

/// File copy job using the Strategy Pattern
#[derive(Debug, Serialize, Deserialize, Job)]
pub struct FileCopyJob {
    pub sources: SdPathBatch,
    pub destination: SdPath,
    #[serde(default)]
    pub options: CopyOptions,

    // Internal state for resumption
    #[serde(default)]
    pub completed_indices: Vec<usize>,
    #[serde(skip, default = "Instant::now")]
    started_at: Instant,
}

impl Job for FileCopyJob {
    const NAME: &'static str = "file_copy";
    const RESUMABLE: bool = true;
    const DESCRIPTION: Option<&'static str> = Some("Copy or move files to a destination");
}

#[async_trait::async_trait]
impl JobHandler for FileCopyJob {
    type Output = FileCopyOutput;

    async fn run(&mut self, ctx: JobContext<'_>) -> JobResult<Self::Output> {
        ctx.log(format!(
            "Starting copy operation on {} files",
            self.sources.paths.len()
        ));

        // Phase 1: Initializing
        let progress = CopyProgress {
            phase: CopyPhase::Initializing,
            current_file: String::new(),
            files_copied: 0,
            total_files: 0,
            bytes_copied: 0,
            total_bytes: 0,
            current_operation: "Initializing copy operation".to_string(),
            estimated_remaining: None,
            preparation_complete: false,
            error_count: 0,
        };
        ctx.progress(Progress::generic(progress.to_generic_progress()));

        // Group by device for efficient processing
        let by_device: HashMap<Uuid, Vec<SdPath>> = self
            .sources
            .by_device()
            .into_iter()
            .map(|(device_id, paths)| (device_id, paths.into_iter().cloned().collect()))
            .collect();

        let mut copied_count = 0;
        let mut total_bytes = 0u64;
        let mut failed_copies = Vec::new();
        let is_move = self.options.delete_after_copy;
        let volume_manager = ctx.volume_manager();

        // Phase 2: Database Query - Try to get instant estimates
        let progress = CopyProgress {
            phase: CopyPhase::DatabaseQuery,
            current_file: String::new(),
            files_copied: 0,
            total_files: 0,
            bytes_copied: 0,
            total_bytes: 0,
            current_operation: "Querying database for file information...".to_string(),
            estimated_remaining: None,
            preparation_complete: false,
            error_count: 0,
        };
        ctx.progress(Progress::generic(progress.to_generic_progress()));

        // Try to get estimates from database
        let db_estimates = {
            let db_query = CopyDatabaseQuery::new(ctx.library_db().clone());
            match db_query.get_estimates_for_paths(&self.sources.paths).await {
                Ok(estimates) => {
                    ctx.log(format!(
                        "Database estimates: {} files, {} bytes ({:.0}% coverage)",
                        estimates.file_count,
                        estimates.total_size,
                        estimates.confidence() * 100.0
                    ));
                    Some(estimates)
                }
                Err(e) => {
                    ctx.log(format!("Database query failed, will calculate from filesystem: {}", e));
                    None
                }
            }
        };

        // Use database estimates if available, otherwise use source path count for initial display
        let (estimated_files, estimated_bytes) = if let Some(ref estimates) = db_estimates {
            if estimates.is_complete() {
                // We have complete information from database
                (estimates.file_count as usize, estimates.total_size)
            } else {
                // Partial information - still useful for initial display
                (self.sources.paths.len().max(estimates.file_count as usize), estimates.total_size)
            }
        } else {
            (self.sources.paths.len(), 0)
        };

        // Phase 3: Preparation - Calculate actual total size
        let progress = CopyProgress {
            phase: CopyPhase::Preparation,
            current_file: String::new(),
            files_copied: 0,
            total_files: estimated_files,
            bytes_copied: 0,
            total_bytes: estimated_bytes,
            current_operation: if db_estimates.is_some() {
                "Verifying file sizes...".to_string()
            } else {
                "Calculating total size...".to_string()
            },
            estimated_remaining: None,
            preparation_complete: false,
            error_count: 0,
        };
        ctx.progress(Progress::generic(progress.to_generic_progress()));

        // Calculate actual file count and total size
        let actual_file_count = self.count_total_files().await?;
        let estimated_total_bytes = self.calculate_total_size(&ctx).await?;

        ctx.log(format!(
            "Preparing to copy {} files ({}) from {} source paths",
            actual_file_count,
            format_bytes(estimated_total_bytes),
            self.sources.paths.len()
        ));

        // Update progress with calculated size and file count
        let progress = CopyProgress {
            phase: CopyPhase::Preparation,
            current_file: String::new(),
            files_copied: 0,
            total_files: actual_file_count,
            bytes_copied: 0,
            total_bytes: estimated_total_bytes,
            current_operation: "Preparation complete".to_string(),
            estimated_remaining: None,
            preparation_complete: true,
            error_count: 0,
        };
        ctx.progress(Progress::generic(progress.to_generic_progress()));

        // Create progress aggregator for tracking overall progress
        let mut progress_aggregator = ProgressAggregator::new(&ctx, actual_file_count, estimated_total_bytes);

        // Process each source using the appropriate strategy
        for (index, source) in self.sources.paths.iter().enumerate() {
            ctx.check_interrupt().await?;

            // Skip files that have already been completed (resume logic)
            if self.completed_indices.contains(&index) {
                ctx.log(format!("Skipping already completed file: {}", source.display()));
                
                // Update progress aggregator to account for already completed files
                let files_in_source = if let Some(local_path) = source.as_local_path() {
                    let file_size = self.get_path_size(local_path).await.unwrap_or(0);
                    let file_count = self.count_files_in_path(local_path).await.unwrap_or(1);
                    progress_aggregator.skip_completed_file(file_size, file_count);
                    total_bytes += file_size;
                    file_count
                } else {
                    1
                };
                
                copied_count += files_in_source; // Count actual files as copied for progress tracking
                continue;
            }

            let final_destination = if self.sources.paths.len() > 1 {
                // Multiple sources: destination must be a directory
                self.destination.join(source.path.file_name().unwrap_or_default())
            } else {
                // Single source: check if destination is a directory
                if let Some(dest_path) = self.destination.as_local_path() {
                    if dest_path.is_dir() {
                        // Destination is a directory, join with source filename
                        self.destination.join(source.path.file_name().unwrap_or_default())
                    } else {
                        // Destination is a file path, use as-is
                        self.destination.clone()
                    }
                } else {
                    // Non-local destination, assume file copy
                    self.destination.clone()
                }
            };

            // Count files in this source path for accurate progress tracking
            let files_in_source = if let Some(local_path) = source.as_local_path() {
                self.count_files_in_path(local_path).await.unwrap_or(1)
            } else {
                1
            };

            // Update aggregator with current file info
            let operation_description = CopyStrategyRouter::describe_strategy(
                source,
                &final_destination,
                is_move,
                &self.options.copy_method,
                volume_manager.as_deref(),
            ).await;
            
            progress_aggregator.start_file(source.display(), operation_description);
            progress_aggregator.set_error_count(failed_copies.len());

            // Update progress - show files already completed
            let files_completed_count = *progress_aggregator.files_completed.lock().unwrap();
            let bytes_completed_snapshot = *progress_aggregator.bytes_completed_before_current.lock().unwrap();
            let progress = CopyProgress {
                phase: CopyPhase::Copying,
                current_file: source.display(),
                files_copied: files_completed_count,
                total_files: actual_file_count,
                bytes_copied: bytes_completed_snapshot,
                total_bytes: estimated_total_bytes,
                current_operation: progress_aggregator.current_operation.clone(),
                estimated_remaining: None,
                preparation_complete: true,
                error_count: failed_copies.len(),
            };
            ctx.progress(Progress::generic(progress.to_generic_progress()));

            // 1. Select the strategy
            let strategy = CopyStrategyRouter::select_strategy(
                source,
                &final_destination,
                is_move,
                &self.options.copy_method,
                volume_manager.as_deref(),
            ).await;

            // 2. Execute the strategy with progress callback
            match strategy.execute(&ctx, source, &final_destination, self.options.verify_checksum, Some(&progress_aggregator.create_callback())).await {
                Ok(bytes) => {
                    // Mark source as complete (bytes/files already updated by callback)
                    progress_aggregator.complete_source();
                    
                    // Update totals
                    copied_count += files_in_source;
                    total_bytes += bytes;

                    // Track successful completion for resume
                    self.completed_indices.push(index);

                    // If this is a move operation and the strategy didn't handle deletion,
                    // we need to delete the source after successful copy
                    if is_move && source.device_id == final_destination.device_id {
                        // For same-device moves, LocalMoveStrategy handles deletion atomically
                        // For cross-volume moves, LocalStreamCopyStrategy needs manual deletion
                        if let Some(vm) = volume_manager.as_deref() {
                            if let (Some(source_path), Some(dest_path)) = 
                                (source.as_local_path(), final_destination.as_local_path()) {
                                if !vm.same_volume(source_path, dest_path).await {
                                    // Cross-volume move - delete source
                                    if let Err(e) = self.delete_source_file(source_path).await {
                                        failed_copies.push(CopyError {
                                            source: source.path.clone(),
                                            destination: final_destination.path.clone(),
                                            error: format!("Copy succeeded but failed to delete source: {}", e),
                                        });
                                        ctx.add_non_critical_error(format!(
                                            "Failed to delete source after move {}: {}",
                                            source.display(),
                                            e
                                        ));
                                    }
                                }
                            }
                        }
                    }
                }
                Err(e) => {
                    failed_copies.push(CopyError {
                        source: source.path.clone(),
                        destination: final_destination.path.clone(),
                        error: e.to_string(),
                    });
                    ctx.add_non_critical_error(format!(
                        "Failed to {} {}: {}",
                        if is_move { "move" } else { "copy" },
                        source.display(),
                        e
                    ));
                }
            }

            // Checkpoint every 20 files to save completed_indices
            if copied_count % 20 == 0 {
                ctx.checkpoint().await?;
            }
        }

        // Phase 4: Complete
        let progress = CopyProgress {
            phase: CopyPhase::Complete,
            current_file: String::new(),
            files_copied: copied_count,
            total_files: actual_file_count,
            bytes_copied: total_bytes,
            total_bytes: estimated_total_bytes,
            current_operation: "Copy operation complete".to_string(),
            estimated_remaining: None,
            preparation_complete: true,
            error_count: failed_copies.len(),
        };
        ctx.progress(Progress::generic(progress.to_generic_progress()));

        ctx.log(format!(
            "Copy operation completed: {} copied, {} failed",
            copied_count,
            failed_copies.len()
        ));

        Ok(FileCopyOutput {
            copied_count,
            failed_count: failed_copies.len(),
            total_bytes,
            duration: self.started_at.elapsed(),
            failed_copies,
            is_move_operation: self.options.delete_after_copy,
        })
    }
}

/// Copy operation phases
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub enum CopyPhase {
    Initializing,
    DatabaseQuery,
    Preparation,
    Copying,
    Complete,
}

impl CopyPhase {
    fn as_str(&self) -> &'static str {
        match self {
            CopyPhase::Initializing => "Initializing",
            CopyPhase::DatabaseQuery => "Database Query",
            CopyPhase::Preparation => "Preparation",
            CopyPhase::Copying => "Copying",
            CopyPhase::Complete => "Complete",
        }
    }
}

/// Copy progress information
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CopyProgress {
    pub phase: CopyPhase,
    pub current_file: String,
    pub files_copied: usize,
    pub total_files: usize,
    pub bytes_copied: u64,
    pub total_bytes: u64,
    pub current_operation: String,
    pub estimated_remaining: Option<Duration>,
    pub preparation_complete: bool,
    pub error_count: usize,
}

impl JobProgress for CopyProgress {}

/// Progress aggregator that tracks overall copy job progress
struct ProgressAggregator<'a> {
    ctx: &'a JobContext<'a>,
    current_file_index: usize,
    total_files: usize,
    bytes_completed_before_current: Arc<Mutex<u64>>,
    total_bytes: u64,
    current_file_path: String,
    current_operation: String,
    error_count: usize,
    files_completed: Arc<Mutex<usize>>,
}

impl<'a> ProgressAggregator<'a> {
    fn new(
        ctx: &'a JobContext<'a>,
        total_files: usize,
        total_bytes: u64,
    ) -> Self {
        Self {
            ctx,
            current_file_index: 0,
            total_files,
            bytes_completed_before_current: Arc::new(Mutex::new(0)),
            total_bytes,
            current_file_path: String::new(),
            current_operation: String::new(),
            error_count: 0,
            files_completed: Arc::new(Mutex::new(0)),
        }
    }

    /// Start processing a new file
    fn start_file(&mut self, file_path: String, current_operation: String) {
        self.current_file_path = file_path;
        self.current_operation = current_operation;
    }

    /// Complete the current source item and update index
    fn complete_source(&mut self) {
        self.current_file_index += 1;
        // Note: bytes and file counts are already updated by the callback
    }
    
    /// Account for files that were already completed (for resume)
    fn skip_completed_file(&mut self, bytes_copied: u64, files_in_operation: usize) {
        *self.bytes_completed_before_current.lock().unwrap() += bytes_copied;
        *self.files_completed.lock().unwrap() += files_in_operation;
    }

    /// Create a progress callback for strategy implementations
    fn create_callback(&self) -> Box<dyn Fn(u64, u64) + Send + Sync + 'a> {
        let ctx = self.ctx;
        let files_completed = self.files_completed.clone();
        let total_files = self.total_files;
        let bytes_before = self.bytes_completed_before_current.clone();
        let total_bytes = self.total_bytes;
        let current_file = self.current_file_path.clone();
        let current_operation = self.current_operation.clone();
        let error_count = self.error_count;

        Box::new(move |bytes_value: u64, signal_value: u64| {
            // NEW SIGNAL: A signal_value of u64::MAX means a file has finished.
            // The bytes_value will be the size of the completed file.
            if signal_value == u64::MAX {
                let mut files = files_completed.lock().unwrap();
                *files += 1;
                let mut bytes = bytes_before.lock().unwrap();
                *bytes += bytes_value; // Add the completed file's size to the total
                ctx.log(format!("File completed. Total files: {}/{}, Total bytes: {}", 
                    *files, total_files, *bytes));
                return;
            }
            
            // Normal byte-level progress update
            let bytes_before_snapshot = *bytes_before.lock().unwrap();
            let total_bytes_copied = bytes_before_snapshot + bytes_value;
            let files_completed_count = *files_completed.lock().unwrap();
            
            let copy_progress = CopyProgress {
                phase: CopyPhase::Copying,
                current_file: current_file.clone(),
                files_copied: files_completed_count,
                total_files,
                bytes_copied: total_bytes_copied,
                total_bytes,
                current_operation: current_operation.clone(),
                estimated_remaining: None,
                preparation_complete: true,
                error_count,
            };
            
            // Log progress details every 100MB
            if total_bytes_copied % (100 * 1024 * 1024) < bytes_value {
                ctx.log(format!("Progress update: {} / {} bytes ({:.1}%)", 
                    total_bytes_copied, total_bytes, 
                    (total_bytes_copied as f64 / total_bytes as f64) * 100.0));
            }
            
            ctx.progress(Progress::generic(copy_progress.to_generic_progress()));
        })
    }

    fn set_error_count(&mut self, count: usize) {
        self.error_count = count;
    }
}

impl ToGenericProgress for CopyProgress {
    fn to_generic_progress(&self) -> GenericProgress {
        // Calculate percentage based on bytes if available, otherwise use file count
        let percentage = if self.total_bytes > 0 {
            (self.bytes_copied as f32 / self.total_bytes as f32).clamp(0.0, 1.0)
        } else if self.total_files > 0 {
            (self.files_copied as f32 / self.total_files as f32).clamp(0.0, 1.0)
        } else {
            0.0
        };
        

        // Create appropriate message based on phase
        let message = match self.phase {
            CopyPhase::Initializing => "Initializing copy operation...".to_string(),
            CopyPhase::DatabaseQuery => "Querying database for file information...".to_string(),
            CopyPhase::Preparation => {
                if self.total_files > 0 {
                    format!("Preparing to copy {} files...", self.total_files)
                } else {
                    "Preparing copy operation...".to_string()
                }
            }
            CopyPhase::Copying => {
                if !self.current_file.is_empty() {
                    format!("Copying: {}", self.current_file)
                } else {
                    self.current_operation.clone()
                }
            }
            CopyPhase::Complete => format!("Copy complete: {} files", self.files_copied),
        };

        // Build generic progress
        let mut progress = GenericProgress::new(percentage, self.phase.as_str(), message);
        
        // Only set completion if we're not using byte-based progress
        // (to avoid overriding the percentage)
        if self.total_bytes == 0 {
            progress = progress.with_completion(self.files_copied as u64, self.total_files as u64);
        } else {
            // For byte-based progress, just set the completion counts without recalculating percentage
            progress.completion.completed = self.files_copied as u64;
            progress.completion.total = self.total_files as u64;
        }
        
        progress = progress.with_bytes(self.bytes_copied, self.total_bytes);

        // Add performance metrics if we're actively copying
        if self.phase == CopyPhase::Copying && self.bytes_copied > 0 {
            // Calculate rate if we have timing information
            // For now, just pass through the estimated remaining time
            progress = progress.with_performance(
                0.0, // Rate will be calculated by job system
                self.estimated_remaining,
                None, // Elapsed time tracked by job system
            );
        }

        // Add error count if any
        if self.error_count > 0 {
            progress = progress.with_errors(self.error_count as u64, 0);
        }

        // Add current file path if available
        if !self.current_file.is_empty() && self.phase == CopyPhase::Copying {
            // Convert current file string to SdPath if possible
            // For now, we'll skip this as we'd need device_id context
        }

        progress
    }
}

/// Error information for failed copies
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CopyError {
    pub source: PathBuf,
    pub destination: PathBuf,
    pub error: String,
}

impl FileCopyJob {
    /// Create a new file copy job with sources and destination
    pub fn new(sources: SdPathBatch, destination: SdPath) -> Self {
        Self {
            sources,
            destination,
            options: Default::default(),
            completed_indices: Vec::new(),
            started_at: Instant::now(),
        }
    }

    /// Create an empty job (used by derive macro)
    pub fn empty() -> Self {
        Self {
            sources: SdPathBatch::new(Vec::new()),
            destination: SdPath::new(uuid::Uuid::new_v4(), PathBuf::new()),
            options: Default::default(),
            completed_indices: Vec::new(),
            started_at: Instant::now(),
        }
    }

    /// Create from individual paths
    pub fn from_paths(sources: Vec<SdPath>, destination: SdPath) -> Self {
        Self::new(SdPathBatch::new(sources), destination)
    }

    /// Set copy options
    pub fn with_options(mut self, options: CopyOptions) -> Self {
        self.options = options;
        self
    }

    /// Create a move job using the copy job with delete_after_copy
    pub fn new_move(sources: SdPathBatch, destination: SdPath, move_mode: MoveMode) -> Self {
        let mut options = CopyOptions::default();
        options.delete_after_copy = true;
        options.move_mode = Some(move_mode);
        Self {
            sources,
            destination,
            options,
            completed_indices: Vec::new(),
            started_at: Instant::now(),
        }
    }

    /// Create a rename operation
    pub fn new_rename(source: SdPath, new_name: String) -> Self {
        let destination = SdPath::new(
            source.device_id,
            source.path.with_file_name(new_name)
        );

        Self::new_move(
            SdPathBatch::new(vec![source]),
            destination,
            MoveMode::Rename
        )
    }

    /// Calculate total size for progress reporting
    async fn calculate_total_size(&self, ctx: &JobContext<'_>) -> JobResult<u64> {
        let mut total = 0u64;

        for source in &self.sources.paths {
            if let Some(local_path) = source.as_local_path() {
                total += self.get_path_size(local_path).await.unwrap_or(0);
            }
        }

        Ok(total)
    }

    /// Count total number of files to be copied (including files within directories)
    async fn count_total_files(&self) -> JobResult<usize> {
        let mut total_count = 0;

        for source in &self.sources.paths {
            if let Some(local_path) = source.as_local_path() {
                total_count += self.count_files_in_path(local_path).await.unwrap_or(0);
            }
        }

        Ok(total_count)
    }

    /// Count files in a path (recursive for directories)
    async fn count_files_in_path(&self, path: &std::path::Path) -> Result<usize, std::io::Error> {
        let mut count = 0;
        let mut stack = vec![path.to_path_buf()];

        while let Some(current_path) = stack.pop() {
            let metadata = tokio::fs::metadata(&current_path).await?;

            if metadata.is_file() {
                count += 1;
            } else if metadata.is_dir() {
                let mut dir = tokio::fs::read_dir(&current_path).await?;
                while let Some(entry) = dir.next_entry().await? {
                    stack.push(entry.path());
                }
            }
        }

        Ok(count)
    }

    /// List all files in a directory (recursive)
    async fn list_files_in_directory(&self, path: &std::path::Path) -> JobResult<Vec<PathBuf>> {
        let mut files = Vec::new();
        let mut stack = vec![path.to_path_buf()];

        while let Some(current_path) = stack.pop() {
            let metadata = tokio::fs::metadata(&current_path).await
                .map_err(|e| JobError::execution(format!("Failed to read metadata: {}", e)))?;

            if metadata.is_file() {
                files.push(current_path);
            } else if metadata.is_dir() {
                let mut dir = tokio::fs::read_dir(&current_path).await
                    .map_err(|e| JobError::execution(format!("Failed to read directory: {}", e)))?;
                while let Some(entry) = dir.next_entry().await
                    .map_err(|e| JobError::execution(format!("Failed to read directory entry: {}", e)))? {
                    stack.push(entry.path());
                }
            }
        }

        Ok(files)
    }

    /// Get size of a path (file or directory) using iterative approach
    async fn get_path_size(&self, path: &std::path::Path) -> Result<u64, std::io::Error> {
        let mut total = 0u64;
        let mut stack = vec![path.to_path_buf()];

        while let Some(current_path) = stack.pop() {
            let metadata = tokio::fs::metadata(&current_path).await?;

            if metadata.is_file() {
                total += metadata.len();
            } else if metadata.is_dir() {
                let mut dir = tokio::fs::read_dir(&current_path).await?;
                while let Some(entry) = dir.next_entry().await? {
                    stack.push(entry.path());
                }
            }
        }

        Ok(total)
    }

    /// Delete source file after successful cross-volume move
    async fn delete_source_file(&self, source: &std::path::Path) -> Result<(), std::io::Error> {
        let metadata = tokio::fs::metadata(source).await?;

        if metadata.is_file() {
            tokio::fs::remove_file(source).await
        } else if metadata.is_dir() {
            tokio::fs::remove_dir_all(source).await
        } else {
            Ok(())
        }
    }
}

/// Output from file copy job
#[derive(Debug, Serialize, Deserialize)]
pub struct FileCopyOutput {
    pub copied_count: usize,
    pub failed_count: usize,
    pub total_bytes: u64,
    pub duration: Duration,
    pub failed_copies: Vec<CopyError>,
    pub is_move_operation: bool,
}

impl From<FileCopyOutput> for JobOutput {
    fn from(output: FileCopyOutput) -> Self {
        if output.is_move_operation {
            JobOutput::FileMove {
                moved_count: output.copied_count,
                failed_count: output.failed_count,
                total_bytes: output.total_bytes,
            }
        } else {
            JobOutput::FileCopy {
                copied_count: output.copied_count,
                total_bytes: output.total_bytes,
            }
        }
    }
}

/// Backward compatibility wrapper for move operations
#[derive(Debug, Serialize, Deserialize, Job)]
pub struct MoveJob {
    pub sources: SdPathBatch,
    pub destination: SdPath,
    pub mode: MoveMode,
    pub overwrite: bool,
    pub preserve_timestamps: bool,
}

impl Job for MoveJob {
    const NAME: &'static str = "move_files";
    const RESUMABLE: bool = true;
    const DESCRIPTION: Option<&'static str> = Some("Move or rename files and directories");
}

#[async_trait::async_trait]
impl JobHandler for MoveJob {
    type Output = MoveOutput;

    async fn run(&mut self, ctx: JobContext<'_>) -> JobResult<Self::Output> {
        // Convert to FileCopyJob with move options
        let mut copy_options = CopyOptions::default();
        copy_options.delete_after_copy = true;
        copy_options.move_mode = Some(self.mode.clone());
        copy_options.overwrite = self.overwrite;
        copy_options.preserve_timestamps = self.preserve_timestamps;

        let mut copy_job = FileCopyJob {
            sources: self.sources.clone(),
            destination: self.destination.clone(),
            options: copy_options,
            completed_indices: Vec::new(),
            started_at: Instant::now(),
        };

        // Run the copy job
        let copy_output = copy_job.run(ctx).await?;

        // Convert output to move format
        Ok(MoveOutput {
            moved_count: copy_output.copied_count,
            failed_count: copy_output.failed_count,
            total_bytes: copy_output.total_bytes,
            duration: copy_output.duration,
            failed_moves: copy_output.failed_copies.into_iter().map(|e| MoveError {
                source: e.source,
                destination: e.destination,
                error: e.error,
            }).collect(),
        })
    }
}

impl MoveJob {
    /// Create a new move job
    pub fn new(sources: SdPathBatch, destination: SdPath, mode: MoveMode) -> Self {
        Self {
            sources,
            destination,
            mode,
            overwrite: false,
            preserve_timestamps: true,
        }
    }

    /// Create an empty job (used by derive macro)
    pub fn empty() -> Self {
        Self {
            sources: SdPathBatch::new(Vec::new()),
            destination: SdPath::new(uuid::Uuid::new_v4(), PathBuf::new()),
            mode: MoveMode::Move,
            overwrite: false,
            preserve_timestamps: true,
        }
    }

    /// Create a rename operation
    pub fn rename(source: SdPath, new_name: String) -> Self {
        let destination = SdPath::new(
            source.device_id,
            source.path.with_file_name(new_name)
        );

        Self::new(
            SdPathBatch::new(vec![source]),
            destination,
            MoveMode::Rename
        )
    }
}

/// Error information for failed moves
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MoveError {
    pub source: PathBuf,
    pub destination: PathBuf,
    pub error: String,
}

/// Output from move operations
#[derive(Debug, Serialize, Deserialize)]
pub struct MoveOutput {
    pub moved_count: usize,
    pub failed_count: usize,
    pub total_bytes: u64,
    pub duration: Duration,
    pub failed_moves: Vec<MoveError>,
}

impl From<MoveOutput> for JobOutput {
    fn from(output: MoveOutput) -> Self {
        JobOutput::FileMove {
            moved_count: output.moved_count,
            failed_count: output.failed_count,
            total_bytes: output.total_bytes,
        }
    }
}

// Helper function to format bytes in human-readable format
fn format_bytes(bytes: u64) -> String {
    const UNITS: &[&str] = &["B", "KB", "MB", "GB", "TB"];
    let mut size = bytes as f64;
    let mut unit_idx = 0;
    
    while size >= 1024.0 && unit_idx < UNITS.len() - 1 {
        size /= 1024.0;
        unit_idx += 1;
    }
    
    if unit_idx == 0 {
        format!("{} {}", size as u64, UNITS[unit_idx])
    } else {
        format!("{:.2} {}", size, UNITS[unit_idx])
    }
}```

## src/operations/files/copy/database.rs

```rust
//! Database query support for copy operations
//!
//! Provides instant size and file count estimates by querying
//! Spacedrive's indexed data, enabling immediate progress feedback.

use crate::{
	infrastructure::database::entities::{entry, location},
	shared::types::SdPath,
};
use anyhow::Result;
use sea_orm::{prelude::*, Condition, DatabaseConnection, QuerySelect};
use std::path::{Path, PathBuf};

/// Database query engine for copy preparation
pub struct CopyDatabaseQuery {
	db: DatabaseConnection,
}

impl CopyDatabaseQuery {
	pub fn new(db: DatabaseConnection) -> Self {
		Self { db }
	}

	/// Get instant estimates for multiple source paths
	pub async fn get_estimates_for_paths(&self, sources: &[SdPath]) -> Result<PathEstimates> {
		let mut total_files = 0u64;
		let mut total_bytes = 0u64;
		let mut indexed_paths = 0u64;

		for source in sources {
			if let Some(local_path) = source.as_local_path() {
				if let Some(estimates) = self.get_path_estimates(local_path).await? {
					total_files += estimates.file_count;
					total_bytes += estimates.total_size;
					indexed_paths += 1;
				}
			}
		}

		Ok(PathEstimates {
			file_count: total_files,
			total_size: total_bytes,
			indexed_paths,
			total_paths: sources.len() as u64,
		})
	}

	/// Get estimates for a single path
	async fn get_path_estimates(&self, path: &Path) -> Result<Option<SinglePathEstimate>> {
		let path_str = path.to_string_lossy().to_string();

		// Find the location that contains this path
		// For now, we'll do a simple prefix match
		let locations_list = location::Entity::find().all(&self.db).await?;

		let location = locations_list
			.into_iter()
			.filter(|loc| path_str.starts_with(&loc.path))
			.max_by_key(|loc| loc.path.len()); // Get the most specific match

		let location = match location {
			Some(loc) => loc,
			None => return Ok(None), // Path not in any indexed location
		};

		let location_path = PathBuf::from(&location.path);

		// Calculate the relative path within the location
		let relative_path = path
			.strip_prefix(&location_path)
			.map(|p| p.to_string_lossy().to_string())
			.unwrap_or_default();

		// If we're querying the entire location, use cached stats
		if relative_path.is_empty() {
			return Ok(Some(SinglePathEstimate {
				file_count: location.total_file_count as u64,
				total_size: location.total_byte_size as u64,
			}));
		}

		// For specific paths within locations, we need to query entries
		// This is a simplified version - in reality we'd need more complex queries
		// to handle directory aggregation properly
		println!("relative_path: {}", relative_path);

		// Split the relative path to get parent directory and name
		let (parent_path, name) = if let Some(pos) = relative_path.rfind('/') {
			(
				relative_path[..pos].to_string(),
				relative_path[pos + 1..].to_string(),
			)
		} else {
			(String::new(), relative_path)
		};

		// Query for the specific entry
		let entry = entry::Entity::find()
			.filter(
				Condition::all()
					.add(entry::Column::LocationId.eq(location.id))
					.add(entry::Column::RelativePath.eq(parent_path))
					.add(entry::Column::Name.eq(name)),
			)
			.one(&self.db)
			.await?;

		match entry {
			Some(entry) => {
				let (file_count, total_size) = match entry.kind {
					0 => (1u64, entry.size as u64), // File
					1 => {
						// Directory
						// Use pre-calculated aggregate values
						let files = entry.file_count as u64;
						let size = entry.aggregate_size as u64;
						(files, size)
					}
					_ => (0, 0), // Symlink or other
				};

				Ok(Some(SinglePathEstimate {
					file_count,
					total_size,
				}))
			}
			None => {
				// Path exists in location but not yet indexed
				Ok(None)
			}
		}
	}
}

/// Aggregate estimates for multiple paths
#[derive(Debug, Clone)]
pub struct PathEstimates {
	pub file_count: u64,
	pub total_size: u64,
	pub indexed_paths: u64,
	pub total_paths: u64,
}

impl PathEstimates {
	/// Check if we have complete information from the database
	pub fn is_complete(&self) -> bool {
		self.indexed_paths == self.total_paths
	}

	/// Get a confidence score (0.0 to 1.0) for the estimates
	pub fn confidence(&self) -> f32 {
		if self.total_paths == 0 {
			0.0
		} else {
			self.indexed_paths as f32 / self.total_paths as f32
		}
	}
}

/// Estimates for a single path
#[derive(Debug, Clone)]
pub struct SinglePathEstimate {
	pub file_count: u64,
	pub total_size: u64,
}

#[cfg(test)]
mod tests {
	use super::*;

	#[test]
	fn test_path_estimates() {
		let estimates = PathEstimates {
			file_count: 100,
			total_size: 1024 * 1024 * 100, // 100MB
			indexed_paths: 3,
			total_paths: 4,
		};

		assert!(!estimates.is_complete());
		assert_eq!(estimates.confidence(), 0.75);
	}
}
```

## src/operations/files/copy/mod.rs

```rust
//! Modular file copy operations using the Strategy Pattern

pub mod action;
pub mod database;
pub mod input;
pub mod job;
pub mod output;
pub mod routing;
pub mod strategy;

pub use job::{FileCopyJob, CopyOptions, MoveMode, CopyProgress, CopyError};
pub use output::FileCopyActionOutput;
pub use strategy::{CopyStrategy, LocalMoveStrategy, LocalStreamCopyStrategy, RemoteTransferStrategy};
pub use routing::CopyStrategyRouter;

// Re-export for backward compatibility
pub use job::MoveJob;```

## src/operations/files/copy/output.rs

```rust
//! File copy operation output types

use crate::infrastructure::actions::output::ActionOutputTrait;
use serde::{Deserialize, Serialize};
use uuid::Uuid;

/// Output from file copy action dispatch
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct FileCopyActionOutput {
    pub job_id: Uuid,
    pub sources_count: usize,
    pub destination: String,
}

impl FileCopyActionOutput {
    pub fn new(job_id: Uuid, sources_count: usize, destination: String) -> Self {
        Self {
            job_id,
            sources_count,
            destination,
        }
    }
}

impl ActionOutputTrait for FileCopyActionOutput {
    fn to_json(&self) -> serde_json::Value {
        serde_json::to_value(self).unwrap_or(serde_json::Value::Null)
    }
    
    fn display_message(&self) -> String {
        format!(
            "Dispatched file copy job {} for {} source(s) to {}",
            self.job_id, self.sources_count, self.destination
        )
    }
    
    fn output_type(&self) -> &'static str {
        "file.copy.dispatched"
    }
}```

## src/operations/files/copy/input.rs

```rust
//! Core input types for file copy operations

use super::job::CopyOptions;
use serde::{Deserialize, Serialize};
use std::path::PathBuf;

/// Copy method preference for file operations
#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]
pub enum CopyMethod {
    /// Automatically select the best method based on source and destination
    Auto,
    /// Use atomic move (rename) for same-volume operations
    AtomicMove,
    /// Use streaming copy for cross-volume operations
    StreamingCopy,
}

impl Default for CopyMethod {
    fn default() -> Self {
        CopyMethod::Auto
    }
}

/// Core input structure for file copy operations
/// This is the canonical interface that all external APIs (CLI, GraphQL, REST) convert to
#[derive(Debug, Clone, PartialEq, Eq)]
pub struct FileCopyInput {
    /// Source files or directories to copy
    pub sources: Vec<PathBuf>,
    
    /// Destination path
    pub destination: PathBuf,
    
    /// Whether to overwrite existing files
    pub overwrite: bool,
    
    /// Whether to verify checksums during copy
    pub verify_checksum: bool,
    
    /// Whether to preserve file timestamps
    pub preserve_timestamps: bool,
    
    /// Whether to delete source files after copying (move operation)
    pub move_files: bool,
    
    /// Preferred copy method to use
    pub copy_method: CopyMethod,
}

impl FileCopyInput {
    /// Create a new FileCopyInput with default options
    pub fn new<D: Into<PathBuf>>(sources: Vec<PathBuf>, destination: D) -> Self {
        Self {
            sources,
            destination: destination.into(),
            overwrite: false,
            verify_checksum: false,
            preserve_timestamps: true,
            move_files: false,
            copy_method: CopyMethod::Auto,
        }
    }
    
    /// Create a single file copy input
    pub fn single_file<S: Into<PathBuf>, D: Into<PathBuf>>(source: S, destination: D) -> Self {
        Self::new(vec![source.into()], destination)
    }
    
    /// Set overwrite option
    pub fn with_overwrite(mut self, overwrite: bool) -> Self {
        self.overwrite = overwrite;
        self
    }
    
    /// Set checksum verification option
    pub fn with_verification(mut self, verify: bool) -> Self {
        self.verify_checksum = verify;
        self
    }
    
    /// Set timestamp preservation option
    pub fn with_timestamp_preservation(mut self, preserve: bool) -> Self {
        self.preserve_timestamps = preserve;
        self
    }
    
    /// Set move files option
    pub fn with_move(mut self, move_files: bool) -> Self {
        self.move_files = move_files;
        self
    }
    
    /// Set copy method preference
    pub fn with_copy_method(mut self, copy_method: CopyMethod) -> Self {
        self.copy_method = copy_method;
        self
    }
    
    /// Convert to CopyOptions for the job system
    pub fn to_copy_options(&self) -> CopyOptions {
        CopyOptions {
            overwrite: self.overwrite,
            verify_checksum: self.verify_checksum,
            preserve_timestamps: self.preserve_timestamps,
            delete_after_copy: self.move_files,
            move_mode: None, // Will be determined by job system
            copy_method: self.copy_method.clone(),
        }
    }
    
    /// Validate the input
    pub fn validate(&self) -> Result<(), Vec<String>> {
        let mut errors = Vec::new();
        
        if self.sources.is_empty() {
            errors.push("At least one source file must be specified".to_string());
        }
        
        // Validate each source path (basic validation - existence check done in builder)
        for source in &self.sources {
            if source.as_os_str().is_empty() {
                errors.push("Source path cannot be empty".to_string());
            }
        }
        
        // Validate destination
        if self.destination.as_os_str().is_empty() {
            errors.push("Destination path cannot be empty".to_string());
        }
        
        if errors.is_empty() {
            Ok(())
        } else {
            Err(errors)
        }
    }
    
    /// Get a summary string for logging/display
    pub fn summary(&self) -> String {
        let operation = if self.move_files { "Move" } else { "Copy" };
        let source_count = self.sources.len();
        let source_desc = if source_count == 1 {
            "1 source".to_string()
        } else {
            format!("{} sources", source_count)
        };
        
        format!(
            "{} {} to {}",
            operation,
            source_desc,
            self.destination.display()
        )
    }
}

impl Default for FileCopyInput {
    fn default() -> Self {
        Self {
            sources: Vec::new(),
            destination: PathBuf::new(),
            overwrite: false,
            verify_checksum: false,
            preserve_timestamps: true,
            move_files: false,
            copy_method: CopyMethod::Auto,
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_new_input() {
        let input = FileCopyInput::new(
            vec!["/file1.txt".into(), "/file2.txt".into()],
            "/dest/"
        );
        
        assert_eq!(input.sources.len(), 2);
        assert_eq!(input.destination, PathBuf::from("/dest/"));
        assert!(!input.overwrite);
        assert!(input.preserve_timestamps);
        assert!(!input.move_files);
    }
    
    #[test]
    fn test_single_file() {
        let input = FileCopyInput::single_file("/source.txt", "/dest.txt");
        
        assert_eq!(input.sources, vec![PathBuf::from("/source.txt")]);
        assert_eq!(input.destination, PathBuf::from("/dest.txt"));
    }
    
    #[test]
    fn test_fluent_api() {
        let input = FileCopyInput::single_file("/source.txt", "/dest.txt")
            .with_overwrite(true)
            .with_verification(true)
            .with_timestamp_preservation(false)
            .with_move(true);
            
        assert!(input.overwrite);
        assert!(input.verify_checksum);
        assert!(!input.preserve_timestamps);
        assert!(input.move_files);
    }
    
    #[test]
    fn test_validation_empty_sources() {
        let input = FileCopyInput::default();
        let result = input.validate();
        
        assert!(result.is_err());
        let errors = result.unwrap_err();
        assert!(errors.iter().any(|e| e.contains("At least one source")));
    }
    
    #[test]
    fn test_validation_empty_destination() {
        let mut input = FileCopyInput::default();
        input.sources = vec!["/file.txt".into()];
        
        let result = input.validate();
        assert!(result.is_err());
        let errors = result.unwrap_err();
        assert!(errors.iter().any(|e| e.contains("Destination path cannot be empty")));
    }
    
    #[test]
    fn test_validation_success() {
        let input = FileCopyInput::new(vec!["/file.txt".into()], "/dest/");
        assert!(input.validate().is_ok());
    }
    
    #[test]
    fn test_summary() {
        let input = FileCopyInput::new(vec!["/file1.txt".into(), "/file2.txt".into()], "/dest/");
        assert_eq!(input.summary(), "Copy 2 sources to /dest/");
        
        let move_input = input.with_move(true);
        assert_eq!(move_input.summary(), "Move 2 sources to /dest/");
        
        let single_input = FileCopyInput::single_file("/file.txt", "/dest.txt");
        assert_eq!(single_input.summary(), "Copy 1 source to /dest.txt");
    }
    
    #[test]
    fn test_to_copy_options() {
        let input = FileCopyInput::single_file("/source.txt", "/dest.txt")
            .with_overwrite(true)
            .with_verification(true)
            .with_timestamp_preservation(false)
            .with_move(true);
            
        let options = input.to_copy_options();
        assert!(options.overwrite);
        assert!(options.verify_checksum);
        assert!(!options.preserve_timestamps);
        assert!(options.delete_after_copy);
    }
}```

## src/operations/files/copy/routing.rs

```rust
//! Strategy router for selecting the optimal copy method

use super::{
    input::CopyMethod,
    strategy::{CopyStrategy, LocalMoveStrategy, LocalStreamCopyStrategy, RemoteTransferStrategy},
};
use crate::{shared::types::SdPath, volume::VolumeManager};
use std::sync::Arc;

pub struct CopyStrategyRouter;

impl CopyStrategyRouter {
    /// Selects the optimal copy strategy based on source, destination, and volume info
    pub async fn select_strategy(
        source: &SdPath,
        destination: &SdPath,
        is_move: bool,
        copy_method: &CopyMethod,
        volume_manager: Option<&VolumeManager>,
    ) -> Box<dyn CopyStrategy> {
        // Cross-device transfer - always use network strategy
        if source.device_id != destination.device_id {
            return Box::new(RemoteTransferStrategy);
        }

        // For same-device operations, respect user's method preference
        match copy_method {
            CopyMethod::AtomicMove => {
                // User explicitly wants atomic move - validate it's possible
                if is_move {
                    return Box::new(LocalMoveStrategy);
                } else {
                    // Cannot do atomic move for copy operations, fall back to streaming
                    return Box::new(LocalStreamCopyStrategy);
                }
            }
            CopyMethod::StreamingCopy => {
                // User explicitly wants streaming copy
                return Box::new(LocalStreamCopyStrategy);
            }
            CopyMethod::Auto => {
                // Auto-select based on optimal strategy (original logic)
                // Same-device operation - get local paths for volume analysis
                let (source_path, dest_path) = match (source.as_local_path(), destination.as_local_path()) {
                    (Some(s), Some(d)) => (s, d),
                    _ => {
                        // Fallback to streaming copy if paths aren't local
                        return Box::new(LocalStreamCopyStrategy);
                    }
                };

                // Check if paths are on the same volume
                if let Some(vm) = volume_manager {
                    if vm.same_volume(source_path, dest_path).await {
                        // Same volume
                        if is_move {
                            // Use atomic move for same-volume moves
                            return Box::new(LocalMoveStrategy);
                        }
                        // For same-volume copies, we could add optimized copy strategies here
                        // (e.g., reflink on filesystems that support it)
                        // For now, fall through to streaming copy
                    }
                } else {
                    // No volume manager available - make best guess
                    // If it's a move operation on the same device, try atomic move
                    if is_move {
                        return Box::new(LocalMoveStrategy);
                    }
                }

                // Default to streaming copy for cross-volume or non-move same-volume
                Box::new(LocalStreamCopyStrategy)
            }
        }
    }

    /// Provides a human-readable description of the selected strategy
    pub async fn describe_strategy(
        source: &SdPath,
        destination: &SdPath,
        is_move: bool,
        copy_method: &CopyMethod,
        volume_manager: Option<&VolumeManager>,
    ) -> String {
        if source.device_id != destination.device_id {
            return if is_move {
                "Cross-device move".to_string()
            } else {
                "Cross-device transfer".to_string()
            };
        }

        // For same-device operations, include user preference info
        let method_prefix = match copy_method {
            CopyMethod::Auto => "",
            CopyMethod::AtomicMove => "User-requested atomic ",
            CopyMethod::StreamingCopy => "User-requested streaming ",
        };

        match copy_method {
            CopyMethod::AtomicMove => {
                if is_move {
                    format!("{}move", method_prefix)
                } else {
                    format!("{}copy (fallback to streaming)", method_prefix)
                }
            }
            CopyMethod::StreamingCopy => {
                if is_move {
                    format!("{}move", method_prefix)
                } else {
                    format!("{}copy", method_prefix)
                }
            }
            CopyMethod::Auto => {
                // Auto-select - use original logic for description
                let (source_path, dest_path) = match (source.as_local_path(), destination.as_local_path()) {
                    (Some(s), Some(d)) => (s, d),
                    _ => {
                        return "Streaming copy".to_string();
                    }
                };

                if let Some(vm) = volume_manager {
                    if vm.same_volume(source_path, dest_path).await {
                        if is_move {
                            return "Atomic move".to_string();
                        } else {
                            return "Same-volume copy".to_string();
                        }
                    } else {
                        return if is_move {
                            "Cross-volume move".to_string()
                        } else {
                            "Cross-volume streaming copy".to_string()
                        };
                    }
                }

                // Fallback description
                if is_move {
                    "Local move".to_string()
                } else {
                    "Local copy".to_string()
                }
            }
        }
    }

    /// Estimates the performance characteristics of the selected strategy
    pub async fn estimate_performance(
        source: &SdPath,
        destination: &SdPath,
        is_move: bool,
        copy_method: &CopyMethod,
        volume_manager: Option<&VolumeManager>,
    ) -> PerformanceEstimate {
        // Cross-device transfers always use network
        if source.device_id != destination.device_id {
            return PerformanceEstimate {
                speed_category: SpeedCategory::Network,
                supports_resume: true,
                requires_network: true,
                is_atomic: false,
            };
        }

        // For same-device operations, consider user's method preference
        match copy_method {
            CopyMethod::AtomicMove => {
                if is_move {
                    PerformanceEstimate {
                        speed_category: SpeedCategory::Instant,
                        supports_resume: false,
                        requires_network: false,
                        is_atomic: true,
                    }
                } else {
                    // Fallback to streaming for copy operations
                    PerformanceEstimate {
                        speed_category: SpeedCategory::LocalDisk,
                        supports_resume: false,
                        requires_network: false,
                        is_atomic: false,
                    }
                }
            }
            CopyMethod::StreamingCopy => {
                PerformanceEstimate {
                    speed_category: SpeedCategory::LocalDisk,
                    supports_resume: false,
                    requires_network: false,
                    is_atomic: false,
                }
            }
            CopyMethod::Auto => {
                // Auto-select - use original performance estimation logic
                let (source_path, dest_path) = match (source.as_local_path(), destination.as_local_path()) {
                    (Some(s), Some(d)) => (s, d),
                    _ => {
                        return PerformanceEstimate {
                            speed_category: SpeedCategory::LocalDisk,
                            supports_resume: false,
                            requires_network: false,
                            is_atomic: false,
                        };
                    }
                };

                if let Some(vm) = volume_manager {
                    if vm.same_volume(source_path, dest_path).await {
                        if is_move {
                            return PerformanceEstimate {
                                speed_category: SpeedCategory::Instant,
                                supports_resume: false,
                                requires_network: false,
                                is_atomic: true,
                            };
                        } else {
                            // Could be optimized with filesystem features
                            let source_vol = vm.volume_for_path(source_path).await;
                            let supports_fast_copy = source_vol
                                .map(|v| v.supports_fast_copy())
                                .unwrap_or(false);

                            return PerformanceEstimate {
                                speed_category: if supports_fast_copy {
                                    SpeedCategory::FastLocal
                                } else {
                                    SpeedCategory::LocalDisk
                                },
                                supports_resume: false,
                                requires_network: false,
                                is_atomic: supports_fast_copy,
                            };
                        }
                    } else {
                        // Cross-volume on same device
                        return PerformanceEstimate {
                            speed_category: SpeedCategory::LocalDisk,
                            supports_resume: true,
                            requires_network: false,
                            is_atomic: false,
                        };
                    }
                }

                // Fallback estimate
                PerformanceEstimate {
                    speed_category: if is_move {
                        SpeedCategory::FastLocal
                    } else {
                        SpeedCategory::LocalDisk
                    },
                    supports_resume: false,
                    requires_network: false,
                    is_atomic: is_move,
                }
            }
        }
    }
}

/// Performance characteristics of a copy strategy
#[derive(Debug, Clone)]
pub struct PerformanceEstimate {
    pub speed_category: SpeedCategory,
    pub supports_resume: bool,
    pub requires_network: bool,
    pub is_atomic: bool,
}

/// Categories of copy operation speed
#[derive(Debug, Clone, PartialEq)]
pub enum SpeedCategory {
    /// Instant operations (like atomic moves)
    Instant,
    /// Fast local operations (reflinks, same-volume copies)
    FastLocal,
    /// Regular disk-to-disk operations
    LocalDisk,
    /// Network transfers
    Network,
}```

## src/operations/files/validation/action.rs

```rust
//! File validation action handler

use crate::{
    context::CoreContext,
    infrastructure::actions::{
        error::{ActionError, ActionResult}, 
        handler::ActionHandler, 
        output::ActionOutput,
    },
    register_action_handler,
    shared::types::{SdPath, SdPathBatch},
};
use super::job::{ValidationJob, ValidationMode};
use async_trait::async_trait;
use std::sync::Arc;
use uuid::Uuid;

#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct ValidationAction {
    pub paths: Vec<std::path::PathBuf>,
    pub verify_checksums: bool,
    pub deep_scan: bool,
}

pub struct ValidationHandler;

impl ValidationHandler {
    pub fn new() -> Self {
        Self
    }
}

#[async_trait]
impl ActionHandler for ValidationHandler {
    async fn validate(
        &self,
        _context: Arc<CoreContext>,
        action: &crate::infrastructure::actions::Action,
    ) -> ActionResult<()> {
        if let crate::infrastructure::actions::Action::FileValidate { action, .. } = action {
            if action.paths.is_empty() {
                return Err(ActionError::Validation {
                    field: "paths".to_string(),
                    message: "At least one path must be specified".to_string(),
                });
            }
            Ok(())
        } else {
            Err(ActionError::InvalidActionType)
        }
    }

    async fn execute(
        &self,
        context: Arc<CoreContext>,
        action: crate::infrastructure::actions::Action,
    ) -> ActionResult<ActionOutput> {
        if let crate::infrastructure::actions::Action::FileValidate { library_id, action } = action {
            let library_manager = &context.library_manager;
            
            let library = library_manager.get_library(library_id).await
                .ok_or(ActionError::Internal(format!("Library not found: {}", library_id)))?;

            // Convert paths to SdPath and create job
            let targets = action.paths
                .into_iter()
                .map(|path| SdPath::local(path))
                .collect();

            // Determine validation mode based on action parameters
            let mode = if action.deep_scan {
                ValidationMode::Complete
            } else if action.verify_checksums {
                ValidationMode::Integrity
            } else {
                ValidationMode::Basic
            };

            let job = ValidationJob::new(SdPathBatch::new(targets), mode);

            // Dispatch the job directly
            let job_handle = library
                .jobs()
                .dispatch(job)
                .await
                .map_err(ActionError::Job)?;

            Ok(ActionOutput::success("File validation job dispatched successfully"))
        } else {
            Err(ActionError::InvalidActionType)
        }
    }

    fn can_handle(&self, action: &crate::infrastructure::actions::Action) -> bool {
        matches!(action, crate::infrastructure::actions::Action::FileValidate { .. })
    }

    fn supported_actions() -> &'static [&'static str] {
        &["file.validate"]
    }
}

register_action_handler!(ValidationHandler, "file.validate");```

## src/operations/files/validation/job.rs

```rust
//! File validation and integrity checking job

use crate::{
	domain::content_identity::ContentHashGenerator,
	infrastructure::jobs::prelude::*,
	shared::types::{SdPath, SdPathBatch},
};
use serde::{Deserialize, Serialize};
use std::{
	collections::HashMap,
	path::PathBuf,
	time::{Duration, Instant},
};
use tokio::fs;
use uuid::Uuid;

/// File validation modes
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum ValidationMode {
	/// Check file accessibility and basic metadata
	Basic,
	/// Verify file integrity using CAS ID
	Integrity,
	/// Check for corruption patterns
	Corruption,
	/// Full validation including content verification
	Complete,
}

/// Validation severity levels
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum ValidationSeverity {
	Info,
	Warning,
	Error,
	Critical,
}

/// File validation issue
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ValidationIssue {
	pub path: SdPath,
	pub issue_type: String,
	pub severity: ValidationSeverity,
	pub description: String,
	pub suggested_action: Option<String>,
}

/// File validation job
#[derive(Debug, Serialize, Deserialize)]
pub struct ValidationJob {
	pub targets: SdPathBatch,
	pub mode: ValidationMode,
	pub verify_against_index: bool,
	pub check_permissions: bool,

	// Internal state for resumption
	#[serde(skip)]
	validated_files: Vec<PathBuf>,
	#[serde(skip, default = "Instant::now")]
	started_at: Instant,
}

/// Validation progress information
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ValidationProgress {
	pub current_file: String,
	pub files_validated: usize,
	pub total_files: usize,
	pub issues_found: usize,
	pub bytes_validated: u64,
	pub current_operation: String,
}

impl JobProgress for ValidationProgress {}

impl Job for ValidationJob {
	const NAME: &'static str = "file_validation";
	const RESUMABLE: bool = true;
	const DESCRIPTION: Option<&'static str> = Some("Validate file integrity and accessibility");
}

#[async_trait::async_trait]
impl JobHandler for ValidationJob {
	type Output = ValidationOutput;

	async fn run(&mut self, ctx: JobContext<'_>) -> JobResult<Self::Output> {
		ctx.log(format!(
			"Starting file validation with mode: {:?}",
			self.mode
		));

		let mut issues = Vec::new();
		let mut validated_count = 0;
		let mut total_bytes_validated = 0u64;

		// Collect all files to validate
		let files_to_validate = self.collect_files(&ctx).await?;
		let total_files = files_to_validate.len();

		ctx.log(format!("Found {} files to validate", total_files));

		for (index, file_info) in files_to_validate.iter().enumerate() {
			ctx.check_interrupt().await?;

			ctx.progress(Progress::structured(ValidationProgress {
				current_file: file_info.path.display(),
				files_validated: validated_count,
				total_files,
				issues_found: issues.len(),
				bytes_validated: total_bytes_validated,
				current_operation: self.get_operation_name(),
			}));

			match self.validate_file(file_info, &ctx).await {
				Ok(file_issues) => {
					issues.extend(file_issues);
					validated_count += 1;
					total_bytes_validated += file_info.size;
				}
				Err(e) => {
					issues.push(ValidationIssue {
						path: file_info.path.clone(),
						issue_type: "ValidationError".to_string(),
						severity: ValidationSeverity::Error,
						description: format!("Failed to validate file: {}", e),
						suggested_action: Some(
							"Check file permissions and accessibility".to_string(),
						),
					});
					ctx.add_non_critical_error(format!(
						"Failed to validate {}: {}",
						file_info.path.display(),
						e
					));
				}
			}

			// Checkpoint every 50 files
			if index % 50 == 0 {
				ctx.checkpoint().await?;
			}
		}

		let error_count = issues
			.iter()
			.filter(|i| {
				matches!(
					i.severity,
					ValidationSeverity::Error | ValidationSeverity::Critical
				)
			})
			.count();
		let warning_count = issues
			.iter()
			.filter(|i| matches!(i.severity, ValidationSeverity::Warning))
			.count();

		ctx.log(format!(
			"Validation completed: {} files validated, {} errors, {} warnings",
			validated_count, error_count, warning_count
		));

		Ok(ValidationOutput {
			validated_count,
			total_files,
			issues,
			total_bytes_validated,
			duration: self.started_at.elapsed(),
			validation_mode: self.mode.clone(),
		})
	}
}

/// File information for validation
#[derive(Debug, Clone)]
struct FileValidationInfo {
	path: SdPath,
	size: u64,
	modified: Option<std::time::SystemTime>,
	permissions: Option<std::fs::Permissions>,
}

impl ValidationJob {
	/// Create a new validation job
	pub fn new(targets: SdPathBatch, mode: ValidationMode) -> Self {
		Self {
			targets,
			mode,
			verify_against_index: true,
			check_permissions: true,
			validated_files: Vec::new(),
			started_at: Instant::now(),
		}
	}

	/// Set whether to verify against the file index
	pub fn with_index_verification(mut self, verify: bool) -> Self {
		self.verify_against_index = verify;
		self
	}

	/// Set whether to check file permissions
	pub fn with_permission_check(mut self, check: bool) -> Self {
		self.check_permissions = check;
		self
	}

	/// Collect all files to validate
	async fn collect_files(&self, ctx: &JobContext<'_>) -> JobResult<Vec<FileValidationInfo>> {
		let mut files = Vec::new();

		for target in &self.targets.paths {
			ctx.check_interrupt().await?;

			if let Some(local_path) = target.as_local_path() {
				self.collect_files_recursive(local_path, target, &mut files, ctx)
					.await?;
			}
		}

		Ok(files)
	}

	/// Collect files from a directory using iterative approach
	async fn collect_files_recursive(
		&self,
		path: &std::path::Path,
		sd_path: &SdPath,
		files: &mut Vec<FileValidationInfo>,
		ctx: &JobContext<'_>,
	) -> JobResult<()> {
		let mut stack = vec![(path.to_path_buf(), sd_path.clone())];

		while let Some((current_path, current_sd_path)) = stack.pop() {
			ctx.check_interrupt().await?;

			let metadata = fs::metadata(&current_path).await?;

			if metadata.is_file() {
				files.push(FileValidationInfo {
					path: current_sd_path,
					size: metadata.len(),
					modified: metadata.modified().ok(),
					permissions: std::fs::metadata(&current_path)
						.ok()
						.map(|m| m.permissions()),
				});
			} else if metadata.is_dir() {
				let mut dir = fs::read_dir(&current_path).await?;

				while let Some(entry) = dir.next_entry().await? {
					let entry_path = entry.path();
					let entry_sd_path = current_sd_path.join(entry.file_name());
					stack.push((entry_path, entry_sd_path));
				}
			}
		}

		Ok(())
	}

	/// Validate a single file
	async fn validate_file(
		&self,
		file_info: &FileValidationInfo,
		ctx: &JobContext<'_>,
	) -> JobResult<Vec<ValidationIssue>> {
		let mut issues = Vec::new();

		if let Some(local_path) = file_info.path.as_local_path() {
			// Basic validation
			issues.extend(self.validate_basic(file_info, local_path).await?);

			// Mode-specific validation
			match self.mode {
				ValidationMode::Basic => {
					// Basic validation already done
				}
				ValidationMode::Integrity => {
					issues.extend(self.validate_integrity(file_info, local_path, ctx).await?);
				}
				ValidationMode::Corruption => {
					issues.extend(self.validate_corruption(file_info, local_path).await?);
				}
				ValidationMode::Complete => {
					issues.extend(self.validate_integrity(file_info, local_path, ctx).await?);
					issues.extend(self.validate_corruption(file_info, local_path).await?);
					issues.extend(self.validate_comprehensive(file_info, local_path).await?);
				}
			}
		}

		Ok(issues)
	}

	/// Basic file validation
	async fn validate_basic(
		&self,
		file_info: &FileValidationInfo,
		local_path: &std::path::Path,
	) -> JobResult<Vec<ValidationIssue>> {
		let mut issues = Vec::new();

		// Check if file exists and is accessible
		if !fs::try_exists(local_path).await.unwrap_or(false) {
			issues.push(ValidationIssue {
				path: file_info.path.clone(),
				issue_type: "FileNotFound".to_string(),
				severity: ValidationSeverity::Critical,
				description: "File does not exist or is not accessible".to_string(),
				suggested_action: Some("Check if file was moved or deleted".to_string()),
			});
			return Ok(issues);
		}

		// Check metadata consistency
		if let Ok(current_metadata) = fs::metadata(local_path).await {
			if current_metadata.len() != file_info.size {
				issues.push(ValidationIssue {
					path: file_info.path.clone(),
					issue_type: "SizeMismatch".to_string(),
					severity: ValidationSeverity::Warning,
					description: format!(
						"File size changed: expected {}, found {}",
						file_info.size,
						current_metadata.len()
					),
					suggested_action: Some("File may have been modified".to_string()),
				});
			}

			// Check modification time if available
			if let (Some(expected_modified), Ok(current_modified)) =
				(file_info.modified, current_metadata.modified())
			{
				if expected_modified != current_modified {
					issues.push(ValidationIssue {
						path: file_info.path.clone(),
						issue_type: "ModificationTimeChanged".to_string(),
						severity: ValidationSeverity::Info,
						description: "File modification time has changed".to_string(),
						suggested_action: None,
					});
				}
			}
		}

		// Check permissions if enabled
		if self.check_permissions {
			if let Some(ref expected_permissions) = file_info.permissions {
				if let Ok(current_metadata) = std::fs::metadata(local_path) {
					let current_permissions = current_metadata.permissions();
					if current_permissions.readonly() != expected_permissions.readonly() {
						issues.push(ValidationIssue {
							path: file_info.path.clone(),
							issue_type: "PermissionChanged".to_string(),
							severity: ValidationSeverity::Warning,
							description: "File permissions have changed".to_string(),
							suggested_action: Some(
								"Check if permission change was intentional".to_string(),
							),
						});
					}
				}
			}
		}

		Ok(issues)
	}

	/// Integrity validation using CAS ID
	async fn validate_integrity(
		&self,
		file_info: &FileValidationInfo,
		local_path: &std::path::Path,
		ctx: &JobContext<'_>,
	) -> JobResult<Vec<ValidationIssue>> {
		let mut issues = Vec::new();

		// For integrity validation, we would need to compare against stored CAS ID
		// This is a placeholder implementation
		match ContentHashGenerator::generate_full_hash(local_path).await {
			Ok(current_cas_id) => {
				// Here we would compare against the stored CAS ID from the database
				// For now, we just verify that we can generate one
				ctx.log(format!(
					"Generated CAS ID for validation: {}",
					&current_cas_id[..16]
				));
			}
			Err(e) => {
				issues.push(ValidationIssue {
					path: file_info.path.clone(),
					issue_type: "IntegrityCheckFailed".to_string(),
					severity: ValidationSeverity::Error,
					description: format!("Failed to compute content hash: {}", e),
					suggested_action: Some("File may be corrupted or inaccessible".to_string()),
				});
			}
		}

		Ok(issues)
	}

	/// Check for file corruption patterns
	async fn validate_corruption(
		&self,
		file_info: &FileValidationInfo,
		local_path: &std::path::Path,
	) -> JobResult<Vec<ValidationIssue>> {
		let mut issues = Vec::new();

		// Check for zero-byte files (unless expected)
		if file_info.size == 0 {
			issues.push(ValidationIssue {
				path: file_info.path.clone(),
				issue_type: "EmptyFile".to_string(),
				severity: ValidationSeverity::Warning,
				description: "File is empty (0 bytes)".to_string(),
				suggested_action: Some("Verify if this is expected".to_string()),
			});
		}

		// Check for extremely large files that might indicate corruption
		if file_info.size > 100 * 1024 * 1024 * 1024 {
			// 100GB
			issues.push(ValidationIssue {
				path: file_info.path.clone(),
				issue_type: "UnusuallyLargeFile".to_string(),
				severity: ValidationSeverity::Info,
				description: format!("File is very large: {} bytes", file_info.size),
				suggested_action: Some("Verify file integrity if unexpected".to_string()),
			});
		}

		// Check file extension vs content (basic check)
		if let Some(extension) = local_path.extension().and_then(|e| e.to_str()) {
			match extension.to_lowercase().as_str() {
				"txt" | "md" | "json" | "xml" | "csv" => {
					// Text files - check for binary content in beginning
					if let Ok(mut file) = tokio::fs::File::open(local_path).await {
						use tokio::io::AsyncReadExt;
						let mut buffer = [0; 1024];
						if let Ok(bytes_read) = file.read(&mut buffer).await {
							let content = &buffer[..bytes_read];
							if content
								.iter()
								.any(|&b| b < 32 && b != 9 && b != 10 && b != 13)
							{
								issues.push(ValidationIssue {
									path: file_info.path.clone(),
									issue_type: "SuspiciousContent".to_string(),
									severity: ValidationSeverity::Warning,
									description: "Text file contains binary data".to_string(),
									suggested_action: Some(
										"Verify file type is correct".to_string(),
									),
								});
							}
						}
					}
				}
				_ => {} // Other file types - could add more specific checks
			}
		}

		Ok(issues)
	}

	/// Comprehensive validation
	async fn validate_comprehensive(
		&self,
		file_info: &FileValidationInfo,
		local_path: &std::path::Path,
	) -> JobResult<Vec<ValidationIssue>> {
		let mut issues = Vec::new();

		// Check file name validity
		if let Some(filename) = local_path.file_name().and_then(|n| n.to_str()) {
			// Check for problematic characters
			let problematic_chars = ['<', '>', ':', '"', '|', '?', '*'];
			if problematic_chars.iter().any(|&c| filename.contains(c)) {
				issues.push(ValidationIssue {
					path: file_info.path.clone(),
					issue_type: "ProblematicFilename".to_string(),
					severity: ValidationSeverity::Warning,
					description: "Filename contains characters that may cause issues".to_string(),
					suggested_action: Some("Consider renaming file".to_string()),
				});
			}

			// Check for very long filenames
			if filename.len() > 255 {
				issues.push(ValidationIssue {
					path: file_info.path.clone(),
					issue_type: "LongFilename".to_string(),
					severity: ValidationSeverity::Warning,
					description: "Filename is very long and may not be compatible with all systems"
						.to_string(),
					suggested_action: Some("Consider shortening filename".to_string()),
				});
			}
		}

		// Check path depth
		let path_components = local_path.components().count();
		if path_components > 32 {
			issues.push(ValidationIssue {
				path: file_info.path.clone(),
				issue_type: "DeepPath".to_string(),
				severity: ValidationSeverity::Info,
				description: "File is located very deep in directory structure".to_string(),
				suggested_action: Some("Consider reorganizing directory structure".to_string()),
			});
		}

		Ok(issues)
	}

	/// Get operation name for progress display
	fn get_operation_name(&self) -> String {
		match self.mode {
			ValidationMode::Basic => "Basic validation".to_string(),
			ValidationMode::Integrity => "Integrity checking".to_string(),
			ValidationMode::Corruption => "Corruption detection".to_string(),
			ValidationMode::Complete => "Complete validation".to_string(),
		}
	}
}

/// Job output for file validation
#[derive(Debug, Serialize, Deserialize)]
pub struct ValidationOutput {
	pub validated_count: usize,
	pub total_files: usize,
	pub issues: Vec<ValidationIssue>,
	pub total_bytes_validated: u64,
	pub duration: Duration,
	pub validation_mode: ValidationMode,
}

impl From<ValidationOutput> for JobOutput {
	fn from(output: ValidationOutput) -> Self {
		JobOutput::FileValidation {
			validated_count: output.validated_count,
			issues_found: output.issues.len(),
			total_bytes_validated: output.total_bytes_validated,
		}
	}
}
```

## src/operations/files/validation/mod.rs

```rust
//! File validation operations

pub mod action;
pub mod job;

pub use job::*;
pub use action::ValidationAction;```

## src/operations/indexing/action.rs

```rust
//! Indexing action handler

use crate::{
    context::CoreContext,
    infrastructure::actions::{
        error::{ActionError, ActionResult}, 
        handler::ActionHandler, 
        output::ActionOutput,
    },
    register_action_handler,
    shared::types::SdPath,
};
use super::job::{IndexerJob, IndexMode, IndexScope};
use async_trait::async_trait;
use std::sync::Arc;
use uuid::Uuid;

#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct IndexingAction {
    pub paths: Vec<std::path::PathBuf>,
    pub recursive: bool,
    pub include_hidden: bool,
}

pub struct IndexingHandler;

impl IndexingHandler {
    pub fn new() -> Self {
        Self
    }
}

#[async_trait]
impl ActionHandler for IndexingHandler {
    async fn validate(
        &self,
        _context: Arc<CoreContext>,
        action: &crate::infrastructure::actions::Action,
    ) -> ActionResult<()> {
        if let crate::infrastructure::actions::Action::Index { action, .. } = action {
            if action.paths.is_empty() {
                return Err(ActionError::Validation {
                    field: "paths".to_string(),
                    message: "At least one path must be specified".to_string(),
                });
            }
            Ok(())
        } else {
            Err(ActionError::InvalidActionType)
        }
    }

    async fn execute(
        &self,
        context: Arc<CoreContext>,
        action: crate::infrastructure::actions::Action,
    ) -> ActionResult<ActionOutput> {
        if let crate::infrastructure::actions::Action::Index { library_id, action } = action {
            let library_manager = &context.library_manager;
            
            let library = library_manager.get_library(library_id).await
                .ok_or(ActionError::Internal(format!("Library not found: {}", library_id)))?;

            // TODO: For multiple paths, we might want to create multiple jobs or handle this differently
            // For now, just take the first path
            let first_path = action.paths.into_iter().next()
                .ok_or(ActionError::Validation {
                    field: "paths".to_string(),
                    message: "At least one path must be specified".to_string(),
                })?;

            // Create indexer job directly
            // TODO: Need location_id - for now using a placeholder
            let job = IndexerJob::from_location(
                Uuid::new_v4(), // placeholder location_id
                SdPath::local(first_path), 
                IndexMode::Content // default mode
            );

            // Dispatch the job directly
            let job_handle = library
                .jobs()
                .dispatch(job)
                .await
                .map_err(ActionError::Job)?;

            Ok(ActionOutput::success("Indexing job dispatched successfully"))
        } else {
            Err(ActionError::InvalidActionType)
        }
    }

    fn can_handle(&self, action: &crate::infrastructure::actions::Action) -> bool {
        matches!(action, crate::infrastructure::actions::Action::Index { .. })
    }

    fn supported_actions() -> &'static [&'static str] {
        &["indexing.index"]
    }
}

register_action_handler!(IndexingHandler, "indexing.index");```

## src/operations/indexing/job.rs

```rust
//! Main indexer job implementation

use crate::{infrastructure::jobs::prelude::*, shared::types::SdPath};
use serde::{Deserialize, Serialize};
use std::{collections::HashMap, path::PathBuf, sync::Arc, time::Duration};
use tokio::sync::RwLock;
use uuid::Uuid;

use super::{
	entry::EntryMetadata,
	metrics::{IndexerMetrics, PhaseTimer},
	phases,
	state::{IndexError, IndexerProgress, IndexerState, IndexerStats, Phase, IndexPhase},
};

/// Indexing mode determines the depth of indexing
#[derive(Debug, Clone, Copy, PartialEq, Eq, PartialOrd, Ord, Serialize, Deserialize)]
pub enum IndexMode {
	/// Just filesystem metadata (fastest)
	Shallow,
	/// Generate content identities (moderate)
	Content,
	/// Full indexing with thumbnails and text extraction (slowest)
	Deep,
}

/// Indexing scope determines how much of the directory tree to process
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
pub enum IndexScope {
	/// Index only the current directory (single level)
	Current,
	/// Index recursively through all subdirectories
	Recursive,
}

impl Default for IndexScope {
	fn default() -> Self {
		IndexScope::Recursive
	}
}

impl From<&str> for IndexScope {
	fn from(s: &str) -> Self {
		match s.to_lowercase().as_str() {
			"current" => IndexScope::Current,
			"recursive" => IndexScope::Recursive,
			_ => IndexScope::Recursive,
		}
	}
}

impl std::fmt::Display for IndexScope {
	fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
		match self {
			IndexScope::Current => write!(f, "current"),
			IndexScope::Recursive => write!(f, "recursive"),
		}
	}
}

/// Determines whether indexing results are persisted to database or kept in memory
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
pub enum IndexPersistence {
	/// Write all results to database (normal operation)
	Persistent,
	/// Keep results in memory only (for unmanaged paths)
	Ephemeral,
}

impl Default for IndexPersistence {
	fn default() -> Self {
		IndexPersistence::Persistent
	}
}

/// Enhanced configuration for indexer jobs
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct IndexerJobConfig {
	pub location_id: Option<Uuid>, // None for ephemeral indexing
	pub path: SdPath,
	pub mode: IndexMode,
	pub scope: IndexScope,
	pub persistence: IndexPersistence,
	pub max_depth: Option<u32>, // Override for Current scope or depth limiting
}

impl IndexerJobConfig {
	/// Create a new configuration for persistent recursive indexing (traditional)
	pub fn new(location_id: Uuid, path: SdPath, mode: IndexMode) -> Self {
		Self {
			location_id: Some(location_id),
			path,
			mode,
			scope: IndexScope::Recursive,
			persistence: IndexPersistence::Persistent,
			max_depth: None,
		}
	}

	/// Create configuration for UI directory navigation (quick current scan)
	pub fn ui_navigation(location_id: Uuid, path: SdPath) -> Self {
		Self {
			location_id: Some(location_id),
			path,
			mode: IndexMode::Shallow,
			scope: IndexScope::Current,
			persistence: IndexPersistence::Persistent,
			max_depth: Some(1),
		}
	}

	/// Create configuration for ephemeral path browsing (outside managed locations)
	pub fn ephemeral_browse(path: SdPath, scope: IndexScope) -> Self {
		Self {
			location_id: None,
			path,
			mode: IndexMode::Shallow,
			scope,
			persistence: IndexPersistence::Ephemeral,
			max_depth: if scope == IndexScope::Current {
				Some(1)
			} else {
				None
			},
		}
	}

	/// Check if this is an ephemeral (non-persistent) job
	pub fn is_ephemeral(&self) -> bool {
		self.persistence == IndexPersistence::Ephemeral
	}

	/// Check if this is a current scope (single level) job
	pub fn is_current_scope(&self) -> bool {
		self.scope == IndexScope::Current
	}
}

/// In-memory storage for ephemeral indexing results
#[derive(Debug)]
pub struct EphemeralIndex {
	pub entries: HashMap<PathBuf, EntryMetadata>,
	pub content_identities: HashMap<String, EphemeralContentIdentity>,
	pub created_at: std::time::Instant,
	pub last_accessed: std::time::Instant,
	pub root_path: PathBuf,
	pub stats: IndexerStats,
}

/// Simplified content identity for ephemeral storage
#[derive(Debug, Clone)]
pub struct EphemeralContentIdentity {
	pub cas_id: String,
	pub mime_type: Option<String>,
	pub file_size: u64,
	pub entry_count: u32,
}

impl EphemeralIndex {
	pub fn new(root_path: PathBuf) -> Self {
		let now = std::time::Instant::now();
		Self {
			entries: HashMap::new(),
			content_identities: HashMap::new(),
			created_at: now,
			last_accessed: now,
			root_path,
			stats: IndexerStats::default(),
		}
	}

	pub fn add_entry(&mut self, path: PathBuf, metadata: EntryMetadata) {
		self.entries.insert(path, metadata);
		self.last_accessed = std::time::Instant::now();
	}

	pub fn get_entry(&mut self, path: &PathBuf) -> Option<&EntryMetadata> {
		self.last_accessed = std::time::Instant::now();
		self.entries.get(path)
	}

	pub fn add_content_identity(&mut self, cas_id: String, content: EphemeralContentIdentity) {
		self.content_identities.insert(cas_id, content);
		self.last_accessed = std::time::Instant::now();
	}

	pub fn age(&self) -> Duration {
		self.created_at.elapsed()
	}

	pub fn idle_time(&self) -> Duration {
		self.last_accessed.elapsed()
	}
}

/// Indexer job - discovers and indexes files in a location
#[derive(Debug, Serialize, Deserialize, Job)]
pub struct IndexerJob {
	pub config: IndexerJobConfig,

	// Resumable state
	#[serde(skip_serializing_if = "Option::is_none")]
	state: Option<IndexerState>,

	// Ephemeral storage for non-persistent jobs
	#[serde(skip)]
	ephemeral_index: Option<Arc<RwLock<EphemeralIndex>>>,

	// Performance tracking
	#[serde(skip)]
	timer: Option<PhaseTimer>,
	#[serde(skip)]
	db_operations: (u64, u64), // (reads, writes)
	#[serde(skip)]
	batch_info: (u64, usize), // (count, total_size)
}

impl Job for IndexerJob {
	const NAME: &'static str = "indexer";
	const RESUMABLE: bool = true;
	const DESCRIPTION: Option<&'static str> = Some("Index files in a location");
}

impl JobProgress for IndexerProgress {}

#[async_trait::async_trait]
impl JobHandler for IndexerJob {
	type Output = IndexerOutput;

	async fn run(&mut self, ctx: JobContext<'_>) -> JobResult<Self::Output> {
		// Initialize timer
		if self.timer.is_none() {
			self.timer = Some(PhaseTimer::new());
		}

		// Initialize ephemeral index if needed
		if self.config.is_ephemeral() && self.ephemeral_index.is_none() {
			let root_path =
				self.config.path.as_local_path().ok_or_else(|| {
					JobError::execution("Path not accessible locally".to_string())
				})?;
			self.ephemeral_index = Some(Arc::new(RwLock::new(EphemeralIndex::new(
				root_path.to_path_buf(),
			))));
			ctx.log("Initialized ephemeral index for non-persistent job");
		}

		// Initialize or restore state
		let state = match &mut self.state {
			Some(state) => {
				ctx.log("Resuming indexer from saved state");
				state
			}
			None => {
				ctx.log(format!(
					"Starting new indexer job (scope: {}, persistence: {:?})",
					self.config.scope, self.config.persistence
				));
				self.state = Some(IndexerState::new(&self.config.path));
				self.state.as_mut().unwrap()
			}
		};

		// Get local path for operations
		let root_path =
			self.config.path.as_local_path().ok_or_else(|| {
				JobError::execution("Location root path is not local".to_string())
			})?;

		// Main state machine loop
		loop {
			ctx.check_interrupt().await?;

			let current_phase = state.phase.clone();
			match current_phase {
				Phase::Discovery => {
					// Use scope-aware discovery
					if self.config.is_current_scope() {
						Self::run_current_scope_discovery_static(state, &ctx, root_path).await?;
					} else {
						phases::run_discovery_phase(state, &ctx, root_path).await?;
					}

					// Track batch info
					self.batch_info.0 = state.entry_batches.len() as u64;
					self.batch_info.1 = state.entry_batches.iter().map(|b| b.len()).sum();

					// Start processing timer
					if let Some(timer) = &mut self.timer {
						timer.start_processing();
					}
				}

				Phase::Processing => {
					if self.config.is_ephemeral() {
						let ephemeral_index = self.ephemeral_index.clone().ok_or_else(|| {
							JobError::execution("Ephemeral index not initialized".to_string())
						})?;
						Self::run_ephemeral_processing_static(state, &ctx, ephemeral_index).await?;
					} else {
						phases::run_processing_phase(
							self.config
								.location_id
								.expect("Location ID required for persistent jobs"),
							state,
							&ctx,
							self.config.mode,
							root_path,
						)
						.await?;

						// Update DB operation counts
						self.db_operations.1 += state.entry_batches.len() as u64 * 100; // Estimate
					}
				}

				Phase::Aggregation => {
					if !self.config.is_ephemeral() {
						phases::run_aggregation_phase(
							self.config
								.location_id
								.expect("Location ID required for persistent jobs"),
							state,
							&ctx,
						)
						.await?;
					} else {
						// Skip aggregation for ephemeral jobs
						ctx.log("Skipping aggregation phase for ephemeral job");
						state.phase = Phase::ContentIdentification;
						continue;
					}

					// Start content timer
					if let Some(timer) = &mut self.timer {
						timer.start_content();
					}
				}

				Phase::ContentIdentification => {
					if self.config.mode >= IndexMode::Content {
						if self.config.is_ephemeral() {
							let ephemeral_index =
								self.ephemeral_index.clone().ok_or_else(|| {
									JobError::execution(
										"Ephemeral index not initialized".to_string(),
									)
								})?;
							Self::run_ephemeral_content_phase_static(state, &ctx, ephemeral_index)
								.await?;
						} else {
							let library_id = ctx.library().id();
							phases::run_content_phase(state, &ctx, library_id).await?;
							self.db_operations.1 += state.entries_for_content.len() as u64;
						}
					} else {
						ctx.log("Skipping content identification phase (mode=Shallow)");
						state.phase = Phase::Complete;
					}
				}

				Phase::Complete => break,
			}

			// Checkpoint after each phase (only for persistent jobs)
			if !self.config.is_ephemeral() {
				ctx.checkpoint().await?;
			}
		}

		// Send final progress update
		let final_progress = IndexerProgress {
			phase: IndexPhase::Finalizing,
			current_path: "Completed".to_string(),
			total_found: state.stats,
			processing_rate: 0.0,
			estimated_remaining: None,
			scope: None,
			persistence: None,
			is_ephemeral: false,
		};
		ctx.progress(Progress::generic(final_progress.to_generic_progress()));
		
		// Calculate final metrics
		let metrics = if let Some(timer) = &self.timer {
			IndexerMetrics::calculate(&state.stats, timer, self.db_operations, self.batch_info)
		} else {
			IndexerMetrics::default()
		};

		// Log summary
		ctx.log(&metrics.format_summary());

		// Generate final output
		Ok(IndexerOutput {
			location_id: self.config.location_id,
			stats: state.stats,
			duration: state.started_at.elapsed(),
			errors: state.errors.clone(),
			metrics: Some(metrics),
			ephemeral_results: if self.config.is_ephemeral() {
				self.ephemeral_index.clone()
			} else {
				None
			},
		})
	}

	async fn on_resume(&mut self, ctx: &JobContext<'_>) -> JobResult {
		// State is already loaded from serialization
		if let Some(state) = &self.state {
			ctx.log(format!("Resuming indexer in {:?} phase", state.phase));
			ctx.log(format!(
				"Progress: {} files, {} dirs, {} errors so far",
				state.stats.files, state.stats.dirs, state.stats.errors
			));

			// Reinitialize timer for resumed job
			self.timer = Some(PhaseTimer::new());
		}
		Ok(())
	}

	async fn on_pause(&mut self, ctx: &JobContext<'_>) -> JobResult {
		ctx.log("Pausing indexer job - state will be preserved");
		Ok(())
	}

	async fn on_cancel(&mut self, ctx: &JobContext<'_>) -> JobResult {
		ctx.log("Cancelling indexer job");
		if let Some(state) = &self.state {
			ctx.log(format!(
				"Final stats: {} files, {} dirs indexed before cancellation",
				state.stats.files, state.stats.dirs
			));
		}
		Ok(())
	}
}

impl IndexerJob {
	/// Create a new indexer job with enhanced configuration
	pub fn new(config: IndexerJobConfig) -> Self {
		Self {
			config,
			state: None,
			ephemeral_index: None,
			timer: None,
			db_operations: (0, 0),
			batch_info: (0, 0),
		}
	}

	/// Create a traditional persistent recursive indexer job
	pub fn from_location(location_id: Uuid, root_path: SdPath, mode: IndexMode) -> Self {
		Self::new(IndexerJobConfig::new(location_id, root_path, mode))
	}

	/// Create a shallow indexer job (metadata only)
	pub fn shallow(location_id: Uuid, root_path: SdPath) -> Self {
		Self::from_location(location_id, root_path, IndexMode::Shallow)
	}

	/// Create a content indexer job (with CAS IDs)
	pub fn with_content(location_id: Uuid, root_path: SdPath) -> Self {
		Self::from_location(location_id, root_path, IndexMode::Content)
	}

	/// Create a deep indexer job (full processing)
	pub fn deep(location_id: Uuid, root_path: SdPath) -> Self {
		Self::from_location(location_id, root_path, IndexMode::Deep)
	}

	/// Create a UI navigation job (current scope, quick scan)
	pub fn ui_navigation(location_id: Uuid, path: SdPath) -> Self {
		Self::new(IndexerJobConfig::ui_navigation(location_id, path))
	}

	/// Create an ephemeral browsing job (no database writes)
	pub fn ephemeral_browse(path: SdPath, scope: IndexScope) -> Self {
		Self::new(IndexerJobConfig::ephemeral_browse(path, scope))
	}

	/// Run current scope discovery (single level only)
	async fn run_current_scope_discovery_static(
		state: &mut IndexerState,
		ctx: &JobContext<'_>,
		root_path: &std::path::Path,
	) -> JobResult<()> {
		use super::entry::EntryProcessor;
		use super::state::{DirEntry, EntryKind};
		use tokio::fs;

		ctx.log("Starting current scope discovery (single level)");

		let mut entries = fs::read_dir(root_path)
			.await
			.map_err(|e| JobError::execution(format!("Failed to read directory: {}", e)))?;

		while let Some(entry) = entries
			.next_entry()
			.await
			.map_err(|e| JobError::execution(format!("Failed to read directory entry: {}", e)))?
		{
			let path = entry.path();
			let metadata = entry
				.metadata()
				.await
				.map_err(|e| JobError::execution(format!("Failed to read metadata: {}", e)))?;

			let entry_kind = if metadata.is_dir() {
				EntryKind::Directory
			} else if metadata.is_symlink() {
				EntryKind::Symlink
			} else {
				EntryKind::File
			};

			let dir_entry = DirEntry {
				path: path.clone(),
				kind: entry_kind,
				size: metadata.len(),
				modified: metadata.modified().ok(),
				inode: EntryProcessor::get_inode(&metadata),
			};

			state.pending_entries.push(dir_entry);
			state.items_since_last_update += 1;

			// Update stats
			match entry_kind {
				EntryKind::File => state.stats.files += 1,
				EntryKind::Directory => state.stats.dirs += 1,
				EntryKind::Symlink => state.stats.symlinks += 1,
			}
		}

		// Create single batch and move to processing
		if !state.pending_entries.is_empty() {
			let batch = state.create_batch();
			state.entry_batches.push(batch);
		}

		state.phase = Phase::Processing;
		ctx.log(format!(
			"Current scope discovery complete: {} entries found",
			state.stats.files + state.stats.dirs
		));

		Ok(())
	}

	/// Run ephemeral processing (store in memory instead of database)
	async fn run_ephemeral_processing_static(
		state: &mut IndexerState,
		ctx: &JobContext<'_>,
		ephemeral_index: Arc<RwLock<EphemeralIndex>>,
	) -> JobResult<()> {
		use super::entry::EntryProcessor;

		ctx.log("Starting ephemeral processing");

		for batch in &state.entry_batches {
			for entry in batch {
				// Extract metadata
				let metadata = EntryProcessor::extract_metadata(&entry.path)
					.await
					.map_err(|e| {
						JobError::execution(format!("Failed to extract metadata: {}", e))
					})?;

				// Store in ephemeral index
				{
					let mut index = ephemeral_index.write().await;
					index.add_entry(entry.path.clone(), metadata);
					index.stats = state.stats; // Update stats
				}
			}
		}

		state.entry_batches.clear();
		state.phase = Phase::ContentIdentification;

		ctx.log("Ephemeral processing complete");
		Ok(())
	}

	/// Run ephemeral content identification
	async fn run_ephemeral_content_phase_static(
		state: &mut IndexerState,
		ctx: &JobContext<'_>,
		_ephemeral_index: Arc<RwLock<EphemeralIndex>>,
	) -> JobResult<()> {
		ctx.log("Starting ephemeral content identification");

		// For ephemeral jobs, we can skip heavy content processing or do it lightly
		// This is mainly for demonstration - in practice you might generate CAS IDs

		state.phase = Phase::Complete;
		ctx.log("Ephemeral content identification complete");

		Ok(())
	}
}

/// Job output with comprehensive results
#[derive(Debug, Serialize, Deserialize)]
pub struct IndexerOutput {
	pub location_id: Option<Uuid>,
	pub stats: IndexerStats,
	pub duration: Duration,
	pub errors: Vec<IndexError>,
	pub metrics: Option<IndexerMetrics>,
	#[serde(skip)]
	pub ephemeral_results: Option<Arc<RwLock<EphemeralIndex>>>,
}

impl From<IndexerOutput> for JobOutput {
	fn from(output: IndexerOutput) -> Self {
		JobOutput::Indexed {
			total_files: output.stats.files,
			total_dirs: output.stats.dirs,
			total_bytes: output.stats.bytes,
		}
	}
}
```

## src/operations/indexing/change_detection/mod.rs

```rust
//! Change detection for incremental indexing
//!
//! This module provides efficient change detection using:
//! - Inode tracking for move/rename detection
//! - Modification time comparison
//! - Size verification
//! - Directory hierarchy tracking

use super::state::EntryKind;
use crate::infrastructure::{database::entities, jobs::prelude::JobContext};
use sea_orm::{ColumnTrait, EntityTrait, QueryFilter, QuerySelect};
use std::{
	collections::HashMap,
	path::{Path, PathBuf},
	time::SystemTime,
};

/// Represents a change detected in the file system
#[derive(Debug, Clone)]
pub enum Change {
	/// New file/directory not in database
	New(PathBuf),

	/// File/directory modified (content or metadata changed)
	Modified {
		path: PathBuf,
		entry_id: i32,
		old_modified: Option<SystemTime>,
		new_modified: Option<SystemTime>,
	},

	/// File/directory moved or renamed (same inode, different path)
	Moved {
		old_path: PathBuf,
		new_path: PathBuf,
		entry_id: i32,
		inode: u64,
	},

	/// File/directory deleted (exists in DB but not on disk)
	Deleted { path: PathBuf, entry_id: i32 },
}

/// Tracks changes between database state and file system
pub struct ChangeDetector {
	/// Maps paths to their database entries
	path_to_entry: HashMap<PathBuf, DatabaseEntry>,

	/// Maps inodes to paths (for detecting moves)
	inode_to_path: HashMap<u64, PathBuf>,

	/// Precision for timestamp comparison (some filesystems have lower precision)
	timestamp_precision_ms: i64,
}

#[derive(Debug, Clone)]
struct DatabaseEntry {
	id: i32,
	path: PathBuf,
	kind: EntryKind,
	size: u64,
	modified: Option<SystemTime>,
	inode: Option<u64>,
}

impl ChangeDetector {
	/// Create a new change detector
	pub fn new() -> Self {
		Self {
			path_to_entry: HashMap::new(),
			inode_to_path: HashMap::new(),
			timestamp_precision_ms: 1, // Default to 1ms precision
		}
	}

	/// Load existing entries from database for a location, scoped to indexing path
	pub async fn load_existing_entries(
		&mut self,
		ctx: &JobContext<'_>,
		location_id: i32,
		indexing_path: &Path,
	) -> Result<(), crate::infrastructure::jobs::prelude::JobError> {
		use crate::infrastructure::jobs::prelude::JobError;
		use super::persistence::{DatabasePersistence, IndexPersistence};

		// Create a database persistence instance to leverage the scoped query logic
		let persistence = DatabasePersistence::new(ctx, location_id, 0); // device_id not needed for query
		
		// Use the scoped query method
		let existing_entries = persistence.get_existing_entries(indexing_path).await?;
		
		// Process the results into our internal data structures
		for (full_path, (id, inode, modified_time)) in existing_entries {
			// Determine entry kind from the path (we could query this, but for change detection we mainly care about existence)
			// For now, we'll assume File for simplicity since change detection primarily cares about path/inode/timestamp
			let entry_kind = if full_path.is_dir() {
				EntryKind::Directory
			} else {
				EntryKind::File
			};

			// We don't have size from the scoped query, but it's not critical for change detection
			// The actual size comparison happens during processing when we have fresh metadata
			let db_entry = DatabaseEntry {
				id,
				path: full_path.clone(),
				kind: entry_kind,
				size: 0, // Will be verified during actual change detection
				modified: modified_time,
				inode,
			};

			// Track by path
			self.path_to_entry.insert(full_path.clone(), db_entry);

			// Track by inode if available
			if let Some(inode_val) = inode {
				self.inode_to_path.insert(inode_val, full_path);
			}
		}

		ctx.log(format!(
			"Loaded {} existing entries for change detection",
			self.path_to_entry.len()
		));

		Ok(())
	}

	/// Check if a path represents a change
	pub fn check_path(
		&self,
		path: &Path,
		metadata: &std::fs::Metadata,
		inode: Option<u64>,
	) -> Option<Change> {
		// Check if path exists in database
		if let Some(db_entry) = self.path_to_entry.get(path) {
			// Check for modifications
			if self.is_modified(db_entry, metadata) {
				return Some(Change::Modified {
					path: path.to_path_buf(),
					entry_id: db_entry.id,
					old_modified: db_entry.modified,
					new_modified: metadata.modified().ok(),
				});
			}

			// No change for this path
			return None;
		}

		// Path not in database - check if it's a move
		if let Some(inode_val) = inode {
			if let Some(old_path) = self.inode_to_path.get(&inode_val) {
				if old_path != path {
					// Same inode, different path - it's a move
					if let Some(db_entry) = self.path_to_entry.get(old_path) {
						return Some(Change::Moved {
							old_path: old_path.clone(),
							new_path: path.to_path_buf(),
							entry_id: db_entry.id,
							inode: inode_val,
						});
					}
				}
			}
		}

		// New file/directory
		Some(Change::New(path.to_path_buf()))
	}

	/// Find deleted entries (in DB but not seen during scan)
	pub fn find_deleted(&self, seen_paths: &std::collections::HashSet<PathBuf>) -> Vec<Change> {
		self.path_to_entry
			.iter()
			.filter(|(path, _)| !seen_paths.contains(*path))
			.map(|(path, entry)| Change::Deleted {
				path: path.clone(),
				entry_id: entry.id,
			})
			.collect()
	}

	/// Check if an entry has been modified
	fn is_modified(&self, db_entry: &DatabaseEntry, metadata: &std::fs::Metadata) -> bool {
		// Check size first (fast)
		if db_entry.size != metadata.len() {
			return true;
		}

		// Check modification time
		if let (Some(db_modified), Ok(fs_modified)) = (db_entry.modified, metadata.modified()) {
			// Compare with precision tolerance
			let db_time = db_modified
				.duration_since(SystemTime::UNIX_EPOCH)
				.unwrap_or_default()
				.as_millis() as i64;
			let fs_time = fs_modified
				.duration_since(SystemTime::UNIX_EPOCH)
				.unwrap_or_default()
				.as_millis() as i64;

			if (db_time - fs_time).abs() > self.timestamp_precision_ms {
				return true;
			}
		}

		false
	}

	/// Set timestamp precision for comparison (in milliseconds)
	pub fn set_timestamp_precision(&mut self, precision_ms: i64) {
		self.timestamp_precision_ms = precision_ms;
	}

	/// Get the number of tracked entries
	pub fn entry_count(&self) -> usize {
		self.path_to_entry.len()
	}
}

// #[cfg(test)]
// mod tests {
//     use super::*;

//     #[test]
//     fn test_change_detection() {
//         let mut detector = ChangeDetector::new();

//         // Add a test entry
//         let path = PathBuf::from("/test/file.txt");
//         let db_entry = DatabaseEntry {
//             id: 1,
//             path: path.clone(),
//             kind: EntryKind::File,
//             size: 1000,
//             modified: Some(SystemTime::now()),
//             inode: Some(12345),
//         };

//         detector.path_to_entry.insert(path.clone(), db_entry);
//         detector.inode_to_path.insert(12345, path.clone());

//         // Test new file detection
//         let new_path = PathBuf::from("/test/new_file.txt");
//         let metadata = std::fs::Metadata::default(); // Would use real metadata in practice

//         match detector.check_path(&new_path, &metadata, None) {
//             Some(Change::New(p)) => assert_eq!(p, new_path),
//             _ => panic!("Expected new file detection"),
//         }
//     }
// }
```

## src/operations/indexing/persistence.rs

```rust
//! Persistence abstraction layer for indexing operations
//!
//! This module provides a unified interface for storing indexing results
//! either persistently in the database or ephemerally in memory.

use crate::{
	file_type::FileTypeRegistry,
	infrastructure::{
		database::entities,
		jobs::prelude::{JobContext, JobError, JobResult},
	},
};
use sea_orm::{ActiveModelTrait, ActiveValue::Set};
use std::{collections::HashMap, path::Path, sync::Arc};
use tokio::sync::RwLock;
use uuid::Uuid;

use super::{
	job::{EphemeralContentIdentity, EphemeralIndex},
	state::{DirEntry, EntryKind},
};

/// Abstraction for storing indexing results
#[async_trait::async_trait]
pub trait IndexPersistence: Send + Sync {
	/// Store an entry and return its ID
	async fn store_entry(
		&self,
		entry: &DirEntry,
		location_id: Option<i32>,
		location_root_path: &Path,
	) -> JobResult<i32>;

	/// Store content identity and link to entry
	async fn store_content_identity(
		&self,
		entry_id: i32,
		path: &Path,
		cas_id: String,
	) -> JobResult<()>;

	/// Get existing entries for change detection, scoped to the indexing path
	async fn get_existing_entries(
		&self,
		indexing_path: &Path,
	) -> JobResult<HashMap<std::path::PathBuf, (i32, Option<u64>, Option<std::time::SystemTime>)>>;

	/// Update an existing entry
	async fn update_entry(&self, entry_id: i32, entry: &DirEntry) -> JobResult<()>;

	/// Check if this persistence layer supports operations
	fn is_persistent(&self) -> bool;
}

/// Database-backed persistence implementation
pub struct DatabasePersistence<'a> {
	ctx: &'a JobContext<'a>,
	location_id: i32,
	device_id: i32,
	entry_id_cache: Arc<RwLock<HashMap<std::path::PathBuf, i32>>>,
}

impl<'a> DatabasePersistence<'a> {
	pub fn new(ctx: &'a JobContext<'a>, location_id: i32, device_id: i32) -> Self {
		Self {
			ctx,
			location_id,
			device_id,
			entry_id_cache: Arc::new(RwLock::new(HashMap::new())),
		}
	}
}

#[async_trait::async_trait]
impl<'a> IndexPersistence for DatabasePersistence<'a> {
	async fn store_entry(
		&self,
		entry: &DirEntry,
		_location_id: Option<i32>,
		location_root_path: &Path,
	) -> JobResult<i32> {
		use super::entry::EntryProcessor;

		// Calculate relative directory path from location root (without filename)
		let relative_path = if let Ok(rel_path) = entry.path.strip_prefix(location_root_path) {
			// Get parent directory relative to location root
			if let Some(parent) = rel_path.parent() {
				if parent == std::path::Path::new("") {
					String::new()
				} else {
					parent.to_string_lossy().to_string()
				}
			} else {
				String::new()
			}
		} else {
			String::new()
		};

		// Extract file extension (without dot) for files, None for directories
		let extension = match entry.kind {
			EntryKind::File => entry
				.path
				.extension()
				.and_then(|ext| ext.to_str())
				.map(|ext| ext.to_lowercase()),
			EntryKind::Directory | EntryKind::Symlink => None,
		};

		// Get file name without extension (stem)
		let name = entry
			.path
			.file_stem()
			.map(|stem| stem.to_string_lossy().to_string())
			.unwrap_or_else(|| {
				entry
					.path
					.file_name()
					.map(|n| n.to_string_lossy().to_string())
					.unwrap_or_else(|| "unknown".to_string())
			});

		// Convert timestamps
		let modified_at = entry
			.modified
			.and_then(|t| {
				chrono::DateTime::from_timestamp(
					t.duration_since(std::time::UNIX_EPOCH).ok()?.as_secs() as i64,
					0,
				)
			})
			.unwrap_or_else(|| chrono::Utc::now());

		// Determine if UUID should be assigned immediately
		// - Directories: Assign UUID immediately (no content to identify)
		// - Empty files: Assign UUID immediately (size = 0, no content to hash)
		// - Regular files: Assign UUID after content identification completes
		let should_assign_uuid = entry.kind == EntryKind::Directory || entry.size == 0;
		let entry_uuid = if should_assign_uuid {
			Some(Uuid::new_v4())
		} else {
			None // Will be assigned during content identification phase
		};

		// Create entry
		let new_entry = entities::entry::ActiveModel {
			uuid: Set(entry_uuid),
			location_id: Set(self.location_id),
			relative_path: Set(relative_path),
			name: Set(name),
			kind: Set(EntryProcessor::entry_kind_to_int(entry.kind)),
			extension: Set(extension),
			metadata_id: Set(None), // User metadata only created when user adds metadata
			content_id: Set(None),  // Will be set later if content indexing is enabled
			size: Set(entry.size as i64),
			aggregate_size: Set(0), // Will be calculated in aggregation phase
			child_count: Set(0),    // Will be calculated in aggregation phase
			file_count: Set(0),     // Will be calculated in aggregation phase
			created_at: Set(chrono::Utc::now()),
			modified_at: Set(modified_at),
			accessed_at: Set(None),
			permissions: Set(None), // TODO: Could extract from metadata
			inode: Set(entry.inode.map(|i| i as i64)),
			..Default::default()
		};

		let result = new_entry
			.insert(self.ctx.library_db())
			.await
			.map_err(|e| JobError::execution(format!("Failed to create entry: {}", e)))?;

		// Cache the entry ID for potential children
		{
			let mut cache = self.entry_id_cache.write().await;
			cache.insert(entry.path.clone(), result.id);
		}

		Ok(result.id)
	}

	async fn store_content_identity(
		&self,
		entry_id: i32,
		path: &Path,
		cas_id: String,
	) -> JobResult<()> {
		use super::entry::EntryProcessor;

		// Use the library ID from the context
		let library_id = self.ctx.library().id();

		// Delegate to existing implementation with the library_id
		EntryProcessor::link_to_content_identity(self.ctx, entry_id, path, cas_id, library_id).await
	}

	async fn get_existing_entries(
		&self,
		indexing_path: &Path,
	) -> JobResult<HashMap<std::path::PathBuf, (i32, Option<u64>, Option<std::time::SystemTime>)>>
	{
		use sea_orm::{ColumnTrait, EntityTrait, QueryFilter};

		// Get location root to calculate relative path for the indexing scope
		let location_record = entities::location::Entity::find_by_id(self.location_id)
			.one(self.ctx.library_db())
			.await
			.map_err(|e| JobError::execution(format!("Failed to find location: {}", e)))?
			.ok_or_else(|| JobError::execution("Location not found".to_string()))?;

		// Parse the location path to determine the root
		let location_root = std::path::Path::new(&location_record.path);

		// Calculate the relative path being indexed
		let relative_indexing_path = if let Ok(rel_path) = indexing_path.strip_prefix(location_root)
		{
			if rel_path == std::path::Path::new("") {
				// Indexing entire location - use optimized query for root
				None
			} else {
				// Indexing subpath - scope the query
				Some(rel_path.to_string_lossy().to_string())
			}
		} else {
			return Err(JobError::execution(format!(
				"Indexing path {} is not within location root {}",
				indexing_path.display(),
				location_root.display()
			)));
		};

		// Build scoped query based on indexing path
		// NOTE: This query benefits from these indexes:
		// CREATE INDEX idx_entries_location_relative_path ON entries(location_id, relative_path);
		// CREATE INDEX idx_entries_location_path_prefix ON entries(location_id, relative_path varchar_pattern_ops); -- PostgreSQL
		let mut query = entities::entry::Entity::find()
			.filter(entities::entry::Column::LocationId.eq(self.location_id));

		// If indexing a subpath, filter entries to that subtree only
		if let Some(ref rel_path) = relative_indexing_path {
			// Include entries that are:
			// 1. Direct children: relative_path = rel_path
			// 2. Descendants: relative_path starts with rel_path + "/"
			query = query.filter(
				entities::entry::Column::RelativePath
					.eq(rel_path)
					.or(entities::entry::Column::RelativePath.like(format!("{}/%", rel_path)))
					.or(entities::entry::Column::RelativePath.like(format!("{}\\%", rel_path))), // Windows paths
			);
		}

		let existing_entries = query
			.all(self.ctx.library_db())
			.await
			.map_err(|e| JobError::execution(format!("Failed to query existing entries: {}", e)))?;

		let mut result = HashMap::new();

		// Log the scope for debugging
		if let Some(rel_path) = &relative_indexing_path {
			self.ctx.log(format!(
				"Loading {} existing entries for subpath: {}",
				existing_entries.len(),
				rel_path
			));
		} else {
			self.ctx.log(format!(
				"Loading {} existing entries for entire location",
				existing_entries.len()
			));
		}

		for entry in existing_entries {
			// Reconstruct full path from relative_path and name
			let full_path = if entry.relative_path.is_empty() {
				location_root.join(&entry.name)
			} else {
				location_root.join(&entry.relative_path).join(&entry.name)
			};

			// Convert timestamp to SystemTime for comparison
			let modified_time =
				entry
					.modified_at
					.timestamp()
					.try_into()
					.ok()
					.and_then(|secs: u64| {
						std::time::UNIX_EPOCH.checked_add(std::time::Duration::from_secs(secs))
					});

			result.insert(
				full_path,
				(entry.id, entry.inode.map(|i| i as u64), modified_time),
			);
		}

		Ok(result)
	}

	async fn update_entry(&self, entry_id: i32, entry: &DirEntry) -> JobResult<()> {
		use super::entry::EntryProcessor;

		// Delegate to existing implementation
		EntryProcessor::update_entry(self.ctx, entry_id, entry).await
	}

	fn is_persistent(&self) -> bool {
		true
	}
}

/// In-memory ephemeral persistence implementation
pub struct EphemeralPersistence {
	index: Arc<RwLock<EphemeralIndex>>,
	next_entry_id: Arc<RwLock<i32>>,
}

impl EphemeralPersistence {
	pub fn new(index: Arc<RwLock<EphemeralIndex>>) -> Self {
		Self {
			index,
			next_entry_id: Arc::new(RwLock::new(1)),
		}
	}

	async fn get_next_id(&self) -> i32 {
		let mut id = self.next_entry_id.write().await;
		let current = *id;
		*id += 1;
		current
	}
}

#[async_trait::async_trait]
impl IndexPersistence for EphemeralPersistence {
	async fn store_entry(
		&self,
		entry: &DirEntry,
		_location_id: Option<i32>,
		_location_root_path: &Path,
	) -> JobResult<i32> {
		use super::entry::EntryProcessor;

		// Extract full metadata
		let metadata = EntryProcessor::extract_metadata(&entry.path)
			.await
			.map_err(|e| JobError::execution(format!("Failed to extract metadata: {}", e)))?;

		// Store in ephemeral index
		{
			let mut index = self.index.write().await;
			index.add_entry(entry.path.clone(), metadata);

			// Update stats
			match entry.kind {
				EntryKind::File => index.stats.files += 1,
				EntryKind::Directory => index.stats.dirs += 1,
				EntryKind::Symlink => index.stats.symlinks += 1,
			}
			index.stats.bytes += entry.size;
		}

		Ok(self.get_next_id().await)
	}

	async fn store_content_identity(
		&self,
		_entry_id: i32,
		path: &Path,
		cas_id: String,
	) -> JobResult<()> {
		// Get file size
		let file_size = tokio::fs::metadata(path)
			.await
			.map(|m| m.len())
			.unwrap_or(0);

		// Detect file type using the file type registry
		let registry = FileTypeRegistry::default();
		let mime_type = if let Ok(result) = registry.identify(path).await {
			result.file_type.primary_mime_type().map(|s| s.to_string())
		} else {
			None
		};

		let content_identity = EphemeralContentIdentity {
			cas_id: cas_id.clone(),
			mime_type,
			file_size,
			entry_count: 1,
		};

		{
			let mut index = self.index.write().await;
			index.add_content_identity(cas_id, content_identity);
		}

		Ok(())
	}

	async fn get_existing_entries(
		&self,
		_indexing_path: &Path,
	) -> JobResult<HashMap<std::path::PathBuf, (i32, Option<u64>, Option<std::time::SystemTime>)>>
	{
		// Ephemeral persistence doesn't support change detection
		Ok(HashMap::new())
	}

	async fn update_entry(&self, _entry_id: i32, _entry: &DirEntry) -> JobResult<()> {
		// Updates not needed for ephemeral storage
		Ok(())
	}

	fn is_persistent(&self) -> bool {
		false
	}
}

/// Factory for creating appropriate persistence implementations
pub struct PersistenceFactory;

impl PersistenceFactory {
	/// Create a database persistence instance
	pub fn database<'a>(
		ctx: &'a JobContext<'a>,
		location_id: i32,
		device_id: i32,
	) -> Box<dyn IndexPersistence + 'a> {
		Box::new(DatabasePersistence::new(ctx, location_id, device_id))
	}

	/// Create an ephemeral persistence instance
	pub fn ephemeral(
		index: Arc<RwLock<EphemeralIndex>>,
	) -> Box<dyn IndexPersistence + Send + Sync> {
		Box::new(EphemeralPersistence::new(index))
	}
}
```

## src/operations/indexing/metrics.rs

```rust
//! Performance metrics and monitoring for the indexer

use std::time::{Duration, Instant};
use serde::{Deserialize, Serialize};

/// Comprehensive metrics for indexing operations
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct IndexerMetrics {
    // Timing
    pub total_duration: Duration,
    pub discovery_duration: Duration,
    pub processing_duration: Duration,
    pub content_duration: Duration,
    
    // Throughput
    pub files_per_second: f32,
    pub bytes_per_second: f64,
    pub dirs_per_second: f32,
    
    // Database operations
    pub db_writes: u64,
    pub db_reads: u64,
    pub batch_count: u64,
    pub avg_batch_size: f32,
    
    // Error tracking
    pub total_errors: u64,
    pub critical_errors: u64,
    pub non_critical_errors: u64,
    pub skipped_paths: u64,
    
    // Memory usage (if available)
    pub peak_memory_bytes: Option<u64>,
    pub avg_memory_bytes: Option<u64>,
}

impl Default for IndexerMetrics {
    fn default() -> Self {
        Self {
            total_duration: Duration::default(),
            discovery_duration: Duration::default(),
            processing_duration: Duration::default(),
            content_duration: Duration::default(),
            files_per_second: 0.0,
            bytes_per_second: 0.0,
            dirs_per_second: 0.0,
            db_writes: 0,
            db_reads: 0,
            batch_count: 0,
            avg_batch_size: 0.0,
            total_errors: 0,
            critical_errors: 0,
            non_critical_errors: 0,
            skipped_paths: 0,
            peak_memory_bytes: None,
            avg_memory_bytes: None,
        }
    }
}

/// Tracks timing for different phases
#[derive(Debug)]
pub struct PhaseTimer {
    phase_start: Instant,
    discovery_start: Option<Instant>,
    processing_start: Option<Instant>,
    content_start: Option<Instant>,
}

impl PhaseTimer {
    pub fn new() -> Self {
        Self {
            phase_start: Instant::now(),
            discovery_start: Some(Instant::now()),
            processing_start: None,
            content_start: None,
        }
    }
    
    pub fn start_processing(&mut self) {
        self.processing_start = Some(Instant::now());
    }
    
    pub fn start_content(&mut self) {
        self.content_start = Some(Instant::now());
    }
    
    pub fn get_durations(&self) -> (Duration, Duration, Duration, Duration) {
        let total = self.phase_start.elapsed();
        
        let discovery = self.discovery_start
            .and_then(|start| self.processing_start.map(|_| start.elapsed()))
            .unwrap_or_default();
            
        let processing = self.processing_start
            .and_then(|start| self.content_start.map(|_| start.elapsed()))
            .unwrap_or_default();
            
        let content = self.content_start
            .map(|start| start.elapsed())
            .unwrap_or_default();
        
        (total, discovery, processing, content)
    }
}

impl IndexerMetrics {
    /// Calculate final metrics from state and timer
    pub fn calculate(
        stats: &super::state::IndexerStats,
        timer: &PhaseTimer,
        db_operations: (u64, u64), // (reads, writes)
        batch_info: (u64, usize), // (count, total_size)
    ) -> Self {
        let (total, discovery, processing, content) = timer.get_durations();
        
        let total_secs = total.as_secs_f32();
        let (db_reads, db_writes) = db_operations;
        let (batch_count, total_batch_size) = batch_info;
        
        Self {
            total_duration: total,
            discovery_duration: discovery,
            processing_duration: processing,
            content_duration: content,
            
            files_per_second: if total_secs > 0.0 { stats.files as f32 / total_secs } else { 0.0 },
            bytes_per_second: if total_secs > 0.0 { stats.bytes as f64 / total_secs as f64 } else { 0.0 },
            dirs_per_second: if total_secs > 0.0 { stats.dirs as f32 / total_secs } else { 0.0 },
            
            db_writes,
            db_reads,
            batch_count,
            avg_batch_size: if batch_count > 0 { total_batch_size as f32 / batch_count as f32 } else { 0.0 },
            
            total_errors: stats.errors,
            critical_errors: 0, // TODO: Track separately
            non_critical_errors: stats.errors,
            skipped_paths: stats.skipped,
            
            peak_memory_bytes: None, // TODO: Implement memory tracking
            avg_memory_bytes: None,
        }
    }
    
    /// Format metrics for logging
    pub fn format_summary(&self) -> String {
        format!(
            "Indexing completed in {:.2}s:\n\
             - Files: {} ({:.1}/s)\n\
             - Directories: {} ({:.1}/s)\n\
             - Total size: {:.2} GB ({:.2} MB/s)\n\
             - Database writes: {} in {} batches (avg {:.1} items/batch)\n\
             - Errors: {} (skipped {} paths)\n\
             - Phase timing: discovery {:.1}s, processing {:.1}s, content {:.1}s",
            self.total_duration.as_secs_f32(),
            self.files_per_second * self.total_duration.as_secs_f32(),
            self.files_per_second,
            self.dirs_per_second * self.total_duration.as_secs_f32(),
            self.dirs_per_second,
            self.bytes_per_second * self.total_duration.as_secs_f64() / 1_073_741_824.0,
            self.bytes_per_second / 1_048_576.0,
            self.db_writes,
            self.batch_count,
            self.avg_batch_size,
            self.total_errors,
            self.skipped_paths,
            self.discovery_duration.as_secs_f32(),
            self.processing_duration.as_secs_f32(),
            self.content_duration.as_secs_f32(),
        )
    }
}```

## src/operations/indexing/mod.rs

```rust
//! Production-ready indexing system for Spacedrive
//! 
//! This module implements a sophisticated file indexing system with:
//! - Multi-phase processing (discovery, processing, content identification)
//! - Full resumability with checkpoint support
//! - Incremental indexing with change detection
//! - Efficient batch processing
//! - Comprehensive error handling
//! - Performance monitoring and metrics

pub mod action;
pub mod job;
pub mod state;
pub mod entry;
pub mod filters;
pub mod metrics;
pub mod phases;
pub mod progress;
pub mod change_detection;
pub mod persistence;

// Re-exports for convenience
pub use job::{
    IndexerJob, IndexMode, IndexScope, IndexPersistence, 
    IndexerJobConfig, EphemeralIndex, EphemeralContentIdentity,
    IndexerOutput
};
pub use state::{IndexerState, IndexerProgress, IndexPhase, IndexerStats};
pub use entry::{EntryProcessor, EntryMetadata};
pub use filters::should_skip_path;
pub use metrics::IndexerMetrics;
pub use persistence::{IndexPersistence as PersistenceTrait, PersistenceFactory};
pub use action::IndexingAction;

// Rules system will be integrated here in the future
// pub mod rules;```

## src/operations/indexing/state.rs

```rust
//! Indexer state management and progress tracking

use serde::{Deserialize, Serialize};
use std::{
    collections::{HashMap, HashSet, VecDeque},
    path::PathBuf,
    time::{Duration, Instant},
};
use crate::shared::types::SdPath;

/// Indexer progress information
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct IndexerProgress {
    pub phase: IndexPhase,
    pub current_path: String,
    pub total_found: IndexerStats,
    pub processing_rate: f32,
    pub estimated_remaining: Option<Duration>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub scope: Option<super::job::IndexScope>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub persistence: Option<super::job::IndexPersistence>,
    pub is_ephemeral: bool,
}

/// Statistics collected during indexing
#[derive(Debug, Clone, Copy, Default, Serialize, Deserialize)]
pub struct IndexerStats {
    pub files: u64,
    pub dirs: u64,
    pub bytes: u64,
    pub symlinks: u64,
    pub skipped: u64,
    pub errors: u64,
}

/// Current phase of the indexing operation
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum IndexPhase {
    Discovery { dirs_queued: usize },
    Processing { batch: usize, total_batches: usize },
    ContentIdentification { current: usize, total: usize },
    Finalizing,
}

/// Internal phases for state machine
#[derive(Debug, Clone, Serialize, Deserialize)]
pub(crate) enum Phase {
    Discovery,
    Processing,
    Aggregation,
    ContentIdentification,
    Complete,
}

/// Directory entry found during discovery
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DirEntry {
    pub path: PathBuf,
    pub kind: EntryKind,
    pub size: u64,
    pub modified: Option<std::time::SystemTime>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub inode: Option<u64>,
}

#[derive(Debug, Clone, Copy, PartialEq, Serialize, Deserialize)]
pub enum EntryKind {
    File,
    Directory,
    Symlink,
}

/// Errors that occur during indexing
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum IndexError {
    ReadDir { path: String, error: String },
    CreateEntry { path: String, error: String },
    ContentId { path: String, error: String },
    FilterCheck { path: String, error: String },
}

/// Resumable indexer state
#[derive(Debug, Serialize, Deserialize)]
pub struct IndexerState {
    pub(crate) phase: Phase,
    #[serde(skip, default = "Instant::now")]
    pub(crate) started_at: Instant,
    
    // Discovery phase
    pub(crate) dirs_to_walk: VecDeque<PathBuf>,
    pub(crate) pending_entries: Vec<DirEntry>,
    pub(crate) seen_paths: HashSet<PathBuf>,
    
    // Processing phase  
    pub(crate) entry_batches: Vec<Vec<DirEntry>>,
    
    // Content phase
    pub(crate) entries_for_content: Vec<(i32, PathBuf)>, // (entry_id, path)
    
    // Database operations
    pub(crate) entry_id_cache: HashMap<PathBuf, i32>, // path -> entry_id for parent lookups
    
    // Change detection
    pub(crate) existing_entries: HashMap<PathBuf, (i32, Option<u64>, Option<std::time::SystemTime>)>, // path -> (id, inode, modified)
    
    // Statistics
    pub(crate) stats: IndexerStats,
    pub(crate) errors: Vec<IndexError>,
    
    // Performance tracking
    #[serde(skip, default = "Instant::now")]
    pub(crate) last_progress_time: Instant,
    pub(crate) items_since_last_update: u64,
    
    // Configuration
    pub(crate) batch_size: usize,
}

impl IndexerState {
    pub fn new(root_path: &SdPath) -> Self {
        let mut dirs_to_walk = VecDeque::new();
        if let Some(path) = root_path.as_local_path() {
            dirs_to_walk.push_back(path.to_path_buf());
        }
        
        Self {
            phase: Phase::Discovery,
            started_at: Instant::now(),
            dirs_to_walk,
            pending_entries: Vec::new(),
            seen_paths: HashSet::new(),
            entry_batches: Vec::new(),
            entries_for_content: Vec::new(),
            entry_id_cache: HashMap::new(),
            existing_entries: HashMap::new(),
            stats: Default::default(),
            errors: Vec::new(),
            last_progress_time: Instant::now(),
            items_since_last_update: 0,
            batch_size: 1000,
        }
    }
    
    pub fn calculate_rate(&mut self) -> f32 {
        let elapsed = self.last_progress_time.elapsed();
        if elapsed.as_secs() > 0 {
            let rate = self.items_since_last_update as f32 / elapsed.as_secs_f32();
            self.last_progress_time = Instant::now();
            self.items_since_last_update = 0;
            rate
        } else {
            0.0
        }
    }
    
    pub fn estimate_remaining(&self) -> Option<Duration> {
        // TODO: Implement based on current rate and remaining work
        None
    }
    
    pub fn add_error(&mut self, error: IndexError) {
        self.stats.errors += 1;
        self.errors.push(error);
    }
    
    pub fn should_create_batch(&self) -> bool {
        self.pending_entries.len() >= self.batch_size
    }
    
    pub fn create_batch(&mut self) -> Vec<DirEntry> {
        std::mem::take(&mut self.pending_entries)
    }
}```

## src/operations/indexing/filters.rs

```rust
//! Filtering logic for the indexer
//! 
//! This module provides hardcoded filtering rules that will eventually
//! be replaced by the full indexer rules system.

use std::path::Path;

/// Common directories to skip during indexing
const SKIP_DIRECTORIES: &[&str] = &[
    // Development
    "node_modules",
    "target",
    "dist",
    "build",
    ".git",
    ".svn",
    ".hg",
    "__pycache__",
    ".pytest_cache",
    ".mypy_cache",
    ".tox",
    ".nox",
    ".coverage",
    ".hypothesis",
    
    // IDEs
    ".idea",
    ".vscode",
    ".vs",
    
    // OS specific
    "$RECYCLE.BIN",
    "System Volume Information",
    ".Trash",
    ".Trash-1000",
    
    // Package managers
    ".npm",
    ".yarn",
    ".pnpm-store",
    "bower_components",
    ".cargo",
    ".rustup",
    ".gradle",
    ".m2",
    
    // Cache directories
    ".cache",
    "Cache",
    "Caches",
    "CachedData",
    "Code Cache",
    
    // Temporary
    "tmp",
    "temp",
    ".tmp",
    ".temp",
];

/// Common files to skip during indexing
const SKIP_FILES: &[&str] = &[
    // OS specific
    ".DS_Store",
    "Thumbs.db",
    "desktop.ini",
    ".directory",
    ".Spotlight-V100",
    ".Trashes",
    ".fseventsd",
    ".TemporaryItems",
    "ehthumbs.db",
    "ehthumbs_vista.db",
    
    // Editor
    "*.swp",
    "*.swo",
    "*~",
    ".*.swp",
    ".*.swo",
    
    // Logs
    "*.log",
    "*.log.*",
    
    // Lock files
    "*.lock",
    "package-lock.json",
    "yarn.lock",
    "pnpm-lock.yaml",
    "Cargo.lock",
    "Gemfile.lock",
    "poetry.lock",
    "composer.lock",
];

/// File extensions to skip (without the dot)
const SKIP_EXTENSIONS: &[&str] = &[
    // Temporary
    "tmp",
    "temp",
    "cache",
    "bak",
    "backup",
    "old",
    
    // System
    "sys",
    "dll",
    "so",
    "dylib",
];

/// Determines if a path should be skipped during indexing
/// 
/// This is a temporary implementation that will be replaced by the
/// full indexer rules system. The rules system will allow users to
/// customize these patterns per location.
/// 
/// TODO: Replace with IndexerRuleEngine when rules system is implemented
pub fn should_skip_path(path: &Path) -> bool {
    // Get the file/directory name
    let Some(name) = path.file_name().and_then(|n| n.to_str()) else {
        return false;
    };
    
    // Skip hidden files/directories (Unix-style)
    if name.starts_with('.') && name != "." && name != ".." {
        // Allow .config and .local as they often contain user data
        if path.is_dir() && (name == ".config" || name == ".local") {
            return false;
        }
        return true;
    }
    
    // Check if it's a directory to skip
    if path.is_dir() && SKIP_DIRECTORIES.contains(&name) {
        return true;
    }
    
    // Check if it's a file to skip
    if path.is_file() {
        // Check exact filename matches
        if SKIP_FILES.contains(&name) {
            return true;
        }
        
        // Check patterns that end with wildcards
        for pattern in SKIP_FILES {
            if pattern.ends_with('*') {
                let prefix = &pattern[..pattern.len() - 1];
                if name.starts_with(prefix) {
                    return true;
                }
            } else if pattern.starts_with('*') {
                let suffix = &pattern[1..];
                if name.ends_with(suffix) {
                    return true;
                }
            }
        }
        
        // Check extensions
        if let Some(ext) = path.extension().and_then(|e| e.to_str()) {
            if SKIP_EXTENSIONS.contains(&ext) {
                return true;
            }
        }
    }
    
    // Check file size (skip files over 4GB for now)
    if path.is_file() {
        if let Ok(metadata) = std::fs::metadata(path) {
            if metadata.len() > 4 * 1024 * 1024 * 1024 {
                return true;
            }
        }
    }
    
    false
}

/// Additional filtering context that can be used for more sophisticated filtering
pub struct FilterContext {
    pub is_git_repo: bool,
    pub has_gitignore: bool,
    pub parent_ignored: bool,
}

impl Default for FilterContext {
    fn default() -> Self {
        Self {
            is_git_repo: false,
            has_gitignore: false,
            parent_ignored: false,
        }
    }
}

/// Future integration point for the rules engine
/// 
/// When the rules system is implemented, this function will delegate to it:
/// ```ignore
/// pub fn should_skip_path_with_rules(
///     path: &Path,
///     rule_engine: &IndexerRuleEngine,
///     context: &FilterContext,
/// ) -> RuleDecision {
///     rule_engine.evaluate(path, context)
/// }
/// ```

#[cfg(test)]
mod tests {
    use super::*;
    use std::path::PathBuf;
    
    #[test]
    fn test_skip_hidden_files() {
        assert!(should_skip_path(Path::new(".hidden")));
        assert!(should_skip_path(Path::new(".DS_Store")));
        assert!(!should_skip_path(Path::new("normal_file.txt")));
    }
    
    #[test]
    fn test_skip_directories() {
        assert!(should_skip_path(Path::new("node_modules")));
        assert!(should_skip_path(Path::new("target")));
        assert!(should_skip_path(Path::new(".git")));
        assert!(!should_skip_path(Path::new("src")));
    }
    
    #[test]
    fn test_skip_system_files() {
        assert!(should_skip_path(Path::new("Thumbs.db")));
        assert!(should_skip_path(Path::new("desktop.ini")));
    }
    
    #[test]
    fn test_allow_config_dirs() {
        let config = PathBuf::from(".config");
        assert!(!should_skip_path(&config));
        
        let local = PathBuf::from(".local");
        assert!(!should_skip_path(&local));
    }
}```

## src/operations/indexing/progress.rs

```rust
//! IndexerProgress to GenericProgress conversion

use super::state::{IndexPhase, IndexerProgress};
use crate::{
	infrastructure::jobs::generic_progress::{GenericProgress, ToGenericProgress},
	shared::types::SdPath,
};
use std::path::PathBuf;

impl ToGenericProgress for IndexerProgress {
	fn to_generic_progress(&self) -> GenericProgress {
		let (percentage, completion_info, phase_name) = match &self.phase {
			IndexPhase::Discovery { dirs_queued } => {
				// Discovery phase - 0-20% range
				let _message =
					format!("Discovering files and directories ({} queued)", dirs_queued);
				// Start at 5% to show immediate progress
				let percentage = if *dirs_queued > 0 { 0.05 } else { 0.1 };
				(percentage, (0, 0), "Discovery".to_string())
			}
			IndexPhase::Processing {
				batch,
				total_batches,
			} => {
				// Processing phase - show batch progress (20-60% of total)
				let batch_progress = if *total_batches > 0 {
					*batch as f32 / *total_batches as f32
				} else {
					0.0
				};
				// Map to 20-60% range
				let percentage = 0.2 + (batch_progress * 0.4);
				let _message = format!("Processing entries (batch {}/{})", batch, total_batches);
				(
					percentage,
					(*batch as u64, *total_batches as u64),
					"Processing".to_string(),
				)
			}
			IndexPhase::ContentIdentification { current, total } => {
				// Content ID phase - show item progress (70-98% of total)
				let content_progress = if *total > 0 {
					(*current as f32 / *total as f32).min(1.0)
				} else {
					0.0
				};
				// Map to 70-98% range, never reach 100% in this phase
				let percentage = 0.7 + (content_progress * 0.28);
				let _message = format!("Generating content identities ({}/{})", current, total);
				(
					percentage,
					(*current as u64, *total as u64),
					"Content Identification".to_string(),
				)
			}
			IndexPhase::Finalizing => {
				// Final phase - 99% (reserve 100% for actual completion)
				let _message = "Finalizing index data...".to_string();
				(0.99, (0, 0), "Finalizing".to_string())
			}
		};

		// Convert current_path string to SdPath if possible
		let current_path = if !self.current_path.is_empty() {
			// For now, create a simple SdPath - this would need proper device UUID in real implementation
			Some(SdPath::new(
				uuid::Uuid::nil(), // TODO: Get actual device UUID
				PathBuf::from(&self.current_path),
			))
		} else {
			None
		};

		// Create the generic progress
		let mut progress = GenericProgress::new(percentage, &phase_name, &self.current_path)
			.with_completion(completion_info.0, completion_info.1)
			.with_bytes(self.total_found.bytes, self.total_found.bytes) // Total bytes found so far
			.with_performance(
				self.processing_rate,
				self.estimated_remaining,
				None, // Could calculate elapsed time from start
			)
			.with_errors(self.total_found.errors, 0) // No separate warning count in IndexerStats
			.with_metadata(self); // Include original indexer progress as metadata

		// Set current path if available
		if let Some(path) = current_path {
			progress = progress.with_current_path(path);
		}

		progress
	}
}

#[cfg(test)]
mod tests {
	use super::*;
	use crate::operations::indexing::state::{IndexPhase, IndexerStats};
	use std::time::Duration;

	#[test]
	fn test_discovery_phase_conversion() {
		let indexer_progress = IndexerProgress {
			phase: IndexPhase::Discovery { dirs_queued: 42 },
			current_path: "/home/user/documents".to_string(),
			total_found: IndexerStats::default(),
			processing_rate: 0.0,
			estimated_remaining: None,
			scope: None,
			persistence: None,
			is_ephemeral: false,
		};

		let generic = indexer_progress.to_generic_progress();
		assert_eq!(generic.phase, "Discovery");
		assert_eq!(generic.percentage, 0.0);
		assert!(generic.message.contains("42 queued"));
	}

	#[test]
	fn test_processing_phase_conversion() {
		let indexer_progress = IndexerProgress {
			phase: IndexPhase::Processing {
				batch: 3,
				total_batches: 10,
			},
			current_path: "/home/user/photos".to_string(),
			total_found: IndexerStats {
				files: 150,
				dirs: 20,
				bytes: 1024 * 1024 * 500, // 500MB
				symlinks: 5,
				skipped: 2,
				errors: 1,
			},
			processing_rate: 25.5,
			estimated_remaining: Some(Duration::from_secs(120)),
			scope: None,
			persistence: None,
			is_ephemeral: false,
		};

		let generic = indexer_progress.to_generic_progress();
		assert_eq!(generic.phase, "Processing");
		assert_eq!(generic.percentage, 0.3); // 3/10
		assert_eq!(generic.completion.completed, 3);
		assert_eq!(generic.completion.total, 10);
		assert_eq!(generic.performance.rate, 25.5);
		assert_eq!(
			generic.performance.estimated_remaining,
			Some(Duration::from_secs(120))
		);
		assert_eq!(generic.performance.error_count, 1);
	}

	#[test]
	fn test_content_identification_conversion() {
		let indexer_progress = IndexerProgress {
			phase: IndexPhase::ContentIdentification {
				current: 75,
				total: 100,
			},
			current_path: "/home/user/videos/movie.mp4".to_string(),
			total_found: IndexerStats::default(),
			processing_rate: 12.0,
			estimated_remaining: Some(Duration::from_secs(30)),
			scope: None,
			persistence: None,
			is_ephemeral: false,
		};

		let generic = indexer_progress.to_generic_progress();
		assert_eq!(generic.phase, "Content Identification");
		assert_eq!(generic.percentage, 0.75); // 75/100
		assert_eq!(generic.completion.completed, 75);
		assert_eq!(generic.completion.total, 100);
	}

	#[test]
	fn test_finalizing_phase_conversion() {
		let indexer_progress = IndexerProgress {
			phase: IndexPhase::Finalizing,
			current_path: "Aggregating directory data...".to_string(),
			total_found: IndexerStats::default(),
			processing_rate: 0.0,
			estimated_remaining: Some(Duration::from_secs(5)),
			scope: None,
			persistence: None,
			is_ephemeral: false,
		};

		let generic = indexer_progress.to_generic_progress();
		assert_eq!(generic.phase, "Finalizing");
		assert_eq!(generic.percentage, 0.95); // Nearly complete
	}
}
```

## src/operations/indexing/entry.rs

```rust
//! Entry processing and metadata extraction

use super::state::{DirEntry, EntryKind, IndexerState};
use crate::{
	file_type::FileTypeRegistry,
	infrastructure::{
		database::entities::{self},
		jobs::prelude::{JobContext, JobError},
	},
};
use sea_orm::{ActiveModelTrait, ActiveValue::Set, ColumnTrait, EntityTrait, QueryFilter};
use std::path::{Path, PathBuf};
use uuid::Uuid;

/// Metadata about a file system entry
#[derive(Debug, Clone)]
pub struct EntryMetadata {
	pub path: PathBuf,
	pub kind: EntryKind,
	pub size: u64,
	pub modified: Option<std::time::SystemTime>,
	pub accessed: Option<std::time::SystemTime>,
	pub created: Option<std::time::SystemTime>,
	pub inode: Option<u64>,
	pub permissions: Option<u32>,
	pub is_hidden: bool,
}

impl From<DirEntry> for EntryMetadata {
	fn from(entry: DirEntry) -> Self {
		Self {
			path: entry.path.clone(),
			kind: entry.kind,
			size: entry.size,
			modified: entry.modified,
			accessed: None,
			created: None,
			inode: entry.inode,
			permissions: None,
			is_hidden: entry
				.path
				.file_name()
				.and_then(|n| n.to_str())
				.map(|n| n.starts_with('.'))
				.unwrap_or(false),
		}
	}
}

/// Handles entry creation and updates in the database
pub struct EntryProcessor;

impl EntryProcessor {
	/// Get platform-specific inode
	#[cfg(unix)]
	pub fn get_inode(metadata: &std::fs::Metadata) -> Option<u64> {
		use std::os::unix::fs::MetadataExt;
		Some(metadata.ino())
	}

	#[cfg(windows)]
	pub fn get_inode(metadata: &std::fs::Metadata) -> Option<u64> {
		// Windows doesn't have inodes, but we can use file index
		use std::os::windows::fs::MetadataExt;
		Some(metadata.file_index().unwrap_or(0))
	}

	#[cfg(not(any(unix, windows)))]
	pub fn get_inode(_metadata: &std::fs::Metadata) -> Option<u64> {
		None
	}

	/// Extract detailed metadata from a path
	pub async fn extract_metadata(path: &Path) -> Result<EntryMetadata, std::io::Error> {
		let metadata = tokio::fs::metadata(path).await?;

		let kind = if metadata.is_dir() {
			EntryKind::Directory
		} else if metadata.is_symlink() {
			EntryKind::Symlink
		} else {
			EntryKind::File
		};

		let inode = Self::get_inode(&metadata);

		#[cfg(unix)]
		let permissions = {
			use std::os::unix::fs::MetadataExt;
			Some(metadata.mode())
		};

		#[cfg(not(unix))]
		let permissions = None;

		Ok(EntryMetadata {
			path: path.to_path_buf(),
			kind,
			size: metadata.len(),
			modified: metadata.modified().ok(),
			accessed: metadata.accessed().ok(),
			created: metadata.created().ok(),
			inode,
			permissions,
			is_hidden: path
				.file_name()
				.and_then(|n| n.to_str())
				.map(|n| n.starts_with('.'))
				.unwrap_or(false),
		})
	}

	/// Create an entry record in the database
	pub async fn create_entry(
		state: &mut IndexerState,
		ctx: &JobContext<'_>,
		entry: &DirEntry,
		location_id: i32,
		device_id: i32,
		location_root_path: &Path,
	) -> Result<i32, JobError> {
		// Calculate relative directory path from location root (without filename)
		let relative_path = if let Ok(rel_path) = entry.path.strip_prefix(location_root_path) {
			// Get parent directory relative to location root
			if let Some(parent) = rel_path.parent() {
				if parent == std::path::Path::new("") {
					String::new()
				} else {
					parent.to_string_lossy().to_string()
				}
			} else {
				String::new()
			}
		} else {
			String::new()
		};

		// Extract file extension (without dot) for files, None for directories
		let extension = match entry.kind {
			EntryKind::File => entry
				.path
				.extension()
				.and_then(|ext| ext.to_str())
				.map(|ext| ext.to_lowercase()),
			EntryKind::Directory | EntryKind::Symlink => None,
		};

		// Get file name without extension (stem)
		let name = entry
			.path
			.file_stem()
			.map(|stem| stem.to_string_lossy().to_string())
			.unwrap_or_else(|| {
				entry
					.path
					.file_name()
					.map(|n| n.to_string_lossy().to_string())
					.unwrap_or_else(|| "unknown".to_string())
			});

		// Convert timestamps
		let modified_at = entry
			.modified
			.and_then(|t| {
				chrono::DateTime::from_timestamp(
					t.duration_since(std::time::UNIX_EPOCH).ok()?.as_secs() as i64,
					0,
				)
			})
			.unwrap_or_else(|| chrono::Utc::now());

		// Determine if UUID should be assigned immediately
		// - Directories: Assign UUID immediately (no content to identify)
		// - Empty files: Assign UUID immediately (size = 0, no content to hash)
		// - Regular files: Assign UUID after content identification completes
		let should_assign_uuid = entry.kind == EntryKind::Directory || entry.size == 0;
		let entry_uuid = if should_assign_uuid {
			Some(Uuid::new_v4())
		} else {
			None // Will be assigned during content identification phase
		};

		// Create entry
		let new_entry = entities::entry::ActiveModel {
			uuid: Set(entry_uuid),
			location_id: Set(location_id),
			relative_path: Set(relative_path),
			name: Set(name),
			kind: Set(Self::entry_kind_to_int(entry.kind)),
			extension: Set(extension),
			metadata_id: Set(None), // User metadata only created when user adds metadata
			content_id: Set(None),  // Will be set later during content identification phase
			size: Set(entry.size as i64),
			aggregate_size: Set(0), // Will be calculated in aggregation phase
			child_count: Set(0),    // Will be calculated in aggregation phase
			file_count: Set(0),     // Will be calculated in aggregation phase
			created_at: Set(chrono::Utc::now()),
			modified_at: Set(modified_at),
			accessed_at: Set(None),
			permissions: Set(None), // TODO: Could extract from metadata
			inode: Set(entry.inode.map(|i| i as i64)),
			..Default::default()
		};

		let result = new_entry
			.insert(ctx.library_db())
			.await
			.map_err(|e| JobError::execution(format!("Failed to create entry: {}", e)))?;

		// Cache the entry ID for potential children
		state.entry_id_cache.insert(entry.path.clone(), result.id);

		Ok(result.id)
	}

	/// Update an existing entry
	pub async fn update_entry(
		ctx: &JobContext<'_>,
		entry_id: i32,
		entry: &DirEntry,
	) -> Result<(), JobError> {
		let db_entry = entities::entry::Entity::find_by_id(entry_id)
			.one(ctx.library_db())
			.await
			.map_err(|e| JobError::execution(format!("Failed to find entry: {}", e)))?
			.ok_or_else(|| JobError::execution("Entry not found for update".to_string()))?;

		let mut entry_active: entities::entry::ActiveModel = db_entry.into();

		// Update modifiable fields
		entry_active.size = Set(entry.size as i64);
		if let Some(modified) = entry.modified {
			if let Some(timestamp) = chrono::DateTime::from_timestamp(
				modified
					.duration_since(std::time::UNIX_EPOCH)
					.ok()
					.map(|d| d.as_secs() as i64)
					.unwrap_or(0),
				0,
			) {
				entry_active.modified_at = Set(timestamp);
			}
		}

		if let Some(inode) = entry.inode {
			entry_active.inode = Set(Some(inode as i64));
		}

		entry_active
			.update(ctx.library_db())
			.await
			.map_err(|e| JobError::execution(format!("Failed to update entry: {}", e)))?;

		Ok(())
	}

	/// Convert EntryKind to integer for database storage
	pub fn entry_kind_to_int(kind: EntryKind) -> i32 {
		match kind {
			EntryKind::File => 0,
			EntryKind::Directory => 1,
			EntryKind::Symlink => 2,
		}
	}

	/// Create or find content identity and link to entry with deterministic UUID
	/// This method implements the content identification phase logic
	pub async fn link_to_content_identity(
		ctx: &JobContext<'_>,
		entry_id: i32,
		path: &Path,
		content_hash: String,
		library_id: Uuid,
	) -> Result<(), JobError> {
		// Check if content identity already exists by content_hash
		let existing = entities::content_identity::Entity::find()
			.filter(entities::content_identity::Column::ContentHash.eq(&content_hash))
			.one(ctx.library_db())
			.await
			.map_err(|e| JobError::execution(format!("Failed to query content identity: {}", e)))?;

		let content_id = if let Some(existing) = existing {
			// Increment entry count for existing content
			let existing_id = existing.id;
			let mut existing_active: entities::content_identity::ActiveModel = existing.into();
			existing_active.entry_count = Set(existing_active.entry_count.unwrap() + 1);
			existing_active.last_verified_at = Set(chrono::Utc::now());

			existing_active
				.update(ctx.library_db())
				.await
				.map_err(|e| {
					JobError::execution(format!("Failed to update content identity: {}", e))
				})?;

			existing_id
		} else {
			// Create new content identity with deterministic UUID (ready for sync)
			let file_size = tokio::fs::metadata(path)
				.await
				.map(|m| m.len() as i64)
				.unwrap_or(0);

			// Generate deterministic UUID from content_hash + library_id
			let deterministic_uuid = {
				const LIBRARY_NAMESPACE: uuid::Uuid = uuid::Uuid::from_bytes([
					0x6b, 0xa7, 0xb8, 0x10, 0x9d, 0xad, 0x11, 0xd1, 0x80, 0xb4, 0x00, 0xc0, 0x4f,
					0xd4, 0x30, 0xc8,
				]);
				// We use v5 to ensure the UUID is deterministic and unique within the library
				let namespace = uuid::Uuid::new_v5(&LIBRARY_NAMESPACE, library_id.as_bytes());
				uuid::Uuid::new_v5(&namespace, content_hash.as_bytes())
			};

			// Detect file type using the file type registry
			let registry = FileTypeRegistry::default();
			let file_type_result = registry.identify(path).await;

			let (kind_id, mime_type_id) = match file_type_result {
				Ok(result) => {
					// Get content kind ID directly from the enum
					let kind_id = result.file_type.category as i32;

					// Handle MIME type - upsert if found
					let mime_type_id = if let Some(mime_str) = result.file_type.primary_mime_type()
					{
						// Check if MIME type already exists
						let existing = entities::mime_type::Entity::find()
							.filter(entities::mime_type::Column::MimeType.eq(mime_str))
							.one(ctx.library_db())
							.await
							.map_err(|e| {
								JobError::execution(format!("Failed to query mime type: {}", e))
							})?;

						match existing {
							Some(mime_record) => Some(mime_record.id),
							None => {
								// Create new MIME type entry
								let new_mime = entities::mime_type::ActiveModel {
									uuid: Set(Uuid::new_v4()),
									mime_type: Set(mime_str.to_string()),
									created_at: Set(chrono::Utc::now()),
									..Default::default()
								};

								let mime_result =
									new_mime.insert(ctx.library_db()).await.map_err(|e| {
										JobError::execution(format!(
											"Failed to create mime type: {}",
											e
										))
									})?;

								Some(mime_result.id)
							}
						}
					} else {
						None
					};

					(kind_id, mime_type_id)
				}
				Err(_) => {
					// If identification fails, fall back to "unknown" (0)
					(0, None)
				}
			};

			let new_content = entities::content_identity::ActiveModel {
				uuid: Set(Some(deterministic_uuid)), // Deterministic UUID for sync
				integrity_hash: Set(None),           // Generated later by validate job
				content_hash: Set(content_hash.clone()),
				mime_type_id: Set(mime_type_id),
				kind_id: Set(kind_id),
				media_data: Set(None),   // Set during media analysis
				text_content: Set(None), // TODO: Extract text content for indexing
				total_size: Set(file_size),
				entry_count: Set(1),
				first_seen_at: Set(chrono::Utc::now()),
				last_verified_at: Set(chrono::Utc::now()),
				..Default::default()
			};

			// Try to insert, but handle unique constraint violations
			let result = match new_content.insert(ctx.library_db()).await {
				Ok(model) => model,
				Err(e) => {
					// Check if it's a unique constraint violation
					if e.to_string().contains("UNIQUE constraint failed") {
						// Another job created it - find and use the existing one
						let existing = entities::content_identity::Entity::find()
							.filter(entities::content_identity::Column::ContentHash.eq(&content_hash))
							.one(ctx.library_db())
							.await
							.map_err(|e| JobError::execution(format!("Failed to find existing content identity: {}", e)))?
							.ok_or_else(|| JobError::execution("Content identity should exist after unique constraint violation".to_string()))?;
						
						// Update entry count
						let mut existing_active: entities::content_identity::ActiveModel = existing.clone().into();
						existing_active.entry_count = Set(existing.entry_count + 1);
						existing_active.last_verified_at = Set(chrono::Utc::now());
						
						existing_active
							.update(ctx.library_db())
							.await
							.map_err(|e| JobError::execution(format!("Failed to update content identity: {}", e)))?;
						
						existing
					} else {
						return Err(JobError::execution(format!("Failed to create content identity: {}", e)));
					}
				}
			};

			result.id
		};

		// Update Entry with content_id AND assign UUID (now ready for sync)
		let entry = entities::entry::Entity::find_by_id(entry_id)
			.one(ctx.library_db())
			.await
			.map_err(|e| JobError::execution(format!("Failed to find entry: {}", e)))?
			.ok_or_else(|| JobError::execution("Entry not found after creation".to_string()))?;

		let mut entry_active: entities::entry::ActiveModel = entry.into();
		entry_active.content_id = Set(Some(content_id));

		// Assign UUID if not already assigned (Entry now ready for sync)
		if let Set(None) = entry_active.uuid {
			entry_active.uuid = Set(Some(Uuid::new_v4()));
		}

		entry_active.update(ctx.library_db()).await.map_err(|e| {
			JobError::execution(format!("Failed to link content identity to entry: {}", e))
		})?;

		Ok(())
	}
}
```

## src/operations/indexing/phases/aggregation.rs

```rust
//! Directory size aggregation phase

use crate::{
    infrastructure::{
        jobs::prelude::{JobContext, JobError, Progress},
        jobs::generic_progress::ToGenericProgress,
        database::entities,
    },
    operations::indexing::{
        state::{IndexerState, IndexPhase, Phase, IndexerProgress},
    },
};
use sea_orm::{EntityTrait, QueryFilter, ColumnTrait, QueryOrder, DbErr, DatabaseConnection, ActiveModelTrait, ActiveValue::Set};
use std::collections::HashMap;
use uuid::Uuid;

/// Run the directory aggregation phase
pub async fn run_aggregation_phase(
    location_id: Uuid,
    state: &mut IndexerState,
    ctx: &JobContext<'_>,
) -> Result<(), JobError> {
    ctx.log("Starting directory size aggregation phase");
    
    // Get the location record
    let location_record = entities::location::Entity::find()
        .filter(entities::location::Column::Uuid.eq(location_id))
        .one(ctx.library_db())
        .await
        .map_err(|e| JobError::execution(format!("Failed to find location: {}", e)))?
        .ok_or_else(|| JobError::execution("Location not found in database".to_string()))?;
    
    let location_id_i32 = location_record.id;
    
    // Find all directories in this location, ordered by path depth (deepest first)
    let mut directories = entities::entry::Entity::find()
        .filter(entities::entry::Column::LocationId.eq(location_id_i32))
        .filter(entities::entry::Column::Kind.eq(1)) // Directory
        .all(ctx.library_db())
        .await
        .map_err(|e| JobError::execution(format!("Failed to query directories: {}", e)))?;
    
    // Sort by path depth (deepest first) to ensure we process children before parents
    directories.sort_by(|a, b| {
        let a_depth = a.relative_path.matches('/').count() + 1;
        let b_depth = b.relative_path.matches('/').count() + 1;
        b_depth.cmp(&a_depth)
    });
    
    let total_dirs = directories.len();
    ctx.log(format!("Found {} directories to aggregate", total_dirs));
    
    // Process directories from leaves to root
    let mut processed = 0;
    let aggregator = DirectoryAggregator::new(ctx.library_db().clone());
    
    for directory in directories {
        ctx.check_interrupt().await?;
        
        processed += 1;
        let indexer_progress = IndexerProgress {
            phase: IndexPhase::Finalizing,
            current_path: format!("Aggregating directory {}/{}: {}", processed, total_dirs, directory.name),
            total_found: state.stats,
            processing_rate: state.calculate_rate(),
            estimated_remaining: state.estimate_remaining(),
            scope: None,
            persistence: None,
            is_ephemeral: false,
        };
        ctx.progress(Progress::generic(indexer_progress.to_generic_progress()));
        
        // Calculate aggregate values for this directory
        match aggregator.aggregate_directory(&directory).await {
            Ok((aggregate_size, child_count, file_count)) => {
                // Update the directory entry
                let directory_name = directory.name.clone();
                let mut active_dir: entities::entry::ActiveModel = directory.into();
                active_dir.aggregate_size = Set(aggregate_size);
                active_dir.child_count = Set(child_count);
                active_dir.file_count = Set(file_count);
                
                active_dir.update(ctx.library_db()).await
                    .map_err(|e| JobError::execution(format!("Failed to update directory aggregates: {}", e)))?;
                
                ctx.log(format!("âœ… Aggregated {}: {} bytes, {} children, {} files", 
                    directory_name, aggregate_size, child_count, file_count));
            }
            Err(e) => {
                ctx.add_non_critical_error(format!("Failed to aggregate directory {}: {}", directory.name, e));
            }
        }
        
        // Checkpoint periodically
        if processed % 100 == 0 {
            ctx.checkpoint_with_state(state).await?;
        }
    }
    
    ctx.log(format!("Directory aggregation complete: {} directories processed", processed));
    state.phase = Phase::ContentIdentification;
    Ok(())
}

struct DirectoryAggregator {
    db: DatabaseConnection,
}

impl DirectoryAggregator {
    fn new(db: DatabaseConnection) -> Self {
        Self { db }
    }
    
    /// Calculate aggregate size, child count, and file count for a directory
    async fn aggregate_directory(&self, directory: &entities::entry::Model) -> Result<(i64, i32, i32), DbErr> {
        // Build the path for children of this directory
        let children_path = if directory.relative_path.is_empty() {
            directory.name.clone()
        } else {
            format!("{}/{}", directory.relative_path, directory.name)
        };
        
        // Get all direct children (entries whose relative_path equals this directory's full path)
        let children = entities::entry::Entity::find()
            .filter(entities::entry::Column::LocationId.eq(directory.location_id))
            .filter(entities::entry::Column::RelativePath.eq(&children_path))
            .all(&self.db)
            .await?;
        
        let mut aggregate_size = 0i64;
        let child_count = children.len() as i32;
        let mut file_count = 0i32;
        
        for child in children {
            match child.kind {
                0 => { // File
                    aggregate_size += child.size;
                    file_count += 1;
                }
                1 => { // Directory
                    aggregate_size += child.aggregate_size;
                    file_count += child.file_count;
                }
                2 => { // Symlink - count as file
                    aggregate_size += child.size;
                    file_count += 1;
                }
                _ => {} // Unknown type, skip
            }
        }
        
        Ok((aggregate_size, child_count, file_count))
    }
}

/// One-time migration to calculate all directory sizes for existing data
pub async fn migrate_directory_sizes(db: &DatabaseConnection) -> Result<(), DbErr> {
    // Get all locations
    let locations = entities::location::Entity::find().all(db).await?;
    
    for location in locations {
        tracing::info!("Migrating directory sizes for location: {}", location.name.as_deref().unwrap_or("Unknown"));
        
        // Find all directories in this location, ordered by path depth (deepest first)
        let directories = entities::entry::Entity::find()
            .filter(entities::entry::Column::LocationId.eq(location.id))
            .filter(entities::entry::Column::Kind.eq(1)) // Directory
            .order_by_desc(entities::entry::Column::RelativePath)
            .all(db)
            .await?;
        
        let aggregator = DirectoryAggregator::new(db.clone());
        
        for directory in directories {
            match aggregator.aggregate_directory(&directory).await {
                Ok((aggregate_size, child_count, file_count)) => {
                    let mut active_dir: entities::entry::ActiveModel = directory.into();
                    active_dir.aggregate_size = Set(aggregate_size);
                    active_dir.child_count = Set(child_count);
                    active_dir.file_count = Set(file_count);
                    
                    active_dir.update(db).await?;
                }
                Err(e) => {
                    tracing::warn!("Failed to aggregate directory {}: {}", directory.name, e);
                }
            }
        }
    }
    
    Ok(())
}```

## src/operations/indexing/phases/discovery.rs

```rust
//! Discovery phase - walks directories and collects entries

use crate::{
    infrastructure::jobs::prelude::{JobContext, JobError, Progress},
    infrastructure::jobs::generic_progress::ToGenericProgress,
    operations::indexing::{
        state::{IndexerState, DirEntry, EntryKind, IndexPhase, IndexError, IndexerProgress},
        filters::should_skip_path,
        entry::EntryProcessor,
    },
};
use std::path::Path;

/// Run the discovery phase of indexing
pub async fn run_discovery_phase(
    state: &mut IndexerState,
    ctx: &JobContext<'_>,
    root_path: &Path,
) -> Result<(), JobError> {
    ctx.log(format!("Discovery phase starting from: {}", root_path.display()));
    ctx.log(format!("Initial directories to walk: {}", state.dirs_to_walk.len()));
    
    let mut skipped_count = 0u64;
    
    while let Some(dir_path) = state.dirs_to_walk.pop_front() {
        ctx.check_interrupt().await?;
        
        // Skip if already seen (handles symlink loops)
        if !state.seen_paths.insert(dir_path.clone()) {
            continue;
        }
        
        // Check if we should skip this directory
        if should_skip_path(&dir_path) {
            state.stats.skipped += 1;
            skipped_count += 1;
            ctx.log(format!("Skipping directory: {}", dir_path.display()));
            continue;
        }
        
        // Update progress
        let indexer_progress = IndexerProgress {
            phase: IndexPhase::Discovery { 
                dirs_queued: state.dirs_to_walk.len() 
            },
            current_path: dir_path.to_string_lossy().to_string(),
            total_found: state.stats,
            processing_rate: state.calculate_rate(),
            estimated_remaining: state.estimate_remaining(),
            scope: None,
            persistence: None,
            is_ephemeral: false,
        };
        ctx.progress(Progress::generic(indexer_progress.to_generic_progress()));
        
        // Read directory entries
        match read_directory(&dir_path).await {
            Ok(entries) => {
                let entry_count = entries.len();
                let mut added_count = 0;
                
                for entry in entries {
                    // Skip filtered entries
                    if should_skip_path(&entry.path) {
                        state.stats.skipped += 1;
                        skipped_count += 1;
                        continue;
                    }
                    
                    match entry.kind {
                        EntryKind::Directory => {
                            state.dirs_to_walk.push_back(entry.path.clone());
                            state.stats.dirs += 1;
                            state.pending_entries.push(entry);
                            added_count += 1;
                        }
                        EntryKind::File => {
                            state.stats.bytes += entry.size;
                            state.stats.files += 1;
                            state.pending_entries.push(entry);
                            added_count += 1;
                        }
                        EntryKind::Symlink => {
                            state.stats.symlinks += 1;
                            state.pending_entries.push(entry);
                            added_count += 1;
                        }
                    }
                }
                
                if added_count > 0 {
                    ctx.log(format!(
                        "Found {} entries in {} ({} filtered)",
                        entry_count,
                        dir_path.display(),
                        entry_count - added_count
                    ));
                }
                
                // Batch entries
                if state.should_create_batch() {
                    let batch = state.create_batch();
                    state.entry_batches.push(batch);
                }
            }
            Err(e) => {
                let error_msg = format!("Failed to read {}: {}", dir_path.display(), e);
                ctx.add_non_critical_error(error_msg);
                state.add_error(IndexError::ReadDir { 
                    path: dir_path.to_string_lossy().to_string(), 
                    error: e.to_string() 
                });
            }
        }
        
        // Update rate tracking
        state.items_since_last_update += 1;
        
        // Periodic checkpoint
        if state.stats.files % 5000 == 0 {
            ctx.checkpoint_with_state(state).await?;
        }
    }
    
    // Final batch
    if !state.pending_entries.is_empty() {
        let final_batch_size = state.pending_entries.len();
        ctx.log(format!("Creating final batch with {} entries", final_batch_size));
        let batch = state.create_batch();
        state.entry_batches.push(batch);
    }
    
    ctx.log(format!(
        "Discovery complete: {} files, {} dirs, {} symlinks, {} skipped, {} batches created", 
        state.stats.files, state.stats.dirs, state.stats.symlinks,
        skipped_count, state.entry_batches.len()
    ));
    
    state.phase = crate::operations::indexing::state::Phase::Processing;
    Ok(())
}

/// Read a directory and extract metadata
async fn read_directory(path: &Path) -> Result<Vec<DirEntry>, std::io::Error> {
    let mut entries = Vec::new();
    let mut dir = tokio::fs::read_dir(path).await?;
    
    while let Some(entry) = dir.next_entry().await? {
        let metadata = match entry.metadata().await {
            Ok(m) => m,
            Err(_) => continue, // Skip entries we can't read
        };
        
        let kind = if metadata.is_dir() {
            EntryKind::Directory
        } else if metadata.is_symlink() {
            EntryKind::Symlink
        } else {
            EntryKind::File
        };
        
        // Extract inode if available
        let inode = EntryProcessor::get_inode(&metadata);
        
        entries.push(DirEntry {
            path: entry.path(),
            kind,
            size: metadata.len(),
            modified: metadata.modified().ok(),
            inode,
        });
    }
    
    Ok(entries)
}```

## src/operations/indexing/phases/mod.rs

```rust
//! Indexer phases implementation
//! 
//! The indexer operates in distinct phases for clarity and resumability:
//! 1. Discovery - Walk directories and collect entries
//! 2. Processing - Create/update database records
//! 3. Aggregation - Calculate directory sizes
//! 4. Content - Generate content identities

pub mod discovery;
pub mod processing;
pub mod aggregation;
pub mod content;

pub use discovery::run_discovery_phase;
pub use processing::run_processing_phase;
pub use aggregation::run_aggregation_phase;
pub use content::run_content_phase;```

## src/operations/indexing/phases/processing.rs

```rust
//! Processing phase - creates/updates database entries

use crate::{
    infrastructure::{
        jobs::prelude::{JobContext, JobError, Progress},
        jobs::generic_progress::ToGenericProgress,
        database::entities,
    },
    operations::indexing::{
        state::{IndexerState, IndexPhase, IndexError, EntryKind, IndexerProgress},
        entry::EntryProcessor,
        IndexMode,
        change_detection::{ChangeDetector, Change},
    },
};
use sea_orm::{EntityTrait, QueryFilter, ColumnTrait};
use uuid::Uuid;
use std::path::Path;

/// Run the processing phase of indexing
pub async fn run_processing_phase(
    location_id: Uuid,
    state: &mut IndexerState,
    ctx: &JobContext<'_>,
    mode: IndexMode,
    location_root_path: &Path,
) -> Result<(), JobError> {
    let total_batches = state.entry_batches.len();
    ctx.log(format!("Processing phase starting with {} batches", total_batches));
    
    // Get the actual location record from database
    let location_record = entities::location::Entity::find()
        .filter(entities::location::Column::Uuid.eq(location_id))
        .one(ctx.library_db())
        .await
        .map_err(|e| JobError::execution(format!("Failed to find location: {}", e)))?
        .ok_or_else(|| JobError::execution("Location not found in database".to_string()))?;
    
    let device_id = location_record.device_id;
    let location_id_i32 = location_record.id;
    ctx.log(format!("Found location record: device_id={}, location_id={}", device_id, location_id_i32));
    
    // Load existing entries for change detection scoped to the indexing path
    // Note: location_root_path is the actual path being indexed (could be a subpath of the location)
    let mut change_detector = ChangeDetector::new();
    if !state.existing_entries.is_empty() || mode != IndexMode::Shallow {
        ctx.log("Loading existing entries for change detection...");
        change_detector.load_existing_entries(ctx, location_id_i32, location_root_path).await?;
        ctx.log(format!("Loaded {} existing entries", change_detector.entry_count()));
    }
    
    let mut total_processed = 0;
    let mut batch_number = 0;
    
    while let Some(mut batch) = state.entry_batches.pop() {
        ctx.check_interrupt().await?;
        
        batch_number += 1;
        let batch_size = batch.len();
        
        let indexer_progress = IndexerProgress {
            phase: IndexPhase::Processing { 
                batch: batch_number, 
                total_batches 
            },
            current_path: format!("Batch {}/{}", batch_number, total_batches),
            total_found: state.stats,
            processing_rate: state.calculate_rate(),
            estimated_remaining: state.estimate_remaining(),
            scope: None,
            persistence: None,
            is_ephemeral: false,
        };
        ctx.progress(Progress::generic(indexer_progress.to_generic_progress()));
        
        // Sort batch by path depth first, then by type to ensure parents are processed before children
        batch.sort_by(|a, b| {
            let a_depth = a.path.components().count();
            let b_depth = b.path.components().count();
            
            // First sort by depth (parents before children)
            match a_depth.cmp(&b_depth) {
                std::cmp::Ordering::Equal => {
                    // Then sort by type (directories before files at same depth)
                    let a_priority = match a.kind {
                        EntryKind::Directory => 0,
                        EntryKind::Symlink => 1,
                        EntryKind::File => 2,
                    };
                    let b_priority = match b.kind {
                        EntryKind::Directory => 0,
                        EntryKind::Symlink => 1,
                        EntryKind::File => 2,
                    };
                    a_priority.cmp(&b_priority)
                }
                other => other,
            }
        });
        
        // Process batch - check for changes and create/update entries
        for entry in batch {
            // Get metadata for change detection
            let metadata = match std::fs::metadata(&entry.path) {
                Ok(m) => m,
                Err(e) => {
                    ctx.add_non_critical_error(format!("Failed to get metadata for {}: {}", entry.path.display(), e));
                    continue;
                }
            };
            
            // Check for changes
            let change = change_detector.check_path(&entry.path, &metadata, entry.inode);
            
            match change {
                Some(Change::New(_)) => {
                    // Create new entry
                    match EntryProcessor::create_entry(state, ctx, &entry, location_id_i32, device_id, location_root_path).await {
                        Ok(entry_id) => {
                            ctx.log(format!("âœ… Created entry {}: {}", entry_id, entry.path.display()));
                            total_processed += 1;
                            
                            // Track for content identification if needed
                            if mode >= IndexMode::Content && entry.kind == EntryKind::File {
                                state.entries_for_content.push((entry_id, entry.path));
                            }
                        }
                        Err(e) => {
                            let error_msg = format!("Failed to create entry for {}: {}", entry.path.display(), e);
                            ctx.add_non_critical_error(error_msg);
                            state.add_error(IndexError::CreateEntry { 
                                path: entry.path.to_string_lossy().to_string(), 
                                error: e.to_string() 
                            });
                        }
                    }
                }
                
                Some(Change::Modified { entry_id, .. }) => {
                    // Update existing entry
                    match EntryProcessor::update_entry(ctx, entry_id, &entry).await {
                        Ok(()) => {
                            ctx.log(format!("ðŸ“ Updated entry {}: {}", entry_id, entry.path.display()));
                            total_processed += 1;
                            
                            // Re-process content if needed
                            if mode >= IndexMode::Content && entry.kind == EntryKind::File {
                                state.entries_for_content.push((entry_id, entry.path));
                            }
                        }
                        Err(e) => {
                            let error_msg = format!("Failed to update entry {}: {}", entry_id, e);
                            ctx.add_non_critical_error(error_msg);
                            state.add_error(IndexError::CreateEntry { 
                                path: entry.path.to_string_lossy().to_string(), 
                                error: e.to_string() 
                            });
                        }
                    }
                }
                
                Some(Change::Moved { old_path, new_path, entry_id, .. }) => {
                    // Handle move - update path in database
                    ctx.log(format!("ðŸ”„ Detected move: {} -> {}", old_path.display(), new_path.display()));
                    // TODO: Implement move handling
                    total_processed += 1;
                }
                
                Some(Change::Deleted { .. }) => {
                    // This shouldn't happen during processing of found entries
                    // Deleted entries are handled after processing
                }
                
                None => {
                    // No change - skip
                    ctx.log(format!("â­ï¸  No change for: {}", entry.path.display()));
                }
            }
        }
        
        ctx.log(format!("Processed batch {}/{}: {} entries", batch_number, total_batches, batch_size));
        ctx.checkpoint_with_state(state).await?;
    }
    
    // Handle deleted entries
    if change_detector.entry_count() > 0 {
        ctx.log("Checking for deleted entries...");
        let seen_paths: std::collections::HashSet<_> = state.seen_paths.iter().cloned().collect();
        let deleted = change_detector.find_deleted(&seen_paths);
        
        if !deleted.is_empty() {
            ctx.log(format!("Found {} deleted entries", deleted.len()));
            for change in deleted {
                if let Change::Deleted { path, entry_id } = change {
                    ctx.log(format!("âŒ Marking as deleted: {} (id: {})", path.display(), entry_id));
                    // TODO: Handle deletion (mark as deleted or remove from DB)
                }
            }
        }
    }
    
    ctx.log(format!("Processing phase complete: {} entries processed", total_processed));
    state.phase = crate::operations::indexing::state::Phase::Aggregation;
    Ok(())
}```

## src/operations/indexing/phases/content.rs

```rust
//! Content identification phase - generates CAS IDs and links content

use crate::{
    infrastructure::jobs::prelude::{JobContext, JobError, Progress},
    infrastructure::jobs::generic_progress::ToGenericProgress,
    operations::indexing::{
        state::{IndexerState, IndexPhase, IndexError, IndexerProgress},
        entry::EntryProcessor,
    },
    domain::content_identity::ContentHashGenerator,
};

/// Run the content identification phase
pub async fn run_content_phase(
    state: &mut IndexerState,
    ctx: &JobContext<'_>,
    library_id: uuid::Uuid,
) -> Result<(), JobError> {
    let total = state.entries_for_content.len();
    ctx.log(format!("Content identification phase starting with {} files", total));
    
    if total == 0 {
        ctx.log("No files to process for content identification");
        state.phase = crate::operations::indexing::state::Phase::Complete;
        return Ok(());
    }
    
    let mut processed = 0;
    let mut success_count = 0;
    let mut error_count = 0;
    
    // Process in chunks for better performance and memory usage
    const CHUNK_SIZE: usize = 100;
    
    while !state.entries_for_content.is_empty() {
        ctx.check_interrupt().await?;
        
        let chunk_size = CHUNK_SIZE.min(state.entries_for_content.len());
        let chunk: Vec<_> = state.entries_for_content.drain(..chunk_size).collect();
        let chunk_len = chunk.len();
        
        // Report progress BEFORE processing (using current processed count)
        let indexer_progress = IndexerProgress {
            phase: IndexPhase::ContentIdentification { 
                current: processed, 
                total 
            },
            current_path: format!("Generating content identities ({}/{})", processed, total),
            total_found: state.stats,
            processing_rate: state.calculate_rate(),
            estimated_remaining: state.estimate_remaining(),
            scope: None,
            persistence: None,
            is_ephemeral: false,
        };
        ctx.progress(Progress::generic(indexer_progress.to_generic_progress()));
        
        // Process chunk in parallel for better performance
        let content_hash_futures: Vec<_> = chunk.iter()
            .map(|(entry_id, path)| async move {
                let hash_result = ContentHashGenerator::generate_content_hash(path).await;
                (*entry_id, path.clone(), hash_result)
            })
            .collect();
        
        // Wait for all content hash generations to complete
        let hash_results = futures::future::join_all(content_hash_futures).await;
        
        // Process results
        for (entry_id, path, hash_result) in hash_results {
            match hash_result {
                Ok(content_hash) => {
                    match EntryProcessor::link_to_content_identity(ctx, entry_id, &path, content_hash.clone(), library_id).await {
                        Ok(()) => {
                            ctx.log(format!("âœ… Created content identity for {}: {}", path.display(), content_hash));
                            success_count += 1;
                        }
                        Err(e) => {
                            let error_msg = format!("Failed to create content identity for {}: {}", path.display(), e);
                            ctx.add_non_critical_error(error_msg);
                            state.add_error(IndexError::ContentId { 
                                path: path.to_string_lossy().to_string(), 
                                error: e.to_string() 
                            });
                            error_count += 1;
                        }
                    }
                }
                Err(e) => {
                    let error_msg = format!("Failed to generate content hash for {}: {}", path.display(), e);
                    ctx.add_non_critical_error(error_msg);
                    state.add_error(IndexError::ContentId { 
                        path: path.to_string_lossy().to_string(), 
                        error: e.to_string() 
                    });
                    error_count += 1;
                }
            }
        }
        
        // Update processed count AFTER processing chunk
        processed += chunk_len;
        
        // Update rate tracking
        state.items_since_last_update += chunk_len as u64;
        
        // Periodic checkpoint
        if processed % 1000 == 0 || processed == total {
            ctx.checkpoint_with_state(state).await?;
        }
    }
    
    ctx.log(format!(
        "Content identification complete: {} successful, {} errors out of {} total",
        success_count, error_count, total
    ));
    
    state.phase = crate::operations::indexing::state::Phase::Complete;
    Ok(())
}```

## src/operations/volumes/track/action.rs

```rust
//! Track volume action
//!
//! This action tracks a volume within a library, allowing Spacedrive to monitor
//! and index files on the volume.

use crate::{
    infrastructure::actions::{error::ActionError, output::ActionOutput},
    volume::VolumeFingerprint,
};
use serde::{Deserialize, Serialize};
use uuid::Uuid;

/// Input for tracking a volume
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct VolumeTrackAction {
    /// The fingerprint of the volume to track
    pub fingerprint: VolumeFingerprint,
    
    /// The library ID to track the volume in
    pub library_id: Uuid,
    
    /// Optional name for the tracked volume
    pub name: Option<String>,
}

impl VolumeTrackAction {
    /// Execute the volume tracking action
    pub async fn execute(
        &self,
        core: &crate::Core,
    ) -> Result<ActionOutput, ActionError> {
        // Get the library
        let library = core
            .libraries
            .get_library(self.library_id)
            .await
            .ok_or_else(|| ActionError::InvalidInput("Library not found".to_string()))?;
            
        // Check if volume exists
        let volume = core
            .volumes
            .get_volume(&self.fingerprint)
            .await
            .ok_or_else(|| ActionError::InvalidInput("Volume not found".to_string()))?;
            
        // TODO: Implement actual volume tracking in library
        // For now, just verify the volume exists and is mounted
        if !volume.is_mounted {
            return Err(ActionError::InvalidInput(
                "Cannot track unmounted volume".to_string()
            ));
        }
        
        Ok(ActionOutput::VolumeTracked {
            fingerprint: self.fingerprint.clone(),
            library_id: self.library_id,
            volume_name: volume.name,
        })
    }
}```

## src/operations/volumes/track/handler.rs

```rust
//! Handler for volume tracking action

use crate::{
    context::CoreContext,
    infrastructure::actions::{
        Action, error::{ActionError, ActionResult}, handler::ActionHandler, output::ActionOutput,
    },
};
use async_trait::async_trait;
use std::sync::Arc;

pub struct VolumeTrackHandler;

impl VolumeTrackHandler {
    pub fn new() -> Self {
        Self
    }
}

#[async_trait]
impl ActionHandler for VolumeTrackHandler {
    async fn execute(
        &self,
        context: Arc<CoreContext>,
        action: Action,
    ) -> ActionResult<ActionOutput> {
        match action {
            Action::VolumeTrack { action } => {
                // Execute the action using the volume manager from context
                let library = context
                    .library_manager
                    .get_library(action.library_id)
                    .await
                    .ok_or_else(|| ActionError::InvalidInput("Library not found".to_string()))?;
                    
                let volume = context
                    .volume_manager
                    .get_volume(&action.fingerprint)
                    .await
                    .ok_or_else(|| ActionError::InvalidInput("Volume not found".to_string()))?;
                    
                if !volume.is_mounted {
                    return Err(ActionError::InvalidInput(
                        "Cannot track unmounted volume".to_string()
                    ));
                }
                
                // Track the volume in the database
                let tracked = context
                    .volume_manager
                    .track_volume(&library, &action.fingerprint, action.name.clone())
                    .await
                    .map_err(|e| match e {
                        crate::volume::VolumeError::AlreadyTracked(_) => {
                            ActionError::InvalidInput("Volume is already tracked in this library".to_string())
                        }
                        crate::volume::VolumeError::NotFound(_) => {
                            ActionError::InvalidInput("Volume not found".to_string())
                        }
                        crate::volume::VolumeError::Database(msg) => {
                            ActionError::Internal(format!("Database error: {}", msg))
                        }
                        _ => ActionError::Internal(e.to_string()),
                    })?;
                
                Ok(ActionOutput::VolumeTracked {
                    fingerprint: action.fingerprint,
                    library_id: action.library_id,
                    volume_name: tracked.display_name.unwrap_or(volume.name),
                })
            }
            _ => Err(ActionError::InvalidActionType),
        }
    }
    
    fn can_handle(&self, action: &Action) -> bool {
        matches!(action, Action::VolumeTrack { .. })
    }
    
    fn supported_actions() -> &'static [&'static str]
    where
        Self: Sized
    {
        &["volume.track"]
    }
}

// Register the handler
crate::register_action_handler!(VolumeTrackHandler, "volume.track");```

## src/operations/volumes/track/mod.rs

```rust
pub mod action;
pub mod handler;```

## src/operations/volumes/untrack/action.rs

```rust
//! Untrack volume action
//!
//! This action removes volume tracking from a library.

use crate::{
    infrastructure::actions::{error::ActionError, output::ActionOutput},
    volume::VolumeFingerprint,
};
use serde::{Deserialize, Serialize};
use uuid::Uuid;

/// Input for untracking a volume
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct VolumeUntrackAction {
    /// The fingerprint of the volume to untrack
    pub fingerprint: VolumeFingerprint,
    
    /// The library ID to untrack the volume from
    pub library_id: Uuid,
}

impl VolumeUntrackAction {
    /// Execute the volume untracking action
    pub async fn execute(
        &self,
        core: &crate::Core,
    ) -> Result<ActionOutput, ActionError> {
        // Get the library
        let _library = core
            .libraries
            .get_library(self.library_id)
            .await
            .ok_or_else(|| ActionError::InvalidInput("Library not found".to_string()))?;
            
        // TODO: Implement actual volume untracking from library
        // For now, just verify the library exists
        
        Ok(ActionOutput::VolumeUntracked {
            fingerprint: self.fingerprint.clone(),
            library_id: self.library_id,
        })
    }
}```

## src/operations/volumes/untrack/handler.rs

```rust
//! Handler for volume untracking action

use crate::{
    context::CoreContext,
    infrastructure::actions::{
        Action, error::{ActionError, ActionResult}, handler::ActionHandler, output::ActionOutput,
    },
};
use async_trait::async_trait;
use std::sync::Arc;

pub struct VolumeUntrackHandler;

impl VolumeUntrackHandler {
    pub fn new() -> Self {
        Self
    }
}

#[async_trait]
impl ActionHandler for VolumeUntrackHandler {
    async fn execute(
        &self,
        context: Arc<CoreContext>,
        action: Action,
    ) -> ActionResult<ActionOutput> {
        match action {
            Action::VolumeUntrack { action } => {
                // Verify library exists
                let library = context
                    .library_manager
                    .get_library(action.library_id)
                    .await
                    .ok_or_else(|| ActionError::InvalidInput("Library not found".to_string()))?;
                    
                // Untrack the volume from the database
                context
                    .volume_manager
                    .untrack_volume(&library, &action.fingerprint)
                    .await
                    .map_err(|e| match e {
                        crate::volume::VolumeError::NotTracked(_) => {
                            ActionError::InvalidInput("Volume is not tracked in this library".to_string())
                        }
                        crate::volume::VolumeError::Database(msg) => {
                            ActionError::Internal(format!("Database error: {}", msg))
                        }
                        _ => ActionError::Internal(e.to_string()),
                    })?;
                
                Ok(ActionOutput::VolumeUntracked {
                    fingerprint: action.fingerprint,
                    library_id: action.library_id,
                })
            }
            _ => Err(ActionError::InvalidActionType),
        }
    }
    
    fn can_handle(&self, action: &Action) -> bool {
        matches!(action, Action::VolumeUntrack { .. })
    }
    
    fn supported_actions() -> &'static [&'static str]
    where
        Self: Sized
    {
        &["volume.untrack"]
    }
}

// Register the handler
crate::register_action_handler!(VolumeUntrackHandler, "volume.untrack");```

## src/operations/volumes/untrack/mod.rs

```rust
pub mod action;
pub mod handler;```

## src/operations/volumes/mod.rs

```rust
//! Volume operations module
//!
//! This module provides operations for managing volumes in Spacedrive:
//! - Tracking/untracking volumes in libraries
//! - Speed testing volume performance

pub mod speed_test;
pub mod track;
pub mod untrack;

pub use speed_test::action::VolumeSpeedTestAction;
pub use track::action::VolumeTrackAction;
pub use untrack::action::VolumeUntrackAction;```

## src/operations/volumes/speed_test/action.rs

```rust
//! Volume speed test action
//!
//! This action tests the read/write performance of a volume.

use crate::{
	infrastructure::actions::{error::ActionError, output::ActionOutput},
	volume::VolumeFingerprint,
};
use serde::{Deserialize, Serialize};

/// Input for volume speed testing
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct VolumeSpeedTestAction {
	/// The fingerprint of the volume to test
	pub fingerprint: VolumeFingerprint,
}

impl VolumeSpeedTestAction {
	/// Execute the volume speed test action
	pub async fn execute(&self, core: &crate::Core) -> Result<ActionOutput, ActionError> {
		// Run the speed test through the volume manager
		core.volumes
			.run_speed_test(&self.fingerprint)
			.await
			.map_err(|e| ActionError::Internal(e.to_string()))?;

		// Get the updated volume with speed test results
		let volume = core
			.volumes
			.get_volume(&self.fingerprint)
			.await
			.ok_or_else(|| {
				ActionError::InvalidInput("Volume not found after speed test".to_string())
			})?;

		Ok(ActionOutput::VolumeSpeedTested {
			fingerprint: self.fingerprint.clone(),
			read_speed_mbps: volume.read_speed_mbps.map(|v| v as u32),
			write_speed_mbps: volume.write_speed_mbps.map(|v| v as u32),
		})
	}
}
```

## src/operations/volumes/speed_test/handler.rs

```rust
//! Handler for volume speed test action

use crate::{
	context::CoreContext,
	infrastructure::actions::{
		error::{ActionError, ActionResult},
		handler::ActionHandler,
		output::ActionOutput,
		Action,
	},
};
use async_trait::async_trait;
use std::sync::Arc;

pub struct VolumeSpeedTestHandler;

impl VolumeSpeedTestHandler {
	pub fn new() -> Self {
		Self
	}
}

#[async_trait]
impl ActionHandler for VolumeSpeedTestHandler {
	async fn execute(
		&self,
		context: Arc<CoreContext>,
		action: Action,
	) -> ActionResult<ActionOutput> {
		match action {
			Action::VolumeSpeedTest { action } => {
				// Run speed test through volume manager
				context
					.volume_manager
					.run_speed_test(&action.fingerprint)
					.await
					.map_err(|e| ActionError::Internal(e.to_string()))?;

				// Get updated volume with results
				let volume = context
					.volume_manager
					.get_volume(&action.fingerprint)
					.await
					.ok_or_else(|| {
						ActionError::InvalidInput("Volume not found after speed test".to_string())
					})?;

				// Extract speed test results
				let read_speed = volume.read_speed_mbps.unwrap_or(0);
				let write_speed = volume.write_speed_mbps.unwrap_or(0);

				// Save results to database for all libraries where this volume is tracked
				let libraries = context.library_manager.get_open_libraries().await;
				if let Err(e) = context
					.volume_manager
					.save_speed_test_results(
						&action.fingerprint,
						read_speed,
						write_speed,
						&libraries,
					)
					.await
				{
					// Log error but don't fail the action since the speed test itself succeeded
					tracing::warn!("Failed to save speed test results to database: {}", e);
				}

				Ok(ActionOutput::VolumeSpeedTested {
					fingerprint: action.fingerprint,
					read_speed_mbps: Some(read_speed as u32),
					write_speed_mbps: Some(write_speed as u32),
				})
			}
			_ => Err(ActionError::InvalidActionType),
		}
	}

	fn can_handle(&self, action: &Action) -> bool {
		matches!(action, Action::VolumeSpeedTest { .. })
	}

	fn supported_actions() -> &'static [&'static str]
	where
		Self: Sized,
	{
		&["volume.speed_test"]
	}
}

// Register the handler
crate::register_action_handler!(VolumeSpeedTestHandler, "volume.speed_test");
```

## src/operations/volumes/speed_test/mod.rs

```rust
pub mod action;
pub mod handler;```

## src/operations/metadata/action.rs

```rust
//! Metadata operations action handler

use crate::{
    context::CoreContext,
    infrastructure::actions::{
        error::{ActionError, ActionResult}, 
        handler::ActionHandler, 
        output::ActionOutput,
    },
    register_action_handler,
};
use async_trait::async_trait;
use std::sync::Arc;
use uuid::Uuid;

#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct MetadataAction {
    pub paths: Vec<std::path::PathBuf>,
    pub extract_exif: bool,
    pub extract_xmp: bool,
}

pub struct MetadataHandler;

impl MetadataHandler {
    pub fn new() -> Self {
        Self
    }
}

#[async_trait]
impl ActionHandler for MetadataHandler {
    async fn validate(
        &self,
        _context: Arc<CoreContext>,
        action: &crate::infrastructure::actions::Action,
    ) -> ActionResult<()> {
        if let crate::infrastructure::actions::Action::MetadataOperation { action, .. } = action {
            if action.paths.is_empty() {
                return Err(ActionError::Validation {
                    field: "paths".to_string(),
                    message: "At least one path must be specified".to_string(),
                });
            }
            Ok(())
        } else {
            Err(ActionError::InvalidActionType)
        }
    }

    async fn execute(
        &self,
        context: Arc<CoreContext>,
        action: crate::infrastructure::actions::Action,
    ) -> ActionResult<ActionOutput> {
        if let crate::infrastructure::actions::Action::MetadataOperation { library_id, action } = action {
            let library_manager = &context.library_manager;
            
            let job_params = serde_json::json!({
                "paths": action.paths,
                "extract_exif": action.extract_exif,
                "extract_xmp": action.extract_xmp
            });

            let library = library_manager.get_library(library_id).await
                .ok_or(ActionError::Internal(format!("Library not found: {}", library_id)))?;

            let job_handle = library
                .jobs()
                .dispatch_by_name("extract_metadata", job_params)
                .await
                .map_err(ActionError::Job)?;

            Ok(ActionOutput::success("Metadata extraction job dispatched successfully"))
        } else {
            Err(ActionError::InvalidActionType)
        }
    }

    fn can_handle(&self, action: &crate::infrastructure::actions::Action) -> bool {
        matches!(action, crate::infrastructure::actions::Action::MetadataOperation { .. })
    }

    fn supported_actions() -> &'static [&'static str] {
        &["metadata.extract"]
    }
}

register_action_handler!(MetadataHandler, "metadata.extract");```

## src/operations/metadata/mod.rs

```rust
//! Metadata operations for hierarchical metadata management

pub mod action;

use chrono::{DateTime, Utc};
use sea_orm::{ActiveModelTrait, ColumnTrait, DatabaseConnection, EntityTrait, QueryFilter, Set};
use serde::{Deserialize, Serialize};
use std::sync::Arc;
use uuid::Uuid;

use crate::infrastructure::database::entities::{
	content_identity::{self, Entity as ContentIdentity, Model as ContentIdentityModel},
	entry::{self, Entity as Entry, Model as EntryModel},
	tag::{self, Entity as Tag, Model as TagModel},
	user_metadata::{
		self, ActiveModel as UserMetadataActiveModel, Entity as UserMetadata,
		Model as UserMetadataModel,
	},
	user_metadata_tag::{
		self, ActiveModel as UserMetadataTagActiveModel, Entity as UserMetadataTag,
	},
};

pub use action::MetadataAction;
use crate::shared::errors::Result;

#[derive(Clone, Debug, PartialEq, Eq, Serialize, Deserialize)]
pub enum MetadataTarget {
	/// Metadata for this specific file instance (syncs with Index domain)
	Entry(Uuid),
	/// Metadata for all instances of this content within library (syncs with UserMetadata domain)
	Content(Uuid),
}

#[derive(Clone, Debug, PartialEq, Eq, Serialize, Deserialize)]
pub enum MetadataScope {
	Entry,   // File-specific (higher priority)
	Content, // Content-universal (lower priority)
}

#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct MetadataDisplay {
	pub notes: Vec<MetadataNote>, // Both entry and content notes shown
	pub tags: Vec<MetadataTag>,   // Both entry and content tags shown
	pub favorite: bool,           // Entry-level overrides content-level
	pub hidden: bool,             // Entry-level overrides content-level
	pub custom_data: Option<serde_json::Value>, // Entry-level overrides content-level
}

#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct MetadataNote {
	pub content: String,
	pub scope: MetadataScope,
	pub created_at: DateTime<Utc>,
}

#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct MetadataTag {
	pub tag: TagModel,
	pub scope: MetadataScope,
	pub created_at: DateTime<Utc>,
}

#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct MetadataUpdate {
	pub notes: Option<String>,
	pub favorite: Option<bool>,
	pub hidden: Option<bool>,
	pub custom_data: Option<serde_json::Value>,
	pub tag_uuids: Option<Vec<Uuid>>,
}

pub struct MetadataService {
	library_db: Arc<DatabaseConnection>,
	current_device_uuid: Uuid,
}

impl MetadataService {
	pub fn new(library_db: Arc<DatabaseConnection>, current_device_uuid: Uuid) -> Self {
		Self {
			library_db,
			current_device_uuid,
		}
	}

	/// Add metadata (notes, tags, favorites) with flexible targeting
	pub async fn add_metadata(
		&self,
		target: MetadataTarget,
		metadata_update: MetadataUpdate,
	) -> Result<UserMetadataModel> {
		match target {
			MetadataTarget::Entry(entry_uuid) => {
				// File-specific metadata - create entry-scoped UserMetadata
				let user_metadata = UserMetadataActiveModel {
					uuid: Set(Uuid::new_v4()),
					entry_uuid: Set(Some(entry_uuid)),
					content_identity_uuid: Set(None), // Mutually exclusive
					notes: Set(metadata_update.notes),
					favorite: Set(metadata_update.favorite.unwrap_or(false)),
					hidden: Set(metadata_update.hidden.unwrap_or(false)),
					custom_data: Set(metadata_update.custom_data.unwrap_or_default()),
					created_at: Set(Utc::now()),
					updated_at: Set(Utc::now()),
					..Default::default()
				}
				.insert(&*self.library_db)
				.await?;

				// Add tags if provided
				if let Some(tag_uuids) = metadata_update.tag_uuids {
					self.add_tags_to_metadata(user_metadata.id, tag_uuids)
						.await?;
				}

				Ok(user_metadata)
			}

			MetadataTarget::Content(content_identity_uuid) => {
				// Content-universal metadata - create content-scoped UserMetadata
				let user_metadata = UserMetadataActiveModel {
					uuid: Set(Uuid::new_v4()),
					entry_uuid: Set(None), // Mutually exclusive
					content_identity_uuid: Set(Some(content_identity_uuid)),
					notes: Set(metadata_update.notes),
					favorite: Set(metadata_update.favorite.unwrap_or(false)),
					hidden: Set(metadata_update.hidden.unwrap_or(false)),
					custom_data: Set(metadata_update.custom_data.unwrap_or_default()),
					created_at: Set(Utc::now()),
					updated_at: Set(Utc::now()),
					..Default::default()
				}
				.insert(&*self.library_db)
				.await?;

				// Add tags if provided
				if let Some(tag_uuids) = metadata_update.tag_uuids {
					self.add_tags_to_metadata(user_metadata.id, tag_uuids)
						.await?;
				}

				Ok(user_metadata)
			}
		}
	}

	/// Get hierarchical metadata display for an entry (both entry and content metadata shown)
	pub async fn get_entry_metadata_display(&self, entry_uuid: Uuid) -> Result<MetadataDisplay> {
		let mut display = MetadataDisplay {
			notes: Vec::new(),
			tags: Vec::new(),
			favorite: false,
			hidden: false,
			custom_data: None,
		};

		// Get entry-specific metadata
		let entry_metadata = UserMetadata::find()
			.filter(user_metadata::Column::EntryUuid.eq(entry_uuid))
			.find_with_related(Tag)
			.all(&*self.library_db)
			.await?;

		for (metadata, tags) in entry_metadata {
			// Notes - show both levels
			if let Some(notes) = metadata.notes {
				display.notes.push(MetadataNote {
					content: notes,
					scope: MetadataScope::Entry,
					created_at: metadata.created_at,
				});
			}

			// Tags - show both levels
			for tag in tags {
				display.tags.push(MetadataTag {
					tag,
					scope: MetadataScope::Entry,
					created_at: metadata.created_at,
				});
			}

			// Favorites/Hidden - entry overrides (higher priority)
			display.favorite = metadata.favorite;
			display.hidden = metadata.hidden;
			display.custom_data = Some(metadata.custom_data);
		}

		// Get content-level metadata if entry has content identity
		if let Some(entry) = Entry::find()
			.filter(entry::Column::Uuid.eq(entry_uuid))
			.one(&*self.library_db)
			.await?
		{
			if let Some(content_id) = entry.content_id {
				if let Some(content_identity) = ContentIdentity::find_by_id(content_id)
					.one(&*self.library_db)
					.await?
				{
					if let Some(content_uuid) = content_identity.uuid {
						let content_metadata = UserMetadata::find()
							.filter(user_metadata::Column::ContentIdentityUuid.eq(content_uuid))
							.find_with_related(Tag)
							.all(&*self.library_db)
							.await?;

						for (metadata, tags) in content_metadata {
							// Notes - show both levels
							if let Some(notes) = metadata.notes {
								display.notes.push(MetadataNote {
									content: notes,
									scope: MetadataScope::Content,
									created_at: metadata.created_at,
								});
							}

							// Tags - show both levels
							for tag in tags {
								display.tags.push(MetadataTag {
									tag,
									scope: MetadataScope::Content,
									created_at: metadata.created_at,
								});
							}

							// Favorites/Hidden - only use if no entry-level override
							if !display.favorite && metadata.favorite {
								display.favorite = true;
							}
							if !display.hidden && metadata.hidden {
								display.hidden = true;
							}
							if display.custom_data.is_none() {
								display.custom_data = Some(metadata.custom_data);
							}
						}
					}
				}
			}
		}

		Ok(display)
	}

	/// Promote entry-level metadata to content-level ("Apply to all instances")
	pub async fn promote_to_content(
		&self,
		entry_metadata_id: i32,
		content_identity_uuid: Uuid,
	) -> Result<UserMetadataModel> {
		// Get existing entry-level metadata
		let entry_metadata = UserMetadata::find_by_id(entry_metadata_id)
			.one(&*self.library_db)
			.await?
			.ok_or_else(|| {
				crate::shared::errors::CoreError::NotFound("Metadata not found".to_string())
			})?;

		// Create new content-level metadata (entry-level remains for hierarchy)
		let content_metadata = UserMetadataActiveModel {
			uuid: Set(Uuid::new_v4()),
			entry_uuid: Set(None),
			content_identity_uuid: Set(Some(content_identity_uuid)),
			notes: Set(entry_metadata.notes.clone()),
			favorite: Set(entry_metadata.favorite),
			hidden: Set(entry_metadata.hidden),
			custom_data: Set(entry_metadata.custom_data.clone()),
			created_at: Set(Utc::now()),
			updated_at: Set(Utc::now()),
			..Default::default()
		}
		.insert(&*self.library_db)
		.await?;

		// Copy tags to new content-level metadata
		let entry_tags = UserMetadataTag::find()
			.filter(user_metadata_tag::Column::UserMetadataId.eq(entry_metadata_id))
			.all(&*self.library_db)
			.await?;

		for entry_tag in entry_tags {
			UserMetadataTagActiveModel {
				user_metadata_id: Set(content_metadata.id),
				tag_uuid: Set(entry_tag.tag_uuid),
				created_at: Set(Utc::now()),
				device_uuid: Set(self.current_device_uuid),
				..Default::default()
			}
			.insert(&*self.library_db)
			.await?;
		}

		Ok(content_metadata)
	}

	async fn add_tags_to_metadata(&self, metadata_id: i32, tag_uuids: Vec<Uuid>) -> Result<()> {
		for tag_uuid in tag_uuids {
			UserMetadataTagActiveModel {
				user_metadata_id: Set(metadata_id),
				tag_uuid: Set(tag_uuid),
				created_at: Set(Utc::now()),
				device_uuid: Set(self.current_device_uuid),
				..Default::default()
			}
			.insert(&*self.library_db)
			.await?;
		}
		Ok(())
	}
}
```

## src/operations/devices/revoke/action.rs

```rust
//! Device revoke action handler

use crate::{
    context::CoreContext,
    infrastructure::actions::{
        error::{ActionError, ActionResult},
        handler::ActionHandler,
        output::ActionOutput,
        Action,
    },
    register_action_handler,
};
use async_trait::async_trait;
use serde::{Deserialize, Serialize};
use std::sync::Arc;
use uuid::Uuid;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DeviceRevokeAction {
    pub device_id: Uuid,
    pub reason: Option<String>,
}

pub struct DeviceRevokeHandler;

impl DeviceRevokeHandler {
    pub fn new() -> Self {
        Self
    }
}

#[async_trait]
impl ActionHandler for DeviceRevokeHandler {
    async fn validate(
        &self,
        context: Arc<CoreContext>,
        action: &Action,
    ) -> ActionResult<()> {
        if let Action::DeviceRevoke { action, .. } = action {
            // Don't allow revoking self
            let current_device = context.device_manager.to_device()
                .map_err(|e| ActionError::Internal(format!("Failed to get current device: {}", e)))?;
            
            if current_device.id == action.device_id {
                return Err(ActionError::Validation {
                    field: "device_id".to_string(),
                    message: "Cannot revoke current device".to_string(),
                });
            }
            Ok(())
        } else {
            Err(ActionError::InvalidActionType)
        }
    }

    async fn execute(
        &self,
        context: Arc<CoreContext>,
        action: Action,
    ) -> ActionResult<ActionOutput> {
        if let Action::DeviceRevoke { library_id, action } = action {
            let library_manager = &context.library_manager;
            
            // Get the specific library
            let library = library_manager
                .get_library(library_id)
                .await
                .ok_or(ActionError::LibraryNotFound(library_id))?;

            // Remove device from database
            use crate::infrastructure::database::entities;
            use sea_orm::{ColumnTrait, EntityTrait, QueryFilter, ModelTrait};
            
            let device = entities::device::Entity::find()
                .filter(entities::device::Column::Uuid.eq(action.device_id))
                .one(library.db().conn())
                .await
                .map_err(|e| ActionError::Internal(format!("Database error: {}", e)))?
                .ok_or_else(|| ActionError::Internal(format!("Device not found: {}", action.device_id)))?;

            let device_name = device.name.clone();
            
            // Delete the device
            device.delete(library.db().conn())
                .await
                .map_err(|e| ActionError::Internal(format!("Failed to delete device: {}", e)))?;

            // TODO: Also revoke any active network connections for this device
            // This would involve the networking/P2P system

            let output = super::output::DeviceRevokeOutput {
                device_id: action.device_id,
                device_name,
                reason: action.reason,
            };

            Ok(ActionOutput::from_trait(output))
        } else {
            Err(ActionError::InvalidActionType)
        }
    }

    fn can_handle(&self, action: &Action) -> bool {
        matches!(action, Action::DeviceRevoke { .. })
    }

    fn supported_actions() -> &'static [&'static str] {
        &["device.revoke"]
    }
}

register_action_handler!(DeviceRevokeHandler, "device.revoke");```

## src/operations/devices/revoke/mod.rs

```rust
//! Device revoke operation

pub mod action;
pub mod output;```

## src/operations/devices/revoke/output.rs

```rust
//! Device revoke operation output

use crate::infrastructure::actions::output::ActionOutputTrait;
use serde::{Deserialize, Serialize};
use uuid::Uuid;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DeviceRevokeOutput {
    pub device_id: Uuid,
    pub device_name: String,
    pub reason: Option<String>,
}

impl ActionOutputTrait for DeviceRevokeOutput {
    fn to_json(&self) -> serde_json::Value {
        serde_json::to_value(self).unwrap_or(serde_json::Value::Null)
    }

    fn display_message(&self) -> String {
        match &self.reason {
            Some(reason) => format!(
                "Revoked device '{}' ({}): {}",
                self.device_name, self.device_id, reason
            ),
            None => format!(
                "Revoked device '{}' ({})",
                self.device_name, self.device_id
            ),
        }
    }

    fn output_type(&self) -> &'static str {
        "device.revoke.output"
    }
}```

## src/operations/devices/mod.rs

```rust
//! Device operations

pub mod revoke;```

## src/operations/media/thumbnail/action.rs

```rust
//! Thumbnail generation action handler

use crate::{
    context::CoreContext,
    infrastructure::actions::{
        error::{ActionError, ActionResult}, 
        handler::ActionHandler, 
        output::ActionOutput,
    },
    register_action_handler,
};
use super::job::{ThumbnailJob, ThumbnailJobConfig};
use async_trait::async_trait;
use std::sync::Arc;
use uuid::Uuid;

#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct ThumbnailAction {
    pub paths: Vec<std::path::PathBuf>,
    pub size: u32,
    pub quality: u8,
}

pub struct ThumbnailHandler;

impl ThumbnailHandler {
    pub fn new() -> Self {
        Self
    }
}

#[async_trait]
impl ActionHandler for ThumbnailHandler {
    async fn validate(
        &self,
        _context: Arc<CoreContext>,
        action: &crate::infrastructure::actions::Action,
    ) -> ActionResult<()> {
        if let crate::infrastructure::actions::Action::GenerateThumbnails { action, .. } = action {
            if action.paths.is_empty() {
                return Err(ActionError::Validation {
                    field: "paths".to_string(),
                    message: "At least one path must be specified".to_string(),
                });
            }
            Ok(())
        } else {
            Err(ActionError::InvalidActionType)
        }
    }

    async fn execute(
        &self,
        context: Arc<CoreContext>,
        action: crate::infrastructure::actions::Action,
    ) -> ActionResult<ActionOutput> {
        if let crate::infrastructure::actions::Action::GenerateThumbnails { library_id, action } = action {
            let library_manager = &context.library_manager;
            
            let library = library_manager.get_library(library_id).await
                .ok_or(ActionError::Internal(format!("Library not found: {}", library_id)))?;

            // Create thumbnail job config
            let config = ThumbnailJobConfig {
                sizes: vec![action.size],
                quality: action.quality,
                regenerate: false,
                ..Default::default()
            };

            // TODO: Convert paths to entry IDs by querying the database
            // For now, create a job that processes all suitable entries
            let job = ThumbnailJob::new(config);

            // Dispatch the job directly
            let job_handle = library
                .jobs()
                .dispatch(job)
                .await
                .map_err(ActionError::Job)?;

            Ok(ActionOutput::success("Thumbnail generation job dispatched successfully"))
        } else {
            Err(ActionError::InvalidActionType)
        }
    }

    fn can_handle(&self, action: &crate::infrastructure::actions::Action) -> bool {
        matches!(action, crate::infrastructure::actions::Action::GenerateThumbnails { .. })
    }

    fn supported_actions() -> &'static [&'static str] {
        &["media.thumbnail"]
    }
}

register_action_handler!(ThumbnailHandler, "media.thumbnail");```

## src/operations/media/thumbnail/job.rs

```rust
//! Thumbnail generation job implementation

use crate::infrastructure::jobs::prelude::*;
use serde::{Deserialize, Serialize};
use std::time::Duration;
use uuid::Uuid;

use super::{
	error::{ThumbnailError, ThumbnailResult},
	generator::ThumbnailGenerator,
	state::{ThumbnailEntry, ThumbnailPhase, ThumbnailState, ThumbnailStats},
};

/// Configuration for thumbnail generation job
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ThumbnailJobConfig {
	/// Target thumbnail sizes to generate
	pub sizes: Vec<u32>,

	/// Quality setting (0-100)
	pub quality: u8,

	/// Whether to regenerate existing thumbnails
	pub regenerate: bool,

	/// Batch size for processing
	pub batch_size: usize,

	/// Maximum concurrent thumbnail generations
	pub max_concurrent: usize,
}

impl Default for ThumbnailJobConfig {
	fn default() -> Self {
		Self {
			sizes: vec![128, 256, 512],
			quality: 85,
			regenerate: false,
			batch_size: 50,
			max_concurrent: 4,
		}
	}
}

/// Progress information for thumbnail generation
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ThumbnailProgress {
	pub phase: ThumbnailPhase,
	pub generated_count: u64,
	pub skipped_count: u64,
	pub error_count: u64,
	pub total_count: u64,
	pub current_file: Option<String>,
	pub estimated_time_remaining: Option<Duration>,
}

impl JobProgress for ThumbnailProgress {}

/// Thumbnail generation job
#[derive(Debug, Serialize, Deserialize)]
pub struct ThumbnailJob {
	/// Entry IDs to process for thumbnails (if None, process all suitable entries)
	pub entry_ids: Option<Vec<Uuid>>,

	/// Job configuration
	pub config: ThumbnailJobConfig,

	// Resumable state
	#[serde(skip_serializing_if = "Option::is_none")]
	state: Option<ThumbnailState>,
}

impl Job for ThumbnailJob {
	const NAME: &'static str = "thumbnail_generation";
	const RESUMABLE: bool = true;
	const DESCRIPTION: Option<&'static str> = Some("Generate thumbnails for media files");
}

/// Output from thumbnail generation job
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ThumbnailOutput {
	pub stats: ThumbnailStats,
	pub duration: Duration,
	pub errors: Vec<String>,
}

impl From<ThumbnailOutput> for JobOutput {
	fn from(output: ThumbnailOutput) -> Self {
		JobOutput::ThumbnailGeneration {
			generated_count: output.stats.generated_count,
			skipped_count: output.stats.skipped_count,
			error_count: output.stats.error_count,
			total_size_bytes: output.stats.thumbnails_size_bytes,
		}
	}
}

#[async_trait::async_trait]
impl JobHandler for ThumbnailJob {
	type Output = ThumbnailOutput;

	async fn run(&mut self, ctx: JobContext<'_>) -> JobResult<Self::Output> {
		// Initialize or restore state
		{
			let _state = self.get_or_create_state(&ctx).await?;
		}

		// Run each phase sequentially
		// Discovery phase
		if self.state.as_ref().unwrap().phase == ThumbnailPhase::Discovery {
			Self::run_discovery_phase_static(
				&self.config,
				&self.entry_ids,
				self.state.as_mut().unwrap(),
				&ctx,
			)
			.await?;
		}

		// Processing phase
		if self.state.as_ref().unwrap().phase == ThumbnailPhase::Processing {
			Self::run_processing_phase_static(&self.config, self.state.as_mut().unwrap(), &ctx)
				.await?;
		}

		// Cleanup phase
		if self.state.as_ref().unwrap().phase == ThumbnailPhase::Cleanup {
			Self::run_cleanup_phase_static(self.state.as_mut().unwrap(), &ctx).await?;
		}

		// Mark as complete and return results
		let state = self.state.as_mut().unwrap();
		state.phase = ThumbnailPhase::Complete;

		Ok(ThumbnailOutput {
			stats: state.stats.clone(),
			duration: state.started_at.elapsed(),
			errors: state.errors.clone(),
		})
	}

	async fn on_resume(&mut self, ctx: &JobContext<'_>) -> JobResult {
		if let Some(state) = &self.state {
			ctx.log(format!("Resuming thumbnail job in {:?} phase", state.phase));
			ctx.log(format!(
				"Progress: {} generated, {} skipped, {} errors",
				state.stats.generated_count, state.stats.skipped_count, state.stats.error_count
			));
		}
		Ok(())
	}

	async fn on_pause(&mut self, ctx: &JobContext<'_>) -> JobResult {
		ctx.log("Pausing thumbnail generation job - state will be preserved");
		Ok(())
	}

	async fn on_cancel(&mut self, ctx: &JobContext<'_>) -> JobResult {
		ctx.log("Cancelling thumbnail generation job");
		if let Some(state) = &self.state {
			ctx.log(format!(
				"Generated {} thumbnails before cancellation",
				state.stats.generated_count
			));
		}
		Ok(())
	}
}

impl ThumbnailJob {
	/// Create a new thumbnail job for all suitable entries
	pub fn new(config: ThumbnailJobConfig) -> Self {
		Self {
			entry_ids: None,
			config,
			state: None,
		}
	}

	/// Create a thumbnail job for specific entry IDs
	pub fn for_entries(entry_ids: Vec<Uuid>, config: ThumbnailJobConfig) -> Self {
		Self {
			entry_ids: Some(entry_ids),
			config,
			state: None,
		}
	}

	/// Create a thumbnail job with default configuration
	pub fn with_defaults() -> Self {
		Self::new(ThumbnailJobConfig::default())
	}

	/// Get or create the job state
	async fn get_or_create_state(
		&mut self,
		ctx: &JobContext<'_>,
	) -> JobResult<&mut ThumbnailState> {
		if self.state.is_none() {
			ctx.log("Initializing new thumbnail generation state");
			self.state = Some(ThumbnailState::new());
		}
		Ok(self.state.as_mut().unwrap())
	}

	/// Discovery phase: Find entries that need thumbnails
	async fn run_discovery_phase_static(
		config: &ThumbnailJobConfig,
		entry_ids: &Option<Vec<Uuid>>,
		state: &mut ThumbnailState,
		ctx: &JobContext<'_>,
	) -> JobResult<()> {
		ctx.progress(Progress::indeterminate(
			"Discovering files for thumbnail generation",
		));
		ctx.log("Starting thumbnail discovery phase");

		// Build MIME type conditions based on available features
		let mut mime_conditions = vec![
			"e.mime_type LIKE 'image/%'",
			"e.mime_type = 'application/pdf'",
		];

		#[cfg(feature = "ffmpeg")]
		{
			mime_conditions.push("e.mime_type LIKE 'video/%'");
		}

		let mime_condition = mime_conditions.join(" OR ");

		let query = if let Some(ref entry_ids) = entry_ids {
			format!(
				"SELECT e.id, ci.cas_id, e.mime_type, e.size, e.relative_path
                 FROM entries e
                 JOIN content_identity ci ON e.content_id = ci.id
                 WHERE e.id IN ({})
                 AND ci.cas_id IS NOT NULL
                 AND ({})
                 ORDER BY e.size DESC",
				entry_ids
					.iter()
					.map(|id| format!("'{}'", id))
					.collect::<Vec<_>>()
					.join(", "),
				mime_condition
			)
		} else {
			format!(
				"SELECT e.id, ci.cas_id, e.mime_type, e.size, e.relative_path
                 FROM entries e
                 JOIN content_identity ci ON e.content_id = ci.id
                 WHERE ci.cas_id IS NOT NULL
                 AND ({})
                 ORDER BY e.size DESC",
				mime_condition
			)
		};

		// This is a placeholder - in real implementation, we'd use the database
		// For now, we'll create some mock entries
		let entries = Self::mock_database_query_static(&query).await?;

		// Filter entries that already have thumbnails (unless regenerating)
		for entry in entries {
			if !config.regenerate
				&& Self::has_all_thumbnails_static(&entry.cas_id, config, ctx).await?
			{
				state.record_skipped();
				continue;
			}

			state.pending_entries.push(entry);
		}

		state.stats.discovered_count = state.pending_entries.len() as u64;

		// Create batches for processing
		state.batches = state
			.pending_entries
			.chunks(config.batch_size)
			.map(|chunk| chunk.to_vec())
			.collect();

		state.phase = ThumbnailPhase::Processing;

		ctx.log(format!(
			"Discovery complete: {} entries found, {} batches created",
			state.stats.discovered_count,
			state.batches.len()
		));
		ctx.progress(Progress::count(0, state.batches.len()));

		Ok(())
	}

	/// Processing phase: Generate thumbnails in batches
	async fn run_processing_phase_static(
		config: &ThumbnailJobConfig,
		state: &mut ThumbnailState,
		ctx: &JobContext<'_>,
	) -> JobResult<()> {
		ctx.log("Starting thumbnail processing phase");

		let batches = state.batches.clone(); // Clone to avoid borrowing issues
		let total_batches = batches.len();

		for (batch_idx, batch) in batches.iter().enumerate() {
			ctx.check_interrupt().await?;

			ctx.log(format!(
				"Processing batch {} of {} ({} entries)",
				batch_idx + 1,
				total_batches,
				batch.len()
			));

			// Process entries in the batch concurrently
			let tasks = batch
				.iter()
				.map(|entry| Self::generate_thumbnails_for_entry_static(entry, config, ctx));

			let results = futures::future::join_all(tasks).await;

			// Process results
			for (entry, result) in batch.iter().zip(results.iter()) {
				match result {
					Ok(thumbnail_size) => {
						state.record_generated(*thumbnail_size);
						ctx.log(format!("Generated thumbnail for {}", entry.relative_path));
					}
					Err(e) => {
						let error_msg = format!(
							"Failed to generate thumbnail for {}: {}",
							entry.relative_path, e
						);
						state.add_error(error_msg.clone());
						ctx.add_non_critical_error(
							crate::infrastructure::jobs::error::JobError::execution(error_msg),
						);
					}
				}
			}

			// Update progress
			ctx.progress(Progress::count(batch_idx + 1, total_batches));

			// Update detailed progress
			let progress = ThumbnailProgress {
				phase: state.phase.clone(),
				generated_count: state.stats.generated_count,
				skipped_count: state.stats.skipped_count,
				error_count: state.stats.error_count,
				total_count: state.stats.discovered_count,
				current_file: batch.last().map(|e| e.relative_path.clone()),
				estimated_time_remaining: None, // TODO: Calculate ETA
			};
			let progress_json = serde_json::to_value(progress).unwrap_or(serde_json::Value::Null);
			ctx.progress(Progress::Structured(progress_json));

			// Checkpoint every 10 batches
			if batch_idx % 10 == 0 {
				ctx.checkpoint().await?;
			}
		}

		state.phase = ThumbnailPhase::Cleanup;
		ctx.log("Processing phase complete");

		Ok(())
	}

	/// Cleanup phase: Remove orphaned thumbnails
	async fn run_cleanup_phase_static(
		_state: &mut ThumbnailState,
		ctx: &JobContext<'_>,
	) -> JobResult<()> {
		ctx.log("Starting cleanup phase");
		ctx.progress(Progress::indeterminate("Cleaning up orphaned thumbnails"));

		// TODO: Implement cleanup logic
		// - Find thumbnails that don't have corresponding entries
		// - Remove old thumbnails if regenerating

		ctx.log("Cleanup phase complete");
		Ok(())
	}

	/// Check if all required thumbnails exist for a CAS ID
	async fn has_all_thumbnails_static(
		cas_id: &str,
		config: &ThumbnailJobConfig,
		ctx: &JobContext<'_>,
	) -> JobResult<bool> {
		let library = ctx.library();
		for &size in &config.sizes {
			if !library.has_thumbnail(cas_id, size).await {
				return Ok(false);
			}
		}
		Ok(true)
	}

	/// Generate thumbnails for a single entry
	async fn generate_thumbnails_for_entry_static(
		entry: &ThumbnailEntry,
		config: &ThumbnailJobConfig,
		ctx: &JobContext<'_>,
	) -> ThumbnailResult<u64> {
		use super::generator::ThumbnailGenerator;
		use super::utils::ThumbnailUtils;

		// Validate parameters
		for &size in &config.sizes {
			ThumbnailUtils::validate_thumbnail_params(size, config.quality)?;
		}

		// Create appropriate generator for the file type
		let generator = ThumbnailGenerator::for_mime_type(&entry.mime_type)?;

		// Get the full path to the source file
		let library = ctx.library();
		let source_path = library.path().join(&entry.relative_path);

		if !source_path.exists() {
			return Err(ThumbnailError::FileNotFound(entry.relative_path.clone()));
		}

		let mut total_thumbnail_size = 0u64;

		// Generate thumbnails for each configured size
		for &size in &config.sizes {
			// Skip if thumbnail already exists (unless regenerating)
			if !config.regenerate && library.has_thumbnail(&entry.cas_id, size).await {
				continue;
			}

			// Build thumbnail output path
			let thumbnail_path = library.thumbnail_path(&entry.cas_id, size);

			// Ensure directory exists
			ThumbnailUtils::ensure_thumbnail_dirs(&thumbnail_path).await?;

			// Generate the thumbnail
			let thumbnail_info = generator
				.generate(&source_path, &thumbnail_path, size, config.quality)
				.await?;

			total_thumbnail_size += thumbnail_info.size_bytes as u64;

			ctx.log(format!(
				"Generated {}x{} thumbnail for {} ({}KB)",
				thumbnail_info.dimensions.0,
				thumbnail_info.dimensions.1,
				entry.relative_path,
				thumbnail_info.size_bytes / 1024
			));
		}

		Ok(total_thumbnail_size)
	}

	/// Mock database query for development
	async fn mock_database_query_static(_query: &str) -> JobResult<Vec<ThumbnailEntry>> {
		// TODO: Replace with actual database query
		let mut entries = vec![ThumbnailEntry {
			entry_id: Uuid::new_v4(),
			cas_id: "abc123def456".to_string(),
			mime_type: "image/jpeg".to_string(),
			file_size: 1024 * 1024,
			relative_path: "photos/vacation.jpg".to_string(),
		}];

		// Only add video entry if FFmpeg feature is enabled
		#[cfg(feature = "ffmpeg")]
		{
			entries.push(ThumbnailEntry {
				entry_id: Uuid::new_v4(),
				cas_id: "def456ghi789".to_string(),
				mime_type: "video/mp4".to_string(),
				file_size: 10 * 1024 * 1024,
				relative_path: "videos/movie.mp4".to_string(),
			});
		}

		#[cfg(not(feature = "ffmpeg"))]
		{
			let _ = &mut entries; // Suppress unused variable warning
		}

		Ok(entries)
	}
}
```

## src/operations/media/thumbnail/error.rs

```rust
//! Thumbnail generation errors

use thiserror::Error;

pub type ThumbnailResult<T> = Result<T, ThumbnailError>;

#[derive(Error, Debug)]
pub enum ThumbnailError {
    #[error("I/O error: {0}")]
    Io(#[from] std::io::Error),
    
    #[error("Image processing error: {0}")]
    Image(#[from] image::ImageError),
    
    #[error("Video processing error: {0}")]
    VideoProcessing(String),
    
    #[error("Unsupported format: {0}")]
    UnsupportedFormat(String),
    
    #[error("Invalid thumbnail size: {0}")]
    InvalidSize(u32),
    
    #[error("Invalid quality setting: {0} (must be 0-100)")]
    InvalidQuality(u8),
    
    #[error("File not found: {0}")]
    FileNotFound(String),
    
    #[error("Permission denied: {0}")]
    PermissionDenied(String),
    
    #[error("Thumbnail already exists: {0}")]
    AlreadyExists(String),
    
    #[error("Database error: {0}")]
    Database(String),
    
    #[error("Serialization error: {0}")]
    Serialization(String),
    
    #[error("Other error: {0}")]
    Other(String),
}

impl ThumbnailError {
    pub fn video_processing(msg: impl Into<String>) -> Self {
        Self::VideoProcessing(msg.into())
    }
    
    pub fn unsupported_format(format: impl Into<String>) -> Self {
        Self::UnsupportedFormat(format.into())
    }
    
    pub fn database(msg: impl Into<String>) -> Self {
        Self::Database(msg.into())
    }
    
    pub fn other(msg: impl Into<String>) -> Self {
        Self::Other(msg.into())
    }
}

impl From<ThumbnailError> for crate::infrastructure::jobs::error::JobError {
    fn from(err: ThumbnailError) -> Self {
        crate::infrastructure::jobs::error::JobError::execution(err.to_string())
    }
}```

## src/operations/media/thumbnail/mod.rs

```rust
//! Thumbnail generation system
//!
//! This module provides thumbnail generation capabilities for various media types
//! including images, videos, and documents. It operates as a separate job that
//! can run independently or be triggered after indexing operations.

pub mod action;
mod job;
mod state;
mod generator;
mod error;
mod utils;

pub use job::{ThumbnailJob, ThumbnailJobConfig};
pub use state::{ThumbnailState, ThumbnailPhase, ThumbnailEntry, ThumbnailStats};
pub use generator::{ThumbnailGenerator, ThumbnailInfo, ImageGenerator, VideoGenerator};
pub use error::{ThumbnailError, ThumbnailResult};
pub use utils::ThumbnailUtils;
pub use action::ThumbnailAction;```

## src/operations/media/thumbnail/state.rs

```rust
//! Thumbnail job state management

use serde::{Deserialize, Serialize};
use std::time::{Duration, Instant};
use uuid::Uuid;

/// Phases of thumbnail generation
#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]
pub enum ThumbnailPhase {
    Discovery,
    Processing,
    Cleanup,
    Complete,
}

/// Entry information for thumbnail generation
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ThumbnailEntry {
    pub entry_id: Uuid,
    pub cas_id: String,
    pub mime_type: String,
    pub file_size: u64,
    pub relative_path: String,
}

/// Statistics for thumbnail generation
#[derive(Debug, Clone, Default, Serialize, Deserialize)]
pub struct ThumbnailStats {
    pub discovered_count: u64,
    pub generated_count: u64,
    pub skipped_count: u64,
    pub error_count: u64,
    pub total_size_bytes: u64,
    pub thumbnails_size_bytes: u64,
}

/// State for thumbnail generation job
#[derive(Debug, Clone, Serialize)]
pub struct ThumbnailState {
    pub phase: ThumbnailPhase,
    pub stats: ThumbnailStats,
    pub pending_entries: Vec<ThumbnailEntry>,
    pub batches: Vec<Vec<ThumbnailEntry>>,
    pub current_batch_index: usize,
    pub errors: Vec<String>,
    
    #[serde(skip)]
    pub started_at: Instant,
}

impl ThumbnailState {
    pub fn new() -> Self {
        Self {
            phase: ThumbnailPhase::Discovery,
            stats: ThumbnailStats::default(),
            pending_entries: Vec::new(),
            batches: Vec::new(),
            current_batch_index: 0,
            errors: Vec::new(),
            started_at: Instant::now(),
        }
    }
    
    pub fn add_error(&mut self, error: String) {
        self.errors.push(error);
        self.stats.error_count += 1;
    }
    
    pub fn record_generated(&mut self, thumbnail_size: u64) {
        self.stats.generated_count += 1;
        self.stats.thumbnails_size_bytes += thumbnail_size;
    }
    
    pub fn record_skipped(&mut self) {
        self.stats.skipped_count += 1;
    }
    
    pub fn total_processed(&self) -> u64 {
        self.stats.generated_count + self.stats.skipped_count + self.stats.error_count
    }
    
    pub fn progress_percentage(&self) -> f32 {
        if self.stats.discovered_count == 0 {
            return 0.0;
        }
        (self.total_processed() as f32 / self.stats.discovered_count as f32) * 100.0
    }
}

impl Default for ThumbnailState {
    fn default() -> Self {
        Self::new()
    }
}

// Custom deserialization to handle Instant
impl<'de> Deserialize<'de> for ThumbnailState {
    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
    where
        D: serde::Deserializer<'de>,
    {
        #[derive(Deserialize)]
        struct ThumbnailStateHelper {
            phase: ThumbnailPhase,
            stats: ThumbnailStats,
            pending_entries: Vec<ThumbnailEntry>,
            batches: Vec<Vec<ThumbnailEntry>>,
            current_batch_index: usize,
            errors: Vec<String>,
        }
        
        let helper = ThumbnailStateHelper::deserialize(deserializer)?;
        Ok(Self {
            phase: helper.phase,
            stats: helper.stats,
            pending_entries: helper.pending_entries,
            batches: helper.batches,
            current_batch_index: helper.current_batch_index,
            errors: helper.errors,
            started_at: Instant::now(), // Reset to current time on deserialization
        })
    }
}```

## src/operations/media/thumbnail/utils.rs

```rust
//! Thumbnail utility functions

use super::error::{ThumbnailError, ThumbnailResult};
use std::path::Path;

/// Utility functions for thumbnail operations
pub struct ThumbnailUtils;

impl ThumbnailUtils {
    /// Check if a file type supports thumbnail generation
    pub fn is_thumbnail_supported(mime_type: &str) -> bool {
        match mime_type {
            mime if mime.starts_with("image/") => true,
            mime if mime.starts_with("video/") => {
                #[cfg(feature = "ffmpeg")]
                {
                    true
                }
                #[cfg(not(feature = "ffmpeg"))]
                {
                    false
                }
            },
            "application/pdf" => true,
            _ => false,
        }
    }
    
    /// Get the thumbnail file extension for a given format
    pub fn get_thumbnail_extension(_format: &str) -> &'static str {
        "webp" // All thumbnails are WebP format
    }
    
    /// Validate thumbnail generation parameters
    pub fn validate_thumbnail_params(size: u32, quality: u8) -> ThumbnailResult<()> {
        if size == 0 || size > 4096 {
            return Err(ThumbnailError::InvalidSize(size));
        }
        
        if quality > 100 {
            return Err(ThumbnailError::InvalidQuality(quality));
        }
        
        Ok(())
    }
    
    /// Generate shard path for a CAS ID
    pub fn get_shard_path(cas_id: &str) -> ThumbnailResult<(String, String)> {
        if cas_id.len() < 4 {
            return Err(ThumbnailError::other("CAS ID too short for sharding"));
        }
        
        let shard1 = cas_id[0..2].to_string();
        let shard2 = cas_id[2..4].to_string();
        
        Ok((shard1, shard2))
    }
    
    /// Build thumbnail filename
    pub fn build_thumbnail_filename(cas_id: &str, size: u32) -> String {
        format!("{}_{}.webp", cas_id, size)
    }
    
    /// Build full thumbnail path with sharding
    pub fn build_thumbnail_path(
        thumbnails_dir: &Path,
        cas_id: &str,
        size: u32,
    ) -> ThumbnailResult<std::path::PathBuf> {
        let (shard1, shard2) = Self::get_shard_path(cas_id)?;
        let filename = Self::build_thumbnail_filename(cas_id, size);
        
        Ok(thumbnails_dir
            .join(shard1)
            .join(shard2)
            .join(filename))
    }
    
    /// Check if a thumbnail file exists
    pub async fn thumbnail_exists(path: &Path) -> bool {
        tokio::fs::metadata(path).await.is_ok()
    }
    
    /// Create thumbnail directory structure
    pub async fn ensure_thumbnail_dirs(thumbnail_path: &Path) -> ThumbnailResult<()> {
        if let Some(parent) = thumbnail_path.parent() {
            tokio::fs::create_dir_all(parent).await?;
        }
        Ok(())
    }
    
    /// Calculate total file size for a list of files
    pub async fn calculate_total_size(paths: &[std::path::PathBuf]) -> u64 {
        let mut total_size = 0;
        for path in paths {
            if let Ok(metadata) = tokio::fs::metadata(path).await {
                total_size += metadata.len();
            }
        }
        total_size
    }
    
    /// Clean up orphaned thumbnails (not implemented yet)
    pub async fn cleanup_orphaned_thumbnails(
        _thumbnails_dir: &Path,
        _valid_cas_ids: &[String],
    ) -> ThumbnailResult<u64> {
        // TODO: Implement cleanup logic
        // 1. Scan thumbnail directory
        // 2. Extract CAS IDs from filenames
        // 3. Check against valid_cas_ids
        // 4. Remove orphaned files
        // 5. Return number of cleaned files
        Ok(0)
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_is_thumbnail_supported() {
        assert!(ThumbnailUtils::is_thumbnail_supported("image/jpeg"));
        assert!(ThumbnailUtils::is_thumbnail_supported("image/png"));
        assert!(ThumbnailUtils::is_thumbnail_supported("video/mp4"));
        assert!(ThumbnailUtils::is_thumbnail_supported("application/pdf"));
        assert!(!ThumbnailUtils::is_thumbnail_supported("text/plain"));
        assert!(!ThumbnailUtils::is_thumbnail_supported("application/json"));
    }
    
    #[test]
    fn test_validate_thumbnail_params() {
        assert!(ThumbnailUtils::validate_thumbnail_params(256, 85).is_ok());
        assert!(ThumbnailUtils::validate_thumbnail_params(0, 85).is_err());
        assert!(ThumbnailUtils::validate_thumbnail_params(5000, 85).is_err());
        assert!(ThumbnailUtils::validate_thumbnail_params(256, 101).is_err());
    }
    
    #[test]
    fn test_get_shard_path() {
        let (shard1, shard2) = ThumbnailUtils::get_shard_path("abcdef123456").unwrap();
        assert_eq!(shard1, "ab");
        assert_eq!(shard2, "cd");
        
        assert!(ThumbnailUtils::get_shard_path("abc").is_err());
    }
    
    #[test]
    fn test_build_thumbnail_filename() {
        let filename = ThumbnailUtils::build_thumbnail_filename("abcdef123456", 256);
        assert_eq!(filename, "abcdef123456_256.webp");
    }
    
    #[test]
    fn test_build_thumbnail_path() {
        let base = std::path::Path::new("/thumbnails");
        let path = ThumbnailUtils::build_thumbnail_path(base, "abcdef123456", 256).unwrap();
        assert_eq!(
            path,
            std::path::Path::new("/thumbnails/ab/cd/abcdef123456_256.webp")
        );
    }
}```

## src/operations/media/thumbnail/generator.rs

```rust
//! Thumbnail generation engine using existing Spacedrive crates

use super::error::{ThumbnailError, ThumbnailResult};
use serde::{Deserialize, Serialize};
use std::path::Path;

/// Information about a generated thumbnail
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ThumbnailInfo {
    pub size_bytes: usize,
    pub dimensions: (u32, u32),
    pub format: String,
}

/// Multi-format thumbnail generator
#[derive(Debug)]
pub enum ThumbnailGenerator {
    Image(ImageGenerator),
    Video(VideoGenerator),
    Document(DocumentGenerator),
}

impl ThumbnailGenerator {
    /// Create appropriate generator for a MIME type
    pub fn for_mime_type(mime_type: &str) -> ThumbnailResult<Self> {
        match mime_type {
            mime if mime.starts_with("image/") => Ok(Self::Image(ImageGenerator::new())),
            mime if mime.starts_with("video/") => {
                #[cfg(feature = "ffmpeg")]
                {
                    Ok(Self::Video(VideoGenerator::new()))
                }
                #[cfg(not(feature = "ffmpeg"))]
                {
                    Err(ThumbnailError::other(
                        "Video thumbnail generation requires FFmpeg feature to be enabled"
                    ))
                }
            },
            "application/pdf" => Ok(Self::Document(DocumentGenerator::new())),
            _ => Err(ThumbnailError::unsupported_format(mime_type)),
        }
    }
    
    /// Generate thumbnail
    pub async fn generate(
        &self,
        source_path: &Path,
        output_path: &Path,
        size: u32,
        quality: u8,
    ) -> ThumbnailResult<ThumbnailInfo> {
        match self {
            Self::Image(gen) => gen.generate(source_path, output_path, size, quality).await,
            Self::Video(gen) => gen.generate(source_path, output_path, size, quality).await,
            Self::Document(gen) => gen.generate(source_path, output_path, size, quality).await,
        }
    }
}

/// Image thumbnail generator using sd-images crate
#[derive(Debug)]
pub struct ImageGenerator;

impl ImageGenerator {
    pub fn new() -> Self {
        Self
    }
    
    pub async fn generate(
        &self,
        source_path: &Path,
        output_path: &Path,
        size: u32,
        quality: u8,
    ) -> ThumbnailResult<ThumbnailInfo> {
        if quality > 100 {
            return Err(ThumbnailError::InvalidQuality(quality));
        }
        
        // Ensure output directory exists
        if let Some(parent) = output_path.parent() {
            tokio::fs::create_dir_all(parent).await?;
        }
        
        // Use tokio::task::spawn_blocking for CPU-intensive image processing
        let source_path = source_path.to_path_buf();
        let output_path = output_path.to_path_buf();
        
        let thumbnail_info = tokio::task::spawn_blocking(move || {
            // Use sd-images to load and process the image
            let img = sd_images::format_image(&source_path)
                .map_err(|e| ThumbnailError::other(format!("Failed to load image: {}", e)))?;
            
            // Calculate target dimensions maintaining aspect ratio
            let (original_width, original_height) = (img.width(), img.height());
            let (target_width, target_height) = calculate_dimensions(
                original_width, 
                original_height, 
                size
            );
            
            // Resize using high-quality algorithm
            let thumbnail = img.resize(
                target_width,
                target_height,
                image::imageops::FilterType::Lanczos3,
            );
            
            // Convert to RGB8 for consistency
            let rgb_thumbnail = thumbnail.to_rgb8();
            
            // Encode as WebP using webp crate (same as sd-ffmpeg)
            let webp_encoder = webp::Encoder::from_rgb(&rgb_thumbnail, target_width, target_height);
            let webp_memory = webp_encoder.encode(quality as f32);
            let webp_data = webp_memory.to_vec();
            
            // Write to file
            std::fs::write(&output_path, &webp_data)?;
            
            Ok::<ThumbnailInfo, ThumbnailError>(ThumbnailInfo {
                size_bytes: webp_data.len(),
                dimensions: (target_width, target_height),
                format: "webp".to_string(),
            })
        }).await.map_err(|e| ThumbnailError::other(format!("Task join error: {}", e)))??;
        
        Ok(thumbnail_info)
    }
}

/// Video thumbnail generator using sd-ffmpeg crate
#[derive(Debug)]
pub struct VideoGenerator;

impl VideoGenerator {
    pub fn new() -> Self {
        Self
    }
    
    pub async fn generate(
        &self,
        source_path: &Path,
        output_path: &Path,
        size: u32,
        quality: u8,
    ) -> ThumbnailResult<ThumbnailInfo> {
        #[cfg(feature = "ffmpeg")]
        {
            if quality > 100 {
                return Err(ThumbnailError::InvalidQuality(quality));
            }
            
            // Use sd-ffmpeg thumbnailer with proper configuration
            let thumbnailer = sd_ffmpeg::ThumbnailerBuilder::new()
                .size(sd_ffmpeg::ThumbnailSize::Scale(size))
                .quality(quality as f32)
                .seek_percentage(0.1)
                .map_err(|e| ThumbnailError::other(format!("Invalid seek percentage: {}", e)))?
                .maintain_aspect_ratio(true)
                .prefer_embedded_metadata(true)
                .build();
            
            // Generate thumbnail
            thumbnailer.process(source_path, output_path).await
                .map_err(|e| ThumbnailError::video_processing(format!("FFmpeg processing failed: {}", e)))?;
            
            // Get file size and return info
            let file_size = tokio::fs::metadata(output_path).await?.len() as usize;
            
            // Calculate approximate dimensions (actual dimensions would require parsing FFmpeg output)
            let dimensions = calculate_video_dimensions(size);
            
            Ok(ThumbnailInfo {
                size_bytes: file_size,
                dimensions,
                format: "webp".to_string(),
            })
        }
        
        #[cfg(not(feature = "ffmpeg"))]
        {
            let _ = (source_path, output_path, size, quality); // Suppress unused variable warnings
            Err(ThumbnailError::other(
                "Video thumbnail generation requires FFmpeg feature to be enabled"
            ))
        }
    }
}

/// Document thumbnail generator using sd-images crate (PDF support)
#[derive(Debug)]
pub struct DocumentGenerator;

impl DocumentGenerator {
    pub fn new() -> Self {
        Self
    }
    
    pub async fn generate(
        &self,
        source_path: &Path,
        output_path: &Path,
        size: u32,
        quality: u8,
    ) -> ThumbnailResult<ThumbnailInfo> {
        if quality > 100 {
            return Err(ThumbnailError::InvalidQuality(quality));
        }
        
        // Ensure output directory exists
        if let Some(parent) = output_path.parent() {
            tokio::fs::create_dir_all(parent).await?;
        }
        
        // Use tokio::task::spawn_blocking for CPU-intensive PDF processing
        let source_path = source_path.to_path_buf();
        let output_path = output_path.to_path_buf();
        
        let thumbnail_info = tokio::task::spawn_blocking(move || {
            // Use sd-images to handle PDF (it supports PDF through pdfium-render)
            let img = sd_images::format_image(&source_path)
                .map_err(|e| ThumbnailError::other(format!("Failed to load PDF: {}", e)))?;
            
            // Calculate target dimensions maintaining aspect ratio
            let (original_width, original_height) = (img.width(), img.height());
            let (target_width, target_height) = calculate_dimensions(
                original_width, 
                original_height, 
                size
            );
            
            // Resize using high-quality algorithm
            let thumbnail = img.resize(
                target_width,
                target_height,
                image::imageops::FilterType::Lanczos3,
            );
            
            // Convert to RGB8 for WebP encoding
            let rgb_thumbnail = thumbnail.to_rgb8();
            
            // Encode as WebP
            let webp_encoder = webp::Encoder::from_rgb(&rgb_thumbnail, target_width, target_height);
            let webp_memory = webp_encoder.encode(quality as f32);
            let webp_data = webp_memory.to_vec();
            
            // Write to file
            std::fs::write(&output_path, &webp_data)?;
            
            Ok::<ThumbnailInfo, ThumbnailError>(ThumbnailInfo {
                size_bytes: webp_data.len(),
                dimensions: (target_width, target_height),
                format: "webp".to_string(),
            })
        }).await.map_err(|e| ThumbnailError::other(format!("Task join error: {}", e)))??;
        
        Ok(thumbnail_info)
    }
}

/// Calculate target dimensions maintaining aspect ratio
fn calculate_dimensions(width: u32, height: u32, target_size: u32) -> (u32, u32) {
    let aspect_ratio = width as f32 / height as f32;
    
    if width > height {
        // Landscape
        let target_width = target_size;
        let target_height = (target_size as f32 / aspect_ratio) as u32;
        (target_width, target_height.max(1))
    } else {
        // Portrait or square
        let target_height = target_size;
        let target_width = (target_size as f32 * aspect_ratio) as u32;
        (target_width.max(1), target_height)
    }
}

/// Calculate approximate video thumbnail dimensions
/// In practice, this would need to be extracted from the actual video metadata
fn calculate_video_dimensions(target_size: u32) -> (u32, u32) {
    // Assume 16:9 aspect ratio for now (most common)
    // This is a simplified approach - in practice we'd get actual dimensions from FFmpeg
    let aspect_ratio = 16.0 / 9.0;
    
    let target_width = target_size;
    let target_height = (target_size as f32 / aspect_ratio) as u32;
    
    (target_width, target_height.max(1))
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_calculate_dimensions() {
        // Landscape image
        let (w, h) = calculate_dimensions(1920, 1080, 256);
        assert_eq!(w, 256);
        assert_eq!(h, 144);
        
        // Portrait image
        let (w, h) = calculate_dimensions(1080, 1920, 256);
        assert_eq!(w, 144);
        assert_eq!(h, 256);
        
        // Square image
        let (w, h) = calculate_dimensions(1000, 1000, 256);
        assert_eq!(w, 256);
        assert_eq!(h, 256);
    }
    
    #[test]
    fn test_generator_for_mime_type() {
        assert!(matches!(
            ThumbnailGenerator::for_mime_type("image/jpeg"),
            Ok(ThumbnailGenerator::Image(_))
        ));
        
        assert!(matches!(
            ThumbnailGenerator::for_mime_type("video/mp4"),
            Ok(ThumbnailGenerator::Video(_))
        ));
        
        assert!(matches!(
            ThumbnailGenerator::for_mime_type("application/pdf"),
            Ok(ThumbnailGenerator::Document(_))
        ));
        
        assert!(ThumbnailGenerator::for_mime_type("text/plain").is_err());
    }
}```

## src/operations/media/mod.rs

```rust
//! Media processing operations
//!
//! This module contains jobs for processing media files including:
//! - Thumbnail generation
//! - Video transcoding
//! - Audio metadata extraction
//! - Image optimization

pub mod thumbnail;

pub use thumbnail::ThumbnailJob;```

## src/test_framework/runner.rs

```rust
//! Cargo test subprocess runner implementation

use std::collections::HashMap;
use std::process::Stdio;
use std::time::{Duration, Instant};
use tempfile::TempDir;
use tokio::io::{AsyncBufReadExt, AsyncReadExt, BufReader};
use tokio::process::{Child, Command};
use tokio::time::interval;

/// A single subprocess in a cargo test-based test
pub struct TestProcess {
	pub name: String,
	pub test_function_name: String,
	pub data_dir: TempDir,
	pub child: Option<Child>,
	pub output: String,
}

/// Cargo test-based multi-process test runner
pub struct CargoTestRunner {
	processes: Vec<TestProcess>,
	global_timeout: Duration,
	test_file_name: String,
}

impl CargoTestRunner {
	/// Create a new cargo test runner
	pub fn new() -> Self {
		Self {
			processes: Vec::new(),
			global_timeout: Duration::from_secs(60),
			test_file_name: "test_core_pairing".to_string(),
		}
	}

	/// Create a new cargo test runner for a specific test file
	pub fn for_test_file(test_file_name: impl Into<String>) -> Self {
		Self {
			processes: Vec::new(),
			global_timeout: Duration::from_secs(60),
			test_file_name: test_file_name.into(),
		}
	}

	/// Set global timeout for all operations
	pub fn with_timeout(mut self, timeout: Duration) -> Self {
		self.global_timeout = timeout;
		self
	}

	/// Add a subprocess with a test function name
	pub fn add_subprocess(
		mut self,
		name: impl Into<String>,
		test_function_name: impl Into<String>,
	) -> Self {
		let name = name.into();
		let test_function_name = test_function_name.into();
		let data_dir = TempDir::new().expect("Failed to create temp dir");

		let process = TestProcess {
			name,
			test_function_name,
			data_dir,
			child: None,
			output: String::new(),
		};

		self.processes.push(process);
		self
	}

	/// Run all subprocesses and wait until success condition is met
	pub async fn run_until_success<F>(&mut self, condition: F) -> Result<(), String>
	where
		F: Fn(&HashMap<String, String>) -> bool,
	{
		// Spawn all subprocesses
		self.spawn_all_processes().await?;

		// Wait for success condition
		self.wait_until_condition(condition).await?;

		// Cleanup
		self.kill_all().await;

		Ok(())
	}

	/// Spawn a single subprocess by name
	pub async fn spawn_single_process(&mut self, name: &str) -> Result<(), String> {
		let process = self
			.processes
			.iter_mut()
			.find(|p| p.name == name)
			.ok_or_else(|| format!("Process '{}' not found", name))?;

		let mut command = Command::new("cargo");
		command
			.args(&[
				"test",
				&process.test_function_name,
				"--test",
				&self.test_file_name,
				"--",
				"--nocapture",
				"--ignored", // Run ignored tests
			])
			.env("TEST_ROLE", &process.name)
			.env("TEST_DATA_DIR", process.data_dir.path().to_str().unwrap());

		let child = command
			.stdout(Stdio::inherit())
			.stderr(Stdio::inherit())
			.spawn()
			.map_err(|e| format!("Failed to spawn process '{}': {}", process.name, e))?;

		process.child = Some(child);
		println!(
			"ðŸš€ Spawned cargo test process: {} (test: {})",
			process.name, process.test_function_name
		);

		Ok(())
	}

	/// Wait for success condition without spawning processes
	pub async fn wait_for_success<F>(&mut self, condition: F) -> Result<(), String>
	where
		F: Fn(&HashMap<String, String>) -> bool,
	{
		// Wait for success condition
		self.wait_until_condition(condition).await?;

		// Cleanup
		self.kill_all().await;

		Ok(())
	}

	/// Spawn all subprocesses using cargo test
	async fn spawn_all_processes(&mut self) -> Result<(), String> {
		for process in &mut self.processes {
			let mut command = Command::new("cargo");
			command
				.args(&[
					"test",
					&process.test_function_name,
					"--test",
					&self.test_file_name,
					"--",
					"--nocapture",
					"--ignored", // Run ignored tests
				])
				.env("TEST_ROLE", &process.name)
				.env("TEST_DATA_DIR", process.data_dir.path().to_str().unwrap());

			let child = command
				.stdout(Stdio::inherit())
				.stderr(Stdio::inherit())
				.spawn()
				.map_err(|e| format!("Failed to spawn process '{}': {}", process.name, e))?;

			process.child = Some(child);
			println!(
				"ðŸš€ Spawned cargo test process: {} (test: {})",
				process.name, process.test_function_name
			);
		}

		Ok(())
	}

	/// Wait until the success condition is met
	async fn wait_until_condition<F>(&mut self, condition: F) -> Result<(), String>
	where
		F: Fn(&HashMap<String, String>) -> bool,
	{
		let mut check_interval = interval(Duration::from_millis(100));
		let start_time = Instant::now();

		loop {
			tokio::select! {
				_ = check_interval.tick() => {
					// Read output from all processes
					self.read_all_output().await;

					// Build output map for condition check
					let outputs: HashMap<String, String> = self.processes.iter()
						.map(|p| (p.name.clone(), p.output.clone()))
						.collect();

					// Check condition
					if condition(&outputs) {
						println!("âœ… Success condition met after {:?}", start_time.elapsed());
						return Ok(());
					}

					// Check for timeout
					if start_time.elapsed() > self.global_timeout {
						return Err("Timeout waiting for success condition".to_string());
					}

					// Check for failed processes
					self.check_process_health()?;
				}
			}
		}
	}

	/// Read output from all running processes
	async fn read_all_output(&mut self) {
		// Output is handled via stdio inheritance - just track what we see in output
		for process in &mut self.processes {
			if let Some(child) = &mut process.child {
				// Check if process has exited to capture final output
				if let Ok(Some(_)) = child.try_wait() {
					// Process has exited, mark its output as complete
				}
			}
		}
	}

	/// Check if any processes have failed
	fn check_process_health(&mut self) -> Result<(), String> {
		for process in &mut self.processes {
			if let Some(child) = &mut process.child {
				if let Ok(Some(exit_status)) = child.try_wait() {
					if !exit_status.success() {
						return Err(format!(
							"Process '{}' exited with failure: {:?}",
							process.name,
							exit_status.code()
						));
					}
				}
			}
		}
		Ok(())
	}

	/// Kill all processes
	pub async fn kill_all(&mut self) {
		for process in &mut self.processes {
			if let Some(mut child) = process.child.take() {
				let _ = child.kill().await;
				let _ = child.wait().await;
			}
		}
		println!("ðŸ§¹ Killed all cargo test processes");
	}

	/// Get output from a specific process
	pub fn get_output(&self, name: &str) -> Option<&str> {
		self.processes
			.iter()
			.find(|p| p.name == name)
			.map(|p| p.output.as_str())
	}

	/// Get all outputs as a map
	pub fn get_all_outputs(&self) -> HashMap<String, String> {
		self.processes
			.iter()
			.map(|p| (p.name.clone(), p.output.clone()))
			.collect()
	}
}

impl Drop for CargoTestRunner {
	fn drop(&mut self) {
		// Best effort cleanup
		for process in &mut self.processes {
			if let Some(mut child) = process.child.take() {
				let _ = child.start_kill();
			}
		}
	}
}

impl Default for CargoTestRunner {
	fn default() -> Self {
		Self::new()
	}
}
```

## src/test_framework/mod.rs

```rust
//! Cargo Test Subprocess Framework for Spacedrive
//!
//! This framework allows test logic to remain in test files while still providing
//! subprocess isolation for multi-device networking tests. It uses `cargo test`
//! as the subprocess executor, coordinated via environment variables.

pub mod runner;

pub use runner::CargoTestRunner;
```

## src/keys/library_key_manager.rs

```rust
//! Library encryption key management using OS secure storage
use keyring::{Entry, Error as KeyringError};
use rand::RngCore;
use std::collections::HashMap;
use thiserror::Error;
use uuid::Uuid;

const KEYRING_SERVICE: &str = "SpacedriveLibraryKeys";
const LIBRARY_KEY_LENGTH: usize = 32; // 256 bits

#[derive(Error, Debug)]
pub enum LibraryKeyError {
	#[error("Keyring error: {0}")]
	Keyring(#[from] KeyringError),
	#[error("Invalid key length: expected {LIBRARY_KEY_LENGTH} bytes, got {0}")]
	InvalidKeyLength(usize),
	#[error("Key not found for library: {0}")]
	KeyNotFound(Uuid),
	#[error("Failed to generate random key")]
	RandomKeyGenerationFailed,
	#[error("JSON error: {0}")]
	Json(#[from] serde_json::Error),
}

pub struct LibraryKeyManager {
	keyring_entry: Entry,
}

impl LibraryKeyManager {
	pub fn new() -> Result<Self, LibraryKeyError> {
		let entry = Entry::new(KEYRING_SERVICE, "library_keys_store")?;
		Ok(Self {
			keyring_entry: entry,
		})
	}

	/// Get or create a library encryption key for a given library ID
	pub fn get_or_create_library_key(
		&self,
		library_id: Uuid,
	) -> Result<[u8; LIBRARY_KEY_LENGTH], LibraryKeyError> {
		let mut all_keys = self.load_all_library_keys()?;
		if let Some(key_hex) = all_keys.get(&library_id) {
			let key_bytes =
				hex::decode(key_hex).map_err(|_| LibraryKeyError::InvalidKeyLength(0))?; // Placeholder for actual error
			if key_bytes.len() != LIBRARY_KEY_LENGTH {
				return Err(LibraryKeyError::InvalidKeyLength(key_bytes.len()));
			}
			let mut key = [0u8; LIBRARY_KEY_LENGTH];
			key.copy_from_slice(&key_bytes);
			Ok(key)
		} else {
			let new_key = self.generate_new_library_key()?;
			all_keys.insert(library_id, hex::encode(new_key));
			self.save_all_library_keys(&all_keys)?;
			Ok(new_key)
		}
	}

	/// Get a library encryption key for a given library ID
	pub fn get_library_key(
		&self,
		library_id: Uuid,
	) -> Result<[u8; LIBRARY_KEY_LENGTH], LibraryKeyError> {
		let all_keys = self.load_all_library_keys()?;
		let key_hex = all_keys
			.get(&library_id)
			.ok_or(LibraryKeyError::KeyNotFound(library_id))?;
		let key_bytes = hex::decode(key_hex).map_err(|_| LibraryKeyError::InvalidKeyLength(0))?; // Placeholder for actual error
		if key_bytes.len() != LIBRARY_KEY_LENGTH {
			return Err(LibraryKeyError::InvalidKeyLength(key_bytes.len()));
		}
		let mut key = [0u8; LIBRARY_KEY_LENGTH];
		key.copy_from_slice(&key_bytes);
		Ok(key)
	}

	/// Store a library encryption key for a given library ID
	pub fn store_library_key(
		&self,
		library_id: Uuid,
		key: [u8; LIBRARY_KEY_LENGTH],
	) -> Result<(), LibraryKeyError> {
		let mut all_keys = self.load_all_library_keys()?;
		all_keys.insert(library_id, hex::encode(key));
		self.save_all_library_keys(&all_keys)?;
		Ok(())
	}

	/// Delete a library encryption key for a given library ID
	pub fn delete_library_key(&self, library_id: Uuid) -> Result<(), LibraryKeyError> {
		let mut all_keys = self.load_all_library_keys()?;
		all_keys.remove(&library_id);
		self.save_all_library_keys(&all_keys)?;
		Ok(())
	}

	fn generate_new_library_key(&self) -> Result<[u8; LIBRARY_KEY_LENGTH], LibraryKeyError> {
		use rand::RngCore;
		let mut key = [0u8; LIBRARY_KEY_LENGTH];
		rand::thread_rng()
			.try_fill_bytes(&mut key)
			.map_err(|_| LibraryKeyError::RandomKeyGenerationFailed)?;
		Ok(key)
	}

	fn load_all_library_keys(&self) -> Result<HashMap<Uuid, String>, LibraryKeyError> {
		match self.keyring_entry.get_password() {
			Ok(json_string) => serde_json::from_str(&json_string).map_err(LibraryKeyError::Json),
			Err(KeyringError::NoEntry) => Ok(HashMap::new()),
			Err(e) => Err(LibraryKeyError::Keyring(e)),
		}
	}

	fn save_all_library_keys(&self, keys: &HashMap<Uuid, String>) -> Result<(), LibraryKeyError> {
		let json_string = serde_json::to_string(keys).map_err(LibraryKeyError::Json)?;
		self.keyring_entry.set_password(&json_string)?;
		Ok(())
	}
}

#[cfg(test)]
mod tests {
	use super::*;
	use uuid::Uuid;

	// Helper to clean up keyring entry after tests
	struct TestCleanup {
		manager: LibraryKeyManager,
	}

	impl Drop for TestCleanup {
		fn drop(&mut self) {
			let _ = self.manager.keyring_entry.delete_credential();
		}
	}

	fn create_test_manager() -> (LibraryKeyManager, TestCleanup) {
		let manager = LibraryKeyManager::new().unwrap();
		let cleanup = TestCleanup {
			manager: LibraryKeyManager::new().unwrap(),
		};
		// Ensure a clean state before test
		let _ = manager.keyring_entry.delete_credential();
		(manager, cleanup)
	}

	#[test]
	fn test_generate_and_retrieve_library_key() {
		let (manager, _cleanup) = create_test_manager();
		let library_id = Uuid::new_v4();

		let key1 = manager.get_or_create_library_key(library_id).unwrap();
		let key2 = manager.get_library_key(library_id).unwrap();

		assert_eq!(key1, key2);
		assert_eq!(key1.len(), LIBRARY_KEY_LENGTH);
	}

	#[test]
	fn test_library_key_persistence() {
		let (manager1, _cleanup) = create_test_manager();
		let library_id = Uuid::new_v4();

		let key1 = manager1.get_or_create_library_key(library_id).unwrap();
		drop(manager1); // Simulate application restart

		let (manager2, _cleanup2) = create_test_manager();
		let key2 = manager2.get_library_key(library_id).unwrap();

		assert_eq!(key1, key2);
	}

	#[test]
	fn test_store_and_delete_library_key() {
		let (manager, _cleanup) = create_test_manager();
		let library_id = Uuid::new_v4();
		let mut test_key = [0u8; LIBRARY_KEY_LENGTH];
		rand::thread_rng().fill_bytes(&mut test_key);

		manager.store_library_key(library_id, test_key).unwrap();
		let retrieved_key = manager.get_library_key(library_id).unwrap();
		assert_eq!(test_key, retrieved_key);

		manager.delete_library_key(library_id).unwrap();
		let result = manager.get_library_key(library_id);
		assert!(matches!(result, Err(LibraryKeyError::KeyNotFound(_))));
	}

	#[test]
	fn test_multiple_library_keys() {
		let (manager, _cleanup) = create_test_manager();
		let library_id1 = Uuid::new_v4();
		let library_id2 = Uuid::new_v4();

		let key1 = manager.get_or_create_library_key(library_id1).unwrap();
		let key2 = manager.get_or_create_library_key(library_id2).unwrap();

		assert_ne!(key1, key2);

		let retrieved_key1 = manager.get_library_key(library_id1).unwrap();
		let retrieved_key2 = manager.get_library_key(library_id2).unwrap();

		assert_eq!(key1, retrieved_key1);
		assert_eq!(key2, retrieved_key2);
	}
}
```

## src/keys/mod.rs

```rust
pub mod device_key_manager;
pub mod library_key_manager;
```

## src/keys/device_key_manager.rs

```rust
//! Master encryption key management using OS secure storage

use keyring::{Entry, Error as KeyringError};
use rand::{thread_rng, Rng};
use thiserror::Error;
use uuid::Uuid;

const KEYRING_SERVICE: &str = "Spacedrive";
const DEVICE_KEY_USERNAME: &str = "master_encryption_key";
const MASTER_KEY_LENGTH: usize = 32; // 256 bits

#[derive(Error, Debug)]
pub enum DeviceKeyError {
    #[error("Keyring error: {0}")]
    Keyring(#[from] KeyringError),
    
    #[error("Invalid key format")]
    InvalidKeyFormat,
    
    #[error("Key generation failed")]
    KeyGenerationFailed,
}

pub struct DeviceKeyManager {
    entry: Entry,
}

impl DeviceKeyManager {
    pub fn new() -> Result<Self, DeviceKeyError> {
        let entry = Entry::new(KEYRING_SERVICE, DEVICE_KEY_USERNAME)?;
        Ok(Self { entry })
    }

    #[cfg(test)]
    pub fn new_for_test(service: &str, username: &str) -> Result<Self, DeviceKeyError> {
        let entry = Entry::new(service, username)?;
        Ok(Self { entry })
    }

    pub fn get_or_create_master_key(&self) -> Result<[u8; MASTER_KEY_LENGTH], DeviceKeyError> {
        match self.entry.get_password() {
            Ok(key_hex) => {
                let key_bytes = hex::decode(key_hex)
                    .map_err(|_| DeviceKeyError::InvalidKeyFormat)?;
                
                if key_bytes.len() != MASTER_KEY_LENGTH {
                    return Err(DeviceKeyError::InvalidKeyFormat);
                }
                
                let mut key = [0u8; MASTER_KEY_LENGTH];
                key.copy_from_slice(&key_bytes);
                Ok(key)
            }
            Err(KeyringError::NoEntry) => {
                let key = self.generate_new_master_key()?;
                let key_hex = hex::encode(key);
                self.entry.set_password(&key_hex)?;
                Ok(key)
            }
            Err(e) => Err(DeviceKeyError::Keyring(e)),
        }
    }

    pub fn get_master_key(&self) -> Result<[u8; MASTER_KEY_LENGTH], DeviceKeyError> {
        let key_hex = self.entry.get_password()?;
        let key_bytes = hex::decode(key_hex)
            .map_err(|_| DeviceKeyError::InvalidKeyFormat)?;
        
        if key_bytes.len() != MASTER_KEY_LENGTH {
            return Err(DeviceKeyError::InvalidKeyFormat);
        }
        
        let mut key = [0u8; MASTER_KEY_LENGTH];
        key.copy_from_slice(&key_bytes);
        Ok(key)
    }

    pub fn get_master_key_hex(&self) -> Result<String, DeviceKeyError> {
        let key = self.get_master_key()?;
        Ok(hex::encode(key))
    }

    fn generate_new_master_key(&self) -> Result<[u8; MASTER_KEY_LENGTH], DeviceKeyError> {
        let mut key = [0u8; MASTER_KEY_LENGTH];
        thread_rng().fill(&mut key);
        Ok(key)
    }

    pub fn regenerate_master_key(&self) -> Result<[u8; MASTER_KEY_LENGTH], DeviceKeyError> {
        let key = self.generate_new_master_key()?;
        let key_hex = hex::encode(key);
        self.entry.set_password(&key_hex)?;
        Ok(key)
    }

    pub fn delete_master_key(&self) -> Result<(), DeviceKeyError> {
        self.entry.delete_credential()?;
        Ok(())
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use keyring::Entry;

    const TEST_SERVICE: &str = "SpacedriveTest";
    const TEST_USERNAME: &str = "test_master_key";

    fn create_test_manager() -> DeviceKeyManager {
        let entry = Entry::new(TEST_SERVICE, TEST_USERNAME).unwrap();
        DeviceKeyManager { entry }
    }

    fn cleanup_test_key() {
        let entry = Entry::new(TEST_SERVICE, TEST_USERNAME).unwrap();
        let _ = entry.delete_credential();
    }

    #[test]
    fn test_generate_and_retrieve_master_key() {
        cleanup_test_key();
        let manager = create_test_manager();

        let key1 = manager.get_or_create_master_key().unwrap();
        let key2 = manager.get_master_key().unwrap();

        assert_eq!(key1, key2);
        assert_eq!(key1.len(), MASTER_KEY_LENGTH);

        cleanup_test_key();
    }

    #[test]
    fn test_master_key_persistence() {
        cleanup_test_key();
        let manager1 = create_test_manager();
        let key1 = manager1.get_or_create_master_key().unwrap();

        let manager2 = create_test_manager();
        let key2 = manager2.get_master_key().unwrap();

        assert_eq!(key1, key2);

        cleanup_test_key();
    }

    #[test]
    fn test_regenerate_master_key() {
        cleanup_test_key();
        let manager = create_test_manager();

        let key1 = manager.get_or_create_master_key().unwrap();
        let key2 = manager.regenerate_master_key().unwrap();

        assert_ne!(key1, key2);
        assert_eq!(key2.len(), MASTER_KEY_LENGTH);

        let key3 = manager.get_master_key().unwrap();
        assert_eq!(key2, key3);

        cleanup_test_key();
    }

    #[test]
    fn test_hex_representation() {
        cleanup_test_key();
        let manager = create_test_manager();

        let key = manager.get_or_create_master_key().unwrap();
        let hex_key = manager.get_master_key_hex().unwrap();

        assert_eq!(hex_key.len(), MASTER_KEY_LENGTH * 2);
        assert_eq!(hex::decode(&hex_key).unwrap(), key);

        cleanup_test_key();
    }
}```

## src/device/config.rs

```rust
//! Device configuration persistence

use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use std::path::PathBuf;
use uuid::Uuid;

/// Device configuration stored on disk
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DeviceConfig {
    /// Unique device identifier
    pub id: Uuid,

    /// User-friendly device name
    pub name: String,

    /// When this device was first initialized
    pub created_at: DateTime<Utc>,

    /// Hardware model (if detectable)
    pub hardware_model: Option<String>,

    /// Operating system
    pub os: String,

    /// Spacedrive version that created this config
    pub version: String,
}

impl DeviceConfig {
    /// Create a new device configuration
    pub fn new(name: String, os: String) -> Self {
        Self {
            id: Uuid::new_v4(),
            name,
            created_at: Utc::now(),
            hardware_model: None,
            os,
            version: env!("CARGO_PKG_VERSION").to_string(),
        }
    }

    /// Get the configuration file path for the current platform
    pub fn config_path() -> Result<PathBuf, super::DeviceError> {
        let base_path = if cfg!(target_os = "macos") {
            dirs::data_dir()
                .ok_or(super::DeviceError::ConfigPathNotFound)?
                .join("com.spacedrive")
        } else if cfg!(target_os = "linux") {
            dirs::config_dir()
                .ok_or(super::DeviceError::ConfigPathNotFound)?
                .join("spacedrive")
        } else if cfg!(target_os = "windows") {
            dirs::config_dir()
                .ok_or(super::DeviceError::ConfigPathNotFound)?
                .join("Spacedrive")
        } else {
            return Err(super::DeviceError::UnsupportedPlatform);
        };

        Ok(base_path.join("device.json"))
    }

    /// Load configuration from disk
    pub fn load() -> Result<Self, super::DeviceError> {
        let path = Self::config_path()?;

        if !path.exists() {
            return Err(super::DeviceError::NotInitialized);
        }

        let content = std::fs::read_to_string(&path)?;
        let config: Self = serde_json::from_str(&content)?;

        Ok(config)
    }

    /// Save configuration to disk
    pub fn save(&self) -> Result<(), super::DeviceError> {
        let path = Self::config_path()?;

        // Ensure parent directory exists
        if let Some(parent) = path.parent() {
            std::fs::create_dir_all(parent)?;
        }

        let content = serde_json::to_string_pretty(self)?;
        std::fs::write(&path, content)?;

        Ok(())
    }

    /// Load configuration from a specific directory
    pub fn load_from(data_dir: &PathBuf) -> Result<Self, super::DeviceError> {
        let path = data_dir.join("device.json");

        if !path.exists() {
            return Err(super::DeviceError::NotInitialized);
        }

        let content = std::fs::read_to_string(&path)?;
        let config: Self = serde_json::from_str(&content)?;

        Ok(config)
    }

    /// Save configuration to a specific directory
    pub fn save_to(&self, data_dir: &PathBuf) -> Result<(), super::DeviceError> {
        // Ensure directory exists
        std::fs::create_dir_all(data_dir)?;

        let path = data_dir.join("device.json");
        let content = serde_json::to_string_pretty(self)?;
        std::fs::write(&path, content)?;

        Ok(())
    }
}```

## src/device/manager.rs

```rust
//! Device manager for handling device lifecycle

use super::config::DeviceConfig;
use crate::keys::device_key_manager::{DeviceKeyManager, DeviceKeyError};
use crate::domain::device::{Device, OperatingSystem};
use std::path::PathBuf;
use std::sync::{Arc, RwLock};
use thiserror::Error;
use uuid::Uuid;

/// Errors that can occur during device management
#[derive(Error, Debug)]
pub enum DeviceError {
    #[error("Device not initialized")]
    NotInitialized,
    
    #[error("Config path not found")]
    ConfigPathNotFound,
    
    #[error("Unsupported platform")]
    UnsupportedPlatform,
    
    #[error("IO error: {0}")]
    Io(#[from] std::io::Error),
    
    #[error("Serialization error: {0}")]
    Serialization(#[from] serde_json::Error),
    
    #[error("Lock poisoned")]
    LockPoisoned,
    
    #[error("Master key error: {0}")]
    MasterKey(#[from] DeviceKeyError),
}

/// Manages the current device state
pub struct DeviceManager {
    /// Current device configuration
    config: Arc<RwLock<DeviceConfig>>,
    /// Master encryption key manager
    device_key_manager: DeviceKeyManager,
}

impl DeviceManager {
    /// Initialize the device manager
    /// 
    /// This will either load existing device configuration or create a new one
    pub fn init() -> Result<Self, DeviceError> {
        let config = match DeviceConfig::load() {
            Ok(config) => config,
            Err(DeviceError::NotInitialized) => {
                // Create new device configuration
                let os = detect_os();
                let name = get_device_name();
                let mut config = DeviceConfig::new(name, os);
                
                // Try to detect hardware model
                config.hardware_model = detect_hardware_model();
                
                // Save the new configuration
                config.save()?;
                config
            }
            Err(e) => return Err(e),
        };
        
        let device_key_manager = DeviceKeyManager::new()?;
        // Initialize master key on first run
        device_key_manager.get_or_create_master_key()?;
        
        Ok(Self {
            config: Arc::new(RwLock::new(config)),
            device_key_manager,
        })
    }
    
    /// Initialize the device manager with a custom data directory
    pub fn init_with_path(data_dir: &PathBuf) -> Result<Self, DeviceError> {
        let config = match DeviceConfig::load_from(data_dir) {
            Ok(config) => config,
            Err(DeviceError::NotInitialized) => {
                // Create new device configuration
                let os = detect_os();
                let name = get_device_name();
                let mut config = DeviceConfig::new(name, os);
                
                // Try to detect hardware model
                config.hardware_model = detect_hardware_model();
                
                // Save the new configuration
                config.save_to(data_dir)?;
                config
            }
            Err(e) => return Err(e),
        };
        
        let device_key_manager = DeviceKeyManager::new()?;
        // Initialize master key on first run
        device_key_manager.get_or_create_master_key()?;
        
        Ok(Self {
            config: Arc::new(RwLock::new(config)),
            device_key_manager,
        })
    }
    
    /// Get the current device ID
    pub fn device_id(&self) -> Result<Uuid, DeviceError> {
        self.config
            .read()
            .map(|c| c.id)
            .map_err(|_| DeviceError::LockPoisoned)
    }
    
    /// Get the current device as a domain Device object
    pub async fn current_device(&self) -> Device {
        let config = self.config.read().unwrap();
        Device {
            id: config.id,
            name: config.name.clone(),
            os: parse_os(&config.os),
            hardware_model: config.hardware_model.clone(),
            network_addresses: vec![],
            is_online: true,
            sync_leadership: std::collections::HashMap::new(),
            last_seen_at: chrono::Utc::now(),
            created_at: chrono::Utc::now(),
            updated_at: chrono::Utc::now(),
        }
    }
    
    /// Get the current device configuration
    pub fn config(&self) -> Result<DeviceConfig, DeviceError> {
        self.config
            .read()
            .map(|c| c.clone())
            .map_err(|_| DeviceError::LockPoisoned)
    }
    
    /// Create a Device domain object from current configuration
    pub fn to_device(&self) -> Result<Device, DeviceError> {
        let config = self.config()?;
        
        // Create device with loaded configuration
        let mut device = Device::new(config.name.clone());
        device.id = config.id;
        device.os = parse_os(&config.os);
        device.hardware_model = config.hardware_model.clone();
        device.created_at = config.created_at;
        
        Ok(device)
    }
    
    /// Update device name
    pub fn set_name(&self, name: String) -> Result<(), DeviceError> {
        let mut config = self.config
            .write()
            .map_err(|_| DeviceError::LockPoisoned)?;
        
        config.name = name;
        config.save()?;
        
        Ok(())
    }
    
    /// Get the master encryption key
    pub fn master_key(&self) -> Result<[u8; 32], DeviceError> {
        Ok(self.device_key_manager.get_master_key()?)
    }
    
    /// Get the master encryption key as hex string
    pub fn master_key_hex(&self) -> Result<String, DeviceError> {
        Ok(self.device_key_manager.get_master_key_hex()?)
    }
    
    /// Regenerate the master encryption key (dangerous operation)
    pub fn regenerate_device_key(&self) -> Result<[u8; 32], DeviceError> {
        Ok(self.device_key_manager.regenerate_master_key()?)
    }
}

/// Get the device name from the system
fn get_device_name() -> String {
    whoami::devicename()
}

/// Detect the operating system
fn detect_os() -> String {
    if cfg!(target_os = "macos") {
        "macOS".to_string()
    } else if cfg!(target_os = "windows") {
        "Windows".to_string()
    } else if cfg!(target_os = "linux") {
        "Linux".to_string()
    } else if cfg!(target_os = "ios") {
        "iOS".to_string()
    } else if cfg!(target_os = "android") {
        "Android".to_string()
    } else {
        "Unknown".to_string()
    }
}

/// Parse OS string back to enum
fn parse_os(os: &str) -> OperatingSystem {
    match os {
        "macOS" => OperatingSystem::MacOS,
        "Windows" => OperatingSystem::Windows,
        "Linux" => OperatingSystem::Linux,
        "iOS" => OperatingSystem::IOs,
        "Android" => OperatingSystem::Android,
        _ => OperatingSystem::Other,
    }
}

/// Try to detect hardware model
fn detect_hardware_model() -> Option<String> {
    #[cfg(target_os = "macos")]
    {
        // Try to get model from system_profiler
        use std::process::Command;
        
        let output = Command::new("system_profiler")
            .args(&["SPHardwareDataType", "-json"])
            .output()
            .ok()?;
        
        if output.status.success() {
            let json_str = String::from_utf8_lossy(&output.stdout);
            // Simple extraction - in production would use proper JSON parsing
            if let Some(start) = json_str.find("\"machine_model\":") {
                let substr = &json_str[start + 17..];
                if let Some(end) = substr.find('"') {
                    return Some(substr[..end].to_string());
                }
            }
        }
    }
    
    None
}

#[cfg(test)]
mod tests {
    use super::*;
    use tempfile::TempDir;
    
    #[test]
    fn test_device_config() {
        let config = DeviceConfig::new("Test Device".to_string(), "Linux".to_string());
        assert_eq!(config.name, "Test Device");
        assert_eq!(config.os, "Linux");
        assert!(config.hardware_model.is_none());
    }
}```

## src/device/mod.rs

```rust
//! Device management module
//! 
//! Handles persistent device identification across Spacedrive installations

mod config;
mod manager;
pub use config::DeviceConfig;
pub use manager::{DeviceManager, DeviceError};
pub use crate::keys::device_key_manager::{DeviceKeyManager, DeviceKeyError};

// Re-export domain types
pub use crate::domain::device::{Device, OperatingSystem};```

## src/infrastructure/database/mod.rs

```rust
//! Database infrastructure using SeaORM

use sea_orm::{
    ConnectOptions, Database as SeaDatabase, DatabaseConnection, DbErr,
};
use sea_orm_migration::MigratorTrait;
use std::path::Path;
use std::time::Duration;
use tracing::info;

pub mod entities;
pub mod migration;

/// Database wrapper for Spacedrive
pub struct Database {
    /// SeaORM database connection
    conn: DatabaseConnection,
}

impl Database {
    /// Create a new database at the specified path
    pub async fn create(path: &Path) -> Result<Self, DbErr> {
        // Ensure parent directory exists
        if let Some(parent) = path.parent() {
            std::fs::create_dir_all(parent)
                .map_err(|e| DbErr::Custom(format!("Failed to create directory: {}", e)))?;
        }
        
        let db_url = format!("sqlite://{}?mode=rwc", path.display());
        
        let mut opt = ConnectOptions::new(db_url);
        opt.max_connections(10)
            .min_connections(5)
            .connect_timeout(Duration::from_secs(8))
            .idle_timeout(Duration::from_secs(8))
            .max_lifetime(Duration::from_secs(8))
            .sqlx_logging(false); // We'll use tracing instead
        
        let conn = SeaDatabase::connect(opt).await?;
        
        info!("Created new database at {:?}", path);
        
        Ok(Self { conn })
    }
    
    /// Open an existing database
    pub async fn open(path: &Path) -> Result<Self, DbErr> {
        if !path.exists() {
            return Err(DbErr::Custom(format!(
                "Database does not exist: {}",
                path.display()
            )));
        }
        
        let db_url = format!("sqlite://{}", path.display());
        
        let mut opt = ConnectOptions::new(db_url);
        opt.max_connections(10)
            .min_connections(5)
            .connect_timeout(Duration::from_secs(8))
            .idle_timeout(Duration::from_secs(8))
            .max_lifetime(Duration::from_secs(8))
            .sqlx_logging(false);
        
        let conn = SeaDatabase::connect(opt).await?;
        
        info!("Opened database at {:?}", path);
        
        Ok(Self { conn })
    }
    
    /// Run migrations
    pub async fn migrate(&self) -> Result<(), DbErr> {
        migration::Migrator::up(&self.conn, None).await?;
        info!("Database migrations completed successfully");
        Ok(())
    }
    
    /// Get the database connection
    pub fn conn(&self) -> &DatabaseConnection {
        &self.conn
    }
}```

## src/infrastructure/database/migration/m20240103_000001_create_volumes.rs

```rust
//! Create volumes table for tracking mounted volumes in each library

use sea_orm_migration::prelude::*;

#[derive(DeriveMigrationName)]
pub struct Migration;

#[async_trait::async_trait]
impl MigrationTrait for Migration {
	async fn up(&self, manager: &SchemaManager) -> Result<(), DbErr> {
		// Create volumes table
		manager
			.create_table(
				Table::create()
					.table(Volumes::Table)
					.if_not_exists()
					.col(
						ColumnDef::new(Volumes::Id)
							.integer()
							.not_null()
							.auto_increment()
							.primary_key(),
					)
					.col(ColumnDef::new(Volumes::Uuid).text().not_null().unique_key())
					.col(ColumnDef::new(Volumes::DeviceId).text().not_null()) // Device this volume belongs to
					.col(ColumnDef::new(Volumes::Fingerprint).text().not_null())
					.col(ColumnDef::new(Volumes::DisplayName).text())
					.col(ColumnDef::new(Volumes::TrackedAt).timestamp().not_null())
					.col(ColumnDef::new(Volumes::LastSeenAt).timestamp().not_null())
					.col(
						ColumnDef::new(Volumes::IsOnline)
							.boolean()
							.not_null()
							.default(true),
					)
					.col(ColumnDef::new(Volumes::TotalCapacity).big_integer())
					.col(ColumnDef::new(Volumes::AvailableCapacity).big_integer())
					.col(ColumnDef::new(Volumes::ReadSpeedMbps).integer())
					.col(ColumnDef::new(Volumes::WriteSpeedMbps).integer())
					.col(ColumnDef::new(Volumes::LastSpeedTestAt).timestamp())
					.col(ColumnDef::new(Volumes::FileSystem).text())
					.col(ColumnDef::new(Volumes::MountPoint).text())
					.col(ColumnDef::new(Volumes::IsRemovable).boolean())
					.col(ColumnDef::new(Volumes::IsNetworkDrive).boolean())
					.col(ColumnDef::new(Volumes::DeviceModel).text())
					// Volume classification fields
					.col(
						ColumnDef::new(Volumes::VolumeType)
							.text()
							.not_null()
							.default("Unknown"),
					)
					.col(
						ColumnDef::new(Volumes::IsUserVisible)
							.boolean()
							.not_null()
							.default(true),
					)
					.col(
						ColumnDef::new(Volumes::AutoTrackEligible)
							.boolean()
							.not_null()
							.default(false),
					)
					.foreign_key(
						ForeignKey::create()
							.name("fk_volumes_device_id")
							.from(Volumes::Table, Volumes::DeviceId)
							.to(Devices::Table, Devices::Uuid)
							.on_delete(ForeignKeyAction::Cascade)
							.on_update(ForeignKeyAction::Cascade),
					)
					.to_owned(),
			)
			.await?;

		// Create unique index on device_id + fingerprint (volumes are unique per device)
		manager
			.create_index(
				Index::create()
					.name("idx_volume_device_fingerprint_unique")
					.table(Volumes::Table)
					.col(Volumes::DeviceId)
					.col(Volumes::Fingerprint)
					.unique()
					.to_owned(),
			)
			.await?;

		// Create index on device_id for efficient device queries
		manager
			.create_index(
				Index::create()
					.name("idx_volume_device_id")
					.table(Volumes::Table)
					.col(Volumes::DeviceId)
					.to_owned(),
			)
			.await?;

		// Create index on last_seen_at for efficient queries
		manager
			.create_index(
				Index::create()
					.name("idx_volume_last_seen_at")
					.table(Volumes::Table)
					.col(Volumes::LastSeenAt)
					.to_owned(),
			)
			.await?;

		// Create index on is_online for filtering
		manager
			.create_index(
				Index::create()
					.name("idx_volume_is_online")
					.table(Volumes::Table)
					.col(Volumes::IsOnline)
					.to_owned(),
			)
			.await?;

		Ok(())
	}

	async fn down(&self, manager: &SchemaManager) -> Result<(), DbErr> {
		manager
			.drop_table(Table::drop().table(Volumes::Table).to_owned())
			.await
	}
}

#[derive(DeriveIden)]
enum Volumes {
	Table,
	Id,
	Uuid,
	DeviceId,
	Fingerprint,
	DisplayName,
	TrackedAt,
	LastSeenAt,
	IsOnline,
	TotalCapacity,
	AvailableCapacity,
	ReadSpeedMbps,
	WriteSpeedMbps,
	LastSpeedTestAt,
	FileSystem,
	MountPoint,
	IsRemovable,
	IsNetworkDrive,
	DeviceModel,
	// Volume classification fields
	VolumeType,
	IsUserVisible,
	AutoTrackEligible,
}

#[derive(DeriveIden)]
enum Devices {
	Table,
	Uuid,
}
```

## src/infrastructure/database/migration/m20240101_000001_create_initial_tables.rs

```rust
//! Initial migration to create all tables

use sea_orm_migration::prelude::*;

#[derive(DeriveMigrationName)]
pub struct Migration;

#[async_trait::async_trait]
impl MigrationTrait for Migration {
	async fn up(&self, manager: &SchemaManager) -> Result<(), DbErr> {
		// Libraries table removed - library metadata is stored in library.json

		// Create devices table with hybrid ID system
		manager
			.create_table(
				Table::create()
					.table(Devices::Table)
					.if_not_exists()
					.col(
						ColumnDef::new(Devices::Id)
							.integer()
							.not_null()
							.auto_increment()
							.primary_key(),
					)
					.col(ColumnDef::new(Devices::Uuid).uuid().not_null().unique_key())
					.col(ColumnDef::new(Devices::Name).string().not_null())
					.col(ColumnDef::new(Devices::Os).string().not_null())
					.col(ColumnDef::new(Devices::OsVersion).string())
					.col(ColumnDef::new(Devices::HardwareModel).string())
					.col(ColumnDef::new(Devices::NetworkAddresses).json().not_null())
					.col(
						ColumnDef::new(Devices::IsOnline)
							.boolean()
							.not_null()
							.default(false),
					)
					.col(
						ColumnDef::new(Devices::LastSeenAt)
							.timestamp_with_time_zone()
							.not_null(),
					)
					.col(ColumnDef::new(Devices::Capabilities).json().not_null())
					.col(ColumnDef::new(Devices::SyncLeadership).json().not_null())
					.col(
						ColumnDef::new(Devices::CreatedAt)
							.timestamp_with_time_zone()
							.not_null(),
					)
					.col(
						ColumnDef::new(Devices::UpdatedAt)
							.timestamp_with_time_zone()
							.not_null(),
					)
					.to_owned(),
			)
			.await?;

		// Create content_kinds lookup table
		manager
			.create_table(
				Table::create()
					.table(ContentKinds::Table)
					.if_not_exists()
					.col(
						ColumnDef::new(ContentKinds::Id)
							.integer()
							.not_null()
							.primary_key(),
					)
					.col(
						ColumnDef::new(ContentKinds::Name)
							.string()
							.not_null()
							.unique_key(),
					)
					.to_owned(),
			)
			.await?;

		// Populate content_kinds table with enum values
		let content_kinds = vec![
			(0, "unknown"),
			(1, "image"),
			(2, "video"),
			(3, "audio"),
			(4, "document"),
			(5, "archive"),
			(6, "code"),
			(7, "text"),
			(8, "database"),
			(9, "book"),
			(10, "font"),
			(11, "mesh"),
			(12, "config"),
			(13, "encrypted"),
			(14, "key"),
			(15, "executable"),
			(16, "binary"),
		];

		for (id, name) in content_kinds {
			manager
				.exec_stmt(
					Query::insert()
						.into_table(ContentKinds::Table)
						.columns([ContentKinds::Id, ContentKinds::Name])
						.values_panic([id.into(), name.into()])
						.to_owned(),
				)
				.await?;
		}

		// Create mime_types table for runtime discovered MIME types
		manager
			.create_table(
				Table::create()
					.table(MimeTypes::Table)
					.if_not_exists()
					.col(
						ColumnDef::new(MimeTypes::Id)
							.integer()
							.not_null()
							.auto_increment()
							.primary_key(),
					)
					.col(
						ColumnDef::new(MimeTypes::Uuid)
							.uuid()
							.not_null()
							.unique_key(),
					)
					.col(
						ColumnDef::new(MimeTypes::MimeType)
							.string()
							.not_null()
							.unique_key(),
					)
					.col(
						ColumnDef::new(MimeTypes::CreatedAt)
							.timestamp_with_time_zone()
							.not_null(),
					)
					.to_owned(),
			)
			.await?;

		// Create locations table with hybrid ID system
		manager
			.create_table(
				Table::create()
					.table(Locations::Table)
					.if_not_exists()
					.col(
						ColumnDef::new(Locations::Id)
							.integer()
							.not_null()
							.auto_increment()
							.primary_key(),
					)
					.col(
						ColumnDef::new(Locations::Uuid)
							.uuid()
							.not_null()
							.unique_key(),
					)
					.col(ColumnDef::new(Locations::DeviceId).integer().not_null())
					.col(ColumnDef::new(Locations::Path).string().not_null())
					.col(ColumnDef::new(Locations::Name).string())
					.col(ColumnDef::new(Locations::IndexMode).string().not_null())
					.col(ColumnDef::new(Locations::ScanState).string().not_null())
					.col(ColumnDef::new(Locations::LastScanAt).timestamp_with_time_zone())
					.col(ColumnDef::new(Locations::ErrorMessage).string())
					.col(
						ColumnDef::new(Locations::TotalFileCount)
							.big_integer()
							.not_null()
							.default(0),
					)
					.col(
						ColumnDef::new(Locations::TotalByteSize)
							.big_integer()
							.not_null()
							.default(0),
					)
					.col(
						ColumnDef::new(Locations::CreatedAt)
							.timestamp_with_time_zone()
							.not_null(),
					)
					.col(
						ColumnDef::new(Locations::UpdatedAt)
							.timestamp_with_time_zone()
							.not_null(),
					)
					.foreign_key(
						ForeignKey::create()
							.from(Locations::Table, Locations::DeviceId)
							.to(Devices::Table, Devices::Id)
							.on_delete(ForeignKeyAction::Cascade),
					)
					.to_owned(),
			)
			.await?;

		// Create content_identities table with hybrid ID system and optional UUID
		manager
			.create_table(
				Table::create()
					.table(ContentIdentities::Table)
					.if_not_exists()
					.col(
						ColumnDef::new(ContentIdentities::Id)
							.integer()
							.not_null()
							.auto_increment()
							.primary_key(),
					)
					.col(ColumnDef::new(ContentIdentities::Uuid).uuid()) // Optional until content identification phase
					.col(ColumnDef::new(ContentIdentities::IntegrityHash).string()) // Full hash for file validation
					.col(
						ColumnDef::new(ContentIdentities::ContentHash)
							.string()
							.not_null(),
					) // Fast sampled hash for deduplication
					.col(ColumnDef::new(ContentIdentities::MimeTypeId).integer())
					.col(
						ColumnDef::new(ContentIdentities::KindId)
							.integer()
							.not_null(),
					)
					.col(ColumnDef::new(ContentIdentities::MediaData).json())
					.col(ColumnDef::new(ContentIdentities::TextContent).text())
					.col(
						ColumnDef::new(ContentIdentities::TotalSize)
							.big_integer()
							.not_null()
							.default(0),
					)
					.col(
						ColumnDef::new(ContentIdentities::EntryCount)
							.integer()
							.not_null()
							.default(0),
					)
					.col(
						ColumnDef::new(ContentIdentities::FirstSeenAt)
							.timestamp_with_time_zone()
							.not_null(),
					)
					.col(
						ColumnDef::new(ContentIdentities::LastVerifiedAt)
							.timestamp_with_time_zone()
							.not_null(),
					)
					.foreign_key(
						ForeignKey::create()
							.from(ContentIdentities::Table, ContentIdentities::KindId)
							.to(ContentKinds::Table, ContentKinds::Id)
							.on_delete(ForeignKeyAction::Restrict),
					)
					.foreign_key(
						ForeignKey::create()
							.from(ContentIdentities::Table, ContentIdentities::MimeTypeId)
							.to(MimeTypes::Table, MimeTypes::Id)
							.on_delete(ForeignKeyAction::SetNull),
					)
					.to_owned(),
			)
			.await?;

		// Create user_metadata table with hierarchical scoping
		manager
			.create_table(
				Table::create()
					.table(UserMetadata::Table)
					.if_not_exists()
					.col(
						ColumnDef::new(UserMetadata::Id)
							.integer()
							.not_null()
							.auto_increment()
							.primary_key(),
					)
					.col(
						ColumnDef::new(UserMetadata::Uuid)
							.uuid()
							.not_null()
							.unique_key(),
					)
					// Exactly one of these is set - defines the scope
					.col(ColumnDef::new(UserMetadata::EntryUuid).uuid()) // File-specific metadata
					.col(ColumnDef::new(UserMetadata::ContentIdentityUuid).uuid()) // Content-universal metadata
					.col(ColumnDef::new(UserMetadata::Notes).text())
					.col(
						ColumnDef::new(UserMetadata::Favorite)
							.boolean()
							.not_null()
							.default(false),
					)
					.col(
						ColumnDef::new(UserMetadata::Hidden)
							.boolean()
							.not_null()
							.default(false),
					)
					.col(ColumnDef::new(UserMetadata::CustomData).json().not_null())
					.col(
						ColumnDef::new(UserMetadata::CreatedAt)
							.timestamp_with_time_zone()
							.not_null(),
					)
					.col(
						ColumnDef::new(UserMetadata::UpdatedAt)
							.timestamp_with_time_zone()
							.not_null(),
					)
					.to_owned(),
			)
			.await?;

		// Add constraint to ensure exactly one scope is set - using raw SQL for SQLite compatibility
		if manager.get_database_backend() == sea_orm::DatabaseBackend::Sqlite {
			// SQLite doesn't support adding constraints to existing tables easily
			// The constraint logic will be enforced at the application level
		} else {
			// For other databases that support ALTER TABLE ADD CONSTRAINT
			manager
				.get_connection()
				.execute_unprepared(
					"ALTER TABLE user_metadata ADD CONSTRAINT check_metadata_scope \
					 CHECK ((entry_uuid IS NOT NULL AND content_identity_uuid IS NULL) OR \
					        (entry_uuid IS NULL AND content_identity_uuid IS NOT NULL))"
				)
				.await?;
		}

		// Note: Foreign key constraints for UserMetadata will be added after entries table is created

		// Create entries table with optimized storage
		manager
			.create_table(
				Table::create()
					.table(Entries::Table)
					.if_not_exists()
					.col(
						ColumnDef::new(Entries::Id)
							.integer()
							.not_null()
							.auto_increment()
							.primary_key(),
					)
					.col(ColumnDef::new(Entries::Uuid).uuid()) // Optional until content identification phase
					.col(ColumnDef::new(Entries::LocationId).integer().not_null())
					.col(ColumnDef::new(Entries::RelativePath).string().not_null())
					.col(ColumnDef::new(Entries::Name).string().not_null())
					.col(ColumnDef::new(Entries::Kind).integer().not_null())
					.col(ColumnDef::new(Entries::Extension).string())
					.col(ColumnDef::new(Entries::MetadataId).uuid()) // References UserMetadata.uuid
					.col(ColumnDef::new(Entries::ContentId).integer())
					.col(ColumnDef::new(Entries::Size).big_integer().not_null())
					.col(
						ColumnDef::new(Entries::AggregateSize)
							.big_integer()
							.not_null()
							.default(0),
					)
					.col(
						ColumnDef::new(Entries::ChildCount)
							.integer()
							.not_null()
							.default(0),
					)
					.col(
						ColumnDef::new(Entries::FileCount)
							.integer()
							.not_null()
							.default(0),
					)
					.col(
						ColumnDef::new(Entries::CreatedAt)
							.timestamp_with_time_zone()
							.not_null(),
					)
					.col(
						ColumnDef::new(Entries::ModifiedAt)
							.timestamp_with_time_zone()
							.not_null(),
					)
					.col(ColumnDef::new(Entries::AccessedAt).timestamp_with_time_zone())
					.col(ColumnDef::new(Entries::Permissions).string())
					.col(ColumnDef::new(Entries::Inode).big_integer())
					.foreign_key(
						ForeignKey::create()
							.from(Entries::Table, Entries::LocationId)
							.to(Locations::Table, Locations::Id)
							.on_delete(ForeignKeyAction::Cascade),
					)
					.foreign_key(
						ForeignKey::create()
							.from(Entries::Table, Entries::MetadataId)
							.to(UserMetadata::Table, UserMetadata::Uuid)
							.on_delete(ForeignKeyAction::SetNull), // Allow NULL during sync resolution
					)
					.foreign_key(
						ForeignKey::create()
							.from(Entries::Table, Entries::ContentId)
							.to(ContentIdentities::Table, ContentIdentities::Id)
							.on_delete(ForeignKeyAction::SetNull),
					)
					.to_owned(),
			)
			.await?;

		// Create tags table with hybrid ID system
		manager
			.create_table(
				Table::create()
					.table(Tags::Table)
					.if_not_exists()
					.col(
						ColumnDef::new(Tags::Id)
							.integer()
							.not_null()
							.auto_increment()
							.primary_key(),
					)
					.col(ColumnDef::new(Tags::Uuid).uuid().not_null().unique_key())
					.col(ColumnDef::new(Tags::Name).string().not_null())
					.col(ColumnDef::new(Tags::Color).string())
					.col(ColumnDef::new(Tags::Icon).string())
					.col(
						ColumnDef::new(Tags::CreatedAt)
							.timestamp_with_time_zone()
							.not_null(),
					)
					.col(
						ColumnDef::new(Tags::UpdatedAt)
							.timestamp_with_time_zone()
							.not_null(),
					)
					.to_owned(),
			)
			.await?;

		// Create labels table with hybrid ID system
		manager
			.create_table(
				Table::create()
					.table(Labels::Table)
					.if_not_exists()
					.col(
						ColumnDef::new(Labels::Id)
							.integer()
							.not_null()
							.auto_increment()
							.primary_key(),
					)
					.col(ColumnDef::new(Labels::Uuid).uuid().not_null().unique_key())
					.col(ColumnDef::new(Labels::Name).string().not_null())
					.col(
						ColumnDef::new(Labels::CreatedAt)
							.timestamp_with_time_zone()
							.not_null(),
					)
					.to_owned(),
			)
			.await?;

		// Create user_metadata_tags junction table (renamed for clarity)
		manager
			.create_table(
				Table::create()
					.table(UserMetadataTags::Table)
					.if_not_exists()
					.col(
						ColumnDef::new(UserMetadataTags::UserMetadataId)
							.integer()
							.not_null(),
					)
					.col(ColumnDef::new(UserMetadataTags::TagUuid).uuid().not_null())
					.col(
						ColumnDef::new(UserMetadataTags::CreatedAt)
							.timestamp_with_time_zone()
							.not_null(),
					)
					.col(
						ColumnDef::new(UserMetadataTags::DeviceUuid)
							.uuid()
							.not_null(),
					)
					.primary_key(
						Index::create()
							.col(UserMetadataTags::UserMetadataId)
							.col(UserMetadataTags::TagUuid),
					)
					.foreign_key(
						ForeignKey::create()
							.from(UserMetadataTags::Table, UserMetadataTags::UserMetadataId)
							.to(UserMetadata::Table, UserMetadata::Id)
							.on_delete(ForeignKeyAction::Cascade),
					)
					.foreign_key(
						ForeignKey::create()
							.from(UserMetadataTags::Table, UserMetadataTags::TagUuid)
							.to(Tags::Table, Tags::Uuid)
							.on_delete(ForeignKeyAction::Cascade),
					)
					.foreign_key(
						ForeignKey::create()
							.from(UserMetadataTags::Table, UserMetadataTags::DeviceUuid)
							.to(Devices::Table, Devices::Uuid)
							.on_delete(ForeignKeyAction::Cascade),
					)
					.to_owned(),
			)
			.await?;

		// Create metadata_labels junction table
		manager
			.create_table(
				Table::create()
					.table(MetadataLabels::Table)
					.if_not_exists()
					.col(
						ColumnDef::new(MetadataLabels::MetadataId)
							.integer()
							.not_null(),
					)
					.col(ColumnDef::new(MetadataLabels::LabelId).integer().not_null())
					.primary_key(
						Index::create()
							.col(MetadataLabels::MetadataId)
							.col(MetadataLabels::LabelId),
					)
					.foreign_key(
						ForeignKey::create()
							.from(MetadataLabels::Table, MetadataLabels::MetadataId)
							.to(UserMetadata::Table, UserMetadata::Id)
							.on_delete(ForeignKeyAction::Cascade),
					)
					.foreign_key(
						ForeignKey::create()
							.from(MetadataLabels::Table, MetadataLabels::LabelId)
							.to(Labels::Table, Labels::Id)
							.on_delete(ForeignKeyAction::Cascade),
					)
					.to_owned(),
			)
			.await?;

		// Create indices for better query performance
		manager
			.create_index(
				Index::create()
					.name("idx_entries_location_path")
					.table(Entries::Table)
					.col(Entries::LocationId)
					.col(Entries::RelativePath)
					.to_owned(),
			)
			.await?;

		manager
			.create_index(
				Index::create()
					.name("idx_content_hash")
					.table(ContentIdentities::Table)
					.col(ContentIdentities::ContentHash)
					.unique()
					.to_owned(),
			)
			.await?;

		manager
			.create_index(
				Index::create()
					.name("idx_locations_path")
					.table(Locations::Table)
					.col(Locations::Path)
					.to_owned(),
			)
			.await?;

		// Create indexes for performance on new fields
		manager
			.create_index(
				Index::create()
					.name("idx_user_metadata_entry")
					.table(UserMetadata::Table)
					.col(UserMetadata::EntryUuid)
					.to_owned(),
			)
			.await?;

		manager
			.create_index(
				Index::create()
					.name("idx_user_metadata_content")
					.table(UserMetadata::Table)
					.col(UserMetadata::ContentIdentityUuid)
					.to_owned(),
			)
			.await?;

		manager
			.create_index(
				Index::create()
					.name("idx_user_metadata_tags_metadata")
					.table(UserMetadataTags::Table)
					.col(UserMetadataTags::UserMetadataId)
					.to_owned(),
			)
			.await?;

		manager
			.create_index(
				Index::create()
					.name("idx_user_metadata_tags_tag")
					.table(UserMetadataTags::Table)
					.col(UserMetadataTags::TagUuid)
					.to_owned(),
			)
			.await?;

		// Add unique constraints for optional UUID fields
		manager
			.create_index(
				Index::create()
					.name("idx_content_identities_uuid_unique")
					.table(ContentIdentities::Table)
					.col(ContentIdentities::Uuid)
					.unique()
					.if_not_exists()
					.to_owned(),
			)
			.await?;

		manager
			.create_index(
				Index::create()
					.name("idx_entries_uuid_unique")
					.table(Entries::Table)
					.col(Entries::Uuid)
					.unique()
					.if_not_exists()
					.to_owned(),
			)
			.await?;

		// For SQLite, we skip foreign key constraints since they can't be added later
		// The relationships will be enforced at the application level
		if manager.get_database_backend() != sea_orm::DatabaseBackend::Sqlite {
			// Add foreign key constraints for UserMetadata scoping (non-SQLite databases)
			manager
				.get_connection()
				.execute_unprepared(
					"ALTER TABLE user_metadata ADD CONSTRAINT fk_user_metadata_entry \
					 FOREIGN KEY (entry_uuid) REFERENCES entries(uuid) ON DELETE CASCADE"
				)
				.await?;

			manager
				.get_connection()
				.execute_unprepared(
					"ALTER TABLE user_metadata ADD CONSTRAINT fk_user_metadata_content \
					 FOREIGN KEY (content_identity_uuid) REFERENCES content_identities(uuid) ON DELETE CASCADE"
				)
				.await?;
		}

		Ok(())
	}

	async fn down(&self, manager: &SchemaManager) -> Result<(), DbErr> {
		// Drop tables in reverse order of creation
		manager
			.drop_table(Table::drop().table(MetadataLabels::Table).to_owned())
			.await?;
		manager
			.drop_table(Table::drop().table(UserMetadataTags::Table).to_owned())
			.await?;
		manager
			.drop_table(Table::drop().table(Labels::Table).to_owned())
			.await?;
		manager
			.drop_table(Table::drop().table(Tags::Table).to_owned())
			.await?;
		manager
			.drop_table(Table::drop().table(Entries::Table).to_owned())
			.await?;
		manager
			.drop_table(Table::drop().table(UserMetadata::Table).to_owned())
			.await?;
		manager
			.drop_table(Table::drop().table(ContentIdentities::Table).to_owned())
			.await?;
		manager
			.drop_table(Table::drop().table(MimeTypes::Table).to_owned())
			.await?;
		manager
			.drop_table(Table::drop().table(ContentKinds::Table).to_owned())
			.await?;
		manager
			.drop_table(Table::drop().table(Locations::Table).to_owned())
			.await?;
		manager
			.drop_table(Table::drop().table(Devices::Table).to_owned())
			.await?;
		// Libraries table removed - no need to drop

		Ok(())
	}
}

// Table identifiers
// Libraries enum removed - library metadata stored in library.json

#[derive(Iden)]
enum Devices {
	Table,
	Id,
	Uuid,
	Name,
	Os,
	OsVersion,
	HardwareModel,
	NetworkAddresses,
	IsOnline,
	LastSeenAt,
	Capabilities,
	SyncLeadership,
	CreatedAt,
	UpdatedAt,
}

#[derive(Iden)]
enum ContentKinds {
	Table,
	Id,
	Name,
}

#[derive(Iden)]
enum MimeTypes {
	Table,
	Id,
	Uuid,
	MimeType,
	CreatedAt,
}

#[derive(Iden)]
enum Locations {
	Table,
	Id,
	Uuid,
	DeviceId,
	Path,
	Name,
	IndexMode,
	ScanState,
	LastScanAt,
	ErrorMessage,
	TotalFileCount,
	TotalByteSize,
	CreatedAt,
	UpdatedAt,
}

#[derive(Iden)]
enum ContentIdentities {
	Table,
	Id,
	Uuid,
	IntegrityHash,
	ContentHash,
	MimeTypeId,
	KindId,
	MediaData,
	TextContent,
	TotalSize,
	EntryCount,
	FirstSeenAt,
	LastVerifiedAt,
}

#[derive(Iden)]
enum UserMetadata {
	Table,
	Id,
	Uuid,
	EntryUuid,
	ContentIdentityUuid,
	Notes,
	Favorite,
	Hidden,
	CustomData,
	CreatedAt,
	UpdatedAt,
}

#[derive(Iden)]
enum Entries {
	Table,
	Id,
	Uuid,
	LocationId,
	RelativePath,
	Name,
	Kind,
	Extension,
	MetadataId,
	ContentId,
	Size,
	AggregateSize,
	ChildCount,
	FileCount,
	CreatedAt,
	ModifiedAt,
	AccessedAt,
	Permissions,
	Inode,
}

#[derive(Iden)]
enum Tags {
	Table,
	Id,
	Uuid,
	Name,
	Color,
	Icon,
	CreatedAt,
	UpdatedAt,
}

#[derive(Iden)]
enum Labels {
	Table,
	Id,
	Uuid,
	Name,
	CreatedAt,
}

#[derive(Iden)]
enum UserMetadataTags {
	Table,
	UserMetadataId,
	TagUuid,
	CreatedAt,
	DeviceUuid,
}

#[derive(Iden)]
enum MetadataLabels {
	Table,
	MetadataId,
	LabelId,
}
```

## src/infrastructure/database/migration/mod.rs

```rust
//! Database migrations

use sea_orm_migration::prelude::*;

mod m20240101_000001_create_initial_tables;
mod m20240102_000001_add_audit_log;
mod m20240103_000001_create_volumes;

pub struct Migrator;

#[async_trait::async_trait]
impl MigratorTrait for Migrator {
	fn migrations() -> Vec<Box<dyn MigrationTrait>> {
		vec![
			Box::new(m20240101_000001_create_initial_tables::Migration),
			Box::new(m20240102_000001_add_audit_log::Migration),
			Box::new(m20240103_000001_create_volumes::Migration),
		]
	}
}
```

## src/infrastructure/database/migration/m20240102_000001_add_audit_log.rs

```rust
//! Add audit log table for action tracking

use sea_orm_migration::prelude::*;

#[derive(DeriveMigrationName)]
pub struct Migration;

#[async_trait::async_trait]
impl MigrationTrait for Migration {
    async fn up(&self, manager: &SchemaManager) -> Result<(), DbErr> {
        // Create the table first
        manager
            .create_table(
                Table::create()
                    .table(AuditLog::Table)
                    .if_not_exists()
                    .col(
                        ColumnDef::new(AuditLog::Id)
                            .integer()
                            .not_null()
                            .auto_increment()
                            .primary_key(),
                    )
                    .col(ColumnDef::new(AuditLog::Uuid).text().not_null().unique_key())
                    .col(ColumnDef::new(AuditLog::ActionType).string().not_null())
                    .col(ColumnDef::new(AuditLog::ActorDeviceId).text().not_null())
                    .col(ColumnDef::new(AuditLog::Targets).text().not_null())
                    .col(ColumnDef::new(AuditLog::Status).string().not_null())
                    .col(ColumnDef::new(AuditLog::JobId).text())
                    .col(
                        ColumnDef::new(AuditLog::CreatedAt)
                            .timestamp()
                            .not_null(),
                    )
                    .col(ColumnDef::new(AuditLog::CompletedAt).timestamp())
                    .col(ColumnDef::new(AuditLog::ErrorMessage).text())
                    .col(ColumnDef::new(AuditLog::ResultPayload).text())
                    .to_owned(),
            )
            .await?;

        // Create indexes separately
        manager
            .create_index(
                Index::create()
                    .name("idx_audit_log_action_type")
                    .table(AuditLog::Table)
                    .col(AuditLog::ActionType)
                    .to_owned(),
            )
            .await?;

        manager
            .create_index(
                Index::create()
                    .name("idx_audit_log_actor_device_id")
                    .table(AuditLog::Table)
                    .col(AuditLog::ActorDeviceId)
                    .to_owned(),
            )
            .await?;

        manager
            .create_index(
                Index::create()
                    .name("idx_audit_log_status")
                    .table(AuditLog::Table)
                    .col(AuditLog::Status)
                    .to_owned(),
            )
            .await?;

        manager
            .create_index(
                Index::create()
                    .name("idx_audit_log_job_id")
                    .table(AuditLog::Table)
                    .col(AuditLog::JobId)
                    .to_owned(),
            )
            .await?;

        manager
            .create_index(
                Index::create()
                    .name("idx_audit_log_created_at")
                    .table(AuditLog::Table)
                    .col(AuditLog::CreatedAt)
                    .to_owned(),
            )
            .await
    }

    async fn down(&self, manager: &SchemaManager) -> Result<(), DbErr> {
        manager
            .drop_table(Table::drop().table(AuditLog::Table).to_owned())
            .await
    }
}

#[derive(DeriveIden)]
enum AuditLog {
    Table,
    Id,
    Uuid,
    ActionType,
    ActorDeviceId,
    Targets,
    Status,
    JobId,
    CreatedAt,
    CompletedAt,
    ErrorMessage,
    ResultPayload,
}```

## src/infrastructure/database/entities/content_kind.rs

```rust
//! Content kind entity (lookup table)

use sea_orm::entity::prelude::*;
use serde::{Deserialize, Serialize};

#[derive(Clone, Debug, PartialEq, Eq, DeriveEntityModel, Serialize, Deserialize)]
#[sea_orm(table_name = "content_kinds")]
pub struct Model {
    #[sea_orm(primary_key, auto_increment = false)]
    pub id: i32,
    pub name: String,
}

#[derive(Copy, Clone, Debug, EnumIter, DeriveRelation)]
pub enum Relation {
    #[sea_orm(has_many = "super::content_identity::Entity")]
    ContentIdentities,
}

impl Related<super::content_identity::Entity> for Entity {
    fn to() -> RelationDef {
        Relation::ContentIdentities.def()
    }
}

impl ActiveModelBehavior for ActiveModel {}```

## src/infrastructure/database/entities/device.rs

```rust
//! Device entity

use sea_orm::entity::prelude::*;
use serde::{Deserialize, Serialize};

#[derive(Clone, Debug, PartialEq, Eq, DeriveEntityModel, Serialize, Deserialize)]
#[sea_orm(table_name = "devices")]
pub struct Model {
    #[sea_orm(primary_key)]
    pub id: i32,
    pub uuid: Uuid,
    pub name: String,
    pub os: String,
    pub os_version: Option<String>,
    pub hardware_model: Option<String>,
    pub network_addresses: Json,  // Vec<String> as JSON
    pub is_online: bool,
    pub last_seen_at: DateTimeUtc,
    pub capabilities: Json,  // DeviceCapabilities as JSON
    pub sync_leadership: Json,  // HashMap<Uuid, SyncRole> as JSON
    pub created_at: DateTimeUtc,
    pub updated_at: DateTimeUtc,
}

#[derive(Copy, Clone, Debug, EnumIter, DeriveRelation)]
pub enum Relation {
    #[sea_orm(has_many = "super::location::Entity")]
    Locations,
}

impl Related<super::location::Entity> for Entity {
    fn to() -> RelationDef {
        Relation::Locations.def()
    }
}

impl ActiveModelBehavior for ActiveModel {}```

## src/infrastructure/database/entities/content_identity.rs

```rust
//! Content identity entity

use sea_orm::entity::prelude::*;
use serde::{Deserialize, Serialize};

#[derive(Clone, Debug, PartialEq, Eq, DeriveEntityModel, Serialize, Deserialize)]
#[sea_orm(table_name = "content_identities")]
pub struct Model {
	#[sea_orm(primary_key)]
	pub id: i32,
	pub uuid: Option<Uuid>, // DETERMINISTIC from content_hash (assigned during content identification phase)
	pub integrity_hash: Option<String>, // Full hash for file validation (generated by validate job)
	pub content_hash: String, // Fast sampled hash for deduplication (generated during content identification)
	pub mime_type_id: Option<i32>,
	pub kind_id: i32,             // ContentKind foreign key
	pub media_data: Option<Json>, // MediaData as JSON
	pub text_content: Option<String>,
	pub total_size: i64,  // Size of one instance of this content
	pub entry_count: i32, // Entries in THIS library only
	pub first_seen_at: DateTimeUtc,
	pub last_verified_at: DateTimeUtc,
}

#[derive(Copy, Clone, Debug, EnumIter, DeriveRelation)]
pub enum Relation {
	#[sea_orm(has_many = "super::entry::Entity")]
	Entries,
	#[sea_orm(
		belongs_to = "super::content_kind::Entity",
		from = "Column::KindId",
		to = "super::content_kind::Column::Id"
	)]
	ContentKind,
	#[sea_orm(
		belongs_to = "super::mime_type::Entity",
		from = "Column::MimeTypeId",
		to = "super::mime_type::Column::Id"
	)]
	MimeType,
}

impl Related<super::entry::Entity> for Entity {
	fn to() -> RelationDef {
		Relation::Entries.def()
	}
}

impl Related<super::content_kind::Entity> for Entity {
	fn to() -> RelationDef {
		Relation::ContentKind.def()
	}
}

impl Related<super::mime_type::Entity> for Entity {
	fn to() -> RelationDef {
		Relation::MimeType.def()
	}
}

impl ActiveModelBehavior for ActiveModel {}

impl Model {
	/// Generate deterministic UUID from content_hash for sync consistency within library
	/// Note: ContentIdentity UUIDs are deterministic from content_hash + library_id
	/// This ensures same content in different libraries has different UUIDs
	/// Maintains library isolation while enabling deterministic sync
	pub fn deterministic_uuid(content_hash: &str, library_id: Uuid) -> Uuid {
		const LIBRARY_NAMESPACE: Uuid = Uuid::from_bytes([
			0x6b, 0xa7, 0xb8, 0x10, 0x9d, 0xad, 0x11, 0xd1, 0x80, 0xb4, 0x00, 0xc0, 0x4f, 0xd4,
			0x30, 0xc8,
		]);
		let namespace = Uuid::new_v5(&LIBRARY_NAMESPACE, library_id.as_bytes());
		Uuid::new_v5(&namespace, content_hash.as_bytes())
	}

	/// Calculate combined size on-demand (no need to cache entry_count * total_size)
	pub fn combined_size(&self) -> i64 {
		self.entry_count as i64 * self.total_size
	}
}
```

## src/infrastructure/database/entities/metadata_tag.rs

```rust
//! UserMetadataTag junction entity for hierarchical metadata tagging

use sea_orm::entity::prelude::*;

#[derive(Clone, Debug, PartialEq, Eq, DeriveEntityModel)]
#[sea_orm(table_name = "user_metadata_tags")]
pub struct Model {
    #[sea_orm(primary_key)]
    pub user_metadata_id: i32,
    #[sea_orm(primary_key)]
    pub tag_uuid: Uuid,
    pub created_at: DateTimeUtc,
    pub device_uuid: Uuid,
}

#[derive(Copy, Clone, Debug, EnumIter, DeriveRelation)]
pub enum Relation {
    #[sea_orm(
        belongs_to = "super::user_metadata::Entity",
        from = "Column::UserMetadataId",
        to = "super::user_metadata::Column::Id"
    )]
    UserMetadata,
    #[sea_orm(
        belongs_to = "super::tag::Entity",
        from = "Column::TagUuid",
        to = "super::tag::Column::Uuid"
    )]
    Tag,
    #[sea_orm(
        belongs_to = "super::device::Entity",
        from = "Column::DeviceUuid",
        to = "super::device::Column::Uuid"
    )]
    Device,
}

impl Related<super::user_metadata::Entity> for Entity {
    fn to() -> RelationDef {
        Relation::UserMetadata.def()
    }
}

impl Related<super::tag::Entity> for Entity {
    fn to() -> RelationDef {
        Relation::Tag.def()
    }
}

impl Related<super::device::Entity> for Entity {
    fn to() -> RelationDef {
        Relation::Device.def()
    }
}

impl ActiveModelBehavior for ActiveModel {}```

## src/infrastructure/database/entities/audit_log.rs

```rust
//! Audit log entity for tracking user actions

use sea_orm::entity::prelude::*;
use sea_orm::Set;
use serde::{Deserialize, Serialize};
use uuid::Uuid;

#[derive(Clone, Debug, PartialEq, Eq, DeriveEntityModel, Serialize, Deserialize)]
#[sea_orm(table_name = "audit_log")]
pub struct Model {
    #[sea_orm(primary_key)]
    pub id: i32,
    
    #[sea_orm(unique)]
    pub uuid: String,

    #[sea_orm(indexed)]
    pub action_type: String,

    #[sea_orm(indexed)]
    pub actor_device_id: String,

    pub targets: String,

    #[sea_orm(indexed)]
    pub status: ActionStatus,

    #[sea_orm(indexed, nullable)]
    pub job_id: Option<String>,

    pub created_at: DateTimeUtc,
    pub completed_at: Option<DateTimeUtc>,
    
    pub error_message: Option<String>,
    
    pub result_payload: Option<String>,
}

#[derive(Debug, Clone, PartialEq, Eq, EnumIter, DeriveActiveEnum, Serialize, Deserialize)]
#[sea_orm(rs_type = "String", db_type = "Text")]
pub enum ActionStatus {
    #[sea_orm(string_value = "in_progress")]
    InProgress,
    #[sea_orm(string_value = "completed")]
    Completed,
    #[sea_orm(string_value = "failed")]
    Failed,
}

#[derive(Copy, Clone, Debug, EnumIter, DeriveRelation)]
pub enum Relation {}

impl ActiveModelBehavior for ActiveModel {
    fn new() -> Self {
        Self {
            uuid: Set(Uuid::new_v4().to_string()),
            created_at: Set(chrono::Utc::now()),
            ..ActiveModelTrait::default()
        }
    }
}```

## src/infrastructure/database/entities/mod.rs

```rust
//! Sea-ORM entity definitions
//! 
//! These map our domain models to database tables.

pub mod content_kind;
pub mod mime_type;
pub mod device;
pub mod location;
pub mod entry;
pub mod user_metadata;
pub mod content_identity;
pub mod tag;
pub mod label;
pub mod metadata_tag;
pub use metadata_tag as user_metadata_tag; // Alias for hierarchical metadata operations
pub mod metadata_label;
pub mod audit_log;
pub mod volume;

// Re-export all entities
pub use device::Entity as Device;
pub use location::Entity as Location;
pub use entry::Entity as Entry;
pub use user_metadata::Entity as UserMetadata;
pub use content_identity::Entity as ContentIdentity;
pub use tag::Entity as Tag;
pub use label::Entity as Label;
pub use metadata_tag::Entity as UserMetadataTag;
pub use audit_log::Entity as AuditLog;
pub use volume::Entity as Volume;

// Re-export active models for easy access
pub use device::ActiveModel as DeviceActive;
pub use location::ActiveModel as LocationActive;
pub use entry::ActiveModel as EntryActive;
pub use user_metadata::ActiveModel as UserMetadataActive;
pub use content_identity::ActiveModel as ContentIdentityActive;
pub use tag::ActiveModel as TagActive;
pub use label::ActiveModel as LabelActive;
pub use metadata_tag::ActiveModel as UserMetadataTagActive;
pub use audit_log::ActiveModel as AuditLogActive;
pub use volume::ActiveModel as VolumeActive;```

## src/infrastructure/database/entities/tag.rs

```rust
//! Tag entity

use sea_orm::entity::prelude::*;
use serde::{Deserialize, Serialize};

#[derive(Clone, Debug, PartialEq, Eq, DeriveEntityModel, Serialize, Deserialize)]
#[sea_orm(table_name = "tags")]
pub struct Model {
    #[sea_orm(primary_key)]
    pub id: i32,
    pub uuid: Uuid,
    pub name: String,
    pub color: Option<String>,
    pub icon: Option<String>,
    pub created_at: DateTimeUtc,
    pub updated_at: DateTimeUtc,
}

#[derive(Copy, Clone, Debug, EnumIter, DeriveRelation)]
pub enum Relation {}

impl ActiveModelBehavior for ActiveModel {}```

## src/infrastructure/database/entities/label.rs

```rust
//! Label entity

use sea_orm::entity::prelude::*;
use serde::{Deserialize, Serialize};

#[derive(Clone, Debug, PartialEq, Eq, DeriveEntityModel, Serialize, Deserialize)]
#[sea_orm(table_name = "labels")]
pub struct Model {
    #[sea_orm(primary_key)]
    pub id: i32,
    pub uuid: Uuid,
    pub name: String,
    pub created_at: DateTimeUtc,
}

#[derive(Copy, Clone, Debug, EnumIter, DeriveRelation)]
pub enum Relation {}

impl ActiveModelBehavior for ActiveModel {}```

## src/infrastructure/database/entities/metadata_label.rs

```rust
//! MetadataLabel junction entity

use sea_orm::entity::prelude::*;

#[derive(Clone, Debug, PartialEq, Eq, DeriveEntityModel)]
#[sea_orm(table_name = "metadata_labels")]
pub struct Model {
    #[sea_orm(primary_key)]
    pub metadata_id: i32,
    #[sea_orm(primary_key)]
    pub label_id: i32,
}

#[derive(Copy, Clone, Debug, EnumIter, DeriveRelation)]
pub enum Relation {
    #[sea_orm(
        belongs_to = "super::user_metadata::Entity",
        from = "Column::MetadataId",
        to = "super::user_metadata::Column::Id"
    )]
    UserMetadata,
    #[sea_orm(
        belongs_to = "super::label::Entity",
        from = "Column::LabelId",
        to = "super::label::Column::Id"
    )]
    Label,
}

impl Related<super::user_metadata::Entity> for Entity {
    fn to() -> RelationDef {
        Relation::UserMetadata.def()
    }
}

impl Related<super::label::Entity> for Entity {
    fn to() -> RelationDef {
        Relation::Label.def()
    }
}

impl ActiveModelBehavior for ActiveModel {}```

## src/infrastructure/database/entities/entry.rs

```rust
//! Entry entity

use sea_orm::entity::prelude::*;
use serde::{Deserialize, Serialize};

#[derive(Clone, Debug, PartialEq, Eq, DeriveEntityModel, Serialize, Deserialize)]
#[sea_orm(table_name = "entries")]
pub struct Model {
    #[sea_orm(primary_key)]
    pub id: i32,
    pub uuid: Option<Uuid>, // None until content identification phase complete (sync readiness indicator)
    pub location_id: i32,  // References location table
    pub relative_path: String,  // Directory path within location
    pub name: String,
    pub kind: i32,  // Entry type: 0=File, 1=Directory, 2=Symlink
    pub extension: Option<String>,  // File extension (without dot), None for directories
    pub metadata_id: Option<i32>,  // Optional - only when user adds metadata
    pub content_id: Option<i32>,  // Optional - for deduplication
    pub size: i64,
    pub aggregate_size: i64,  // Total size including all children (for directories)
    pub child_count: i32,  // Total number of direct children
    pub file_count: i32,  // Total number of files in this directory and subdirectories
    pub created_at: DateTimeUtc,
    pub modified_at: DateTimeUtc,
    pub accessed_at: Option<DateTimeUtc>,
    pub permissions: Option<String>,  // Unix permissions as string
    pub inode: Option<i64>,  // Platform-specific file identifier for change detection
}

#[derive(Copy, Clone, Debug, EnumIter, DeriveRelation)]
pub enum Relation {
    #[sea_orm(
        belongs_to = "super::location::Entity",
        from = "Column::LocationId",
        to = "super::location::Column::Id"
    )]
    Location,
    #[sea_orm(
        belongs_to = "super::user_metadata::Entity",
        from = "Column::MetadataId",
        to = "super::user_metadata::Column::Id"
    )]
    UserMetadata,
    #[sea_orm(
        belongs_to = "super::content_identity::Entity",
        from = "Column::ContentId",
        to = "super::content_identity::Column::Id"
    )]
    ContentIdentity,
}

impl Related<super::user_metadata::Entity> for Entity {
    fn to() -> RelationDef {
        Relation::UserMetadata.def()
    }
}

impl Related<super::content_identity::Entity> for Entity {
    fn to() -> RelationDef {
        Relation::ContentIdentity.def()
    }
}

impl Related<super::location::Entity> for Entity {
    fn to() -> RelationDef {
        Relation::Location.def()
    }
}

impl ActiveModelBehavior for ActiveModel {}

#[derive(Clone, Debug, PartialEq, Eq, Serialize, Deserialize)]
pub enum EntryKind {
    File = 0,
    Directory = 1,
    Symlink = 2,
}

impl From<i32> for EntryKind {
    fn from(value: i32) -> Self {
        match value {
            0 => EntryKind::File,
            1 => EntryKind::Directory,
            2 => EntryKind::Symlink,
            _ => EntryKind::File, // Default fallback
        }
    }
}

impl From<EntryKind> for i32 {
    fn from(kind: EntryKind) -> Self {
        kind as i32
    }
}

impl Model {
    /// Get the entry kind as enum
    pub fn entry_kind(&self) -> EntryKind {
        EntryKind::from(self.kind)
    }

    /// UUID Assignment Rules:
    /// - Directories: Assign UUID immediately (no content to identify)
    /// - Empty files: Assign UUID immediately (size = 0, no content to hash)
    /// - Regular files: Assign UUID after content identification completes
    pub fn should_assign_uuid_immediately(&self) -> bool {
        self.entry_kind() == EntryKind::Directory || self.size == 0
    }

    /// Check if this entry is ready for sync (has UUID assigned)
    pub fn is_sync_ready(&self) -> bool {
        self.uuid.is_some()
    }
}```

## src/infrastructure/database/entities/location.rs

```rust
//! Location entity

use sea_orm::entity::prelude::*;
use serde::{Deserialize, Serialize};

#[derive(Clone, Debug, PartialEq, Eq, DeriveEntityModel, Serialize, Deserialize)]
#[sea_orm(table_name = "locations")]
pub struct Model {
    #[sea_orm(primary_key)]
    pub id: i32,
    pub uuid: Uuid,
    pub device_id: i32,
    pub path: String,
    pub name: Option<String>,
    pub index_mode: String,  // "shallow", "content", "deep"
    pub scan_state: String,  // "pending", "scanning", "completed", "error"
    pub last_scan_at: Option<DateTimeUtc>,
    pub error_message: Option<String>,
    pub total_file_count: i64,
    pub total_byte_size: i64,
    pub created_at: DateTimeUtc,
    pub updated_at: DateTimeUtc,
}

#[derive(Copy, Clone, Debug, EnumIter, DeriveRelation)]
pub enum Relation {
    #[sea_orm(
        belongs_to = "super::device::Entity",
        from = "Column::DeviceId",
        to = "super::device::Column::Id"
    )]
    Device,
    #[sea_orm(has_many = "super::entry::Entity")]
    Entries,
}

impl Related<super::device::Entity> for Entity {
    fn to() -> RelationDef {
        Relation::Device.def()
    }
}

impl Related<super::entry::Entity> for Entity {
    fn to() -> RelationDef {
        Relation::Entries.def()
    }
}

impl ActiveModelBehavior for ActiveModel {}```

## src/infrastructure/database/entities/volume.rs

```rust
//! Volume entity

use crate::volume::types::TrackedVolume;
use sea_orm::entity::prelude::*;
use serde::{Deserialize, Serialize};

#[derive(Clone, Debug, PartialEq, Eq, DeriveEntityModel, Serialize, Deserialize)]
#[sea_orm(table_name = "volumes")]
pub struct Model {
	#[sea_orm(primary_key)]
	pub id: i32,
	pub uuid: Uuid,
	pub device_id: Uuid, // Foreign key to devices table
	pub fingerprint: String,
	pub display_name: Option<String>,
	pub tracked_at: DateTimeUtc,
	pub last_seen_at: DateTimeUtc,
	pub is_online: bool,
	pub total_capacity: Option<i64>,
	pub available_capacity: Option<i64>,
	pub read_speed_mbps: Option<i32>,
	pub write_speed_mbps: Option<i32>,
	pub last_speed_test_at: Option<DateTimeUtc>,
	pub file_system: Option<String>,
	pub mount_point: Option<String>,
	pub is_removable: Option<bool>,
	pub is_network_drive: Option<bool>,
	pub device_model: Option<String>,
	/// Volume type classification
	pub volume_type: Option<String>,
	/// Whether volume is visible in default UI
	pub is_user_visible: Option<bool>,
	/// Whether volume is eligible for auto-tracking
	pub auto_track_eligible: Option<bool>,
}

#[derive(Copy, Clone, Debug, EnumIter, DeriveRelation)]
pub enum Relation {
	#[sea_orm(
		belongs_to = "super::device::Entity",
		from = "Column::DeviceId",
		to = "super::device::Column::Uuid"
	)]
	Device,
}

impl Related<super::device::Entity> for Entity {
	fn to() -> RelationDef {
		Relation::Device.def()
	}
}

impl ActiveModelBehavior for ActiveModel {}

impl Model {
	/// Convert database model to tracked volume
	pub fn to_tracked_volume(&self) -> crate::volume::types::TrackedVolume {
		crate::volume::types::TrackedVolume {
			id: self.id,
			uuid: self.uuid,
			device_id: self.device_id,
			fingerprint: crate::volume::VolumeFingerprint(self.fingerprint.clone()),
			display_name: self.display_name.clone(),
			tracked_at: self.tracked_at,
			last_seen_at: self.last_seen_at,
			is_online: self.is_online,
			total_capacity: self.total_capacity.map(|c| c as u64),
			available_capacity: self.available_capacity.map(|c| c as u64),
			read_speed_mbps: self.read_speed_mbps.map(|s| s as u32),
			write_speed_mbps: self.write_speed_mbps.map(|s| s as u32),
			last_speed_test_at: self.last_speed_test_at,
			file_system: self.file_system.clone(),
			mount_point: self.mount_point.clone(),
			is_removable: self.is_removable,
			is_network_drive: self.is_network_drive,
			device_model: self.device_model.clone(),
			volume_type: self.volume_type.as_deref().unwrap_or("Unknown").to_string(),
			is_user_visible: self.is_user_visible,
			auto_track_eligible: self.auto_track_eligible,
		}
	}
}
```

## src/infrastructure/database/entities/user_metadata.rs

```rust
//! User metadata entity

use sea_orm::entity::prelude::*;
use serde::{Deserialize, Serialize};

#[derive(Clone, Debug, PartialEq, Eq, DeriveEntityModel, Serialize, Deserialize)]
#[sea_orm(table_name = "user_metadata")]
pub struct Model {
    #[sea_orm(primary_key)]
    pub id: i32,
    pub uuid: Uuid,
    
    // Exactly one of these is set - defines the scope
    pub entry_uuid: Option<Uuid>, // File-specific metadata (higher priority in hierarchy)
    pub content_identity_uuid: Option<Uuid>, // Content-universal metadata (lower priority in hierarchy)
    
    // All metadata types benefit from scope flexibility
    pub notes: Option<String>,
    pub favorite: bool,
    pub hidden: bool,
    pub custom_data: Json,  // Arbitrary JSON data
    pub created_at: DateTimeUtc,
    pub updated_at: DateTimeUtc,
}

#[derive(Copy, Clone, Debug, EnumIter, DeriveRelation)]
pub enum Relation {
    #[sea_orm(
        belongs_to = "super::entry::Entity",
        from = "Column::EntryUuid",
        to = "super::entry::Column::Uuid"
    )]
    Entry,
    #[sea_orm(
        belongs_to = "super::content_identity::Entity",
        from = "Column::ContentIdentityUuid",
        to = "super::content_identity::Column::Uuid"
    )]
    ContentIdentity,
}

impl Related<super::entry::Entity> for Entity {
    fn to() -> RelationDef {
        Relation::Entry.def()
    }
}

impl Related<super::content_identity::Entity> for Entity {
    fn to() -> RelationDef {
        Relation::ContentIdentity.def()
    }
}

impl Related<super::tag::Entity> for Entity {
    fn to() -> RelationDef {
        super::metadata_tag::Relation::Tag.def()
    }
    
    fn via() -> Option<RelationDef> {
        Some(super::metadata_tag::Relation::UserMetadata.def().rev())
    }
}

impl ActiveModelBehavior for ActiveModel {}

#[derive(Clone, Debug, PartialEq, Eq, Serialize, Deserialize)]
pub enum MetadataScope {
    Entry,          // File-specific (higher priority)
    Content,        // Content-universal (lower priority)
}

impl Model {
    /// Get the scope of this metadata (entry or content-level)
    pub fn scope(&self) -> Option<MetadataScope> {
        if self.entry_uuid.is_some() {
            Some(MetadataScope::Entry)
        } else if self.content_identity_uuid.is_some() {
            Some(MetadataScope::Content)
        } else {
            None // Invalid state - should be caught by DB constraint
        }
    }

    /// Check if this metadata is entry-scoped
    pub fn is_entry_scoped(&self) -> bool {
        self.entry_uuid.is_some()
    }

    /// Check if this metadata is content-scoped
    pub fn is_content_scoped(&self) -> bool {
        self.content_identity_uuid.is_some()
    }
}```

## src/infrastructure/database/entities/mime_type.rs

```rust
//! MIME type entity (runtime discovered)

use sea_orm::entity::prelude::*;
use serde::{Deserialize, Serialize};

#[derive(Clone, Debug, PartialEq, Eq, DeriveEntityModel, Serialize, Deserialize)]
#[sea_orm(table_name = "mime_types")]
pub struct Model {
    #[sea_orm(primary_key)]
    pub id: i32,
    pub uuid: Uuid,
    pub mime_type: String,
    pub created_at: DateTimeUtc,
}

#[derive(Copy, Clone, Debug, EnumIter, DeriveRelation)]
pub enum Relation {
    #[sea_orm(has_many = "super::content_identity::Entity")]
    ContentIdentities,
}

impl Related<super::content_identity::Entity> for Entity {
    fn to() -> RelationDef {
        Relation::ContentIdentities.def()
    }
}

impl ActiveModelBehavior for ActiveModel {}```

## src/infrastructure/cli/output/section.rs

```rust
//! Section builder for complex multi-line outputs

use super::context::OutputContext;
use comfy_table::{Table, presets::UTF8_FULL};
use owo_colors::OwoColorize;
use std::io;

/// Line types for section building
#[derive(Debug)]
enum Line {
    Title(String),
    Status(String, String),
    Item(String, String),
    Text(String),
    Empty,
    Table(Table),
}

/// Fluent builder for output sections
pub struct OutputSection<'a> {
    context: &'a mut OutputContext,
    lines: Vec<Line>,
}

impl<'a> OutputSection<'a> {
    /// Create a new section builder
    pub fn new(context: &'a mut OutputContext) -> Self {
        Self {
            context,
            lines: Vec::new(),
        }
    }

    /// Add a title
    pub fn title(mut self, text: &str) -> Self {
        self.lines.push(Line::Title(text.to_string()));
        self
    }

    /// Add a status line (label: value)
    pub fn status(mut self, label: &str, value: &str) -> Self {
        self.lines.push(Line::Status(label.to_string(), value.to_string()));
        self
    }

    /// Add an item line (label: value with indentation)
    pub fn item(mut self, label: &str, value: &str) -> Self {
        self.lines.push(Line::Item(label.to_string(), value.to_string()));
        self
    }

    /// Add plain text
    pub fn text(mut self, text: &str) -> Self {
        self.lines.push(Line::Text(text.to_string()));
        self
    }

    /// Add an empty line
    pub fn empty_line(mut self) -> Self {
        self.lines.push(Line::Empty);
        self
    }

    /// Add a table
    pub fn table(mut self, table: Table) -> Self {
        self.lines.push(Line::Table(table));
        self
    }

    /// Add a help section
    pub fn help(self) -> HelpSection<'a> {
        HelpSection::new(self)
    }

    /// Render the section
    pub fn render(self) -> io::Result<()> {
        let use_color = self.context.use_color();
        let mut last_was_empty = false;

        for (i, line) in self.lines.iter().enumerate() {
            // Smart spacing: avoid duplicate empty lines
            if matches!(line, Line::Empty) {
                if !last_was_empty && i > 0 {
                    self.context.writeln("")?;
                }
                last_was_empty = true;
                continue;
            }
            last_was_empty = false;

            match line {
                Line::Title(text) => {
                    // Add spacing before title if not first line
                    if i > 0 {
                        self.context.writeln("")?;
                    }
                    if use_color {
                        self.context.writeln(&text.bold().cyan().to_string())?;
                    } else {
                        self.context.writeln(text)?;
                    }
                }
                Line::Status(label, value) => {
                    if use_color {
                        self.context.writeln(&format!("{}: {}", label, value.bold()))?;
                    } else {
                        self.context.writeln(&format!("{}: {}", label, value))?;
                    }
                }
                Line::Item(label, value) => {
                    if use_color {
                        self.context.writeln(&format!("  {}: {}", label, value.bold()))?;
                    } else {
                        self.context.writeln(&format!("  {}: {}", label, value))?;
                    }
                }
                Line::Text(text) => {
                    self.context.writeln(text)?;
                }
                Line::Table(table) => {
                    self.context.write(&table.to_string())?;
                }
                Line::Empty => {} // Handled above
            }
        }

        Ok(())
    }
}

/// Help section builder
pub struct HelpSection<'a> {
    parent: OutputSection<'a>,
    items: Vec<String>,
}

impl<'a> HelpSection<'a> {
    fn new(parent: OutputSection<'a>) -> Self {
        Self {
            parent,
            items: Vec::new(),
        }
    }

    /// Add a help item
    pub fn item(mut self, text: &str) -> Self {
        self.items.push(text.to_string());
        self
    }

    /// Finish help section and return to parent
    pub fn end(mut self) -> OutputSection<'a> {
        if !self.items.is_empty() {
            self.parent.lines.push(Line::Empty);
            
            if self.parent.context.use_emoji() {
                self.parent.lines.push(Line::Text("ðŸ’¡ Tips:".to_string()));
            } else {
                self.parent.lines.push(Line::Text("Tips:".to_string()));
            }
            
            for item in self.items {
                self.parent.lines.push(Line::Text(format!("   â€¢ {}", item)));
            }
        }
        self.parent
    }

    /// Render the help section (consumes self)
    pub fn render(self) -> io::Result<()> {
        self.end().render()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::infrastructure::cli::output::{CliOutput, OutputFormat};

    #[test]
    fn test_section_builder() {
        let (mut output, buffer) = CliOutput::test();
        
        output.section()
            .title("Test Section")
            .status("Status", "Active")
            .empty_line()
            .item("Item 1", "Value 1")
            .item("Item 2", "Value 2")
            .render()
            .unwrap();
        
        let result = String::from_utf8(buffer.lock().unwrap().clone()).unwrap();
        assert!(result.contains("Test Section"));
        assert!(result.contains("Status: Active"));
        assert!(result.contains("Item 1: Value 1"));
    }

    #[test]
    fn test_help_section() {
        let (mut output, buffer) = CliOutput::test();
        
        output.section()
            .title("Commands")
            .help()
                .item("Use 'create' to make new items")
                .item("Use 'delete' to remove items")
            .render()
            .unwrap();
        
        let result = String::from_utf8(buffer.lock().unwrap().clone()).unwrap();
        assert!(result.contains("Commands"));
        assert!(result.contains("Tips:"));
        assert!(result.contains("â€¢ Use 'create' to make new items"));
    }
}```

## src/infrastructure/cli/output/formatters.rs

```rust
//! Output formatters for different output modes

use super::context::OutputContext;
use super::messages::*;
use super::{BULB, CHART, DEVICE, ERROR, FOLDER, INFO, LIBRARY, NETWORK, ROCKET, STOP, SUCCESS, TRASH, WARNING};
use owo_colors::OwoColorize;
use serde_json::json;

/// Core output trait - implemented for each format
pub trait OutputFormatter: Send {
    /// Format a message
    fn format(&self, message: &Message, context: &OutputContext) -> String;
    
    /// Format an error message
    fn format_error(&self, message: &Message, context: &OutputContext) -> String {
        self.format(message, context)
    }
}

/// Human-readable formatter with colors and emojis
pub struct HumanFormatter {
    use_color: bool,
    use_emoji: bool,
}

impl HumanFormatter {
    pub fn new(use_color: bool, use_emoji: bool) -> Self {
        Self { use_color, use_emoji }
    }

    fn emoji(&self, emoji: console::Emoji) -> String {
        if self.use_emoji {
            emoji.to_string()
        } else {
            "".to_string()
        }
    }

    fn colored(&self, text: impl std::fmt::Display) -> ColorHelper {
        ColorHelper {
            text: text.to_string(),
            use_color: self.use_color,
        }
    }
}

struct ColorHelper {
    text: String,
    use_color: bool,
}

impl ColorHelper {
    fn green(self) -> String {
        if self.use_color {
            self.text.green().to_string()
        } else {
            self.text
        }
    }

    fn red(self) -> String {
        if self.use_color {
            self.text.red().to_string()
        } else {
            self.text
        }
    }

    fn yellow(self) -> String {
        if self.use_color {
            self.text.yellow().to_string()
        } else {
            self.text
        }
    }

    fn bright_cyan(self) -> String {
        if self.use_color {
            self.text.bright_cyan().to_string()
        } else {
            self.text
        }
    }

    fn bright_yellow(self) -> String {
        if self.use_color {
            self.text.bright_yellow().to_string()
        } else {
            self.text
        }
    }

    fn bright_blue(self) -> String {
        if self.use_color {
            self.text.bright_blue().to_string()
        } else {
            self.text
        }
    }

    fn bright_green(self) -> String {
        if self.use_color {
            self.text.bright_green().to_string()
        } else {
            self.text
        }
    }

    fn bright_red(self) -> String {
        if self.use_color {
            self.text.bright_red().to_string()
        } else {
            self.text
        }
    }

    fn dimmed(self) -> String {
        if self.use_color {
            self.text.dimmed().to_string()
        } else {
            self.text
        }
    }

    fn bold(self) -> String {
        if self.use_color {
            self.text.bold().to_string()
        } else {
            self.text
        }
    }

    fn cyan(self) -> String {
        if self.use_color {
            self.text.cyan().to_string()
        } else {
            self.text
        }
    }
}

impl OutputFormatter for HumanFormatter {
    fn format(&self, message: &Message, _context: &OutputContext) -> String {
        match message {
            Message::Success(text) => {
                format!("{}{}", self.emoji(SUCCESS), self.colored(text).green())
            }
            Message::Error(text) => {
                format!("{}{}", self.emoji(ERROR), self.colored(text).red())
            }
            Message::Warning(text) => {
                format!("{}{}", self.emoji(WARNING), self.colored(text).yellow())
            }
            Message::Info(text) => {
                format!("{}{}", self.emoji(INFO), text)
            }
            Message::Debug(text) => {
                format!("[DEBUG] {}", self.colored(text).dimmed())
            }

            // Library messages
            Message::LibraryCreated { name, id, path } => {
                format!(
                    "{}Library '{}' created successfully!\n   ID: {}\n   Path: {}\n   Status: {}",
                    self.emoji(SUCCESS),
                    self.colored(name).bright_cyan(),
                    self.colored(id.to_string()).bright_yellow(),
                    self.colored(path.display()).bright_blue(),
                    self.colored("Active").bright_green()
                )
            }
            Message::LibraryList { libraries } => {
                if libraries.is_empty() {
                    format!("{}No libraries found. Create one with: spacedrive library create <name>", self.emoji(LIBRARY))
                } else {
                    let mut output = format!("{}Libraries:\n", self.emoji(LIBRARY));
                    for lib in libraries {
                        output.push_str(&format!(
                            "  {} {} - {}\n",
                            self.colored(&lib.id.to_string()[..8]).dimmed(),
                            self.colored(&lib.name).bright_cyan(),
                            lib.path.display()
                        ));
                    }
                    output.trim_end().to_string()
                }
            }
            Message::CurrentLibrary { library } => {
                match library {
                    Some(lib) => format!(
                        "{}Current library: {}\n   ID: {}\n   Path: {}",
                        self.emoji(LIBRARY),
                        self.colored(&lib.name).bright_cyan(),
                        self.colored(lib.id.to_string()).bright_yellow(),
                        self.colored(lib.path.display()).bright_blue()
                    ),
                    None => format!("{}No current library selected", self.emoji(WARNING)),
                }
            }
            Message::NoLibrariesFound => {
                format!("{}No libraries found. Create one with: spacedrive library create <name>", self.emoji(LIBRARY))
            }

            // Location messages
            Message::LocationAdded { path, id } => {
                format!(
                    "{}Location added successfully!\n   Path: {}\n   ID: {}",
                    self.emoji(SUCCESS),
                    self.colored(path.display()).bright_blue(),
                    self.colored(id.to_string()).bright_yellow()
                )
            }
            Message::LocationList { locations } => {
                if locations.is_empty() {
                    format!("{}No locations found", self.emoji(FOLDER))
                } else {
                    let mut output = format!("{}Locations:\n", self.emoji(FOLDER));
                    for loc in locations {
                        output.push_str(&format!(
                            "  {} - {} files\n",
                            self.colored(loc.path.display()).bright_blue(),
                            loc.indexed_files
                        ));
                    }
                    output.trim_end().to_string()
                }
            }

            // Daemon messages
            Message::DaemonStarting { instance } => {
                format!("{}Starting Spacedrive daemon ({})", self.emoji(ROCKET), instance)
            }
            Message::DaemonStopping { instance } => {
                format!("{}Stopping Spacedrive daemon instance '{}'...", self.emoji(STOP), instance)
            }
            Message::DaemonStopped { instance } => {
                format!("{}Spacedrive daemon instance '{}' stopped", self.emoji(SUCCESS), instance)
            }
            Message::DaemonStarted { instance, pid, socket_path } => {
                format!(
                    "{}Daemon started successfully ({})\n   PID: {}\n   Socket: {}",
                    self.emoji(SUCCESS),
                    instance,
                    self.colored(pid).bright_yellow(),
                    self.colored(socket_path.display()).bright_blue()
                )
            }
            Message::DaemonNotRunning { instance } => {
                format!(
                    "{}Spacedrive daemon instance '{}' is not running\n   Start it with: spacedrive start",
                    self.emoji(ERROR),
                    instance
                )
            }
            Message::DaemonStatus { version, uptime, instance, networking_enabled, libraries } => {
                let mut output = format!(
                    "{}Spacedrive Daemon Status\n",
                    self.emoji(CHART)
                );
                output.push_str(&format!("   Version: {}\n", self.colored(version).bright_green()));
                output.push_str(&format!("   Instance: {}\n", instance));
                output.push_str(&format!("   Uptime: {} seconds\n", uptime));
                output.push_str(&format!("   Networking: {}\n", 
                    if *networking_enabled { 
                        self.colored("Enabled").green()
                    } else { 
                        self.colored("Disabled").red()
                    }
                ));
                output.push_str(&format!("   Libraries: {}", libraries.len()));
                output
            }

            // Network messages
            Message::NetworkingInitialized => {
                format!("{}Networking initialized successfully", self.emoji(SUCCESS))
            }
            Message::NetworkingStarted => {
                format!("{}Networking service started", self.emoji(NETWORK))
            }
            Message::DevicesList { devices } => {
                if devices.is_empty() {
                    format!("{}No devices found", self.emoji(DEVICE))
                } else {
                    let mut output = format!("{}Discovered devices:\n", self.emoji(DEVICE));
                    for device in devices {
                        let status_str = format!("{:?}", device.status);
                        let status_colored = match device.status {
                            DeviceStatus::Online => self.colored(status_str).green(),
                            DeviceStatus::Paired => self.colored(status_str).bright_green(),
                            DeviceStatus::Offline => self.colored(status_str).red(),
                            DeviceStatus::Discovered => self.colored(status_str).yellow(),
                        };
                        let id_display = if device.id.len() >= 8 {
                            &device.id[..8]
                        } else {
                            &device.id
                        };
                        output.push_str(&format!(
                            "  {} {} - {}\n",
                            self.colored(id_display).dimmed(),
                            self.colored(&device.name).bright_cyan(),
                            status_colored
                        ));
                    }
                    output.trim_end().to_string()
                }
            }
            Message::PairingCodeGenerated { code } => {
                format!(
                    "{}Pairing code generated:\n\n   {}\n\n{}Share this code with the device you want to pair",
                    self.emoji(SUCCESS),
                    self.colored(code).bright_green().bold(),
                    self.emoji(BULB)
                )
            }
            Message::PairingSuccess { device_name, device_id } => {
                format!(
                    "{}Successfully paired with {} ({})",
                    self.emoji(SUCCESS),
                    self.colored(device_name).bright_cyan(),
                    self.colored(if device_id.len() >= 8 { &device_id[..8] } else { device_id }).dimmed()
                )
            }

            // Job messages
            Message::JobStarted { name, .. } => {
                format!("{}Job started: {}", self.emoji(ROCKET), self.colored(name).bright_cyan())
            }
            Message::JobCompleted { name, duration, .. } => {
                format!(
                    "{}Job completed: {} ({}s)",
                    self.emoji(SUCCESS),
                    self.colored(name).bright_cyan(),
                    duration
                )
            }
            Message::JobFailed { name, error, .. } => {
                format!(
                    "{}Job failed: {}\n   Error: {}",
                    self.emoji(ERROR),
                    self.colored(name).bright_cyan(),
                    self.colored(error).red()
                )
            }

            // File operation messages
            Message::FileCopied { source, destination } => {
                format!(
                    "{}Copied {} â†’ {}",
                    self.emoji(SUCCESS),
                    self.colored(source.display()).bright_blue(),
                    self.colored(destination.display()).bright_green()
                )
            }
            Message::FileDeleted { path } => {
                format!(
                    "{}Deleted {}",
                    self.emoji(TRASH),
                    self.colored(path.display()).bright_red()
                )
            }

            // Help messages
            Message::HelpText { lines } => {
                let mut output = format!("{}Tips:\n", self.emoji(BULB));
                for line in lines {
                    output.push_str(&format!("   â€¢ {}\n", line));
                }
                output.trim_end().to_string()
            }

            // Progress messages
            Message::IndexingProgress { current, total, location } => {
                format!(
                    "Indexing {}: {}/{} files",
                    self.colored(location).bright_blue(),
                    current,
                    total
                )
            }
            Message::CopyProgress { current, total, current_file } => {
                match current_file {
                    Some(file) => format!(
                        "Copying {}: {}/{} files",
                        self.colored(file).bright_blue(),
                        current,
                        total
                    ),
                    None => format!("Copying: {}/{} files", current, total),
                }
            }
            Message::ValidationProgress { current, total } => {
                format!("Validating: {}/{} files", current, total)
            }

            // Fallback for other messages
            _ => format!("{:?}", message),
        }
    }

    fn format_error(&self, message: &Message, context: &OutputContext) -> String {
        // Errors are always formatted with error styling
        match message {
            Message::Error(text) => {
                format!("{}{}", self.emoji(ERROR), self.colored(text).red())
            }
            _ => self.format(message, context),
        }
    }
}

/// JSON formatter for machine-readable output
pub struct JsonFormatter;

impl OutputFormatter for JsonFormatter {
    fn format(&self, message: &Message, _context: &OutputContext) -> String {
        // Convert message to JSON
        match message {
            Message::Success(text) => json!({
                "type": "success",
                "message": text
            }).to_string(),
            Message::Error(text) => json!({
                "type": "error",
                "message": text
            }).to_string(),
            Message::LibraryCreated { name, id, path } => json!({
                "type": "library_created",
                "success": true,
                "data": {
                    "name": name,
                    "id": id.to_string(),
                    "path": path.to_string_lossy()
                }
            }).to_string(),
            Message::LibraryList { libraries } => json!({
                "type": "library_list",
                "data": libraries.iter().map(|lib| json!({
                    "id": lib.id.to_string(),
                    "name": lib.name,
                    "path": lib.path.to_string_lossy()
                })).collect::<Vec<_>>()
            }).to_string(),
            Message::DevicesList { devices } => json!({
                "type": "devices_list",
                "data": devices.iter().map(|dev| json!({
                    "id": dev.id,
                    "name": dev.name,
                    "status": format!("{:?}", dev.status).to_lowercase()
                })).collect::<Vec<_>>()
            }).to_string(),
            _ => {
                // Fallback to serializing the entire message
                serde_json::to_string(message).unwrap_or_else(|_| {
                    json!({
                        "type": "unknown",
                        "message": format!("{:?}", message)
                    }).to_string()
                })
            }
        }
    }
}```

## src/infrastructure/cli/output/mod.rs

```rust
//! CLI Output System
//!
//! This module provides a structured and consistent output system for the CLI,
//! replacing direct `println!` calls with a type-safe, testable approach.
//!
//! # Features
//! - Multiple output formats (Human, JSON, Quiet)
//! - Color and emoji support with automatic fallback
//! - Testable output with writer injection
//! - Consistent styling across all commands
//! - Structured message types for all CLI outputs

pub mod context;
pub mod formatters;
pub mod messages;
pub mod section;

#[cfg(test)]
mod tests;

pub use context::{ColorMode, OutputContext, OutputFormat, VerbosityLevel};
pub use messages::Message;
pub use section::OutputSection;

use console::Emoji;
use std::io::{self, Write};

// Emoji constants with fallbacks
pub const SUCCESS: Emoji<'_, '_> = Emoji("âœ… ", "[OK] ");
pub const ERROR: Emoji<'_, '_> = Emoji("âŒ ", "[ERROR] ");
pub const WARNING: Emoji<'_, '_> = Emoji("âš ï¸  ", "[WARN] ");
pub const INFO: Emoji<'_, '_> = Emoji("â„¹ï¸  ", "[INFO] ");
pub const SEARCH: Emoji<'_, '_> = Emoji("ðŸ” ", "[SEARCH] ");
pub const FOLDER: Emoji<'_, '_> = Emoji("ðŸ“ ", "[FOLDER] ");
pub const LIBRARY: Emoji<'_, '_> = Emoji("ðŸ“š ", "[LIBRARY] ");
pub const NETWORK: Emoji<'_, '_> = Emoji("ðŸŒ ", "[NETWORK] ");
pub const DEVICE: Emoji<'_, '_> = Emoji("ðŸ’» ", "[DEVICE] ");
pub const ROCKET: Emoji<'_, '_> = Emoji("ðŸš€ ", "[START] ");
pub const STOP: Emoji<'_, '_> = Emoji("ðŸ›‘ ", "[STOP] ");
pub const MAIL: Emoji<'_, '_> = Emoji("ðŸ“¬ ", "[MAIL] ");
pub const TRASH: Emoji<'_, '_> = Emoji("ðŸ—‘ï¸  ", "[DELETE] ");
pub const CHART: Emoji<'_, '_> = Emoji("ðŸ“Š ", "[STATUS] ");
pub const CLOCK: Emoji<'_, '_> = Emoji("ðŸ• ", "[TIME] ");
pub const PAIRING: Emoji<'_, '_> = Emoji("ðŸ”— ", "[PAIR] ");
pub const BULB: Emoji<'_, '_> = Emoji("ðŸ’¡ ", "[TIP] ");
pub const CHECKMARK: Emoji<'_, '_> = Emoji("âœ“ ", "[OK] ");

/// Main CLI output handler
pub struct CliOutput {
    context: OutputContext,
}

impl CliOutput {
    /// Create a new output handler with the given format
    pub fn new(format: OutputFormat) -> Self {
        Self {
            context: OutputContext::new(format),
        }
    }

    /// Create a new output handler with all options
    pub fn with_options(
        format: OutputFormat,
        verbosity: VerbosityLevel,
        color: ColorMode,
    ) -> Self {
        Self {
            context: OutputContext::with_options(format, verbosity, color),
        }
    }

    /// Create a test output handler that captures output to a string
    #[cfg(test)]
    pub fn test() -> (Self, std::sync::Arc<std::sync::Mutex<Vec<u8>>>) {
        let buffer = std::sync::Arc::new(std::sync::Mutex::new(Vec::new()));
        let writer = TestWriter { buffer: buffer.clone() };
        let context = OutputContext::test(Box::new(writer));
        (Self { context }, buffer)
    }

    /// Print a message
    pub fn print(&mut self, message: Message) -> io::Result<()> {
        self.context.print(message)
    }

    /// Print an error message (always prints regardless of verbosity)
    pub fn error(&mut self, message: Message) -> io::Result<()> {
        self.context.error(message)
    }

    /// Print a success message
    pub fn success(&mut self, text: &str) -> io::Result<()> {
        self.print(Message::Success(text.to_string()))
    }

    /// Print an info message
    pub fn info(&mut self, text: &str) -> io::Result<()> {
        self.print(Message::Info(text.to_string()))
    }

    /// Print a warning message
    pub fn warning(&mut self, text: &str) -> io::Result<()> {
        self.print(Message::Warning(text.to_string()))
    }

    /// Create a new output section builder
    pub fn section(&mut self) -> OutputSection {
        OutputSection::new(&mut self.context)
    }

    /// Get the test output buffer
    #[cfg(test)]
    pub fn test_buffer(&self) -> Option<String> {
        None // This will be handled differently
    }

    /// Check if output should use colors
    pub fn use_color(&self) -> bool {
        self.context.use_color()
    }

    /// Check if output should use emojis
    pub fn use_emoji(&self) -> bool {
        self.context.use_emoji()
    }

    /// Get the current output format
    pub fn format(&self) -> &OutputFormat {
        self.context.format()
    }

    /// Get the current verbosity level
    pub fn verbosity(&self) -> &VerbosityLevel {
        self.context.verbosity()
    }
}


// Test writer for capturing output
#[cfg(test)]
struct TestWriter {
    buffer: std::sync::Arc<std::sync::Mutex<Vec<u8>>>,
}

#[cfg(test)]
impl Write for TestWriter {
    fn write(&mut self, buf: &[u8]) -> io::Result<usize> {
        self.buffer.lock().unwrap().extend_from_slice(buf);
        Ok(buf.len())
    }
    
    fn flush(&mut self) -> io::Result<()> {
        Ok(())
    }
}```

## src/infrastructure/cli/output/tests.rs

```rust
//! Tests for the CLI output system

#[cfg(test)]
mod tests {
    use crate::infrastructure::cli::output::*;
    use crate::infrastructure::cli::output::messages::*;
    use std::path::PathBuf;
    use uuid::Uuid;

    #[test]
    fn test_human_format_basic_messages() {
        let (mut output, buffer) = CliOutput::test();
        
        output.success("Operation successful").unwrap();
        output.error(Message::Error("Something went wrong".to_string())).unwrap();
        output.warning("This is a warning").unwrap();
        output.info("Some information").unwrap();
        
        let result = String::from_utf8(buffer.lock().unwrap().clone()).unwrap();
        
        assert!(result.contains("Operation successful"));
        assert!(result.contains("Something went wrong"));
        assert!(result.contains("This is a warning"));
        assert!(result.contains("Some information"));
    }

    #[test]
    fn test_json_format() {
        use std::sync::{Arc, Mutex};
        use super::super::TestWriter;
        
        let buffer = Arc::new(Mutex::new(Vec::new()));
        let writer = TestWriter { buffer: buffer.clone() };
        let mut output = CliOutput {
            context: OutputContext::test(Box::new(writer)),
        };
        output.context.format = OutputFormat::Json;
        output.context.formatter = Box::new(crate::infrastructure::cli::output::formatters::JsonFormatter);
        
        output.print(Message::LibraryCreated {
            name: "Test Library".to_string(),
            id: Uuid::new_v4(),
            path: PathBuf::from("/test/path"),
        }).unwrap();
        
        let result = String::from_utf8(buffer.lock().unwrap().clone()).unwrap();
        let json: serde_json::Value = serde_json::from_str(&result.trim()).unwrap();
        
        assert_eq!(json["type"], "library_created");
        assert_eq!(json["success"], true);
        assert_eq!(json["data"]["name"], "Test Library");
    }

    #[test]
    fn test_quiet_format() {
        use std::sync::{Arc, Mutex};
        use super::super::TestWriter;
        
        let buffer = Arc::new(Mutex::new(Vec::new()));
        let writer = TestWriter { buffer: buffer.clone() };
        let mut output = CliOutput {
            context: OutputContext::test(Box::new(writer)),
        };
        output.context.format = OutputFormat::Quiet;
        
        // Normal messages should not appear in quiet mode
        output.info("This should not appear").unwrap();
        output.success("This also should not appear").unwrap();
        
        // Errors should always appear
        output.error(Message::Error("This error should appear".to_string())).unwrap();
        
        let result = String::from_utf8(buffer.lock().unwrap().clone()).unwrap();
        
        assert!(!result.contains("This should not appear"));
        assert!(!result.contains("This also should not appear"));
        assert!(result.contains("This error should appear"));
    }

    #[test]
    fn test_verbosity_levels() {
        use std::sync::{Arc, Mutex};
        use super::super::TestWriter;
        
        let buffer = Arc::new(Mutex::new(Vec::new()));
        let writer = TestWriter { buffer: buffer.clone() };
        let mut output = CliOutput::with_options(
            OutputFormat::Human,
            VerbosityLevel::Normal,
            ColorMode::Never
        );
        output.context = OutputContext::test(Box::new(writer));
        
        // Normal messages should appear
        output.info("Normal info").unwrap();
        
        // Debug messages should not appear at normal verbosity
        output.print(Message::Debug("Debug info".to_string())).unwrap();
        
        // Progress messages (verbose level) should not appear
        output.print(Message::IndexingProgress {
            current: 10,
            total: 100,
            location: "test".to_string(),
        }).unwrap();
        
        let result = String::from_utf8(buffer.lock().unwrap().clone()).unwrap();
        
        assert!(result.contains("Normal info"));
        assert!(!result.contains("Debug info"));
        assert!(!result.contains("Indexing"));
    }

    #[test]
    fn test_library_messages() {
        let (mut output, buffer) = CliOutput::test();
        
        let lib_id = Uuid::new_v4();
        output.print(Message::LibraryCreated {
            name: "My Library".to_string(),
            id: lib_id,
            path: PathBuf::from("/home/user/library"),
        }).unwrap();
        
        let result = String::from_utf8(buffer.lock().unwrap().clone()).unwrap();
        
        assert!(result.contains("Library 'My Library' created successfully"));
        assert!(result.contains(&lib_id.to_string()));
        assert!(result.contains("/home/user/library"));
    }

    #[test]
    fn test_device_list() {
        let (mut output, buffer) = CliOutput::test();
        
        let devices = vec![
            DeviceInfo {
                id: "device1".to_string(),
                name: "My Computer".to_string(),
                status: DeviceStatus::Online,
                peer_id: None,
            },
            DeviceInfo {
                id: "device2".to_string(),
                name: "My Phone".to_string(),
                status: DeviceStatus::Paired,
                peer_id: Some("peer123".to_string()),
            },
        ];
        
        output.print(Message::DevicesList { devices }).unwrap();
        
        let result = String::from_utf8(buffer.lock().unwrap().clone()).unwrap();
        
        assert!(result.contains("Discovered devices"));
        assert!(result.contains("My Computer"));
        assert!(result.contains("My Phone"));
        assert!(result.contains("Online"));
        assert!(result.contains("Paired"));
    }

    #[test]
    fn test_section_builder_basic() {
        let (mut output, buffer) = CliOutput::test();
        
        output.section()
            .title("Test Section")
            .status("Version", "1.0.0")
            .status("Status", "Running")
            .empty_line()
            .text("Some additional text")
            .render()
            .unwrap();
        
        let result = String::from_utf8(buffer.lock().unwrap().clone()).unwrap();
        
        assert!(result.contains("Test Section"));
        assert!(result.contains("Version: 1.0.0"));
        assert!(result.contains("Status: Running"));
        assert!(result.contains("Some additional text"));
    }

    #[test]
    fn test_section_builder_with_table() {
        let (mut output, buffer) = CliOutput::test();
        
        let mut table = comfy_table::Table::new();
        table.set_header(vec!["ID", "Name", "Status"]);
        table.add_row(vec!["1", "Item 1", "Active"]);
        table.add_row(vec!["2", "Item 2", "Inactive"]);
        
        output.section()
            .title("Items")
            .table(table)
            .render()
            .unwrap();
        
        let result = String::from_utf8(buffer.lock().unwrap().clone()).unwrap();
        
        assert!(result.contains("Items"));
        assert!(result.contains("ID"));
        assert!(result.contains("Name"));
        assert!(result.contains("Status"));
        assert!(result.contains("Item 1"));
        assert!(result.contains("Active"));
    }

    #[test]
    fn test_help_section() {
        let (mut output, buffer) = CliOutput::test();
        
        output.section()
            .title("Available Commands")
            .help()
                .item("create - Create a new item")
                .item("list - List all items")
                .item("delete - Delete an item")
            .render()
            .unwrap();
        
        let result = String::from_utf8(buffer.lock().unwrap().clone()).unwrap();
        
        assert!(result.contains("Available Commands"));
        assert!(result.contains("Tips:"));
        assert!(result.contains("â€¢ create - Create a new item"));
        assert!(result.contains("â€¢ list - List all items"));
        assert!(result.contains("â€¢ delete - Delete an item"));
    }

    #[test]
    fn test_progress_messages() {
        use std::sync::{Arc, Mutex};
        use super::super::TestWriter;
        
        let buffer = Arc::new(Mutex::new(Vec::new()));
        let writer = TestWriter { buffer: buffer.clone() };
        let mut output = CliOutput::with_options(
            OutputFormat::Human,
            VerbosityLevel::Verbose,
            ColorMode::Never
        );
        output.context = OutputContext::test(Box::new(writer));
        output.context.verbosity = VerbosityLevel::Verbose;
        
        output.print(Message::IndexingProgress {
            current: 150,
            total: 1000,
            location: "/home/user/documents".to_string(),
        }).unwrap();
        
        output.print(Message::CopyProgress {
            current: 5,
            total: 10,
            current_file: Some("file.txt".to_string()),
        }).unwrap();
        
        let result = String::from_utf8(buffer.lock().unwrap().clone()).unwrap();
        
        assert!(result.contains("Indexing /home/user/documents: 150/1000 files"));
        assert!(result.contains("Copying file.txt: 5/10 files"));
    }

    #[test]
    fn test_pairing_messages() {
        let (mut output, buffer) = CliOutput::test();
        
        output.print(Message::PairingCodeGenerated {
            code: "ABC123".to_string(),
        }).unwrap();
        
        output.print(Message::PairingSuccess {
            device_name: "John's Phone".to_string(),
            device_id: "device123".to_string(),
        }).unwrap();
        
        let result = String::from_utf8(buffer.lock().unwrap().clone()).unwrap();
        
        assert!(result.contains("Pairing code generated"));
        assert!(result.contains("ABC123"));
        assert!(result.contains("Successfully paired with John's Phone"));
    }

    #[test]
    fn test_job_messages() {
        let (mut output, buffer) = CliOutput::test();
        
        let job_id = Uuid::new_v4();
        
        output.print(Message::JobStarted {
            id: job_id,
            name: "File Copy".to_string(),
        }).unwrap();
        
        output.print(Message::JobCompleted {
            id: job_id,
            name: "File Copy".to_string(),
            duration: 42,
        }).unwrap();
        
        output.print(Message::JobFailed {
            id: job_id,
            name: "File Validation".to_string(),
            error: "Checksum mismatch".to_string(),
        }).unwrap();
        
        let result = String::from_utf8(buffer.lock().unwrap().clone()).unwrap();
        
        assert!(result.contains("Job started: File Copy"));
        assert!(result.contains("Job completed: File Copy (42s)"));
        assert!(result.contains("Job failed: File Validation"));
        assert!(result.contains("Checksum mismatch"));
    }

    #[test]
    fn test_empty_line_deduplication() {
        let (mut output, buffer) = CliOutput::test();
        
        output.section()
            .title("Test")
            .empty_line()
            .empty_line()  // Should be deduplicated
            .empty_line()  // Should be deduplicated
            .text("Content")
            .render()
            .unwrap();
        
        let result = String::from_utf8(buffer.lock().unwrap().clone()).unwrap();
        let lines: Vec<&str> = result.lines().collect();
        
        // Count empty lines between "Test" and "Content"
        let empty_count = lines.iter()
            .skip_while(|&&line| !line.contains("Test"))
            .take_while(|&&line| !line.contains("Content"))
            .filter(|&&line| line.trim().is_empty())
            .count();
        
        assert_eq!(empty_count, 1, "Should have exactly one empty line");
    }

    #[test]
    fn test_color_mode_detection() {
        // Test auto mode (will be false in test environment)
        let ctx = OutputContext::with_options(
            OutputFormat::Human,
            VerbosityLevel::Normal,
            ColorMode::Auto
        );
        // In test environment, should detect no color support
        assert!(!ctx.use_color());
        
        // Test always mode
        let ctx = OutputContext::with_options(
            OutputFormat::Human,
            VerbosityLevel::Normal,
            ColorMode::Always
        );
        assert!(ctx.use_color());
        
        // Test never mode
        let ctx = OutputContext::with_options(
            OutputFormat::Human,
            VerbosityLevel::Normal,
            ColorMode::Never
        );
        assert!(!ctx.use_color());
    }

    #[test]
    fn test_daemon_status_message() {
        let (mut output, buffer) = CliOutput::test();
        
        output.print(Message::DaemonStatus {
            version: "2.0.0".to_string(),
            uptime: 3600,
            instance: "default".to_string(),
            networking_enabled: true,
            libraries: vec![
                LibraryInfo {
                    id: Uuid::new_v4(),
                    name: "Main Library".to_string(),
                    path: PathBuf::from("/libraries/main"),
                }
            ],
        }).unwrap();
        
        let result = String::from_utf8(buffer.lock().unwrap().clone()).unwrap();
        
        assert!(result.contains("Spacedrive Daemon Status"));
        assert!(result.contains("Version: 2.0.0"));
        assert!(result.contains("Instance: default"));
        assert!(result.contains("Uptime: 3600 seconds"));
        assert!(result.contains("Networking: Enabled"));
        assert!(result.contains("Libraries: 1"));
    }
}```

## src/infrastructure/cli/output/messages.rs

```rust
//! Message types for all CLI outputs

use super::VerbosityLevel;
use serde::{Deserialize, Serialize};
use std::path::PathBuf;
use uuid::Uuid;

/// All possible output messages in the system
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(tag = "type")]
pub enum Message {
	// Generic messages
	Success(String),
	Error(String),
	Warning(String),
	Info(String),
	Debug(String),

	// Library messages
	LibraryCreated {
		name: String,
		id: Uuid,
		path: PathBuf,
	},
	LibraryDeleted {
		id: Uuid,
	},
	LibrarySwitched {
		name: String,
		id: Uuid,
	},
	LibraryList {
		libraries: Vec<LibraryInfo>,
	},
	CurrentLibrary {
		library: Option<LibraryInfo>,
	},
	NoLibrariesFound,

	// Location messages
	LocationAdded {
		path: PathBuf,
		id: Uuid,
	},
	LocationRemoved {
		path: PathBuf,
	},
	LocationList {
		locations: Vec<LocationInfo>,
	},
	LocationIndexing {
		path: PathBuf,
		progress: f32,
	},

	// Daemon messages
	DaemonStarting {
		instance: String,
	},
	DaemonStarted {
		instance: String,
		pid: u32,
		socket_path: PathBuf,
	},
	DaemonStopping {
		instance: String,
	},
	DaemonStopped {
		instance: String,
	},
	DaemonNotRunning {
		instance: String,
	},
	DaemonStatus {
		version: String,
		uptime: u64,
		instance: String,
		networking_enabled: bool,
		libraries: Vec<LibraryInfo>,
	},

	// Network messages
	NetworkingInitialized,
	NetworkingStarted,
	NetworkingStopped,
	DeviceDiscovered {
		device: DeviceInfo,
	},
	DevicesList {
		devices: Vec<DeviceInfo>,
	},
	PairingCodeGenerated {
		code: String,
	},
	PairingInProgress {
		device_name: String,
	},
	PairingSuccess {
		device_name: String,
		device_id: String,
	},
	PairingFailed {
		reason: String,
	},
	PairingStatus {
		status: String,
		pending_requests: Vec<PairingRequest>,
	},
	SpacedropSent {
		file_name: String,
		device_name: String,
	},
	SpacedropReceived {
		file_name: String,
		sender_name: String,
	},

	// Job messages
	JobStarted {
		id: Uuid,
		name: String,
	},
	JobProgress {
		id: Uuid,
		name: String,
		progress: f32,
		message: Option<String>,
	},
	JobCompleted {
		id: Uuid,
		name: String,
		duration: u64,
	},
	JobFailed {
		id: Uuid,
		name: String,
		error: String,
	},
	JobList {
		jobs: Vec<JobInfo>,
	},

	// File operation messages
	FileCopied {
		source: PathBuf,
		destination: PathBuf,
	},
	FileDeleted {
		path: PathBuf,
	},
	FileRenamed {
		old_path: PathBuf,
		new_path: PathBuf,
	},
	FileValidated {
		path: PathBuf,
		hash: String,
	},

	// System messages
	SystemInfo {
		version: String,
		platform: String,
		data_dir: PathBuf,
	},
	LogsShowing {
		path: PathBuf,
	},

	// Progress messages
	IndexingProgress {
		current: u64,
		total: u64,
		location: String,
	},
	CopyProgress {
		current: u64,
		total: u64,
		current_file: Option<String>,
	},
	ValidationProgress {
		current: u64,
		total: u64,
	},

	// Help messages
	HelpText {
		lines: Vec<String>,
	},
}

impl Message {
	/// Determine if this is an error message
	pub fn is_error(&self) -> bool {
		matches!(
			self,
			Message::Error(_)
				| Message::DaemonNotRunning { .. }
				| Message::PairingFailed { .. }
				| Message::JobFailed { .. }
		)
	}

	/// Get the verbosity level for this message
	pub fn verbosity_level(&self) -> VerbosityLevel {
		match self {
			Message::Debug(_) => VerbosityLevel::Debug,
			Message::IndexingProgress { .. }
			| Message::CopyProgress { .. }
			| Message::ValidationProgress { .. } => VerbosityLevel::Verbose,
			_ => VerbosityLevel::Normal,
		}
	}
}

// Helper structs for complex messages
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LibraryInfo {
	pub id: Uuid,
	pub name: String,
	pub path: PathBuf,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LocationInfo {
	pub id: Uuid,
	pub path: PathBuf,
	pub indexed_files: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DeviceInfo {
	pub id: String,
	pub name: String,
	pub status: DeviceStatus,
	pub peer_id: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum DeviceStatus {
	Online,
	Offline,
	Paired,
	Discovered,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PairingRequest {
	pub id: String,
	pub device_name: String,
	pub timestamp: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct JobInfo {
	pub id: Uuid,
	pub name: String,
	pub status: JobStatus,
	pub progress: Option<f32>,
	pub started_at: u64,
	pub completed_at: Option<u64>,
}

#[derive(Debug, Clone, Serialize, Deserialize, strum::Display, strum::EnumString)]
#[strum(serialize_all = "lowercase")]
pub enum JobStatus {
	Queued,
	Running,
	Completed,
	Failed,
	Paused,
	Cancelled,
}

impl From<crate::infrastructure::jobs::types::JobStatus> for JobStatus {
	fn from(status: crate::infrastructure::jobs::types::JobStatus) -> Self {
		match status {
			crate::infrastructure::jobs::types::JobStatus::Queued => Self::Queued,
			crate::infrastructure::jobs::types::JobStatus::Running => Self::Running,
			crate::infrastructure::jobs::types::JobStatus::Completed => Self::Completed,
			crate::infrastructure::jobs::types::JobStatus::Failed => Self::Failed,
			crate::infrastructure::jobs::types::JobStatus::Paused => Self::Paused,
			crate::infrastructure::jobs::types::JobStatus::Cancelled => Self::Cancelled,
		}
	}
}
```

## src/infrastructure/cli/output/context.rs

```rust
//! Output context and configuration

use super::formatters::{HumanFormatter, JsonFormatter, OutputFormatter};
use super::messages::Message;
use std::io::{self, Write};
use supports_color::Stream;

/// Output format for CLI commands
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum OutputFormat {
    /// Human-readable output with colors and emojis
    Human,
    /// Machine-readable JSON output
    Json,
    /// Minimal output (errors only)
    Quiet,
}

impl OutputFormat {
    /// Parse from string
    pub fn from_str(s: &str) -> Option<Self> {
        match s.to_lowercase().as_str() {
            "human" => Some(OutputFormat::Human),
            "json" => Some(OutputFormat::Json),
            "quiet" => Some(OutputFormat::Quiet),
            _ => None,
        }
    }
}

/// Verbosity level for output
#[derive(Debug, Clone, Copy, PartialEq, Eq, PartialOrd, Ord)]
pub enum VerbosityLevel {
    /// Errors only
    Quiet = 0,
    /// Default output
    Normal = 1,
    /// Additional information
    Verbose = 2,
    /// Everything including debug info
    Debug = 3,
}

impl VerbosityLevel {
    /// Create from occurrence count (e.g., -v, -vv, -vvv)
    pub fn from_occurrences(count: u8) -> Self {
        match count {
            0 => VerbosityLevel::Normal,
            1 => VerbosityLevel::Verbose,
            2.. => VerbosityLevel::Debug,
        }
    }
}

/// Color output mode
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum ColorMode {
    /// Automatically detect terminal support
    Auto,
    /// Always use colors
    Always,
    /// Never use colors
    Never,
}

/// Global output context passed through CLI operations
pub struct OutputContext {
    pub(super) format: OutputFormat,
    pub(super) verbosity: VerbosityLevel,
    pub(super) color: ColorMode,
    pub(super) writer: Box<dyn Write>,
    pub(super) formatter: Box<dyn OutputFormatter>,
    pub(super) use_emoji: bool,
    pub(super) use_color: bool,
}

impl OutputContext {
    /// Create a new output context with default settings
    pub fn new(format: OutputFormat) -> Self {
        Self::with_options(format, VerbosityLevel::Normal, ColorMode::Auto)
    }

    /// Create a new output context with all options
    pub fn with_options(
        format: OutputFormat,
        verbosity: VerbosityLevel,
        color: ColorMode,
    ) -> Self {
        let writer: Box<dyn Write> = Box::new(io::stdout());
        
        // Determine color support
        let use_color = match color {
            ColorMode::Always => true,
            ColorMode::Never => false,
            ColorMode::Auto => supports_color::on(Stream::Stdout).is_some(),
        };

        // Determine emoji support (use emojis if we have color support)
        let use_emoji = use_color && format != OutputFormat::Json;

        // Create appropriate formatter
        let formatter: Box<dyn OutputFormatter> = match format {
            OutputFormat::Human => Box::new(HumanFormatter::new(use_color, use_emoji)),
            OutputFormat::Json => Box::new(JsonFormatter),
            OutputFormat::Quiet => Box::new(HumanFormatter::new(false, false)),
        };

        Self {
            format,
            verbosity,
            color,
            writer,
            formatter,
            use_emoji,
            use_color,
        }
    }

    /// Create a test context with a custom writer
    #[cfg(test)]
    pub fn test(writer: Box<dyn Write>) -> Self {
        let formatter = Box::new(HumanFormatter::new(false, false));

        Self {
            format: OutputFormat::Human,
            verbosity: VerbosityLevel::Normal,
            color: ColorMode::Never,
            writer,
            formatter,
            use_emoji: false,
            use_color: false,
        }
    }

    /// Check if a message should be printed based on verbosity
    pub fn should_print(&self, message: &Message) -> bool {
        match self.format {
            OutputFormat::Quiet => message.is_error(),
            _ => {
                let msg_level = message.verbosity_level();
                msg_level <= self.verbosity
            }
        }
    }

    /// Print a message
    pub fn print(&mut self, message: Message) -> io::Result<()> {
        if self.should_print(&message) {
            let formatted = self.formatter.format(&message, self);
            writeln!(self.writer, "{}", formatted)?;
        }
        Ok(())
    }

    /// Print an error message (always prints)
    pub fn error(&mut self, message: Message) -> io::Result<()> {
        let formatted = self.formatter.format_error(&message, self);
        writeln!(self.writer, "{}", formatted)?;
        Ok(())
    }

    /// Write raw string (for section output)
    pub fn write(&mut self, content: &str) -> io::Result<()> {
        write!(self.writer, "{}", content)?;
        Ok(())
    }

    /// Write raw string with newline
    pub fn writeln(&mut self, content: &str) -> io::Result<()> {
        writeln!(self.writer, "{}", content)?;
        Ok(())
    }

    /// Get the inner writer
    pub fn into_inner(self) -> Box<dyn Write> {
        self.writer
    }
    
    /// Get a reference to the formatter for testing
    #[cfg(test)]
    pub fn formatter(&self) -> &dyn OutputFormatter {
        self.formatter.as_ref()
    }

    /// Check if colors should be used
    pub fn use_color(&self) -> bool {
        self.use_color
    }

    /// Check if emojis should be used
    pub fn use_emoji(&self) -> bool {
        self.use_emoji
    }

    /// Get the output format
    pub fn format(&self) -> &OutputFormat {
        &self.format
    }

    /// Get the verbosity level
    pub fn verbosity(&self) -> &VerbosityLevel {
        &self.verbosity
    }
}

// Implement Write trait to allow direct writing
impl Write for OutputContext {
    fn write(&mut self, buf: &[u8]) -> io::Result<usize> {
        self.writer.write(buf)
    }

    fn flush(&mut self) -> io::Result<()> {
        self.writer.flush()
    }
}

```

## src/infrastructure/cli/pairing_ui.rs

```rust
//! CLI-specific pairing user interface implementations
//!
//! This module contains CLI-specific implementations for pairing interactions,
//! providing console-based interactions for device pairing.

use async_trait::async_trait;
use crate::networking::{DeviceInfo, NetworkingError as NetworkError, Result};
use crate::networking::PairingState;

/// CLI-specific pairing user interface trait
#[async_trait]
pub trait PairingUserInterface: Send + Sync {
    async fn confirm_pairing(&self, remote_device: &DeviceInfo) -> Result<bool>;
    async fn show_pairing_progress(&self, state: PairingState);
    async fn show_pairing_error(&self, error: &NetworkError);
    async fn show_pairing_code(&self, code: &str, expires_in_seconds: u32);
    async fn prompt_pairing_code(&self) -> Result<[String; 12]>;
}

/// Console-based pairing UI for CLI applications
pub struct ConsolePairingUI;

#[async_trait]
impl PairingUserInterface for ConsolePairingUI {
    async fn confirm_pairing(&self, remote_device: &DeviceInfo) -> Result<bool> {
        use dialoguer::Confirm;
        
        println!("\nðŸ”— Device Pairing Request");
        println!("=========================");
        println!("Device wants to pair with you:");
        println!("  Name: {}", remote_device.device_name);
        println!("  ID: {}", remote_device.device_id);
        println!("  Fingerprint: {}", remote_device.network_fingerprint);
        println!();
        
        let confirmed = Confirm::new()
            .with_prompt("Do you want to pair with this device?")
            .default(false)
            .interact()
            .map_err(|e| NetworkError::AuthenticationFailed(format!("UI interaction failed: {}", e)))?;
        
        if confirmed {
            println!("âœ… Pairing confirmed by user");
        } else {
            println!("âŒ Pairing rejected by user");
        }
        
        Ok(confirmed)
    }
    
    async fn show_pairing_progress(&self, state: PairingState) {
        use colored::*;
        
        let (status, message) = match state {
            PairingState::Idle => ("â¸ï¸", "Waiting to start pairing"),
            PairingState::GeneratingCode => ("ðŸ”„", "Generating pairing code..."),
            PairingState::Broadcasting => ("ðŸ“¡", "Broadcasting pairing availability"),
            PairingState::Scanning => ("ðŸ”", "Scanning for devices to pair with"),
            PairingState::WaitingForConnection => ("â³", "Waiting for connection"),
            PairingState::Connecting => ("ðŸ”—", "Establishing secure connection"),
            PairingState::Authenticating => ("ðŸ”", "Authenticating pairing code"),
            PairingState::ExchangingKeys => ("ðŸ”‘", "Exchanging device information"),
            PairingState::AwaitingConfirmation => ("â³", "Waiting for user confirmation"),
            PairingState::EstablishingSession => ("ðŸ›¡ï¸", "Establishing session keys"),
            PairingState::ChallengeReceived { .. } => ("ðŸ”", "Processing challenge"),
            PairingState::ResponseSent => ("ðŸ“¤", "Response sent"),
            PairingState::Completed => ("âœ…", "Pairing completed successfully"),
            PairingState::Failed { ref reason } => ("âŒ", reason.as_str()),
            PairingState::ResponsePending { .. } => ("ðŸ”„", "Preparing challenge response"),
        };
        
        println!("{} {}", status, message.bright_white());
    }
    
    async fn show_pairing_error(&self, error: &NetworkError) {
        use colored::*;
        
        println!("{} Pairing failed: {}", "âŒ".red(), error.to_string().red());
    }
    
    async fn show_pairing_code(&self, code: &str, expires_in_seconds: u32) {
        use colored::*;
        
        println!("\nðŸ”‘ Your Pairing Code");
        println!("==================");
        println!();
        println!("Share this code with the other device:");
        println!();
        println!("    {}", code.bright_cyan().bold());
        println!();
        println!("â° This code expires in {} seconds", expires_in_seconds.to_string().yellow());
        println!();
        println!("ðŸ’¡ The other device should enter these words to pair with you.");
    }
    
    async fn prompt_pairing_code(&self) -> Result<[String; 12]> {
        use dialoguer::Input;
        use colored::*;
        
        println!("\nðŸ”‘ Enter Pairing Code");
        println!("====================");
        println!("Enter the 12-word pairing code from the other device:");
        println!();
        
        let mut words = Vec::new();
        
        for i in 1..=12 {
            let word: String = Input::new()
                .with_prompt(&format!("Word {}/12", i))
                .interact()
                .map_err(|e| NetworkError::AuthenticationFailed(format!("Input failed: {}", e)))?;
            
            words.push(word.trim().to_lowercase());
        }
        
        // Validate we have exactly 12 words
        if words.len() != 12 {
            return Err(NetworkError::AuthenticationFailed(
                "Must provide exactly 12 words".to_string()
            ));
        }
        
        println!();
        println!("Entered code: {}", words.join(" ").bright_cyan());
        
        Ok([
            words[0].clone(),
            words[1].clone(),
            words[2].clone(),
            words[3].clone(),
            words[4].clone(),
            words[5].clone(),
            words[6].clone(),
            words[7].clone(),
            words[8].clone(),
            words[9].clone(),
            words[10].clone(),
            words[11].clone(),
        ])
    }
}

/// Simple pairing UI for daemon mode (auto-accepts valid requests)
pub struct SimplePairingUI {
    code_sender: Option<tokio::sync::oneshot::Sender<(String, u32)>>,
}

impl SimplePairingUI {
    pub fn new() -> Self {
        Self {
            code_sender: None,
        }
    }
    
    pub fn with_code_sender(mut self, sender: tokio::sync::oneshot::Sender<(String, u32)>) -> Self {
        self.code_sender = Some(sender);
        self
    }
}

#[async_trait]
impl PairingUserInterface for SimplePairingUI {
    async fn show_pairing_error(&self, error: &NetworkError) {
        tracing::error!("Pairing error: {}", error);
    }

    async fn show_pairing_code(&self, code: &str, expires_in_seconds: u32) {
        tracing::info!("Pairing code generated: {} (expires in {} seconds)", code, expires_in_seconds);
        
        // Send the code back to the waiting CLI
        if let Some(sender) = &self.code_sender {
            // We can't move out of self, so we'll log here and let the pairing method handle it differently
            // This is a limitation of the current UI interface design
        }
    }

    async fn prompt_pairing_code(&self) -> Result<[String; 12]> {
        // This should not be called in the CLI daemon context
        Err(NetworkError::AuthenticationFailed(
            "Interactive pairing code input not supported in daemon mode".to_string(),
        ))
    }

    async fn confirm_pairing(&self, remote_device: &DeviceInfo) -> Result<bool> {
        tracing::info!("Auto-accepting pairing with device: {}", remote_device.device_name);
        Ok(true)
    }

    async fn show_pairing_progress(&self, state: PairingState) {
        match state {
            PairingState::GeneratingCode => tracing::info!("Generating pairing code..."),
            PairingState::Broadcasting => tracing::info!("Broadcasting on DHT..."),
            PairingState::Scanning => tracing::info!("Scanning DHT for devices..."),
            PairingState::WaitingForConnection => tracing::info!("Waiting for connection..."),
            PairingState::Connecting => tracing::info!("Establishing connection..."),
            PairingState::Authenticating => tracing::info!("Authenticating..."),
            PairingState::ExchangingKeys => tracing::info!("Exchanging keys..."),
            PairingState::AwaitingConfirmation => tracing::info!("Awaiting confirmation..."),
            PairingState::EstablishingSession => tracing::info!("Establishing session..."),
            PairingState::ChallengeReceived { .. } => tracing::info!("Processing challenge..."),
            PairingState::ResponseSent => tracing::info!("Response sent..."),
            PairingState::Completed => tracing::info!("Pairing completed!"),
            PairingState::Failed { reason } => tracing::error!("Pairing failed: {}", reason),
            _ => {}
        }
    }
}

/// CLI-specific network logger that uses tracing
pub struct CliNetworkLogger;

#[async_trait]
impl crate::networking::NetworkLogger for CliNetworkLogger {
    async fn info(&self, message: &str) {
        tracing::info!("{}", message);
    }
    
    async fn error(&self, message: &str) {
        tracing::error!("{}", message);
    }
    
    async fn debug(&self, message: &str) {
        tracing::debug!("{}", message);
    }
    
    async fn warn(&self, message: &str) {
        tracing::warn!("{}", message);
    }
}```

## src/infrastructure/cli/adapters/copy.rs

```rust
//! CLI adapter for file copy operations

use crate::operations::files::copy::input::{CopyMethod, FileCopyInput};
use clap::{Parser, ValueEnum};
use std::path::PathBuf;

/// CLI-specific copy method values
#[derive(Debug, Clone, ValueEnum)]
pub enum CopyMethodCli {
    /// Automatically select the best method based on source and destination
    Auto,
    /// Use atomic move (rename) for same-volume operations
    AtomicMove,
    /// Use streaming copy for cross-volume operations
    Streaming,
}

impl Default for CopyMethodCli {
    fn default() -> Self {
        CopyMethodCli::Auto
    }
}

impl From<CopyMethodCli> for CopyMethod {
    fn from(cli_method: CopyMethodCli) -> Self {
        match cli_method {
            CopyMethodCli::Auto => CopyMethod::Auto,
            CopyMethodCli::AtomicMove => CopyMethod::AtomicMove,
            CopyMethodCli::Streaming => CopyMethod::StreamingCopy,
        }
    }
}

/// CLI-specific arguments for file copy command
/// This struct handles CLI parsing and converts to the core FileCopyInput type
#[derive(Debug, Clone, Parser)]
pub struct FileCopyCliArgs {
    /// Source files or directories to copy
    pub sources: Vec<PathBuf>,

    /// Destination path
    #[arg(short = 'o', long)]
    pub destination: PathBuf,

    /// Overwrite existing files
    #[arg(long)]
    pub overwrite: bool,

    /// Verify checksums during copy
    #[arg(long)]
    pub verify: bool,

    /// Preserve file timestamps (default: true)
    #[arg(long, default_value = "true")]
    pub preserve_timestamps: bool,

    /// Move files instead of copying (delete source after copy)
    #[arg(long)]
    pub move_files: bool,

    /// Copy method to use (auto, atomic-move, streaming)
    #[arg(long, value_enum, default_value = "auto")]
    pub method: CopyMethodCli,
}

impl From<FileCopyCliArgs> for FileCopyInput {
    fn from(args: FileCopyCliArgs) -> Self {
        Self {
            sources: args.sources,
            destination: args.destination,
            overwrite: args.overwrite,
            verify_checksum: args.verify,
            preserve_timestamps: args.preserve_timestamps,
            move_files: args.move_files,
            copy_method: args.method.into(),
        }
    }
}

impl FileCopyCliArgs {
    /// Convert to core input type
    pub fn to_input(self) -> FileCopyInput {
        self.into()
    }
    
    /// Validate CLI arguments and convert to input
    pub fn validate_and_convert(self) -> Result<FileCopyInput, String> {
        let input = self.to_input();
        
        match input.validate() {
            Ok(()) => Ok(input),
            Err(errors) => Err(errors.join("; ")),
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_cli_to_input_conversion() {
        let cli_args = FileCopyCliArgs {
            sources: vec!["/file1.txt".into(), "/file2.txt".into()],
            destination: "/dest/".into(),
            overwrite: true,
            verify: false,
            preserve_timestamps: true,
            move_files: false,
            method: CopyMethodCli::Auto,
        };

        let input: FileCopyInput = cli_args.into();

        assert_eq!(input.sources.len(), 2);
        assert_eq!(input.destination, PathBuf::from("/dest/"));
        assert!(input.overwrite);
        assert!(!input.verify_checksum);
        assert!(input.preserve_timestamps);
        assert!(!input.move_files);
        assert_eq!(input.copy_method, CopyMethod::Auto);
    }

    #[test]
    fn test_validate_and_convert_success() {
        let cli_args = FileCopyCliArgs {
            sources: vec!["/file.txt".into()],
            destination: "/dest/".into(),
            overwrite: false,
            verify: true,
            preserve_timestamps: false,
            move_files: true,
            method: CopyMethodCli::Streaming,
        };

        let result = cli_args.validate_and_convert();
        assert!(result.is_ok());

        let input = result.unwrap();
        assert_eq!(input.sources, vec![PathBuf::from("/file.txt")]);
        assert!(input.verify_checksum);
        assert!(!input.preserve_timestamps);
        assert!(input.move_files);
        assert_eq!(input.copy_method, CopyMethod::StreamingCopy);
    }

    #[test]
    fn test_validate_and_convert_failure() {
        let cli_args = FileCopyCliArgs {
            sources: vec![], // Empty sources should fail
            destination: "/dest/".into(),
            overwrite: false,
            verify: false,
            preserve_timestamps: true,
            move_files: false,
            method: CopyMethodCli::Auto,
        };

        let result = cli_args.validate_and_convert();
        assert!(result.is_err());
        assert!(result.unwrap_err().contains("At least one source"));
    }

    #[test]
    fn test_default_values() {
        // Test that clap default values work as expected
        let cli_args = FileCopyCliArgs {
            sources: vec!["/file.txt".into()],
            destination: "/dest/".into(),
            overwrite: false,
            verify: false,
            preserve_timestamps: true, // Should default to true
            move_files: false,
            method: CopyMethodCli::Auto, // Should default to Auto
        };

        let input = cli_args.to_input();
        assert!(input.preserve_timestamps); // Default should be true
        assert_eq!(input.copy_method, CopyMethod::Auto); // Default should be Auto
    }

    #[test]
    fn test_copy_method_conversion() {
        // Test all copy method variants
        assert_eq!(CopyMethod::from(CopyMethodCli::Auto), CopyMethod::Auto);
        assert_eq!(CopyMethod::from(CopyMethodCli::AtomicMove), CopyMethod::AtomicMove);
        assert_eq!(CopyMethod::from(CopyMethodCli::Streaming), CopyMethod::StreamingCopy);
    }
}```

## src/infrastructure/cli/adapters/mod.rs

```rust
//! CLI adapters for converting CLI arguments to domain input types

pub mod copy;

// Re-export for convenience
pub use copy::FileCopyCliArgs;```

## src/infrastructure/cli/mod.rs

```rust
pub mod adapters;
pub mod commands;
pub mod daemon;
pub mod output;
pub mod pairing_ui;
pub mod state;
pub mod utils;

use crate::infrastructure::cli::commands::{
	daemon::{handle_daemon_command, DaemonCommands},
	file::{handle_file_command, FileCommands},
	job::{handle_job_command, JobCommands},
	library::{handle_library_command, LibraryCommands},
	location::{handle_location_command, LocationCommands},
	network::{handle_network_command, NetworkCommands},
	system::{handle_system_command, SystemCommands},
	volume::{handle_volume_command, VolumeCommands},
};
use crate::infrastructure::cli::output::{CliOutput, Message};
use clap::{Parser, Subcommand};
use std::path::PathBuf;

#[derive(Parser)]
#[command(name = "spacedrive")]
#[command(about = "Spacedrive v2 CLI", long_about = None)]
pub struct Cli {
	/// Path to Spacedrive data directory
	#[arg(short, long, global = true)]
	pub data_dir: Option<PathBuf>,

	/// Enable debug logging (can be used multiple times for more verbosity)
	#[arg(short = 'v', long, global = true, action = clap::ArgAction::Count)]
	pub verbose: u8,

	/// Output format
	#[arg(short = 'f', long, global = true, value_enum, default_value = "human")]
	pub format: OutputFormatArg,

	/// Disable colors and emojis in output
	#[arg(long, global = true)]
	pub no_color: bool,

	/// Suppress all output except errors
	#[arg(short = 'q', long, global = true)]
	pub quiet: bool,

	/// Daemon instance name (for multiple daemon support)
	#[arg(long, global = true)]
	pub instance: Option<String>,

	#[command(subcommand)]
	pub command: Commands,
}

#[derive(Clone, Copy, Debug, PartialEq, Eq, clap::ValueEnum)]
pub enum OutputFormatArg {
	Human,
	Json,
}

#[derive(Subcommand)]
pub enum Commands {
	/// Start the Spacedrive daemon in the background
	Start {
		/// Run in foreground instead of daemonizing
		#[arg(long)]
		foreground: bool,
		/// Enable networking on startup
		#[arg(long)]
		enable_networking: bool,
	},

	/// Stop the Spacedrive daemon
	Stop {
		/// Remove all data (data directory) after stopping
		#[arg(long)]
		reset: bool,
	},

	/// Check if the daemon is running and show status
	Status,

	/// Daemon lifecycle management (advanced)
	#[command(subcommand)]
	Daemon(DaemonCommands),

	/// Library management
	#[command(subcommand)]
	Library(LibraryCommands),

	/// Location management
	#[command(subcommand)]
	Location(LocationCommands),

	/// Job management and monitoring
	#[command(subcommand)]
	Job(JobCommands),

	/// Network operations and device management
	#[command(subcommand)]
	Network(NetworkCommands),

	/// File operations
	#[command(subcommand)]
	File(FileCommands),

	/// System monitoring and information
	#[command(subcommand)]
	System(SystemCommands),

	/// Volume management and operations
	#[command(subcommand)]
	Volume(VolumeCommands),
}

pub async fn run() -> Result<(), Box<dyn std::error::Error>> {
	let cli = Cli::parse();

	// Set up logging - skip for daemon start commands as they handle their own logging
	let is_daemon_start = matches!(
		&cli.command,
		Commands::Start { .. } | Commands::Daemon(DaemonCommands::Start { .. })
	);
	if !is_daemon_start {
		let log_level = match cli.verbose {
			0 => "info",
			1 => "debug",
			_ => "trace",
		};
		let env_filter =
			tracing_subscriber::EnvFilter::try_from_default_env().unwrap_or_else(|_| {
				// Fallback to hardcoded filters if RUST_LOG not set
				if cli.verbose > 0 {
					// Enable detailed networking and libp2p logging when verbose
					tracing_subscriber::EnvFilter::new(&format!(
						"sd_core_new={},spacedrive_cli={},libp2p=debug",
						log_level, log_level
					))
				} else {
					tracing_subscriber::EnvFilter::new(&format!(
						"sd_core_new={},spacedrive_cli={}",
						log_level, log_level
					))
				}
			});

		tracing_subscriber::fmt().with_env_filter(env_filter).init();
	}

	// Set up output context
	use output::{CliOutput, ColorMode, OutputFormat, VerbosityLevel};

	let output_format = if cli.quiet {
		OutputFormat::Quiet
	} else {
		match cli.format {
			OutputFormatArg::Human => OutputFormat::Human,
			OutputFormatArg::Json => OutputFormat::Json,
		}
	};

	let color_mode = if cli.no_color {
		ColorMode::Never
	} else {
		ColorMode::Auto
	};

	let verbosity = VerbosityLevel::from_occurrences(cli.verbose);

	let mut output = CliOutput::with_options(output_format, verbosity, color_mode);

	// Determine data directory with instance isolation
	let base_data_dir = cli
		.data_dir
		.unwrap_or_else(|| PathBuf::from("./data/spacedrive-cli-data"));

	let data_dir = if let Some(ref instance) = cli.instance {
		base_data_dir.join(format!("instance-{}", instance))
	} else {
		base_data_dir
	};

	// Route to appropriate domain handler
	match &cli.command {
		Commands::Start {
			foreground,
			enable_networking,
		} => {
			// Handle start command
			handle_daemon_command(
				DaemonCommands::Start {
					foreground: *foreground,
					enable_networking: *enable_networking,
				},
				data_dir,
				cli.instance.clone(),
				output,
			)
			.await
		}
		Commands::Stop { reset } => {
			// Handle stop command
			handle_daemon_command(
				DaemonCommands::Stop { reset: *reset },
				data_dir,
				cli.instance.clone(),
				output,
			)
			.await
		}
		Commands::Status => {
			// Handle status command
			handle_daemon_command(
				DaemonCommands::Status,
				data_dir,
				cli.instance.clone(),
				output,
			)
			.await
		}
		Commands::Daemon(daemon_cmd) => {
			// Daemon commands don't need daemon to be running
			handle_daemon_command(daemon_cmd.clone(), data_dir, cli.instance.clone(), output).await
		}
		Commands::Library(library_cmd) => {
			// Check if daemon is running
			if !daemon::Daemon::is_running_instance(cli.instance.clone()) {
				print_daemon_not_running(&cli.instance, &mut output)?;
				return Ok(());
			}
			handle_library_command(library_cmd.clone(), cli.instance.clone(), output).await
		}
		Commands::Location(location_cmd) => {
			// Check if daemon is running
			if !daemon::Daemon::is_running_instance(cli.instance.clone()) {
				print_daemon_not_running(&cli.instance, &mut output)?;
				return Ok(());
			}
			handle_location_command(location_cmd.clone(), cli.instance.clone(), output).await
		}
		Commands::Job(job_cmd) => {
			// Check if daemon is running
			if !daemon::Daemon::is_running_instance(cli.instance.clone()) {
				print_daemon_not_running(&cli.instance, &mut output)?;
				return Ok(());
			}
			handle_job_command(job_cmd.clone(), cli.instance.clone(), output).await
		}
		Commands::Network(network_cmd) => {
			// Check if daemon is running (except for init)
			if !matches!(network_cmd, NetworkCommands::Init) {
				if !daemon::Daemon::is_running_instance(cli.instance.clone()) {
					print_daemon_not_running(&cli.instance, &mut output)?;
					return Ok(());
				}
			}
			handle_network_command(network_cmd.clone(), cli.instance.clone(), output).await
		}
		Commands::File(file_cmd) => {
			// Check if daemon is running
			if !daemon::Daemon::is_running_instance(cli.instance.clone()) {
				print_daemon_not_running(&cli.instance, &mut output)?;
				return Ok(());
			}
			handle_file_command(file_cmd.clone(), cli.instance.clone(), output).await
		}
		Commands::System(system_cmd) => {
			// System commands may or may not need daemon depending on the command
			match system_cmd {
				SystemCommands::Logs { .. } => {
					// Logs command doesn't need daemon to be running
					handle_system_command(system_cmd.clone(), cli.instance.clone(), output).await
				}
				_ => {
					// Other system commands need daemon
					if !daemon::Daemon::is_running_instance(cli.instance.clone()) {
						print_daemon_not_running(&cli.instance, &mut output)?;
						return Ok(());
					}
					handle_system_command(system_cmd.clone(), cli.instance.clone(), output).await
				}
			}
		}
		Commands::Volume(volume_cmd) => {
			// Check if daemon is running
			if !daemon::Daemon::is_running_instance(cli.instance.clone()) {
				print_daemon_not_running(&cli.instance, &mut output)?;
				return Ok(());
			}
			handle_volume_command(volume_cmd.clone(), cli.instance.clone(), output).await
		}
	}
}

fn print_daemon_not_running(
	instance_name: &Option<String>,
	output: &mut CliOutput,
) -> Result<(), Box<dyn std::error::Error>> {
	let instance_display = instance_name.as_deref().unwrap_or("default");
	output.error(Message::DaemonNotRunning {
		instance: instance_display.to_string(),
	})?;

	let start_cmd = if instance_name.is_some() {
		format!("spacedrive --instance {} start", instance_display)
	} else {
		"spacedrive start".to_string()
	};

	output
		.section()
		.help()
		.item(&format!("Start it with: {}", start_cmd))
		.render()?;

	Ok(())
}
```

## src/infrastructure/cli/daemon/types/responses.rs

```rust
//! Response types returned by the daemon

use serde::{Deserialize, Serialize};
use std::path::PathBuf;
use uuid::Uuid;

use super::common::{
	BrowseEntry, ConnectedDeviceInfo, JobInfo, LibraryInfo, LocationInfo, PairingRequestInfo,
	VolumeListItem,
};
use crate::{infrastructure::actions::output::ActionOutput, volume::Volume};

/// Responses from the daemon
#[derive(Debug, Serialize, Deserialize)]
pub enum DaemonResponse {
	Ok,
	Error(String),
	Pong,
	Status(DaemonStatus),
	LibraryCreated {
		id: Uuid,
		name: String,
		path: PathBuf,
	},
	Libraries(Vec<LibraryInfo>),
	CurrentLibrary(Option<LibraryInfo>),
	LocationAdded {
		location_id: Uuid,
		job_id: String,
	},
	LocationIndexed {
		location_id: Uuid,
	},
	Locations(Vec<LocationInfo>),
	BrowseResults {
		path: PathBuf,
		entries: Vec<BrowseEntry>,
		total_files: usize,
		total_dirs: usize,
	},
	Jobs(Vec<JobInfo>),
	JobInfo(Option<JobInfo>),
	CopyStarted {
		job_id: Uuid,
		sources_count: usize,
	},
	Event(String), // Serialized event

	// Networking responses
	ConnectedDevices(Vec<ConnectedDeviceInfo>),
	SpacedropStarted {
		transfer_id: Uuid,
	},

	// Pairing responses
	PairingCodeGenerated {
		code: String,
		expires_in_seconds: u32,
	},
	PairingInProgress,
	PairingStatus {
		status: String,
		remote_device: Option<ConnectedDeviceInfo>,
	},
	PendingPairings(Vec<PairingRequestInfo>),

	// Volume responses
	VolumeList(Vec<Volume>),
	VolumeListWithTracking(Vec<VolumeListItem>),
	Volume(Volume),

	// Action output (generic for all action results)
	ActionOutput(ActionOutput),
}

#[derive(Debug, Serialize, Deserialize)]
pub struct DaemonStatus {
	pub version: String,
	pub uptime_secs: u64,
	pub current_library: Option<Uuid>,
	pub active_jobs: usize,
	pub total_locations: usize,
}
```

## src/infrastructure/cli/daemon/types/commands.rs

```rust
//! Command types that can be sent to the daemon

use serde::{Deserialize, Serialize};
use std::path::PathBuf;
use uuid::Uuid;
use crate::infrastructure::cli::commands::{
    LibraryCommands, LocationCommands, JobCommands, FileCommands, 
    NetworkCommands, SystemCommands, VolumeCommands
};

/// Commands that can be sent to the daemon
#[derive(Debug, Serialize, Deserialize)]
pub enum DaemonCommand {
	// Core management
	Ping,
	Shutdown,
	GetStatus,

	// Library commands
	CreateLibrary {
		name: String,
		path: Option<PathBuf>,
	},
	ListLibraries,
	SwitchLibrary {
		id: Uuid,
	},
	GetCurrentLibrary,

	// Location commands
	AddLocation {
		path: PathBuf,
		name: Option<String>,
	},
	ListLocations,
	RescanLocation {
		id: Uuid,
	},
	RemoveLocation {
		id: Uuid,
	},

	// Job commands
	ListJobs {
		status: Option<String>,
	},
	GetJobInfo {
		id: Uuid,
	},
	PauseJob {
		id: Uuid,
	},
	ResumeJob {
		id: Uuid,
	},
	CancelJob {
		id: Uuid,
	},

	// File operations
	Copy {
		sources: Vec<PathBuf>,
		destination: PathBuf,
		overwrite: bool,
		verify: bool,
		preserve_timestamps: bool,
		move_files: bool,
	},

	// Indexing operations
	Browse {
		path: PathBuf,
		scope: String,
		content: bool,
	},
	IndexAll {
		force: bool,
	},
	IndexLocation {
		location: String,
		force: bool,
	},

	// Subscribe to events
	SubscribeEvents,

	// Networking commands
	InitNetworking,
	StartNetworking,
	StopNetworking,
	ListConnectedDevices,
	RevokeDevice {
		device_id: Uuid,
	},
	SendSpacedrop {
		device_id: Uuid,
		file_path: String,
		sender_name: String,
		message: Option<String>,
	},

	// Pairing commands
	StartPairingAsInitiator,
	StartPairingAsJoiner {
		code: String,
	},
	GetPairingStatus,
	ListPendingPairings,
	AcceptPairing {
		request_id: Uuid,
	},
	RejectPairing {
		request_id: Uuid,
	},

	// Volume commands
	Volume(VolumeCommands),
}```

## src/infrastructure/cli/daemon/types/mod.rs

```rust
//! Type definitions for the daemon

pub mod commands;
pub mod common;
pub mod responses;

pub use commands::DaemonCommand;
pub use common::{
	ConnectedDeviceInfo, DaemonInstance, JobInfo, LibraryInfo, LocationInfo, PairingRequestInfo,
	VolumeListItem,
};
pub use responses::{DaemonResponse, DaemonStatus};
```

## src/infrastructure/cli/daemon/types/common.rs

```rust
//! Common types shared between commands and responses

use crate::volume::{Volume, VolumeFingerprint};
use serde::{Deserialize, Serialize};
use std::path::PathBuf;
use uuid::Uuid;

#[derive(Debug, Serialize, Deserialize)]
pub struct LibraryInfo {
	pub id: Uuid,
	pub name: String,
	pub path: PathBuf,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct LocationInfo {
	pub id: Uuid,
	pub name: String,
	pub path: PathBuf,
	pub status: String,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct JobInfo {
	pub id: Uuid,
	pub name: String,
	pub status: String,
	pub progress: f32,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ConnectedDeviceInfo {
	pub device_id: Uuid,
	pub device_name: String,
	pub device_type: String,
	pub os_version: String,
	pub app_version: String,
	pub peer_id: String,
	pub status: String,
	pub connection_active: bool,
	pub last_seen: String,
	pub connected_at: Option<String>,
	pub bytes_sent: u64,
	pub bytes_received: u64,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct PairingRequestInfo {
	pub request_id: Uuid,
	pub device_id: Uuid,
	pub device_name: String,
	pub received_at: String,
}

/// Daemon instance information
#[derive(Debug)]
pub struct DaemonInstance {
	pub name: Option<String>, // None for default instance
	pub socket_path: PathBuf,
	pub is_running: bool,
}

impl DaemonInstance {
	/// Get instance display name ("default" for None, or the actual name)
	pub fn display_name(&self) -> &str {
		self.name.as_deref().unwrap_or("default")
	}
}

#[derive(Debug, Serialize, Deserialize)]
pub struct BrowseEntry {
	pub name: String,
	pub path: std::path::PathBuf,
	pub is_dir: bool,
	pub size: Option<u64>,
	pub modified: Option<String>,
	pub file_type: Option<String>,
}

/// Represents a volume in the list with tracking information
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct VolumeListItem {
	/// The volume data (either from live detection or reconstructed from tracking info)
	pub volume: Volume,
	/// Whether this volume is tracked in the current library
	pub is_tracked: bool,
	/// Custom name assigned when tracking (if any)
	pub tracked_name: Option<String>,
	/// Whether the volume is currently online/connected
	pub is_online: bool,
	/// When the volume was last seen (for offline volumes)
	pub last_seen_at: Option<chrono::DateTime<chrono::Utc>>,
}
```

## src/infrastructure/cli/daemon/client.rs

```rust
//! Client for communicating with the daemon

use std::path::PathBuf;
use tokio::io::{AsyncBufReadExt, AsyncWriteExt, BufReader};
use tokio::net::UnixStream;

use super::config::DaemonConfig;
use super::types::{DaemonCommand, DaemonResponse};

/// Client for communicating with the daemon
pub struct DaemonClient {
	socket_path: PathBuf,
	instance_name: Option<String>,
}

impl DaemonClient {
	pub fn new() -> Self {
		Self::new_with_instance(None)
	}

	pub fn new_with_instance(instance_name: Option<String>) -> Self {
		let config = DaemonConfig::new(instance_name.clone());
		Self {
			socket_path: config.socket_path,
			instance_name,
		}
	}

	/// Send a command to the daemon
	pub async fn send_command(
		&self,
		cmd: DaemonCommand,
	) -> Result<DaemonResponse, Box<dyn std::error::Error>> {
		let mut stream = UnixStream::connect(&self.socket_path).await?;

		// Send command
		let json = serde_json::to_string(&cmd)?;
		stream.write_all(format!("{}\n", json).as_bytes()).await?;

		// Read response
		let mut reader = BufReader::new(stream);
		let mut line = String::new();
		reader.read_line(&mut line).await?;

		let response: DaemonResponse = serde_json::from_str(line.trim())?;
		Ok(response)
	}

	/// Check if daemon is running
	pub fn is_running(&self) -> bool {
		super::Daemon::is_running_instance(self.instance_name.clone())
	}
}```

## src/infrastructure/cli/daemon/config.rs

```rust
//! Daemon configuration

use std::path::PathBuf;

/// Daemon configuration
pub struct DaemonConfig {
	pub socket_path: PathBuf,
	pub pid_file: PathBuf,
	pub log_file: Option<PathBuf>,
	pub instance_name: Option<String>,
}

impl Default for DaemonConfig {
	fn default() -> Self {
		Self::new(None)
	}
}

impl DaemonConfig {
	/// Create a new daemon config with optional instance name
	pub fn new(instance_name: Option<String>) -> Self {
		let runtime_dir = dirs::runtime_dir()
			.or_else(|| dirs::cache_dir())
			.unwrap_or_else(|| PathBuf::from("/tmp"));

		let (socket_name, pid_name, log_name) = if let Some(ref name) = instance_name {
			(
				format!("spacedrive-{}.sock", name),
				format!("spacedrive-{}.pid", name),
				format!("spacedrive-{}.log", name),
			)
		} else {
			(
				"spacedrive.sock".to_string(),
				"spacedrive.pid".to_string(),
				"spacedrive.log".to_string(),
			)
		};

		Self {
			socket_path: runtime_dir.join(socket_name),
			pid_file: runtime_dir.join(pid_name),
			log_file: Some(runtime_dir.join(log_name)),
			instance_name,
		}
	}

	/// Get instance display name ("default" for None, or the actual name)
	pub fn instance_display_name(&self) -> &str {
		self.instance_name.as_deref().unwrap_or("default")
	}
}```

## src/infrastructure/cli/daemon/mod.rs

```rust
//! Spacedrive daemon implementation
//!
//! The daemon runs in the background and handles all core operations.
//! The CLI communicates with it via Unix domain socket (on Unix) or named pipe (on Windows).

use crate::{infrastructure::cli::state::CliState, Core};
use std::path::PathBuf;
use std::sync::Arc;
use tokio::io::{AsyncBufReadExt, AsyncWriteExt, BufReader};
use tokio::net::{UnixListener, UnixStream};
use tokio::sync::{oneshot, RwLock};
use tracing::{error, info, warn};

pub mod client;
pub mod config;
pub mod handlers;
pub mod services;
pub mod types;

pub use client::DaemonClient;
pub use config::DaemonConfig;
pub use types::*;

use services::{DaemonHelpers, StateService};

/// The daemon server
pub struct Daemon {
	core: Arc<Core>,
	config: DaemonConfig,
	start_time: std::time::Instant,
	shutdown_tx: Arc<tokio::sync::Mutex<Option<oneshot::Sender<()>>>>,
	cli_state: Arc<RwLock<CliState>>,
	data_dir: PathBuf,
	handler_registry: handlers::HandlerRegistry,
	state_service: Arc<StateService>,
}

impl Daemon {
	/// Create a new daemon instance
	pub async fn new(data_dir: PathBuf) -> Result<Self, Box<dyn std::error::Error>> {
		Self::new_with_instance(data_dir, None).await
	}

	/// Create a new daemon instance with optional instance name
	pub async fn new_with_instance(
		data_dir: PathBuf,
		instance_name: Option<String>,
	) -> Result<Self, Box<dyn std::error::Error>> {
		// Set up logging BEFORE initializing Core
		let config = DaemonConfig::new(instance_name.clone());
		if let Some(ref log_file) = config.log_file {
			DaemonHelpers::setup_file_logging(log_file)?;
		}

		let core = Arc::new(Core::new_with_config(data_dir.clone()).await?);

		// Load CLI state
		let cli_state = CliState::load(&data_dir).unwrap_or_default();
		let cli_state = Arc::new(RwLock::new(cli_state));

		// Create state service
		let state_service = Arc::new(StateService::new(cli_state.clone(), data_dir.clone()));

		// Auto-select first library if needed
		state_service.auto_select_library(&core).await?;

		// Create handler registry
		let shutdown_tx = Arc::new(tokio::sync::Mutex::new(None));
		let handler_registry = handlers::HandlerRegistry::new(
			std::time::Instant::now(),
			shutdown_tx.clone(),
			data_dir.clone(),
		);

		Ok(Self {
			core,
			config,
			start_time: std::time::Instant::now(),
			shutdown_tx,
			cli_state,
			data_dir,
			handler_registry,
			state_service,
		})
	}

	/// Create a new daemon instance with networking enabled
	pub async fn new_with_networking(
		data_dir: PathBuf,
	) -> Result<Self, Box<dyn std::error::Error>> {
		Self::new_with_networking_and_instance(data_dir, None).await
	}

	/// Create a new daemon instance with networking enabled and optional instance name
	pub async fn new_with_networking_and_instance(
		data_dir: PathBuf,
		instance_name: Option<String>,
	) -> Result<Self, Box<dyn std::error::Error>> {
		// Set up logging BEFORE initializing Core
		let config = DaemonConfig::new(instance_name.clone());
		if let Some(ref log_file) = config.log_file {
			DaemonHelpers::setup_file_logging(log_file)?;
		}

		let mut core = Core::new_with_config(data_dir.clone()).await?;

		// Initialize networking
		core.init_networking().await?;
		core.start_networking().await?;

		let core = Arc::new(core);

		// Load CLI state
		let cli_state = CliState::load(&data_dir).unwrap_or_default();
		let cli_state = Arc::new(RwLock::new(cli_state));

		// Create state service
		let state_service = Arc::new(StateService::new(cli_state.clone(), data_dir.clone()));

		// Auto-select first library if needed
		state_service.auto_select_library(&core).await?;

		// Create handler registry
		let shutdown_tx = Arc::new(tokio::sync::Mutex::new(None));
		let handler_registry = handlers::HandlerRegistry::new(
			std::time::Instant::now(),
			shutdown_tx.clone(),
			data_dir.clone(),
		);

		Ok(Self {
			core,
			config,
			start_time: std::time::Instant::now(),
			shutdown_tx,
			cli_state,
			data_dir,
			handler_registry,
			state_service,
		})
	}

	/// Start the daemon server
	pub async fn start(self) -> Result<(), Box<dyn std::error::Error>> {
		// Logging is already set up in the constructor

		// Remove old socket if it exists
		if self.config.socket_path.exists() {
			std::fs::remove_file(&self.config.socket_path)?;
		}

		// Write PID file
		std::fs::write(&self.config.pid_file, std::process::id().to_string())?;

		// Create Unix socket
		let listener = UnixListener::bind(&self.config.socket_path)?;
		info!("Daemon listening on {:?}", self.config.socket_path);

		// Emit CoreStarted event to signal daemon is ready
		self.core
			.events
			.emit(crate::infrastructure::events::Event::CoreStarted);

		// Set up shutdown channel
		let (shutdown_tx, mut shutdown_rx) = oneshot::channel();
		*self.shutdown_tx.lock().await = Some(shutdown_tx);

		// Accept connections
		loop {
			tokio::select! {
				Ok((stream, _)) = listener.accept() => {
					let core = self.core.clone();
					let handler_registry = &self.handler_registry;
					let state_service = self.state_service.clone();

					// Handle client directly without spawning background task
					if let Err(e) = handle_client(stream, core, handler_registry, state_service).await {
						error!("Error handling client: {}", e);
					}
				}
				_ = &mut shutdown_rx => {
					info!("Daemon shutting down");
					break;
				}
			}
		}

		// Cleanup
		std::fs::remove_file(&self.config.socket_path).ok();
		std::fs::remove_file(&self.config.pid_file).ok();

		Ok(())
	}

	/// Check if daemon is running
	pub fn is_running() -> bool {
		Self::is_running_instance(None)
	}

	/// Check if daemon instance is running
	pub fn is_running_instance(instance_name: Option<String>) -> bool {
		let config = DaemonConfig::new(instance_name);

		if let Ok(pid_str) = std::fs::read_to_string(&config.pid_file) {
			if let Ok(pid) = pid_str.trim().parse::<u32>() {
				// Check if process is running (Unix only)
				#[cfg(unix)]
				{
					use std::process::Command;
					let output = Command::new("kill")
						.args(&["-0", &pid.to_string()])
						.output();

					if let Ok(output) = output {
						return output.status.success();
					}
				}
			}
		}

		false
	}

	/// Stop a running daemon
	pub async fn stop() -> Result<(), Box<dyn std::error::Error>> {
		Self::stop_instance(None).await
	}

	/// Stop a running daemon instance
	pub async fn stop_instance(
		instance_name: Option<String>,
	) -> Result<(), Box<dyn std::error::Error>> {
		let config = DaemonConfig::new(instance_name.clone());

		// First check if daemon is actually running
		if !Self::is_running_instance(instance_name) {
			return Err(format!(
				"Daemon instance '{}' is not running",
				config.instance_display_name()
			)
			.into());
		}

		// Try to connect and send shutdown command
		match UnixStream::connect(&config.socket_path).await {
			Ok(mut stream) => {
				let cmd = DaemonCommand::Shutdown;
				let json = serde_json::to_string(&cmd)?;
				stream.write_all(format!("{}\n", json).as_bytes()).await?;
				stream.flush().await?;

				// Wait a bit for graceful shutdown
				tokio::time::sleep(std::time::Duration::from_millis(500)).await;
			}
			Err(_) => {
				// If we can't connect to socket, try to kill the process
				if let Ok(pid_str) = std::fs::read_to_string(&config.pid_file) {
					if let Ok(pid) = pid_str.trim().parse::<u32>() {
						#[cfg(unix)]
						{
							use std::process::Command;
							Command::new("kill")
								.args(&["-TERM", &pid.to_string()])
								.output()?;
						}
					}
				}
			}
		}

		// Clean up files
		std::fs::remove_file(&config.socket_path).ok();
		std::fs::remove_file(&config.pid_file).ok();

		Ok(())
	}

	/// Wait for daemon to be ready by attempting to connect
	pub async fn wait_for_ready(
		instance_name: Option<String>,
		timeout_secs: u64,
	) -> Result<bool, Box<dyn std::error::Error>> {
		let config = DaemonConfig::new(instance_name);
		let start = std::time::Instant::now();
		let timeout = std::time::Duration::from_secs(timeout_secs);

		while start.elapsed() < timeout {
			// Try to connect to the socket
			if let Ok(mut stream) = UnixStream::connect(&config.socket_path).await {
				// Try to send a simple ping command to verify it's responsive
				let cmd = DaemonCommand::Ping;
				let json = serde_json::to_string(&cmd)?;

				if stream
					.write_all(format!("{}\n", json).as_bytes())
					.await
					.is_ok()
				{
					if stream.flush().await.is_ok() {
						// Try to read the response
						let mut reader = BufReader::new(stream);
						let mut line = String::new();
						if reader.read_line(&mut line).await.is_ok() {
							if let Ok(response) = serde_json::from_str::<DaemonResponse>(&line) {
								if matches!(response, DaemonResponse::Pong) {
									return Ok(true);
								}
							}
						}
					}
				}
			}

			// Wait a bit before retrying
			tokio::time::sleep(std::time::Duration::from_millis(100)).await;
		}

		Ok(false)
	}

	/// List all daemon instances
	pub fn list_instances() -> Result<Vec<DaemonInstance>, Box<dyn std::error::Error>> {
		let runtime_dir = dirs::runtime_dir()
			.or_else(|| dirs::cache_dir())
			.unwrap_or_else(|| PathBuf::from("/tmp"));

		let mut instances = Vec::new();

		// Find all spacedrive-*.sock files
		if let Ok(entries) = std::fs::read_dir(&runtime_dir) {
			for entry in entries.flatten() {
				let file_name = entry.file_name();
				let file_str = file_name.to_string_lossy();

				if file_str.starts_with("spacedrive") && file_str.ends_with(".sock") {
					let instance_name = if file_str == "spacedrive.sock" {
						None // Default instance
					} else {
						// Extract instance name from spacedrive-{name}.sock
						Some(
							file_str
								.strip_prefix("spacedrive-")
								.and_then(|s| s.strip_suffix(".sock"))
								.unwrap_or("unknown")
								.to_string(),
						)
					};

					let is_running = Self::is_running_instance(instance_name.clone());
					instances.push(DaemonInstance {
						name: instance_name,
						socket_path: entry.path(),
						is_running,
					});
				}
			}
		}

		// Sort by name for consistent output
		instances.sort_by(|a, b| {
			match (&a.name, &b.name) {
				(None, None) => std::cmp::Ordering::Equal,
				(None, Some(_)) => std::cmp::Ordering::Less, // Default first
				(Some(_), None) => std::cmp::Ordering::Greater,
				(Some(a), Some(b)) => a.cmp(b),
			}
		});

		Ok(instances)
	}
}

/// Handle a client connection
async fn handle_client(
	stream: UnixStream,
	core: Arc<Core>,
	handler_registry: &handlers::HandlerRegistry,
	state_service: Arc<StateService>,
) -> Result<(), Box<dyn std::error::Error>> {
	let (reader, mut writer) = stream.into_split();
	let mut reader = BufReader::new(reader);
	let mut line = String::new();

	while reader.read_line(&mut line).await? > 0 {
		let trimmed = line.trim();
		if trimmed.is_empty() {
			line.clear();
			continue;
		}

		let response = match serde_json::from_str::<DaemonCommand>(trimmed) {
			Ok(cmd) => {
				// info!("Handling daemon command: {:?}", cmd);
				handler_registry.handle(cmd, &core, &state_service).await
			}
			Err(e) => DaemonResponse::Error(format!("Invalid command: {}", e)),
		};

		let json = serde_json::to_string(&response)?;
		writer.write_all(format!("{}\n", json).as_bytes()).await?;

		line.clear();
	}

	Ok(())
}
```

## src/infrastructure/cli/daemon/handlers/core.rs

```rust
//! Core command handlers (ping, shutdown, status)

use async_trait::async_trait;
use std::sync::Arc;
use tokio::sync::oneshot;
use tracing::error;

use crate::Core;
use crate::infrastructure::cli::daemon::services::StateService;
use crate::infrastructure::cli::daemon::types::{DaemonCommand, DaemonResponse, DaemonStatus};

use super::CommandHandler;

/// Handler for core daemon commands
pub struct CoreHandler {
	start_time: std::time::Instant,
	shutdown_tx: Arc<tokio::sync::Mutex<Option<oneshot::Sender<()>>>>,
}

impl CoreHandler {
	pub fn new(
		start_time: std::time::Instant,
		shutdown_tx: Arc<tokio::sync::Mutex<Option<oneshot::Sender<()>>>>,
	) -> Self {
		Self {
			start_time,
			shutdown_tx,
		}
	}
}

#[async_trait]
impl CommandHandler for CoreHandler {
	async fn handle(
		&self,
		cmd: DaemonCommand,
		core: &Arc<Core>,
		state_service: &Arc<StateService>,
	) -> DaemonResponse {
		match cmd {
			DaemonCommand::Ping => DaemonResponse::Pong,

			DaemonCommand::Shutdown => {
				// Gracefully shutdown core (this will close all libraries and cleanup locks)
				if let Err(e) = core.shutdown().await {
					error!("Error during core shutdown: {}", e);
				}

				// Trigger daemon shutdown
				let mut shutdown_guard = self.shutdown_tx.lock().await;
				if let Some(tx) = shutdown_guard.take() {
					let _ = tx.send(());
				}

				DaemonResponse::Ok
			}

			DaemonCommand::GetStatus => {
				let current_library = state_service.get_current_library_id().await;

				// TODO: Get actual job and location counts
				DaemonResponse::Status(DaemonStatus {
					version: env!("CARGO_PKG_VERSION").to_string(),
					uptime_secs: self.start_time.elapsed().as_secs(),
					current_library,
					active_jobs: 0,     // TODO: Get from job manager
					total_locations: 0, // TODO: Get from location manager
				})
			}

			_ => DaemonResponse::Error("Invalid command for core handler".to_string()),
		}
	}

	fn can_handle(&self, cmd: &DaemonCommand) -> bool {
		matches!(
			cmd,
			DaemonCommand::Ping | DaemonCommand::Shutdown | DaemonCommand::GetStatus
		)
	}
}```

## src/infrastructure/cli/daemon/handlers/job.rs

```rust
//! Job command handlers

use async_trait::async_trait;
use std::sync::Arc;
use uuid::Uuid;

use crate::infrastructure::cli::daemon::services::StateService;
use crate::infrastructure::cli::daemon::types::{DaemonCommand, DaemonResponse, JobInfo};
use crate::Core;

use super::CommandHandler;

/// Handler for job commands
pub struct JobHandler;

#[async_trait]
impl CommandHandler for JobHandler {
	async fn handle(
		&self,
		cmd: DaemonCommand,
		core: &Arc<Core>,
		state_service: &Arc<StateService>,
	) -> DaemonResponse {
		match cmd {
			DaemonCommand::ListJobs { status } => {
				// Get current library from CLI state
				if let Some(library) = state_service.get_current_library(core).await {
					let job_manager = library.jobs();

					// For running jobs, get from memory for live monitoring
					if let Some(ref status_str) = status {
						if status_str == "running" {
							let running_jobs = job_manager.list_running_jobs().await;
							let infos: Vec<JobInfo> = running_jobs
								.into_iter()
								.map(|j| JobInfo {
									id: j.id,
									name: j.name,
									status: j.status.to_string(),
									progress: j.progress,
								})
								.collect();

							return DaemonResponse::Jobs(infos);
						}
					}

					// For other statuses, query the database
					let status_filter = status.and_then(|s| {
						s.parse::<crate::infrastructure::jobs::types::JobStatus>()
							.ok()
					});

					match job_manager.list_jobs(status_filter).await {
						Ok(jobs) => {
							let infos: Vec<JobInfo> = jobs
								.into_iter()
								.map(|j| JobInfo {
									id: j.id,
									name: j.name,
									status: j.status.to_string(),
									progress: j.progress,
								})
								.collect();

							DaemonResponse::Jobs(infos)
						}
						Err(e) => DaemonResponse::Error(e.to_string()),
					}
				} else {
					DaemonResponse::Error("No library selected".to_string())
				}
			}

			DaemonCommand::GetJobInfo { id } => {
				// Get current library from CLI state
				if let Some(library) = state_service.get_current_library(core).await {
					let job_manager = library.jobs();

					match job_manager.get_job_info(id).await {
						Ok(job) => DaemonResponse::JobInfo(job.map(|j| JobInfo {
							id: j.id,
							name: j.name,
							status: j.status.to_string(),
							progress: j.progress,
						})),
						Err(e) => DaemonResponse::Error(e.to_string()),
					}
				} else {
					DaemonResponse::Error("No library selected".to_string())
				}
			}

			DaemonCommand::PauseJob { id } => {
				// Get current library from CLI state
				if let Some(library) = state_service.get_current_library(core).await {
					let job_manager = library.jobs();
					let job_id = crate::infrastructure::jobs::types::JobId(id);
					
					match job_manager.pause_job(job_id).await {
						Ok(_) => DaemonResponse::Ok,
						Err(e) => DaemonResponse::Error(e.to_string()),
					}
				} else {
					DaemonResponse::Error("No library selected".to_string())
				}
			}

			DaemonCommand::ResumeJob { id } => {
				// Get current library from CLI state
				if let Some(library) = state_service.get_current_library(core).await {
					let job_manager = library.jobs();
					let job_id = crate::infrastructure::jobs::types::JobId(id);
					
					match job_manager.resume_job(job_id).await {
						Ok(_) => DaemonResponse::Ok,
						Err(e) => DaemonResponse::Error(e.to_string()),
					}
				} else {
					DaemonResponse::Error("No library selected".to_string())
				}
			}

			DaemonCommand::CancelJob { id } => {
				// TODO: Implement job cancel when job manager supports it
				DaemonResponse::Error("Job cancel not yet implemented".to_string())
			}

			_ => DaemonResponse::Error("Invalid command for job handler".to_string()),
		}
	}

	fn can_handle(&self, cmd: &DaemonCommand) -> bool {
		matches!(
			cmd,
			DaemonCommand::ListJobs { .. }
				| DaemonCommand::GetJobInfo { .. }
				| DaemonCommand::PauseJob { .. }
				| DaemonCommand::ResumeJob { .. }
				| DaemonCommand::CancelJob { .. }
		)
	}
}
```

## src/infrastructure/cli/daemon/handlers/library.rs

```rust
//! Library command handlers

use async_trait::async_trait;
use std::sync::Arc;
use tracing::warn;
use uuid::Uuid;

use crate::Core;
use crate::infrastructure::cli::daemon::services::StateService;
use crate::infrastructure::cli::daemon::types::{
	DaemonCommand, DaemonResponse, LibraryInfo,
};

use super::CommandHandler;

/// Handler for library commands
pub struct LibraryHandler;

#[async_trait]
impl CommandHandler for LibraryHandler {
	async fn handle(
		&self,
		cmd: DaemonCommand,
		core: &Arc<Core>,
		state_service: &Arc<StateService>,
	) -> DaemonResponse {
		match cmd {
			DaemonCommand::CreateLibrary { name, path } => {
				match core
					.libraries
					.create_library(&name, path, core.context.clone())
					.await
				{
					Ok(library) => DaemonResponse::LibraryCreated {
						id: library.id(),
						name: library.name().await,
						path: library.path().to_path_buf(),
					},
					Err(e) => DaemonResponse::Error(e.to_string()),
				}
			}

			DaemonCommand::ListLibraries => {
				let libraries = core.libraries.list().await;
				let infos: Vec<LibraryInfo> =
					futures::future::join_all(libraries.into_iter().map(|lib| async move {
						LibraryInfo {
							id: lib.id(),
							name: lib.name().await,
							path: lib.path().to_path_buf(),
						}
					}))
					.await;

				DaemonResponse::Libraries(infos)
			}

			DaemonCommand::GetCurrentLibrary => {
				if let Some(library) = state_service.get_current_library(core).await {
					DaemonResponse::CurrentLibrary(Some(LibraryInfo {
						id: library.id(),
						name: library.name().await,
						path: library.path().to_path_buf(),
					}))
				} else {
					DaemonResponse::CurrentLibrary(None)
				}
			}

			DaemonCommand::SwitchLibrary { id } => {
				let libraries = core.libraries.list().await;
				if let Some(library) = libraries.iter().find(|lib| lib.id() == id) {
					match state_service
						.switch_library(library.id(), library.path().to_path_buf())
						.await
					{
						Ok(_) => DaemonResponse::Ok,
						Err(e) => {
							warn!("Failed to save CLI state: {}", e);
							DaemonResponse::Ok // Still return Ok as the switch was successful
						}
					}
				} else {
					DaemonResponse::Error("Library not found".to_string())
				}
			}

			_ => DaemonResponse::Error("Invalid command for library handler".to_string()),
		}
	}

	fn can_handle(&self, cmd: &DaemonCommand) -> bool {
		matches!(
			cmd,
			DaemonCommand::CreateLibrary { .. }
				| DaemonCommand::ListLibraries
				| DaemonCommand::GetCurrentLibrary
				| DaemonCommand::SwitchLibrary { .. }
		)
	}
}```

## src/infrastructure/cli/daemon/handlers/system.rs

```rust
//! System monitoring command handlers

use async_trait::async_trait;
use std::path::PathBuf;
use std::sync::Arc;

use crate::Core;
use crate::infrastructure::cli::daemon::services::StateService;
use crate::infrastructure::cli::daemon::types::{DaemonCommand, DaemonResponse};

use super::CommandHandler;

/// Handler for system monitoring commands
pub struct SystemHandler {
	data_dir: PathBuf,
}

impl SystemHandler {
	pub fn new(data_dir: PathBuf) -> Self {
		Self { data_dir }
	}
}

#[async_trait]
impl CommandHandler for SystemHandler {
	async fn handle(
		&self,
		cmd: DaemonCommand,
		_core: &Arc<Core>,
		_state_service: &Arc<StateService>,
	) -> DaemonResponse {
		match cmd {
			DaemonCommand::SubscribeEvents => {
				// TODO: Implement event subscription
				DaemonResponse::Error("Event subscription not yet implemented".to_string())
			}

			_ => DaemonResponse::Error("Invalid command for system handler".to_string()),
		}
	}

	fn can_handle(&self, cmd: &DaemonCommand) -> bool {
		matches!(cmd, DaemonCommand::SubscribeEvents)
	}
}```

## src/infrastructure/cli/daemon/handlers/mod.rs

```rust
//! Command handlers for the daemon

use async_trait::async_trait;
use std::sync::Arc;

use crate::Core;
use crate::infrastructure::cli::daemon::services::StateService;
use crate::infrastructure::cli::daemon::types::{DaemonCommand, DaemonResponse};

pub mod core;
pub mod file;
pub mod job;
pub mod library;
pub mod location;
pub mod network;
pub mod system;
pub mod volume;

pub use self::core::CoreHandler;
pub use file::FileHandler;
pub use job::JobHandler;
pub use library::LibraryHandler;
pub use location::LocationHandler;
pub use network::NetworkHandler;
pub use system::SystemHandler;
pub use volume::VolumeHandler;

/// Trait for command handlers
#[async_trait]
pub trait CommandHandler: Send + Sync {
	/// Handle a command and return a response
	async fn handle(
		&self,
		cmd: DaemonCommand,
		core: &Arc<Core>,
		state_service: &Arc<StateService>,
	) -> DaemonResponse;

	/// Check if this handler can handle the given command
	fn can_handle(&self, cmd: &DaemonCommand) -> bool;
}

/// Registry for command handlers
pub struct HandlerRegistry {
	handlers: Vec<Box<dyn CommandHandler>>,
}

impl HandlerRegistry {
	/// Create a new handler registry with all handlers
	pub fn new(
		start_time: std::time::Instant,
		shutdown_tx: Arc<tokio::sync::Mutex<Option<tokio::sync::oneshot::Sender<()>>>>,
		data_dir: std::path::PathBuf,
	) -> Self {
		let handlers: Vec<Box<dyn CommandHandler>> = vec![
			Box::new(CoreHandler::new(start_time, shutdown_tx)),
			Box::new(LibraryHandler),
			Box::new(LocationHandler),
			Box::new(JobHandler),
			Box::new(FileHandler),
			Box::new(NetworkHandler),
			Box::new(SystemHandler::new(data_dir)),
			Box::new(VolumeHandler),
		];

		Self { handlers }
	}

	/// Handle a command by finding the appropriate handler
	pub async fn handle(
		&self,
		cmd: DaemonCommand,
		core: &Arc<Core>,
		state_service: &Arc<StateService>,
	) -> DaemonResponse {
		for handler in &self.handlers {
			if handler.can_handle(&cmd) {
				return handler.handle(cmd, core, state_service).await;
			}
		}

		DaemonResponse::Error("No handler found for command".to_string())
	}
}```

## src/infrastructure/cli/daemon/handlers/network.rs

```rust
//! Network operation command handlers

use async_trait::async_trait;
use std::sync::Arc;
use uuid::Uuid;

use crate::Core;
use crate::infrastructure::cli::daemon::services::StateService;
use crate::infrastructure::cli::daemon::types::{
	ConnectedDeviceInfo, DaemonCommand, DaemonResponse, PairingRequestInfo,
};

use super::CommandHandler;

/// Handler for network operation commands
pub struct NetworkHandler;

#[async_trait]
impl CommandHandler for NetworkHandler {
	async fn handle(
		&self,
		cmd: DaemonCommand,
		core: &Arc<Core>,
		state_service: &Arc<StateService>,
	) -> DaemonResponse {
		match cmd {
			DaemonCommand::InitNetworking => {
				// Check if networking is already initialized
				if core.networking().is_some() {
					DaemonResponse::Ok // Networking is already available
				} else {
					// Networking not available - daemon needs to be restarted with networking
					DaemonResponse::Error(
						"Networking not available. Restart daemon with: spacedrive start --enable-networking".to_string()
					)
				}
			}

			DaemonCommand::StartNetworking => match core.start_networking().await {
				Ok(_) => DaemonResponse::Ok,
				Err(e) => DaemonResponse::Error(e.to_string()),
			},

			DaemonCommand::StopNetworking => {
				// TODO: Implement networking stop when available
				DaemonResponse::Error("Stop networking not yet implemented".to_string())
			}

			DaemonCommand::ListConnectedDevices => match core.get_connected_devices_info().await {
				Ok(devices) => {
					let connected_devices: Vec<ConnectedDeviceInfo> = devices
						.into_iter()
						.map(|device| {
							// Get connection status from networking service
							let (
								peer_id,
								connection_active,
								connected_at,
								bytes_sent,
								bytes_received,
							) = if let Some(_networking) = core.networking() {
								// Try to get connection details - this is a simplified version
								// In a real implementation, we'd access the connection registry
								("unknown".to_string(), true, Some("now".to_string()), 0, 0)
							} else {
								("unavailable".to_string(), false, None, 0, 0)
							};

							ConnectedDeviceInfo {
								device_id: device.device_id,
								device_name: device.device_name,
								device_type: format!("{:?}", device.device_type),
								os_version: device.os_version,
								app_version: device.app_version,
								peer_id,
								status: "connected".to_string(),
								connection_active,
								last_seen: device
									.last_seen
									.format("%Y-%m-%d %H:%M:%S UTC")
									.to_string(),
								connected_at,
								bytes_sent,
								bytes_received,
							}
						})
						.collect();

					DaemonResponse::ConnectedDevices(connected_devices)
				}
				Err(e) => DaemonResponse::Error(e.to_string()),
			},

			DaemonCommand::RevokeDevice { device_id } => {
				// Get current library from CLI state
				if let Some(library) = state_service.get_current_library(core).await {
					let library_id = library.id();

					// Get the action manager
					match core.context.get_action_manager().await {
						Some(action_manager) => {
							// Create DeviceRevokeAction
							let action = crate::infrastructure::actions::Action::DeviceRevoke {
								library_id,
								action: crate::operations::devices::revoke::action::DeviceRevokeAction {
									device_id,
									reason: Some("Revoked via CLI".to_string()),
								},
							};

							// Dispatch the action
							match action_manager.dispatch(action).await {
								Ok(_output) => DaemonResponse::Ok,
								Err(e) => DaemonResponse::Error(format!(
									"Failed to revoke device: {}",
									e
								)),
							}
						}
						None => DaemonResponse::Error("Action manager not available".to_string()),
					}
				} else {
					DaemonResponse::Error("No library selected".to_string())
				}
			}

			DaemonCommand::SendSpacedrop {
				device_id,
				file_path,
				sender_name,
				message,
			} => {
				if let Some(networking) = core.networking() {
					let service = &*networking;

					// Create spacedrop request message
					let transfer_id = uuid::Uuid::new_v4();
					let spacedrop_request = serde_json::json!({
						"transfer_id": transfer_id,
						"file_path": file_path,
						"sender_name": sender_name,
						"message": message,
						"file_size": std::fs::metadata(&file_path).map(|m| m.len()).unwrap_or(0)
					});

					match service
						.send_message(
							device_id,
							"spacedrop",
							serde_json::to_vec(&spacedrop_request).unwrap_or_default(),
						)
						.await
					{
						Ok(_) => DaemonResponse::SpacedropStarted { transfer_id },
						Err(e) => DaemonResponse::Error(e.to_string()),
					}
				} else {
					DaemonResponse::Error("Networking not initialized".to_string())
				}
			}

			// Pairing commands
			DaemonCommand::StartPairingAsInitiator => {
				if let Some(networking) = core.networking() {
					let service = &*networking;
					match service.start_pairing_as_initiator().await {
						Ok((code, expires_in_seconds)) => DaemonResponse::PairingCodeGenerated {
							code,
							expires_in_seconds,
						},
						Err(e) => DaemonResponse::Error(e.to_string()),
					}
				} else {
					DaemonResponse::Error("Networking not initialized".to_string())
				}
			}

			DaemonCommand::StartPairingAsJoiner { code } => {
				if let Some(networking) = core.networking() {
					let service = &*networking;
					match service.start_pairing_as_joiner(&code).await {
						Ok(_) => DaemonResponse::PairingInProgress,
						Err(e) => DaemonResponse::Error(e.to_string()),
					}
				} else {
					DaemonResponse::Error("Networking not initialized".to_string())
				}
			}

			DaemonCommand::GetPairingStatus => {
				if let Some(networking) = core.networking() {
					let service = &*networking;
					match service.get_pairing_status().await {
						Ok(sessions) => {
							// Convert sessions to status format for compatibility
							if let Some(session) = sessions.first() {
								let status = match &session.state {
									crate::networking::PairingState::Idle => "idle",
									crate::networking::PairingState::GeneratingCode => {
										"generating_code"
									}
									crate::networking::PairingState::Broadcasting => "broadcasting",
									crate::networking::PairingState::Scanning => "scanning",
									crate::networking::PairingState::WaitingForConnection => {
										"waiting_for_connection"
									}
									crate::networking::PairingState::Connecting => "connecting",
									crate::networking::PairingState::Authenticating => "authenticating",
									crate::networking::PairingState::ExchangingKeys => {
										"exchanging_keys"
									}
									crate::networking::PairingState::AwaitingConfirmation => {
										"awaiting_confirmation"
									}
									crate::networking::PairingState::EstablishingSession => {
										"establishing_session"
									}
									crate::networking::PairingState::ChallengeReceived { .. } => {
										"authenticating"
									}
									crate::networking::PairingState::ResponseSent => "authenticating",
									crate::networking::PairingState::Completed => "completed",
									crate::networking::PairingState::Failed { .. } => "failed",
									crate::networking::PairingState::ResponsePending { .. } => {
										"responding"
									}
								}
								.to_string();

								DaemonResponse::PairingStatus {
									status,
									remote_device: None, // No device info available yet in new system
								}
							} else {
								DaemonResponse::PairingStatus {
									status: "no_active_pairing".to_string(),
									remote_device: None,
								}
							}
						}
						Err(e) => DaemonResponse::Error(e.to_string()),
					}
				} else {
					DaemonResponse::Error("Networking not initialized".to_string())
				}
			}

			DaemonCommand::ListPendingPairings => {
				if let Some(networking) = core.networking() {
					let service = &*networking;
					match service.get_pairing_status().await {
						Ok(sessions) => {
							// Convert active pairing sessions to pending requests
							let pairing_requests: Vec<PairingRequestInfo> = sessions
								.into_iter()
								.filter(|session| {
									matches!(
										session.state,
										crate::networking::PairingState::WaitingForConnection
									)
								})
								.map(|session| PairingRequestInfo {
									request_id: session.id,
									device_id: session.remote_device_id.unwrap_or(session.id),
									device_name: "Unknown Device".to_string(),
									received_at: session.created_at.to_string(),
								})
								.collect();
							DaemonResponse::PendingPairings(pairing_requests)
						}
						Err(e) => DaemonResponse::Error(e.to_string()),
					}
				} else {
					DaemonResponse::Error("Networking not initialized".to_string())
				}
			}

			DaemonCommand::AcceptPairing {
				request_id: _request_id,
			} => {
				// Pairing acceptance is handled automatically in the new system
				DaemonResponse::Ok
			}

			DaemonCommand::RejectPairing {
				request_id: _request_id,
			} => {
				// For now, just acknowledge - in full implementation we'd cancel the session
				DaemonResponse::Ok
			}

			_ => DaemonResponse::Error("Invalid command for network handler".to_string()),
		}
	}

	fn can_handle(&self, cmd: &DaemonCommand) -> bool {
		matches!(
			cmd,
			DaemonCommand::InitNetworking
				| DaemonCommand::StartNetworking
				| DaemonCommand::StopNetworking
				| DaemonCommand::ListConnectedDevices
				| DaemonCommand::RevokeDevice { .. }
				| DaemonCommand::SendSpacedrop { .. }
				| DaemonCommand::StartPairingAsInitiator
				| DaemonCommand::StartPairingAsJoiner { .. }
				| DaemonCommand::GetPairingStatus
				| DaemonCommand::ListPendingPairings
				| DaemonCommand::AcceptPairing { .. }
				| DaemonCommand::RejectPairing { .. }
		)
	}
}```

## src/infrastructure/cli/daemon/handlers/location.rs

```rust
//! Location command handlers

use async_trait::async_trait;
use std::path::PathBuf;
use std::sync::Arc;
use uuid::Uuid;

use crate::infrastructure::cli::daemon::services::StateService;
use crate::infrastructure::cli::daemon::types::{DaemonCommand, DaemonResponse, LocationInfo};
use crate::Core;

use super::CommandHandler;

/// Handler for location commands
pub struct LocationHandler;

#[async_trait]
impl CommandHandler for LocationHandler {
	async fn handle(
		&self,
		cmd: DaemonCommand,
		core: &Arc<Core>,
		state_service: &Arc<StateService>,
	) -> DaemonResponse {
		match cmd {
			DaemonCommand::AddLocation { path, name } => {
				// Get current library from CLI state
				if let Some(library) = state_service.get_current_library(core).await {
					let library_id = library.id();

					// Get the action manager
					match core.context.get_action_manager().await {
						Some(action_manager) => {
							// Create the location add action
							let action = crate::infrastructure::actions::Action::LocationAdd {
								library_id,
								action:
									crate::operations::locations::add::action::LocationAddAction {
										path: path.clone(),
										name,
										mode: crate::operations::indexing::IndexMode::Content,
									},
							};

							// Dispatch the action
							match action_manager.dispatch(action).await {
								Ok(output) => {
									// For now, just return success
									// TODO: Extract location and job IDs when LocationAdd action returns them
									DaemonResponse::Ok
								}
								Err(e) => {
									DaemonResponse::Error(format!("Failed to add location: {}", e))
								}
							}
						}
						None => DaemonResponse::Error("Action manager not available".to_string()),
					}
				} else {
					DaemonResponse::Error("No library selected".to_string())
				}
			}

			DaemonCommand::ListLocations => {
				// Get current library from CLI state
				if let Some(library) = state_service.get_current_library(core).await {
					// For listing, we can directly query the database since it's a read operation
					use crate::infrastructure::database::entities;
					use sea_orm::EntityTrait;

					match entities::location::Entity::find()
						.all(library.db().conn())
						.await
					{
						Ok(locations) => {
							let infos: Vec<LocationInfo> = locations
								.into_iter()
								.map(|loc| LocationInfo {
									id: loc.uuid,
									name: loc.name.unwrap_or_default(),
									path: PathBuf::from(loc.path),
									status: if loc.scan_state == "1" {
										"active"
									} else {
										"idle"
									}
									.to_string(),
								})
								.collect();

							DaemonResponse::Locations(infos)
						}
						Err(e) => DaemonResponse::Error(format!("Failed to list locations: {}", e)),
					}
				} else {
					DaemonResponse::Error("No library selected".to_string())
				}
			}

			DaemonCommand::RemoveLocation { id } => {
				// Get current library from CLI state
				if let Some(library) = state_service.get_current_library(core).await {
					let library_id = library.id();

					// Get the action manager
					match core.context.get_action_manager().await {
						Some(action_manager) => {
							// Create the location remove action
							let action = crate::infrastructure::actions::Action::LocationRemove {
								library_id,
								action: crate::operations::locations::remove::action::LocationRemoveAction {
									location_id: id,
								},
							};

							// Dispatch the action
							match action_manager.dispatch(action).await {
								Ok(_) => DaemonResponse::Ok,
								Err(e) => DaemonResponse::Error(format!(
									"Failed to remove location: {}",
									e
								)),
							}
						}
						None => DaemonResponse::Error("Action manager not available".to_string()),
					}
				} else {
					DaemonResponse::Error("No library selected".to_string())
				}
			}

			DaemonCommand::RescanLocation { id } => {
				// Get current library from CLI state
				if let Some(library) = state_service.get_current_library(core).await {
					let library_id = library.id();

					// Get the action manager
					match core.context.get_action_manager().await {
						Some(action_manager) => {
							// Create LocationRescanAction
							let action = crate::infrastructure::actions::Action::LocationRescan {
								library_id,
								action: crate::operations::locations::rescan::action::LocationRescanAction {
									location_id: id,
									full_rescan: false,
								},
							};

							// Dispatch the action
							match action_manager.dispatch(action).await {
								Ok(output) => {
									// For now, just return success
									// TODO: Extract job ID when LocationRescan action returns it
									DaemonResponse::Ok
								}
								Err(e) => DaemonResponse::Error(format!(
									"Failed to start rescan: {}",
									e
								)),
							}
						}
						None => DaemonResponse::Error("Action manager not available".to_string()),
					}
				} else {
					DaemonResponse::Error("No library selected".to_string())
				}
			}

			_ => DaemonResponse::Error("Invalid command for location handler".to_string()),
		}
	}

	fn can_handle(&self, cmd: &DaemonCommand) -> bool {
		matches!(
			cmd,
			DaemonCommand::AddLocation { .. }
				| DaemonCommand::ListLocations
				| DaemonCommand::RemoveLocation { .. }
				| DaemonCommand::RescanLocation { .. }
		)
	}
}
```

## src/infrastructure/cli/daemon/handlers/volume.rs

```rust
//! Volume command handler for the daemon

use async_trait::async_trait;
use std::sync::Arc;
use tracing::debug;

use crate::{
	infrastructure::{
		actions::{manager::ActionManager, Action},
		cli::{
			commands::VolumeCommands,
			daemon::{
				services::StateService,
				types::{DaemonCommand, DaemonResponse, VolumeListItem},
			},
		},
	},
	operations::volumes::{
		speed_test::action::VolumeSpeedTestAction, track::action::VolumeTrackAction,
		untrack::action::VolumeUntrackAction,
	},
	volume::VolumeFingerprint,
	Core,
};

use super::CommandHandler;

/// Handler for volume commands
pub struct VolumeHandler;

#[async_trait]
impl CommandHandler for VolumeHandler {
	async fn handle(
		&self,
		cmd: DaemonCommand,
		core: &Arc<Core>,
		state_service: &Arc<StateService>,
	) -> DaemonResponse {
		match cmd {
			DaemonCommand::Volume(volume_cmd) => match volume_cmd {
				VolumeCommands::List {
					include_system,
					type_filter,
					show_types,
					show_offline,
				} => {
					// Get all currently detected volumes
					let volumes = core.volumes.get_all_volumes().await;

					// Get current library to check track status
					if let Some(library) = state_service.get_current_library(core).await {
						// Update offline status for tracked volumes
						if let Err(e) = core.volumes.update_offline_volumes(&library).await {
							debug!("Failed to update offline volumes: {}", e);
						}

						// Get tracked volumes for this library
						let tracked_volumes = match core.volumes.get_tracked_volumes(&library).await
						{
							Ok(tracked) => tracked,
							Err(_) => Vec::new(),
						};

						// Create a map of fingerprint -> tracked info for quick lookup
						let tracked_map: std::collections::HashMap<_, _> = tracked_volumes
							.iter()
							.map(|tv| (tv.fingerprint.clone(), tv))
							.collect();

						let mut volume_list_items = Vec::new();

						// Add currently detected volumes
						for volume in volumes {
							let is_tracked = tracked_map.contains_key(&volume.fingerprint);
							let tracked_name = tracked_map
								.get(&volume.fingerprint)
								.and_then(|tv| tv.display_name.clone());

							volume_list_items.push(VolumeListItem {
								volume,
								is_tracked,
								tracked_name,
								is_online: true,    // Currently detected volumes are online
								last_seen_at: None, // Not applicable for online volumes
							});
						}

						// Add offline tracked volumes if requested
						if show_offline {
							for tracked_volume in &tracked_volumes {
								// Skip if this volume is already in the online list by fingerprint
								if volume_list_items.iter().any(|item| {
									item.volume.fingerprint == tracked_volume.fingerprint
								}) {
									continue;
								}

								// Skip if we already added this offline volume (deduplicate offline volumes only)
								let already_added_offline = volume_list_items.iter().any(|item| {
									!item.is_online && // Only check against other offline volumes
									item.volume.device_id == tracked_volume.device_id &&
									item.volume.mount_point.to_string_lossy() == tracked_volume.mount_point.clone().unwrap_or_default()
								});

								if already_added_offline {
									continue;
								}

								// Convert tracked volume to offline volume
								let offline_volume = tracked_volume.to_offline_volume();

								volume_list_items.push(VolumeListItem {
									volume: offline_volume,
									is_tracked: true,
									tracked_name: tracked_volume.display_name.clone(),
									is_online: tracked_volume.is_online,
									last_seen_at: Some(tracked_volume.last_seen_at),
								});
							}
						}

						DaemonResponse::VolumeListWithTracking(volume_list_items)
					} else {
						// No current library, just return basic volume list
						DaemonResponse::VolumeList(volumes)
					}
				}

				VolumeCommands::Get { fingerprint } => {
					let fingerprint = VolumeFingerprint(fingerprint);
					match core.volumes.get_volume(&fingerprint).await {
						Some(volume) => DaemonResponse::Volume(volume),
						None => DaemonResponse::Error("Volume not found".to_string()),
					}
				}

				VolumeCommands::Track { fingerprint, name } => {
					if let Some(library) = state_service.get_current_library(core).await {
						// Try to resolve short ID to full fingerprint
						let resolved_fingerprint = if let Some(volume) =
							core.volumes.get_volume_by_short_id(&fingerprint).await
						{
							volume.fingerprint
						} else {
							// Try as full fingerprint
							VolumeFingerprint::from_hex(fingerprint)
						};

						let action = Action::VolumeTrack {
							action: VolumeTrackAction {
								fingerprint: resolved_fingerprint,
								library_id: library.id(),
								name,
							},
						};

						match core.context.get_action_manager().await {
							Some(action_manager) => match action_manager.dispatch(action).await {
								Ok(action_output) => DaemonResponse::ActionOutput(action_output),
								Err(e) => {
									DaemonResponse::Error(format!("Failed to track volume: {}", e))
								}
							},
							None => {
								DaemonResponse::Error("Action manager not initialized".to_string())
							}
						}
					} else {
						DaemonResponse::Error(
							"No library selected. Use 'library switch' to select a library first."
								.to_string(),
						)
					}
				}

				VolumeCommands::Untrack { fingerprint } => {
					if let Some(library) = state_service.get_current_library(core).await {
						// Try to resolve short ID to full fingerprint
						let resolved_fingerprint = if let Some(volume) =
							core.volumes.get_volume_by_short_id(&fingerprint).await
						{
							volume.fingerprint
						} else {
							// Try as full fingerprint
							VolumeFingerprint::from_hex(fingerprint)
						};

						let action = Action::VolumeUntrack {
							action: VolumeUntrackAction {
								fingerprint: resolved_fingerprint,
								library_id: library.id(),
							},
						};

						match core.context.get_action_manager().await {
							Some(action_manager) => match action_manager.dispatch(action).await {
								Ok(action_output) => DaemonResponse::ActionOutput(action_output),
								Err(e) => DaemonResponse::Error(format!(
									"Failed to untrack volume: {}",
									e
								)),
							},
							None => {
								DaemonResponse::Error("Action manager not initialized".to_string())
							}
						}
					} else {
						DaemonResponse::Error(
							"No library selected. Use 'library switch' to select a library first."
								.to_string(),
						)
					}
				}

				VolumeCommands::SpeedTest { fingerprint } => {
					let action = Action::VolumeSpeedTest {
						action: VolumeSpeedTestAction {
							fingerprint: VolumeFingerprint(fingerprint),
						},
					};

					match core.context.get_action_manager().await {
						Some(action_manager) => match action_manager.dispatch(action).await {
							Ok(output) => DaemonResponse::ActionOutput(output),
							Err(e) => {
								DaemonResponse::Error(format!("Failed to run speed test: {}", e))
							}
						},
						None => DaemonResponse::Error("Action manager not initialized".to_string()),
					}
				}

				VolumeCommands::Refresh => match core.volumes.refresh_volumes().await {
					Ok(_) => {
						let volumes = core.volumes.get_all_volumes().await;
						DaemonResponse::VolumeList(volumes)
					}
					Err(e) => DaemonResponse::Error(format!("Failed to refresh volumes: {}", e)),
				},

				VolumeCommands::FixNames => {
					if let Some(library) = state_service.get_current_library(core).await {
						match core.volumes.update_empty_display_names(&library).await {
							Ok(count) => {
								if count > 0 {
									tracing::info!("Updated display names for {} volumes", count);
								} else {
									tracing::info!("No volumes with empty display names found");
								}
								DaemonResponse::Ok
							}
							Err(e) => DaemonResponse::Error(format!(
								"Failed to update display names: {}",
								e
							)),
						}
					} else {
						DaemonResponse::Error("No library selected".to_string())
					}
				}
			},
			_ => DaemonResponse::Error("Invalid command for volume handler".to_string()),
		}
	}

	fn can_handle(&self, cmd: &DaemonCommand) -> bool {
		matches!(cmd, DaemonCommand::Volume(_))
	}
}
```

## src/infrastructure/cli/daemon/handlers/file.rs

```rust
//! File operation command handlers

use async_trait::async_trait;
use std::path::PathBuf;
use std::sync::Arc;
use uuid::Uuid;

use crate::infrastructure::actions::builder::{ActionBuildError, ActionBuilder};
use crate::infrastructure::cli::daemon::services::StateService;
use crate::infrastructure::cli::daemon::types::{DaemonCommand, DaemonResponse};
use crate::Core;

use super::CommandHandler;

/// Handler for file operation commands
pub struct FileHandler;

#[async_trait]
impl CommandHandler for FileHandler {
	async fn handle(
		&self,
		cmd: DaemonCommand,
		core: &Arc<Core>,
		state_service: &Arc<StateService>,
	) -> DaemonResponse {
		match cmd {
			DaemonCommand::Copy {
				sources,
				destination,
				overwrite,
				verify,
				preserve_timestamps,
				move_files,
			} => {
				// Get current library from CLI state
				if let Some(library) = state_service.get_current_library(core).await {
					let library_id = library.id();

					// Create the copy input
					let input = crate::operations::files::copy::input::FileCopyInput {
						sources: sources.clone(),
						destination: destination.clone(),
						overwrite,
						verify_checksum: verify,
						preserve_timestamps,
						move_files,
						copy_method: crate::operations::files::copy::input::CopyMethod::Auto,
					};

					// Validate input
					if let Err(errors) = input.validate() {
						return DaemonResponse::Error(format!(
							"Invalid copy operation: {}",
							errors.join("; ")
						));
					}

					// Get the action manager
					match core.context.get_action_manager().await {
						Some(action_manager) => {
							// Create the copy action
							let action = match crate::operations::files::copy::action::FileCopyActionBuilder::from_input(input).build() {
								Ok(action) => action,
								Err(e) => {
									return DaemonResponse::Error(format!("Failed to build copy action: {}", e));
								}
							};

							// Create the full Action enum
							let full_action = crate::infrastructure::actions::Action::FileCopy {
								library_id,
								action,
							};

							// Dispatch the action
							match action_manager.dispatch(full_action).await {
								Ok(output) => {
									// For now, just return success
									// TODO: Extract job ID when FileCopy action returns it
									DaemonResponse::Ok
								}
								Err(e) => DaemonResponse::Error(format!(
									"Failed to start copy operation: {}",
									e
								)),
							}
						}
						None => DaemonResponse::Error("Action manager not available".to_string()),
					}
				} else {
					DaemonResponse::Error(
						"No library available. Create or open a library first.".to_string(),
					)
				}
			}

			// Indexing operations
			DaemonCommand::Browse {
				path,
				scope,
				content,
			} => {
				// Browse is a read-only operation that doesn't persist anything
				// For now, we'll do a simple directory listing
				match std::fs::read_dir(&path) {
					Ok(entries) => {
						let mut browse_entries = Vec::new();
						let mut total_files = 0;
						let mut total_dirs = 0;

						for entry in entries.flatten() {
							let metadata = entry.metadata().ok();
							let is_dir = metadata.as_ref().map(|m| m.is_dir()).unwrap_or(false);

							if is_dir {
								total_dirs += 1;
							} else {
								total_files += 1;
							}

							let file_name = entry.file_name().to_string_lossy().to_string();
							let file_path = entry.path();

							let size = if !is_dir {
								metadata.as_ref().map(|m| m.len())
							} else {
								None
							};

							let modified =
								metadata
									.as_ref()
									.and_then(|m| m.modified().ok())
									.map(|time| {
										// Convert to human-readable format
										chrono::DateTime::<chrono::Utc>::from(time)
											.format("%Y-%m-%d %H:%M:%S")
											.to_string()
									});

							let file_type = if is_dir {
								Some("directory".to_string())
							} else {
								// Simple file type detection based on extension
								file_path
									.extension()
									.and_then(|ext| ext.to_str())
									.map(|ext| ext.to_lowercase())
							};

							browse_entries.push(
								crate::infrastructure::cli::daemon::types::common::BrowseEntry {
									name: file_name,
									path: file_path,
									is_dir,
									size,
									modified,
									file_type,
								},
							);
						}

						// Sort entries: directories first, then files, alphabetically
						browse_entries.sort_by(|a, b| match (a.is_dir, b.is_dir) {
							(true, false) => std::cmp::Ordering::Less,
							(false, true) => std::cmp::Ordering::Greater,
							_ => a.name.cmp(&b.name),
						});

						DaemonResponse::BrowseResults {
							path,
							entries: browse_entries,
							total_files,
							total_dirs,
						}
					}
					Err(e) => DaemonResponse::Error(format!("Failed to browse path: {}", e)),
				}
			}

			DaemonCommand::IndexAll { force } => {
				// Get current library from CLI state
				if let Some(library) = state_service.get_current_library(core).await {
					let library_id = library.id();

					// Get the action manager
					match core.context.get_action_manager().await {
						Some(action_manager) => {
							// Create LocationManager
							let location_manager =
								crate::location::manager::LocationManager::new((*core.events).clone());

							// Get all locations for the library
							match location_manager.list_locations(&library).await {
								Ok(locations) => {
									if locations.is_empty() {
										return DaemonResponse::Error(
											"No locations found in library".to_string(),
										);
									}

									let location_count = locations.len();
									let mut success_count = 0;
									let mut errors = Vec::new();

									// Dispatch LocationIndexAction for each location
									for location in locations {
										let action = crate::infrastructure::actions::Action::LocationIndex {
											library_id,
											action: crate::operations::locations::index::action::LocationIndexAction {
												location_id: location.id,
												mode: location.index_mode.into(),
											},
										};

										match action_manager.dispatch(action).await {
											Ok(_output) => {
												success_count += 1;
											}
											Err(e) => {
												errors.push(format!(
													"Location {}: {}",
													location.id, e
												));
											}
										}
									}

									if errors.is_empty() {
										DaemonResponse::Ok
									} else {
										DaemonResponse::Error(format!(
											"Indexed {}/{} locations. Errors: {}",
											success_count,
											location_count,
											errors.join("; ")
										))
									}
								}
								Err(e) => DaemonResponse::Error(format!(
									"Failed to list locations: {}",
									e
								)),
							}
						}
						None => DaemonResponse::Error("Action manager not available".to_string()),
					}
				} else {
					DaemonResponse::Error(
						"No library available. Create or open a library first.".to_string(),
					)
				}
			}

			DaemonCommand::IndexLocation { location, force } => {
				// Get current library from CLI state
				if let Some(library) = state_service.get_current_library(core).await {
					let library_id = library.id();

					// Parse location ID
					match location.parse::<Uuid>() {
						Ok(location_id) => {
							// Get the action manager
							match core.context.get_action_manager().await {
								Some(action_manager) => {
									// Create LocationIndexAction
									let action = crate::infrastructure::actions::Action::LocationIndex {
										library_id,
										action: crate::operations::locations::index::action::LocationIndexAction {
											location_id,
											mode: crate::operations::indexing::IndexMode::Content,
										},
									};

									// Dispatch the action
									match action_manager.dispatch(action).await {
										Ok(_output) => {
											DaemonResponse::LocationIndexed { location_id }
										}
										Err(e) => DaemonResponse::Error(format!(
											"Failed to index location: {}",
											e
										)),
									}
								}
								None => DaemonResponse::Error(
									"Action manager not available".to_string(),
								),
							}
						}
						Err(_) => {
							DaemonResponse::Error(format!("Invalid location ID: {}", location))
						}
					}
				} else {
					DaemonResponse::Error(
						"No library available. Create or open a library first.".to_string(),
					)
				}
			}

			_ => DaemonResponse::Error("Invalid command for file handler".to_string()),
		}
	}

	fn can_handle(&self, cmd: &DaemonCommand) -> bool {
		matches!(
			cmd,
			DaemonCommand::Copy { .. }
				| DaemonCommand::Browse { .. }
				| DaemonCommand::IndexAll { .. }
				| DaemonCommand::IndexLocation { .. }
		)
	}
}
```

## src/infrastructure/cli/daemon/services/helpers.rs

```rust
//! Helper functions for common daemon operations

/// Helper functions for daemon operations
pub struct DaemonHelpers;

impl DaemonHelpers {

	/// Set up file logging for the daemon
	pub fn setup_file_logging(
		log_file: &std::path::Path,
	) -> Result<(), Box<dyn std::error::Error>> {
		use std::fs::OpenOptions;
		use tracing_subscriber::{fmt, prelude::*, EnvFilter};

		// Create log file directory if it doesn't exist
		if let Some(parent) = log_file.parent() {
			std::fs::create_dir_all(parent)?;
		}

		// Open log file for appending
		let file = OpenOptions::new()
			.create(true)
			.append(true)
			.open(log_file)?;

		// Set up file logging with both console and file output
		let file_layer = fmt::layer()
			.with_writer(file)
			.with_ansi(false) // No color codes in file
			.with_target(true)
			.with_thread_ids(true)
			.with_line_number(true);

		let console_layer = fmt::layer()
			.with_writer(std::io::stderr) // Console output to stderr
			.with_target(false); // Less verbose for console

		// Use info level for daemon by default, can be overridden with RUST_LOG
		let filter = EnvFilter::try_from_default_env()
			.unwrap_or_else(|_| EnvFilter::new("info,sd_core_new=debug"));

		// Set up comprehensive logging for the entire daemon process
		tracing_subscriber::registry()
			.with(filter)
			.with(file_layer)
			.with(console_layer)
			.init();

		tracing::info!(
			"Daemon logging initialized, writing to: {}",
			log_file.display()
		);
		tracing::info!("All Core application logs will be captured");
		Ok(())
	}
}```

## src/infrastructure/cli/daemon/services/mod.rs

```rust
//! Service traits and implementations for the daemon

pub mod helpers;
pub mod state;

pub use helpers::DaemonHelpers;
pub use state::StateService;```

## src/infrastructure/cli/daemon/services/state.rs

```rust
//! State management service for the daemon

use crate::infrastructure::cli::state::CliState;
use crate::library::Library;
use crate::Core;
use std::path::PathBuf;
use std::sync::Arc;
use tokio::sync::RwLock;
use uuid::Uuid;

/// Service for managing CLI state
pub struct StateService {
	cli_state: Arc<RwLock<CliState>>,
	data_dir: PathBuf,
}

impl StateService {
	/// Create a new state service
	pub fn new(cli_state: Arc<RwLock<CliState>>, data_dir: PathBuf) -> Self {
		Self {
			cli_state,
			data_dir,
		}
	}

	/// Get the current library from CLI state
	pub async fn get_current_library(&self, core: &Arc<Core>) -> Option<Arc<Library>> {
		let state = self.cli_state.read().await;
		if let Some(current_id) = state.current_library_id {
			core.libraries.get_library(current_id).await
		} else {
			// Fallback to first library if no current library is set
			let libraries = core.libraries.list().await;
			libraries.first().cloned()
		}
	}

	/// Switch to a different library
	pub async fn switch_library(
		&self,
		library_id: Uuid,
		library_path: PathBuf,
	) -> Result<(), Box<dyn std::error::Error>> {
		let mut state = self.cli_state.write().await;
		state.set_current_library(library_id, library_path);

		// Save state to disk
		state.save(&self.data_dir)?;
		Ok(())
	}

	/// Get the current library ID
	pub async fn get_current_library_id(&self) -> Option<Uuid> {
		let state = self.cli_state.read().await;
		state.current_library_id
	}

	/// Auto-select first library if none is set
	pub async fn auto_select_library(
		&self,
		core: &Arc<Core>,
	) -> Result<(), Box<dyn std::error::Error>> {
		let mut state = self.cli_state.write().await;

		if state.current_library_id.is_none() {
			let libraries = core.libraries.list().await;
			if let Some(first_lib) = libraries.first() {
				state.set_current_library(first_lib.id(), first_lib.path().to_path_buf());
				state.save(&self.data_dir)?;
			}
		}

		Ok(())
	}
}```

## src/infrastructure/cli/state.rs

```rust
use serde::{Deserialize, Serialize};
use std::path::{Path, PathBuf};
use uuid::Uuid;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CliState {
    /// Currently selected library ID
    pub current_library_id: Option<Uuid>,
    
    /// Last used library path
    pub last_library_path: Option<PathBuf>,
    
    /// Recent commands for history
    pub command_history: Vec<String>,
    
    /// Maximum history size
    #[serde(default = "default_history_size")]
    pub max_history: usize,
}

fn default_history_size() -> usize {
    100
}

impl Default for CliState {
    fn default() -> Self {
        Self {
            current_library_id: None,
            last_library_path: None,
            command_history: Vec::new(),
            max_history: default_history_size(),
        }
    }
}

impl CliState {
    pub fn load(data_dir: &Path) -> Result<Self, Box<dyn std::error::Error>> {
        let state_file = data_dir.join("cli_state.json");
        
        if state_file.exists() {
            let content = std::fs::read_to_string(&state_file)?;
            let state: Self = serde_json::from_str(&content)?;
            Ok(state)
        } else {
            Ok(Self::default())
        }
    }
    
    pub fn save(&self, data_dir: &Path) -> Result<(), Box<dyn std::error::Error>> {
        let state_file = data_dir.join("cli_state.json");
        
        // Ensure directory exists
        if let Some(parent) = state_file.parent() {
            std::fs::create_dir_all(parent)?;
        }
        
        let content = serde_json::to_string_pretty(self)?;
        std::fs::write(&state_file, content)?;
        
        Ok(())
    }
    
    pub fn add_to_history(&mut self, command: String) {
        self.command_history.push(command);
        
        // Trim history if it exceeds max size
        if self.command_history.len() > self.max_history {
            self.command_history.remove(0);
        }
    }
    
    pub fn set_current_library(&mut self, library_id: Uuid, library_path: PathBuf) {
        self.current_library_id = Some(library_id);
        self.last_library_path = Some(library_path);
    }
    
    pub fn has_current_library(&self) -> bool {
        self.current_library_id.is_some()
    }
}```

## src/infrastructure/cli/commands/job.rs

```rust
//! Job management commands
//!
//! This module handles CLI commands for managing jobs:
//! - Listing jobs with optional filtering
//! - Getting detailed job information
//! - Monitoring job progress in real-time

use crate::infrastructure::cli::daemon::{DaemonClient, DaemonCommand, DaemonResponse};
use crate::infrastructure::cli::output::messages::{
	JobInfo as OutputJobInfo, JobStatus as OutputJobStatus,
};
use crate::infrastructure::cli::output::{CliOutput, Message};
use crate::infrastructure::cli::utils::{format_bytes, progress_styles};
use clap::Subcommand;
use comfy_table::Table;
use indicatif::{MultiProgress, ProgressBar};
use std::collections::HashMap;
use uuid::Uuid;

#[derive(Subcommand, Clone, Debug)]
pub enum JobCommands {
	/// List all jobs
	List {
		/// Filter by status
		#[arg(short, long)]
		status: Option<String>,
		/// Show only recent jobs
		#[arg(short, long)]
		recent: bool,
	},

	/// Get detailed information about a job
	Info {
		/// Job ID (can be partial)
		id: String,
	},

	/// Monitor job progress in real-time
	Monitor {
		/// Specific job ID to monitor
		job_id: Option<String>,
		/// Exit when job completes
		#[arg(short, long)]
		exit_on_complete: bool,
	},

	/// Pause a job
	Pause {
		/// Job ID to pause
		id: String,
	},

	/// Resume a paused job
	Resume {
		/// Job ID to resume
		id: String,
	},

	/// Cancel a job
	Cancel {
		/// Job ID to cancel
		id: String,
		/// Skip confirmation prompt
		#[arg(short, long)]
		yes: bool,
	},

	/// Clear completed or failed jobs
	Clear {
		/// Only clear failed jobs
		#[arg(short, long)]
		failed: bool,
		/// Skip confirmation prompt
		#[arg(short, long)]
		yes: bool,
	},
}

pub async fn handle_job_command(
	cmd: JobCommands,
	instance_name: Option<String>,
	mut output: CliOutput,
) -> Result<(), Box<dyn std::error::Error>> {
	let mut client = DaemonClient::new_with_instance(instance_name.clone());

	match cmd {
		JobCommands::List { status, recent: _ } => {
			let status_filter = status.map(|s| s.to_lowercase());

			match client
				.send_command(DaemonCommand::ListJobs {
					status: status_filter,
				})
				.await
			{
				Ok(DaemonResponse::Jobs(jobs)) => {
					if jobs.is_empty() {
						output.info("No jobs found")?;
					} else {
						let output_jobs: Vec<OutputJobInfo> = jobs
							.into_iter()
							.map(|job| {
								let status = job
									.status
									.parse::<crate::infrastructure::jobs::types::JobStatus>()
									.map(OutputJobStatus::from)
									.unwrap_or(OutputJobStatus::Queued);

								OutputJobInfo {
									id: job.id,
									name: job.name.clone(),
									status,
									progress: Some(job.progress),
									started_at: 0, // TODO: Get actual timestamp from daemon
									completed_at: None,
								}
							})
							.collect();

						if matches!(
							output.format(),
							crate::infrastructure::cli::output::OutputFormat::Json
						) {
							output.print(Message::JobList { jobs: output_jobs })?;
						} else {
							// For human output, use a table
							let mut table = Table::new();
							table.set_header(vec!["ID", "Name", "Status", "Progress"]);

							for job in output_jobs {
								let progress_str = match job.status {
									OutputJobStatus::Running => {
										format!("{}%", (job.progress.unwrap_or(0.0) * 100.0) as u32)
									}
									_ => "-".to_string(),
								};

								table.add_row(vec![
									job.id.to_string(),
									job.name,
									format!("{:?}", job.status),
									progress_str,
								]);
							}

							output.section().table(table).render()?;
						}
					}
				}
				Ok(DaemonResponse::Error(e)) => {
					output.error(Message::Error(format!("Failed to list jobs: {}", e)))?;
				}
				Err(e) => {
					output.error(Message::Error(format!(
						"Failed to communicate with daemon: {}",
						e
					)))?;
				}
				_ => {
					output.error(Message::Error(
						"Unexpected response from daemon".to_string(),
					))?;
				}
			}
		}

		JobCommands::Info { id } => {
			// Parse the job ID string to UUID
			let job_id = match id.parse::<Uuid>() {
				Ok(uuid) => uuid,
				Err(_) => {
					output.error(Message::Error("Invalid job ID format".to_string()))?;
					return Ok(());
				}
			};

			match client
				.send_command(DaemonCommand::GetJobInfo { id: job_id })
				.await
			{
				Ok(DaemonResponse::JobInfo(Some(job))) => {
					output
						.section()
						.title("Job Information")
						.item("ID", &job.id.to_string())
						.item("Name", &job.name)
						.item("Status", &job.status)
						.item("Progress", &format!("{}%", (job.progress * 100.0) as u32))
						.render()?;
				}
				Ok(DaemonResponse::JobInfo(None)) => {
					output.error(Message::Error("Job not found".to_string()))?;
				}
				Ok(DaemonResponse::Error(e)) => {
					output.error(Message::Error(format!("Error: {}", e)))?;
				}
				Err(e) => {
					output.error(Message::Error(format!(
						"Failed to communicate with daemon: {}",
						e
					)))?;
				}
				_ => {
					output.error(Message::Error(
						"Unexpected response from daemon".to_string(),
					))?;
				}
			}
		}

		JobCommands::Monitor {
			job_id,
			exit_on_complete,
		} => {
			monitor_jobs(&mut client, job_id, exit_on_complete, &mut output).await?;
		}

		JobCommands::Pause { id } => {
			let job_id = match id.parse::<Uuid>() {
				Ok(uuid) => uuid,
				Err(_) => {
					output.error(Message::Error("Invalid job ID format".to_string()))?;
					return Ok(());
				}
			};

			match client
				.send_command(DaemonCommand::PauseJob { id: job_id })
				.await
			{
				Ok(DaemonResponse::Ok) => {
					output.success("Job paused successfully")?;
				}
				Ok(DaemonResponse::Error(e)) => {
					output.error(Message::Error(format!("Failed to pause job: {}", e)))?;
				}
				Err(e) => {
					output.error(Message::Error(format!(
						"Failed to communicate with daemon: {}",
						e
					)))?;
				}
				_ => {
					output.error(Message::Error(
						"Unexpected response from daemon".to_string(),
					))?;
				}
			}
		}

		JobCommands::Resume { id } => {
			let job_id = match id.parse::<Uuid>() {
				Ok(uuid) => uuid,
				Err(_) => {
					output.error(Message::Error("Invalid job ID format".to_string()))?;
					return Ok(());
				}
			};

			match client
				.send_command(DaemonCommand::ResumeJob { id: job_id })
				.await
			{
				Ok(DaemonResponse::Ok) => {
					output.success("Job resumed successfully")?;
				}
				Ok(DaemonResponse::Error(e)) => {
					output.error(Message::Error(format!("Failed to resume job: {}", e)))?;
				}
				Err(e) => {
					output.error(Message::Error(format!(
						"Failed to communicate with daemon: {}",
						e
					)))?;
				}
				_ => {
					output.error(Message::Error(
						"Unexpected response from daemon".to_string(),
					))?;
				}
			}
		}

		JobCommands::Cancel { id, yes } => {
			if !yes {
				use dialoguer::Confirm;
				let confirm = Confirm::new()
					.with_prompt(format!("Are you sure you want to cancel job '{}'?", id))
					.default(false)
					.interact()?;

				if !confirm {
					output.info("Operation cancelled")?;
					return Ok(());
				}
			}

			let job_id = match id.parse::<Uuid>() {
				Ok(uuid) => uuid,
				Err(_) => {
					output.error(Message::Error("Invalid job ID format".to_string()))?;
					return Ok(());
				}
			};

			match client
				.send_command(DaemonCommand::CancelJob { id: job_id })
				.await
			{
				Ok(DaemonResponse::Ok) => {
					output.success("Job cancelled successfully")?;
				}
				Ok(DaemonResponse::Error(e)) => {
					output.error(Message::Error(format!("Failed to cancel job: {}", e)))?;
				}
				Err(e) => {
					output.error(Message::Error(format!(
						"Failed to communicate with daemon: {}",
						e
					)))?;
				}
				_ => {
					output.error(Message::Error(
						"Unexpected response from daemon".to_string(),
					))?;
				}
			}
		}

		JobCommands::Clear { failed, yes } => {
			if !yes {
				use dialoguer::Confirm;
				let confirm = Confirm::new()
					.with_prompt(if failed {
						"Are you sure you want to clear failed jobs?"
					} else {
						"Are you sure you want to clear all completed jobs?"
					})
					.default(false)
					.interact()?;

				if !confirm {
					output.info("Operation cancelled")?;
					return Ok(());
				}
			}

			output.error(Message::Error(
				"Clear command not yet implemented".to_string(),
			))?;
			output.info("This command will be available in a future update")?;
		}
	}

	Ok(())
}

/// Monitor jobs through the daemon with CLI output
async fn monitor_jobs(
	client: &mut DaemonClient,
	job_id: Option<String>,
	exit_on_complete: bool,
	output: &mut CliOutput,
) -> Result<(), Box<dyn std::error::Error>> {
	output
		.section()
		.title("Active Job Monitor")
		.text("Press Ctrl+C to exit")
		.empty_line()
		.render()?;

	// Create progress bars for active jobs
	let multi_progress = MultiProgress::new();
	let mut job_bars: HashMap<String, ProgressBar> = HashMap::new();
	let style = progress_styles::basic_style();

	// If monitoring specific job
	if let Some(ref specific_job_id) = job_id {
		output.info(&format!(
			"Monitoring job {}...",
			specific_job_id.chars().take(8).collect::<String>()
		))?;
	} else {
		output.info("Monitoring all jobs...")?;
	}

	let mut jobs_completed = false;

	// Poll for job updates
	loop {
		tokio::select! {
			_ = tokio::time::sleep(std::time::Duration::from_secs(1)) => {
				// Get job list
				match client.send_command(DaemonCommand::ListJobs { status: Some("running".to_string()) }).await {
					Ok(DaemonResponse::Jobs(jobs)) => {
						// Track which jobs are still active
						let mut active_job_ids = std::collections::HashSet::new();
						let mut has_running_jobs = false;

						for job in &jobs {
							// Filter by specific job if requested
							if let Some(ref specific_id) = job_id {
								if !job.id.to_string().starts_with(specific_id) {
									continue;
								}
							}

							active_job_ids.insert(job.id.to_string());
							has_running_jobs = true;

							// Create or update progress bar
							let job_key = job.id.to_string();
							let pb = job_bars.entry(job_key.clone()).or_insert_with(|| {
								let bar = multi_progress.add(ProgressBar::new(100));
								bar.set_style(style.clone());
								bar.set_prefix(format!("[{}]", job.name));
								bar
							});

							// Update progress
							pb.set_position((job.progress * 100.0) as u64);
							pb.set_message(format!(
								"{:.1}% â€¢ {}",
								job.progress * 100.0,
								job.status
							));

							// If job is no longer running, finish the bar
							if job.status == "Completed" {
								pb.finish_with_message("âœ… Completed");
								job_bars.remove(&job_key);

								output.print(Message::JobCompleted {
									id: job.id,
									name: job.name.clone(),
									duration: 0, // TODO: Calculate actual duration
								})?;

								if exit_on_complete {
									jobs_completed = true;
								}
							} else if job.status == "Failed" {
								pb.abandon_with_message("âŒ Failed");
								job_bars.remove(&job_key);

								output.print(Message::JobFailed {
									id: job.id,
									name: job.name.clone(),
									error: "Job failed".to_string(),
								})?;

								if exit_on_complete {
									jobs_completed = true;
								}
							}
						}

						// Clean up progress bars for jobs that are no longer active
						let keys_to_remove: Vec<String> = job_bars.keys()
							.filter(|k| !active_job_ids.contains(*k))
							.cloned()
							.collect();

						for key in keys_to_remove {
							if let Some(pb) = job_bars.remove(&key) {
								pb.finish_and_clear();
							}
						}

						// If we have specific job and no jobs found, exit
						if job_id.is_some() && jobs.is_empty() {
							output.error(Message::Error("Job not found or not running".to_string()))?;
							break;
						}

						// If exit_on_complete is true and no running jobs, exit
						if exit_on_complete && !has_running_jobs {
							if jobs_completed {
								output.success("All monitored jobs completed")?;
							} else {
								output.info("No running jobs found")?;
							}
							break;
						}
					}
					Err(e) => {
						output.error(Message::Error(format!("Error getting job list: {}", e)))?;
						break;
					}
					_ => {}
				}
			}

			_ = tokio::signal::ctrl_c() => {
				output.info("Exiting monitor...")?;
				break;
			}
		}
	}

	Ok(())
}
```

## src/infrastructure/cli/commands/library.rs

```rust
//! Library management commands
//!
//! This module handles CLI commands for managing libraries:
//! - Creating new libraries
//! - Listing existing libraries
//! - Switching between libraries
//! - Getting current library info

use crate::infrastructure::cli::daemon::{DaemonClient, DaemonCommand, DaemonResponse};
use crate::infrastructure::cli::output::messages::LibraryInfo as OutputLibraryInfo;
use crate::infrastructure::cli::output::{CliOutput, Message};
use clap::Subcommand;
use comfy_table::Table;
use std::path::PathBuf;

#[derive(Subcommand, Clone, Debug)]
pub enum LibraryCommands {
	/// Create a new library
	Create {
		/// Library name
		name: String,
		/// Path where to create the library
		#[arg(short, long)]
		path: Option<PathBuf>,
	},

	/// Open and switch to a library
	Open {
		/// Path to the library
		path: PathBuf,
	},

	/// Switch to a different library
	Switch {
		/// Library ID or name
		identifier: String,
	},

	/// List all libraries
	List {
		/// Show detailed information
		#[arg(long)]
		detailed: bool,
	},

	/// Show current library info
	Current,

	/// Close the current library
	Close,

	/// Delete a library
	Delete {
		/// Library ID to delete
		id: String,
		/// Skip confirmation prompt
		#[arg(short, long)]
		yes: bool,
	},
}

pub async fn handle_library_command(
	cmd: LibraryCommands,
	instance_name: Option<String>,
	mut output: CliOutput,
) -> Result<(), Box<dyn std::error::Error>> {
	let mut client = DaemonClient::new_with_instance(instance_name.clone());

	match cmd {
		LibraryCommands::Create { name, path } => {
			output.info(&format!("Creating library '{}'...", name))?;

			match client
				.send_command(DaemonCommand::CreateLibrary {
					name: name.clone(),
					path,
				})
				.await
			{
				Ok(DaemonResponse::LibraryCreated { id, name, path }) => {
					output.print(Message::LibraryCreated { name, id, path })?;
				}
				Ok(DaemonResponse::Error(e)) => {
					output.error(Message::Error(format!("Failed to create library: {}", e)))?;
				}
				Err(e) => {
					output.error(Message::Error(format!(
						"Failed to communicate with daemon: {}",
						e
					)))?;
				}
				_ => {
					output.error(Message::Error(
						"Unexpected response from daemon".to_string(),
					))?;
				}
			}
		}

		LibraryCommands::Open { path } => {
			output.info(&format!("Opening library at {}...", path.display()))?;
			output.error(Message::Error(
				"Open command not yet implemented".to_string(),
			))?;
			output.info("Use 'spacedrive library create' to create a new library")?;
		}

		LibraryCommands::List { detailed } => {
			match client.send_command(DaemonCommand::ListLibraries).await {
				Ok(DaemonResponse::Libraries(libraries)) => {
					if libraries.is_empty() {
						output.print(Message::NoLibrariesFound)?;
					} else {
						let output_libs: Vec<OutputLibraryInfo> = libraries
							.into_iter()
							.map(|lib| OutputLibraryInfo {
								id: lib.id,
								name: lib.name,
								path: lib.path,
							})
							.collect();

						if detailed
							|| matches!(
								output.format(),
								crate::infrastructure::cli::output::OutputFormat::Json
							) {
							output.print(Message::LibraryList {
								libraries: output_libs,
							})?;
						} else {
							// For non-detailed human output, use a table
							let mut table = Table::new();
							table.set_header(vec!["ID", "Name", "Path"]);

							for lib in output_libs {
								table.add_row(vec![
									lib.id.to_string(),
									lib.name,
									lib.path.display().to_string(),
								]);
							}

							output.section().table(table).render()?;
						}
					}
				}
				Ok(DaemonResponse::Error(e)) => {
					output.error(Message::Error(format!("Failed to list libraries: {}", e)))?;
				}
				Err(e) => {
					output.error(Message::Error(format!(
						"Failed to communicate with daemon: {}",
						e
					)))?;
				}
				_ => {
					output.error(Message::Error(
						"Unexpected response from daemon".to_string(),
					))?;
				}
			}
		}

		LibraryCommands::Switch { identifier } => {
			match client
				.send_command(DaemonCommand::SwitchLibrary {
					id: identifier.parse()?,
				})
				.await
			{
				Ok(DaemonResponse::Ok) => {
					output.success("Switched library successfully")?;
				}
				Ok(DaemonResponse::Error(e)) => {
					output.error(Message::Error(format!("Failed to switch library: {}", e)))?;
				}
				Err(e) => {
					output.error(Message::Error(format!(
						"Failed to communicate with daemon: {}",
						e
					)))?;
				}
				_ => {
					output.error(Message::Error(
						"Unexpected response from daemon".to_string(),
					))?;
				}
			}
		}

		LibraryCommands::Current => {
			match client.send_command(DaemonCommand::GetCurrentLibrary).await {
				Ok(DaemonResponse::CurrentLibrary(lib_opt)) => {
					let library = lib_opt.map(|lib| OutputLibraryInfo {
						id: lib.id,
						name: lib.name,
						path: lib.path,
					});
					output.print(Message::CurrentLibrary { library })?;
				}
				Ok(DaemonResponse::Error(e)) => {
					output.error(Message::Error(format!("Error: {}", e)))?;
				}
				Err(e) => {
					output.error(Message::Error(format!(
						"Failed to communicate with daemon: {}",
						e
					)))?;
				}
				_ => {
					output.error(Message::Error(
						"Unexpected response from daemon".to_string(),
					))?;
				}
			}
		}

		LibraryCommands::Close => {
			output.info("Closing current library...")?;
			output.error(Message::Error(
				"Close command not yet implemented".to_string(),
			))?;
			output.info("This command will be available in a future update")?;
		}

		LibraryCommands::Delete { id, yes } => {
			if !yes {
				use dialoguer::Confirm;
				let confirm = Confirm::new()
					.with_prompt(format!("Are you sure you want to delete library '{}'?", id))
					.default(false)
					.interact()?;

				if !confirm {
					output.info("Operation cancelled")?;
					return Ok(());
				}
			}

			output.info(&format!("Deleting library {}...", id))?;
			output.error(Message::Error(
				"Delete command not yet implemented".to_string(),
			))?;
			output.info("This command will be available in a future update")?;
		}
	}

	Ok(())
}
```

## src/infrastructure/cli/commands/system.rs

```rust
//! System monitoring commands
//!
//! This module handles CLI commands for system monitoring and information:
//! - System status checking
//! - Log viewing and following
//! - Real-time monitoring

use crate::infrastructure::cli::daemon::{DaemonClient, DaemonCommand, DaemonConfig, DaemonResponse};
use crate::infrastructure::cli::output::{CliOutput, Message};
use clap::Subcommand;
use std::fs::File;
use std::io::{BufRead, BufReader, Seek, SeekFrom};
use std::time::Duration;
use tokio::time::sleep;

#[derive(Subcommand, Clone, Debug)]
pub enum SystemCommands {
    /// Show system status
    Status,

    /// Monitor daemon logs in real-time
    Logs {
        /// Number of lines to show initially
        #[arg(short, long, default_value = "50")]
        lines: usize,
        /// Follow logs in real-time
        #[arg(short, long)]
        follow: bool,
    },

    /// Monitor all system activity in real-time
    Monitor,
    
    /// Launch Terminal User Interface for real-time monitoring
    Tui,
}

pub async fn handle_system_command(
    cmd: SystemCommands,
    instance_name: Option<String>,
    mut output: CliOutput,
) -> Result<(), Box<dyn std::error::Error>> {
    match cmd {
        SystemCommands::Status => {
            handle_status_command(instance_name, &mut output).await
        }
        SystemCommands::Logs { lines, follow } => {
            handle_logs_command(lines, follow, instance_name, &mut output).await
        }
        SystemCommands::Monitor => {
            handle_monitor_command(instance_name, &mut output).await
        }
        SystemCommands::Tui => {
            handle_tui_command(instance_name, &mut output).await
        }
    }
}

async fn handle_status_command(
    instance_name: Option<String>,
    output: &mut CliOutput,
) -> Result<(), Box<dyn std::error::Error>> {
    let mut client = DaemonClient::new_with_instance(instance_name.clone());
    
    match client.send_command(DaemonCommand::GetStatus).await {
        Ok(DaemonResponse::Status(status)) => {
            let section = output.section()
                .title("System Status")
                .item("Version", &status.version)
                .item("Uptime", &format!("{} seconds", status.uptime_secs));
            
            let section = if let Some(library_id) = status.current_library {
                section.item("Current Library", &library_id.to_string())
            } else {
                section.item("Current Library", "None")
            };
            
            section.item("Active Jobs", &status.active_jobs.to_string())
                .item("Total Locations", &status.total_locations.to_string())
                .empty_line()
                .item("OS", std::env::consts::OS)
                .item("Architecture", std::env::consts::ARCH)
                .render()?;
        }
        Ok(DaemonResponse::Error(e)) => {
            output.error(Message::Error(format!("Failed to get system status: {}", e)))?;
        }
        Err(e) => {
            output.error(Message::Error(format!("Failed to communicate with daemon: {}", e)))?;
        }
        _ => {
            output.error(Message::Error("Unexpected response from daemon".to_string()))?;
        }
    }
    
    Ok(())
}

async fn handle_logs_command(
    lines: usize,
    follow: bool,
    instance_name: Option<String>,
    output: &mut CliOutput,
) -> Result<(), Box<dyn std::error::Error>> {
    // Get the daemon config to find the log file path
    let config = DaemonConfig::new(instance_name.clone());

    let log_file_path = config.log_file.ok_or("No log file configured for daemon")?;

    if !log_file_path.exists() {
        let instance_display = instance_name.as_deref().unwrap_or("default");
        output.error(Message::Error(format!(
            "Log file not found for daemon instance '{}'",
            instance_display
        )))?;
        output.section()
            .text(&format!("Expected at: {}", log_file_path.display()))
            .text("Make sure the daemon is running with logging enabled")
            .render()?;
        return Ok(());
    }

    output.print(Message::LogsShowing { path: log_file_path.clone() })?;
    output.section()
        .text(&format!(
            "Spacedrive Daemon Logs ({}) - Press Ctrl+C to exit",
            instance_name.as_deref().unwrap_or("default")
        ))
        .status("Log file", &log_file_path.display().to_string())
        .text("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        .render()?;

    // Read initial lines
    let file = File::open(&log_file_path)?;
    let reader = BufReader::new(file);
    let all_lines: Vec<String> = reader.lines().collect::<Result<Vec<_>, _>>()?;

    // Show last N lines
    let start_index = if all_lines.len() > lines {
        all_lines.len() - lines
    } else {
        0
    };

    for line in &all_lines[start_index..] {
        // For logs, we intentionally use println! to output directly to stdout
        // This preserves exact log formatting and bypasses the output system's formatting
        println!("{}", format_log_line(line, output));
    }

    if follow {
        // Follow mode - watch for new lines
        let mut file = File::open(&log_file_path)?;
        file.seek(SeekFrom::End(0))?;
        let mut reader = BufReader::new(file);

        loop {
            let mut line = String::new();
            match reader.read_line(&mut line) {
                Ok(0) => {
                    // No new data, sleep and try again
                    sleep(Duration::from_millis(100)).await;
                }
                Ok(_) => {
                    // New line found - use print! for real-time output without newline
                    print!("{}", format_log_line(&line, output));
                }
                Err(e) => {
                    output.error(Message::Error(format!("Error reading log file: {}", e)))?;
                    break;
                }
            }
        }
    }

    Ok(())
}

async fn handle_monitor_command(
    instance_name: Option<String>,
    output: &mut CliOutput,
) -> Result<(), Box<dyn std::error::Error>> {
    // Special case - monitor needs event streaming
    output.info("Job monitor not yet implemented for daemon mode")?;
    output.info("Use 'spacedrive job list' to see current jobs")?;
    Ok(())
}

async fn handle_tui_command(
    instance_name: Option<String>,
    output: &mut CliOutput,
) -> Result<(), Box<dyn std::error::Error>> {
    output.info("Launching Terminal User Interface...")?;
    output.error(Message::Error("TUI command not yet implemented for daemon mode".to_string()))?;
    output.section()
        .text("This command will be available in a future update")
        .text("The TUI will provide real-time monitoring of:")
        .text("- Library overview and statistics")
        .text("- Location management and indexing status")
        .text("- Job progress and monitoring")
        .text("- Event stream and system activity")
        .text("- Interactive controls and navigation")
        .render()?;
    Ok(())
}

fn format_log_line(line: &str, output: &CliOutput) -> String {
    // Basic log formatting - colorize by log level if colors are enabled
    if !output.use_color() {
        return line.to_string();
    }
    
    use owo_colors::OwoColorize;
    if line.contains("ERROR") {
        line.red().to_string()
    } else if line.contains("WARN") {
        line.yellow().to_string()
    } else if line.contains("INFO") {
        line.to_string()
    } else if line.contains("DEBUG") {
        line.dimmed().to_string()
    } else {
        line.to_string()
    }
}```

## src/infrastructure/cli/commands/mod.rs

```rust
pub mod daemon;
pub mod library;
pub mod location;
pub mod job;
pub mod network;
pub mod file;
pub mod system;
pub mod volume;

// Re-export command types for convenience
pub use daemon::DaemonCommands;
pub use library::LibraryCommands;
pub use location::LocationCommands;
pub use job::JobCommands;
pub use network::NetworkCommands;
pub use file::FileCommands;
pub use system::SystemCommands;
pub use volume::VolumeCommands;```

## src/infrastructure/cli/commands/daemon.rs

```rust
//! Daemon lifecycle management commands
//!
//! This module handles CLI commands that manage the daemon itself:
//! - Starting and stopping the daemon
//! - Checking daemon status
//! - Managing multiple daemon instances

use crate::infrastructure::cli::daemon::{
	Daemon, DaemonClient, DaemonCommand, DaemonConfig, DaemonResponse,
};
use crate::infrastructure::cli::output::messages::LibraryInfo as OutputLibraryInfo;
use crate::infrastructure::cli::output::{CliOutput, Message};
use clap::Subcommand;
use comfy_table::Table;
use std::path::PathBuf;

#[derive(Subcommand, Clone, Debug)]
pub enum DaemonCommands {
	/// Start the Spacedrive daemon in the background
	Start {
		/// Run in foreground instead of daemonizing
		#[arg(long)]
		foreground: bool,
		/// Enable networking on startup
		#[arg(long)]
		enable_networking: bool,
	},

	/// Stop the Spacedrive daemon
	Stop {
		/// Remove all data (data directory) after stopping
		#[arg(long)]
		reset: bool,
	},

	/// Check if the daemon is running and show status
	Status,

	/// Manage daemon instances
	#[command(subcommand)]
	Instance(InstanceCommands),
}

#[derive(Subcommand, Clone, Debug)]
pub enum InstanceCommands {
	/// List all daemon instances
	List,
	/// Stop a specific daemon instance
	Stop {
		/// Instance name to stop
		name: String,
	},
	/// Show currently targeted instance
	Current,
}

pub async fn handle_daemon_command(
	cmd: DaemonCommands,
	data_dir: PathBuf,
	instance_name: Option<String>,
	mut output: CliOutput,
) -> Result<(), Box<dyn std::error::Error>> {
	match cmd {
		DaemonCommands::Start {
			foreground,
			enable_networking,
		} => {
			handle_start_daemon(
				data_dir,
				foreground,
				enable_networking,
				instance_name,
				output,
			)
			.await
		}
		DaemonCommands::Stop { reset } => handle_stop_daemon(data_dir, instance_name, reset, output).await,
		DaemonCommands::Status => handle_daemon_status(instance_name, output).await,
		DaemonCommands::Instance(instance_cmd) => {
			handle_instance_command(instance_cmd, output).await
		}
	}
}

async fn handle_start_daemon(
	data_dir: PathBuf,
	foreground: bool,
	enable_networking: bool,
	instance_name: Option<String>,
	mut output: CliOutput,
) -> Result<(), Box<dyn std::error::Error>> {
	if Daemon::is_running_instance(instance_name.clone()) {
		let instance_display = instance_name.as_deref().unwrap_or("default");
		output.warning(&format!(
			"Spacedrive daemon instance '{}' is already running",
			instance_display
		))?;
		return Ok(());
	}

	output.print(Message::DaemonStarting {
		instance: instance_name.as_deref().unwrap_or("default").to_string(),
	})?;

	if foreground {
		// Run in foreground
		if enable_networking {
			// For networking enabled startup, we need a default password
			output.info("Starting daemon with networking enabled...")?;
			output.info("Using master key for secure device authentication.")?;

			match Daemon::new_with_networking_and_instance(data_dir.clone(), instance_name.clone())
				.await
			{
				Ok(daemon) => daemon.start().await?,
				Err(e) => {
					output.error(Message::Error(format!(
						"Failed to start daemon with networking: {}",
						e
					)))?;
					output.info("Falling back to daemon without networking...")?;
					let daemon = Daemon::new_with_instance(data_dir, instance_name.clone()).await?;
					daemon.start().await?;
				}
			}
		} else {
			let daemon = Daemon::new_with_instance(data_dir, instance_name.clone()).await?;
			daemon.start().await?;
		}
	} else {
		// Daemonize (simplified version - in production use proper daemonization)
		use std::process::Command;

		let exe = std::env::current_exe()?;
		let mut cmd = Command::new(exe);
		cmd.arg("daemon")
			.arg("start")
			.arg("--foreground")
			.arg("--data-dir")
			.arg(data_dir);

		if let Some(ref instance) = instance_name {
			cmd.arg("--instance").arg(instance);
		}

		if enable_networking {
			cmd.arg("--enable-networking");
		}

		// Detach from terminal
		#[cfg(unix)]
		{
			use std::os::unix::process::CommandExt;
			cmd.stdin(std::process::Stdio::null())
				.stdout(std::process::Stdio::null())
				.stderr(std::process::Stdio::null());

			unsafe {
				cmd.pre_exec(|| {
					// Create new session
					libc::setsid();
					Ok(())
				});
			}
		}

		cmd.spawn()?;

		// Wait for daemon to be ready (try connecting to socket)
		output.info("Waiting for daemon to initialize...")?;

		if Daemon::wait_for_ready(instance_name.clone(), 10).await? {
			let instance_display = instance_name.as_deref().unwrap_or("default");
			output.success(&format!(
				"Spacedrive daemon instance '{}' started successfully",
				instance_display
			))?;
		} else {
			let instance_display = instance_name.as_deref().unwrap_or("default");
			output.error(Message::Error(format!(
				"Failed to start Spacedrive daemon instance '{}' (timed out waiting for readiness)",
				instance_display
			)))?;
		}
	}

	Ok(())
}

async fn handle_stop_daemon(
	data_dir: PathBuf,
	instance_name: Option<String>,
	reset: bool,
	mut output: CliOutput,
) -> Result<(), Box<dyn std::error::Error>> {
	if !Daemon::is_running_instance(instance_name.clone()) {
		let instance_display = instance_name.as_deref().unwrap_or("default");
		output.warning(&format!(
			"Spacedrive daemon instance '{}' is not running",
			instance_display
		))?;
		return Ok(());
	}

	let instance_display = instance_name.as_deref().unwrap_or("default");
	output.print(Message::DaemonStopping {
		instance: instance_display.to_string(),
	})?;
	Daemon::stop_instance(instance_name.clone()).await?;

	// Wait a bit to ensure it's stopped
	tokio::time::sleep(std::time::Duration::from_secs(1)).await;

	if !Daemon::is_running_instance(instance_name.clone()) {
		output.print(Message::DaemonStopped {
			instance: instance_display.to_string(),
		})?;
		
		// If reset flag is set, remove the data directory
		if reset {
			output.warning("Removing all Spacedrive data...")?;
			
			if data_dir.exists() {
				match std::fs::remove_dir_all(&data_dir) {
					Ok(_) => {
						output.success(&format!(
							"Successfully removed data directory: {}",
							data_dir.display()
						))?;
					}
					Err(e) => {
						output.error(Message::Error(format!(
							"Failed to remove data directory: {}",
							e
						)))?;
						return Err(e.into());
					}
				}
			} else {
				output.info(&format!(
					"Data directory does not exist: {}",
					data_dir.display()
				))?;
			}
		}
	} else {
		output.error(Message::Error(format!(
			"Failed to stop Spacedrive daemon instance '{}'",
			instance_display
		)))?;
	}

	Ok(())
}

async fn handle_daemon_status(
	instance_name: Option<String>,
	mut output: CliOutput,
) -> Result<(), Box<dyn std::error::Error>> {
	let instance_display = instance_name.as_deref().unwrap_or("default");

	if Daemon::is_running_instance(instance_name.clone()) {
		output.success(&format!(
			"Spacedrive daemon instance '{}' is running",
			instance_display
		))?;

		// Try to get more info from daemon
		let client = DaemonClient::new_with_instance(instance_name);

		// Get status
		match client.send_command(DaemonCommand::GetStatus).await {
			Ok(DaemonResponse::Status(status)) => {
				output
					.section()
					.title("Status")
					.status("Version", &status.version)
					.status("Uptime", &format!("{} seconds", status.uptime_secs))
					.status("Active Jobs", &status.active_jobs.to_string())
					.status("Total Locations", &status.total_locations.to_string())
					.render()?;
			}
			Err(e) => {
				output.warning(&format!("Could not get status: {}", e))?;
			}
			_ => {}
		}

		// Get libraries
		match client.send_command(DaemonCommand::ListLibraries).await {
			Ok(DaemonResponse::Libraries(libraries)) => {
				if !libraries.is_empty() {
					output.section().empty_line().title("Libraries").render()?;

					for lib in &libraries {
						output
							.section()
							.text(&format!("   â€¢ {} ({})", lib.name, lib.id))
							.render()?;
					}
				}
			}
			Err(e) => {
				output.warning(&format!("Could not get libraries: {}", e))?;
			}
			_ => {}
		}

		// Get current library
		match client.send_command(DaemonCommand::GetCurrentLibrary).await {
			Ok(DaemonResponse::CurrentLibrary(lib_opt)) => {
				if let Some(lib) = lib_opt {
					output
						.section()
						.empty_line()
						.title("Current Library")
						.item("Name", &lib.name)
						.item("ID", &lib.id.to_string())
						.item("Path", &lib.path.display().to_string())
						.render()?;
				} else {
					output
						.section()
						.empty_line()
						.title("Current Library")
						.text("None selected")
						.render()?;
				}
			}
			Err(e) => {
				output.warning(&format!("Could not get current library: {}", e))?;
			}
			_ => {}
		}
	} else {
		output.error(Message::DaemonNotRunning {
			instance: instance_display.to_string(),
		})?;

		let start_cmd = if instance_name.is_some() {
			format!("spacedrive --instance {} start", instance_display)
		} else {
			"spacedrive start".to_string()
		};

		output
			.section()
			.help()
			.item(&format!("Start it with: {}", start_cmd))
			.render()?;
	}

	Ok(())
}

async fn handle_instance_command(
	cmd: InstanceCommands,
	mut output: CliOutput,
) -> Result<(), Box<dyn std::error::Error>> {
	match cmd {
		InstanceCommands::List => match Daemon::list_instances() {
			Ok(instances) => {
				if instances.is_empty() {
					output.info("No daemon instances found")?;
				} else {
					let mut table = Table::new();
					table.set_header(vec!["Instance", "Status", "Socket Path"]);

					for instance in instances {
						let status = if instance.is_running {
							"Running"
						} else {
							"Stopped"
						};

						table.add_row(vec![
							instance.display_name().to_string(),
							status.to_string(),
							instance.socket_path.display().to_string(),
						]);
					}

					output.section().table(table).render()?;
				}
			}
			Err(e) => {
				output.error(Message::Error(format!("Failed to list instances: {}", e)))?;
			}
		},

		InstanceCommands::Stop { name } => {
			let instance_name = if name == "default" {
				None
			} else {
				Some(name.clone())
			};
			match Daemon::stop_instance(instance_name).await {
				Ok(_) => {
					output.success(&format!("Daemon instance '{}' stopped", name))?;
				}
				Err(e) => {
					output.error(Message::Error(format!(
						"Failed to stop instance '{}': {}",
						name, e
					)))?;
				}
			}
		}

		InstanceCommands::Current => {
			output.info("Current instance functionality not yet implemented")?;
			output.info("Use --instance <name> flag to target specific instances")?;
		}
	}

	Ok(())
}
```

## src/infrastructure/cli/commands/network.rs

```rust
//! Network management commands
//!
//! This module handles CLI commands for networking operations:
//! - Initializing and managing networking services
//! - Device discovery and management
//! - Pairing operations with other devices
//! - Spacedrop file sharing

use crate::infrastructure::cli::daemon::{DaemonClient, DaemonCommand, DaemonResponse};
use crate::infrastructure::cli::output::{CliOutput, Message};
use crate::infrastructure::cli::output::messages::{DeviceInfo as OutputDeviceInfo, DeviceStatus, PairingRequest};
use crate::infrastructure::cli::utils::format_bytes_parts;
use crate::services::networking::DeviceInfo;
use clap::Subcommand;
use comfy_table::{presets::UTF8_FULL, Table};
use std::path::PathBuf;
use uuid::Uuid;

#[derive(Subcommand, Clone, Debug)]
pub enum NetworkCommands {
    /// Initialize networking using master key
    Init,

    /// Start networking services
    Start,

    /// Stop networking services  
    Stop,

    /// List discovered devices
    Devices,

    /// Pairing operations
    Pair {
        #[command(subcommand)]
        action: PairingCommands,
    },

    /// Revoke a paired device
    Revoke {
        /// Device ID to revoke
        device_id: String,
    },

    /// Spacedrop operations
    Spacedrop {
        /// Device ID to send to
        device_id: String,
        /// File path to send
        file_path: PathBuf,
        /// Sender name
        #[arg(short, long)]
        sender: Option<String>,
        /// Optional message
        #[arg(short, long)]
        message: Option<String>,
    },
}

#[derive(Subcommand, Clone, Debug)]
pub enum PairingCommands {
    /// Generate a pairing code and wait for another device to connect (initiator)
    Generate,

    /// Join another device using their pairing code
    Join {
        /// The pairing code from the other device
        code: String,
    },

    /// Show pairing status
    Status,

    /// List pending pairing requests
    ListPending,

    /// Accept a pairing request
    Accept {
        /// Request ID
        request_id: String,
    },

    /// Reject a pairing request
    Reject {
        /// Request ID
        request_id: String,
    },
}

pub async fn handle_network_command(
    cmd: NetworkCommands,
    instance_name: Option<String>,
    mut output: CliOutput,
) -> Result<(), Box<dyn std::error::Error>> {
    let mut client = DaemonClient::new_with_instance(instance_name.clone());

    // Check if daemon is running for most commands
    match &cmd {
        NetworkCommands::Init { .. } => {
            // Init doesn't require daemon to be running
        }
        _ => {
            if !client.is_running() {
                output.error(Message::DaemonNotRunning {
                    instance: instance_name.as_deref().unwrap_or("default").to_string(),
                })?;
                return Ok(());
            }
        }
    }

    match cmd {
        NetworkCommands::Init => {
            match client
                .send_command(DaemonCommand::InitNetworking)
                .await?
            {
                DaemonResponse::Ok => {
                    output.print(Message::NetworkingInitialized)?;
                }
                DaemonResponse::Error(err) => {
                    output.error(Message::Error(err))?;
                }
                _ => {
                    output.error(Message::Error("Unexpected response".to_string()))?;
                }
            }
        }

        NetworkCommands::Start => {
            match client
                .send_command(DaemonCommand::StartNetworking)
                .await?
            {
                DaemonResponse::Ok => {
                    output.print(Message::NetworkingStarted)?;
                }
                DaemonResponse::Error(err) => {
                    output.error(Message::Error(err))?;
                }
                _ => {
                    output.error(Message::Error("Unexpected response".to_string()))?;
                }
            }
        }

        NetworkCommands::Stop => {
            match client
                .send_command(DaemonCommand::StopNetworking)
                .await?
            {
                DaemonResponse::Ok => {
                    output.print(Message::NetworkingStopped)?;
                }
                DaemonResponse::Error(err) => {
                    output.error(Message::Error(err))?;
                }
                _ => {
                    output.error(Message::Error("Unexpected response".to_string()))?;
                }
            }
        }

        NetworkCommands::Devices => {
            match client
                .send_command(DaemonCommand::ListConnectedDevices)
                .await?
            {
                DaemonResponse::ConnectedDevices(devices) => {
                    if devices.is_empty() {
                        output.info("No devices currently connected")?;
                    } else {
                        let output_devices: Vec<OutputDeviceInfo> = devices.into_iter()
                            .map(|device| OutputDeviceInfo {
                                id: device.device_id.to_string(),
                                name: device.device_name.clone(),
                                status: match device.status.as_str() {
                                    "online" => DeviceStatus::Online,
                                    "offline" => DeviceStatus::Offline,
                                    "paired" => DeviceStatus::Paired,
                                    "discovered" => DeviceStatus::Discovered,
                                    _ => DeviceStatus::Offline,
                                },
                                peer_id: None, // TODO: Get from daemon if available
                            })
                            .collect();
                        
                        if matches!(output.format(), crate::infrastructure::cli::output::OutputFormat::Json) {
                            output.print(Message::DevicesList { devices: output_devices })?;
                        } else {
                            // For human output, use a table
                            let mut table = Table::new();
                            table.load_preset(UTF8_FULL);
                            table.set_header(vec!["Device ID", "Name", "Status"]);

                            for device in output_devices {
                                table.add_row(vec![
                                    &device.id[..8.min(device.id.len())],
                                    &device.name,
                                    &format!("{:?}", device.status),
                                ]);
                            }

                            output.section()
                                .title("Connected Devices")
                                .table(table)
                                .render()?;
                        }
                    }
                }
                DaemonResponse::Error(err) => {
                    output.error(Message::Error(err))?;
                }
                _ => {
                    output.error(Message::Error("Unexpected response".to_string()))?;
                }
            }
        }

        NetworkCommands::Revoke { device_id } => {
            // Parse the device ID string to UUID
            let device_uuid = match device_id.parse::<Uuid>() {
                Ok(uuid) => uuid,
                Err(_) => {
                    output.error(Message::Error("Invalid device ID format".to_string()))?;
                    return Ok(());
                }
            };
            
            match client
                .send_command(DaemonCommand::RevokeDevice { device_id: device_uuid })
                .await?
            {
                DaemonResponse::Ok => {
                    output.success(&format!("Device {} revoked", device_id))?;
                }
                DaemonResponse::Error(err) => {
                    output.error(Message::Error(err))?;
                }
                _ => {
                    output.error(Message::Error("Unexpected response".to_string()))?;
                }
            }
        }

        NetworkCommands::Spacedrop {
            device_id,
            file_path,
            sender,
            message,
        } => {
            // Parse device_id to UUID
            let device_uuid = match device_id.parse::<Uuid>() {
                Ok(uuid) => uuid,
                Err(_) => {
                    output.error(Message::Error("Invalid device ID format".to_string()))?;
                    return Ok(());
                }
            };
            
            // Use sender name or default
            let sender_name = sender.unwrap_or_else(|| "Anonymous".to_string());
            
            match client
                .send_command(DaemonCommand::SendSpacedrop {
                    device_id: device_uuid,
                    file_path: file_path.to_string_lossy().to_string(),
                    sender_name: sender_name.clone(),
                    message,
                })
                .await?
            {
                DaemonResponse::SpacedropStarted { transfer_id } => {
                    output.print(Message::SpacedropSent {
                        file_name: file_path.file_name().unwrap_or_default().to_string_lossy().to_string(),
                        device_name: device_id.clone(),
                    })?;
                    output.info(&format!("Transfer ID: {}", transfer_id))?;
                }
                DaemonResponse::Error(err) => {
                    output.error(Message::Error(err))?;
                }
                _ => {
                    output.error(Message::Error("Unexpected response".to_string()))?;
                }
            }
        }

        NetworkCommands::Pair { action } => {
            handle_pairing_command(action, &client, &mut output).await?;
        }
    }

    Ok(())
}

/// Handle pairing-related CLI commands through the daemon
async fn handle_pairing_command(
    action: PairingCommands,
    client: &DaemonClient,
    output: &mut CliOutput,
) -> Result<(), Box<dyn std::error::Error>> {
    match action {
        PairingCommands::Generate => {
            output.info("Generating pairing code...")?;

            match client
                .send_command(DaemonCommand::StartPairingAsInitiator)
                .await?
            {
                DaemonResponse::PairingCodeGenerated {
                    code,
                    expires_in_seconds,
                } => {
                    output.print(Message::PairingCodeGenerated { code: code.clone() })?;
                    
                    output.section()
                        .empty_line()
                        .text(&format!("This code expires in {} seconds", expires_in_seconds))
                        .empty_line()
                        .help()
                            .item(&format!("The other device should run: spacedrive network pair join \"{}\"", code))
                            .item("Pairing will auto-accept valid requests")
                        .render()?;
                }
                DaemonResponse::Error(err) => {
                    output.error(Message::Error(format!("Failed to generate pairing code: {}", err)))?;
                }
                _ => {
                    output.error(Message::Error("Unexpected response from daemon".to_string()))?;
                }
            }
        }

        PairingCommands::Join { code } => {
            output.info("Joining pairing session...")?;
            let code_preview = code.split_whitespace()
                .take(3)
                .collect::<Vec<_>>()
                .join(" ");
            output.info(&format!("Code: {}...", code_preview))?;

            match client
                .send_command(DaemonCommand::StartPairingAsJoiner { code: code.clone() })
                .await?
            {
                DaemonResponse::PairingInProgress => {
                    output.print(Message::PairingInProgress { 
                        device_name: "remote device".to_string() 
                    })?;
                    output.info("Pairing process started - this may take a few moments...")?;

                    // Monitor pairing status
                    output.info("")?;
                    output.info("Monitoring pairing progress...")?;

                    let mut attempts = 0;
                    let max_attempts = 30; // 30 seconds timeout

                    loop {
                        if attempts >= max_attempts {
                            output.warning("Pairing monitoring timed out")?;
                            output.info("Use 'spacedrive network pair status' to check final result")?;
                            break;
                        }

                        tokio::time::sleep(std::time::Duration::from_secs(1)).await;
                        attempts += 1;

                        // Check pairing status with improved error handling
                        match client.send_command(DaemonCommand::GetPairingStatus).await {
                            Ok(DaemonResponse::PairingStatus {
                                status,
                                remote_device,
                            }) => {
                                match status.as_str() {
                                    "completed" => {
                                        if let Some(device) = remote_device {
                                            output.print(Message::PairingSuccess {
                                                device_name: device.device_name,
                                                device_id: device.device_id.to_string(),
                                            })?;
                                        } else {
                                            output.success("Pairing completed successfully!")?;
                                        }
                                        break;
                                    }
                                    s if s.contains("failed") => {
                                        output.print(Message::PairingFailed { reason: s.to_string() })?;
                                        break;
                                    }
                                    "cancelled" | "canceled" => {
                                        output.warning("Pairing was cancelled")?;
                                        break;
                                    }
                                    s @ ("in_progress" | "waiting" | "connecting"
                                    | "authenticating") => {
                                        // Still in progress - show periodic updates
                                        if attempts % 5 == 0 {
                                            output.info(&format!(
                                                "Still pairing... ({}/{}) [{}]",
                                                attempts, max_attempts, s
                                            ))?;
                                        }
                                    }
                                    s => {
                                        // Unknown status - log and continue
                                        if attempts % 10 == 0 {
                                            output.info(&format!(
                                                "Status: {} ({}/{})",
                                                s, attempts, max_attempts
                                            ))?;
                                        }
                                    }
                                }
                            }
                            Ok(DaemonResponse::Error(err)) => {
                                output.error(Message::Error(format!("Daemon error: {}", err)))?;
                                break;
                            }
                            Ok(_) => {
                                // Unexpected response type
                                if attempts % 20 == 0 {
                                    output.warning(&format!(
                                        "Unexpected response type from daemon ({}/{})",
                                        attempts, max_attempts
                                    ))?;
                                }
                            }
                            Err(e) => {
                                // Network/communication error
                                if attempts % 15 == 0 {
                                    output.warning(&format!(
                                        "Connection issue: {} ({}/{})",
                                        e, attempts, max_attempts
                                    ))?;
                                }
                                // Continue trying - daemon might be busy
                            }
                        }
                    }
                }
                DaemonResponse::Error(err) => {
                    output.error(Message::Error(format!("Failed to join pairing session: {}", err)))?;
                }
                response => {
                    output.error(Message::Error(format!(
                        "Unexpected response from daemon: {:?}",
                        response
                    )))?;
                }
            }
        }

        PairingCommands::Status => {
            // Show current pairing status first
            output.info("Checking pairing status...")?;
            output.info("")?;

            match client.send_command(DaemonCommand::GetPairingStatus).await {
                Ok(DaemonResponse::PairingStatus {
                    status,
                    remote_device,
                }) => {
                    let section = output.section()
                        .title("Current Pairing Status")
                        .item("Status", &status);
                    
                    let section = if let Some(device) = remote_device {
                        section.item("Connected Device", &format!("{} ({})", 
                            device.device_name,
                            &device.device_id.to_string()[..8]
                        ))
                    } else {
                        section
                    };
                    section.render()?;
                }
                Ok(DaemonResponse::Error(err)) => {
                    output.warning(&format!("Status check error: {}", err))?;
                }
                _ => {
                    output.warning("Could not determine current status")?;
                }
            }

            // Show pending requests
            match client
                .send_command(DaemonCommand::ListPendingPairings)
                .await?
            {
                DaemonResponse::PendingPairings(requests) => {
                    if requests.is_empty() {
                        output.info("No pending pairing requests")?;
                        output.section()
                            .title("To start pairing:")
                            .help()
                                .item("Generate a code: spacedrive network pair generate")
                                .item("Join with a code: spacedrive network pair join \"<code>\"")
                            .render()?;
                    } else {
                        let pending_requests: Vec<PairingRequest> = requests.into_iter()
                            .map(|req| PairingRequest {
                                id: req.request_id.to_string(),
                                device_name: req.device_name,
                                timestamp: 0, // TODO: Parse from received_at string
                            })
                            .collect();
                        
                        output.print(Message::PairingStatus {
                            status: "active".to_string(),
                            pending_requests,
                        })?;
                    }
                }
                DaemonResponse::Error(err) => {
                    output.error(Message::Error(format!("Failed to get pairing status: {}", err)))?;
                }
                _ => {
                    output.error(Message::Error("Unexpected response from daemon".to_string()))?;
                }
            }
        }

        PairingCommands::ListPending => {
            // List pending pairing requests
            match client
                .send_command(DaemonCommand::ListPendingPairings)
                .await?
            {
                DaemonResponse::PendingPairings(requests) => {
                    if requests.is_empty() {
                        output.info("No pending pairing requests")?;
                    } else {
                        let mut table = Table::new();
                        table.load_preset(UTF8_FULL);
                        table.set_header(vec![
                            "Request ID",
                            "Device ID",
                            "Device Name",
                            "Received At",
                        ]);

                        for request in &requests {
                            table.add_row(vec![
                                &request.request_id.to_string()[..8],
                                &request.device_id.to_string()[..8],
                                &request.device_name,
                                &request.received_at,
                            ]);
                        }

                        output.section()
                            .title("Pending Pairing Requests")
                            .table(table)
                            .empty_line()
                            .help()
                                .item("To accept a request: spacedrive network pair accept <request_id>")
                                .item("To reject a request: spacedrive network pair reject <request_id>")
                            .render()?;
                    }
                }
                DaemonResponse::Error(err) => {
                    output.error(Message::Error(format!("Failed to list pending pairings: {}", err)))?;
                }
                _ => {
                    output.error(Message::Error("Unexpected response from daemon".to_string()))?;
                }
            }
        }

        PairingCommands::Accept { request_id } => {
            let request_uuid = match request_id.parse::<Uuid>() {
                Ok(id) => id,
                Err(_) => {
                    output.error(Message::Error("Invalid request ID format".to_string()))?;
                    return Ok(());
                }
            };
            
            match client
                .send_command(DaemonCommand::AcceptPairing { request_id: request_uuid })
                .await?
            {
                DaemonResponse::Ok => {
                    output.success(&format!("Pairing request {} accepted", request_id))?;
                }
                DaemonResponse::Error(err) => {
                    output.error(Message::Error(format!("Failed to accept pairing request: {}", err)))?;
                }
                _ => {
                    output.error(Message::Error("Unexpected response from daemon".to_string()))?;
                }
            }
        }

        PairingCommands::Reject { request_id } => {
            let request_uuid = match request_id.parse::<Uuid>() {
                Ok(id) => id,
                Err(_) => {
                    output.error(Message::Error("Invalid request ID format".to_string()))?;
                    return Ok(());
                }
            };
            
            match client
                .send_command(DaemonCommand::RejectPairing { request_id: request_uuid })
                .await?
            {
                DaemonResponse::Ok => {
                    output.success(&format!("Pairing request {} rejected", request_id))?;
                }
                DaemonResponse::Error(err) => {
                    output.error(Message::Error(format!("Failed to reject pairing request: {}", err)))?;
                }
                _ => {
                    output.error(Message::Error("Unexpected response from daemon".to_string()))?;
                }
            }
        }
    }

    Ok(())
}

/// Helper function to format file transfer progress
pub fn format_transfer_progress(bytes_transferred: u64, total_bytes: u64) -> String {
    if total_bytes == 0 {
        return "Unknown".to_string();
    }

    let percentage = (bytes_transferred as f64 / total_bytes as f64) * 100.0;
    let (transferred_size, transferred_unit) = format_bytes_parts(bytes_transferred);
    let (total_size, total_unit) = format_bytes_parts(total_bytes);

    format!(
        "{:.1}% ({:.1} {} / {:.1} {})",
        percentage, transferred_size, transferred_unit, total_size, total_unit
    )
}```

## src/infrastructure/cli/commands/location.rs

```rust
//! Location management commands
//!
//! This module handles CLI commands for managing locations:
//! - Adding new locations to libraries
//! - Listing existing locations
//! - Removing locations
//! - Rescanning locations for changes

use crate::infrastructure::cli::daemon::{DaemonClient, DaemonCommand, DaemonResponse};
use crate::infrastructure::cli::output::messages::LocationInfo as OutputLocationInfo;
use crate::infrastructure::cli::output::{CliOutput, Message};
use clap::{Subcommand, ValueEnum};
use comfy_table::Table;
use std::path::PathBuf;
use uuid::Uuid;

// Re-export from the commands module for consistency
#[derive(Clone, Debug, ValueEnum)]
pub enum CliIndexMode {
	/// Only metadata (fast)
	Shallow,
	/// Metadata + content hashing
	Content,
	/// Full analysis including media metadata
	Deep,
}

#[derive(Subcommand, Clone, Debug)]
pub enum LocationCommands {
	/// Add a new location to the current library
	Add {
		/// Path to add as a location
		path: PathBuf,
		/// Custom name for the location
		#[arg(short, long)]
		name: Option<String>,
		/// Indexing mode
		#[arg(short, long, value_enum, default_value = "content")]
		mode: CliIndexMode,
	},

	/// List all locations in the current library
	List,

	/// Get information about a specific location
	Info {
		/// Location ID or path
		identifier: String,
	},

	/// Remove a location from the library
	Remove {
		/// Location ID or path
		identifier: String,
		/// Skip confirmation prompt
		#[arg(short, long)]
		yes: bool,
	},

	/// Rescan a location for changes
	Rescan {
		/// Location ID or path
		identifier: String,
		/// Force full rescan
		#[arg(short, long)]
		force: bool,
	},
}

pub async fn handle_location_command(
	cmd: LocationCommands,
	instance_name: Option<String>,
	mut output: CliOutput,
) -> Result<(), Box<dyn std::error::Error>> {
	let mut client = DaemonClient::new_with_instance(instance_name.clone());

	match cmd {
		LocationCommands::Add { path, name, mode } => {
			output.info(&format!("Adding location {}...", path.display()))?;

			match client
				.send_command(DaemonCommand::AddLocation {
					path: path.clone(),
					name,
				})
				.await
			{
				Ok(DaemonResponse::LocationAdded {
					location_id,
					job_id,
				}) => {
					output.print(Message::LocationAdded {
						path: path.clone(),
						id: location_id,
					})?;

					if !job_id.is_empty() {
						output
							.section()
							.item("Job ID", &job_id.chars().take(8).collect::<String>())
							.empty_line()
							.text("Indexing started...")
							.help()
							.item("To monitor detailed progress, run: spacedrive job monitor")
							.end()
							.render()?;

						// Show basic progress by checking job status periodically
						let mut last_status = String::new();
						for _ in 0..10 {
							// Check for 10 seconds
							tokio::time::sleep(std::time::Duration::from_secs(1)).await;

							if let Ok(uuid) = job_id.parse::<uuid::Uuid>() {
								match client
									.send_command(DaemonCommand::GetJobInfo { id: uuid })
									.await
								{
									Ok(DaemonResponse::JobInfo(Some(job))) => {
										if job.status != last_status {
											output.print(Message::JobProgress {
												id: uuid,
												name: "Indexing".to_string(),
												progress: job.progress,
												message: Some(job.status.clone()),
											})?;
											last_status = job.status.clone();
										}

										if job.status == "completed" || job.status == "failed" {
											break;
										}
									}
									_ => {}
								}
							}
						}
					} else {
						output.warning("Location added but indexing failed to start")?;
					}
				}
				Ok(DaemonResponse::Error(e)) => {
					output.error(Message::Error(format!("Failed to add location: {}", e)))?;
				}
				Err(e) => {
					output.error(Message::Error(format!(
						"Failed to communicate with daemon: {}",
						e
					)))?;
				}
				_ => {
					output.error(Message::Error(
						"Unexpected response from daemon".to_string(),
					))?;
				}
			}
		}

		LocationCommands::List => {
			match client.send_command(DaemonCommand::ListLocations).await {
				Ok(DaemonResponse::Locations(locations)) => {
					if locations.is_empty() {
						output.info(
							"No locations found. Add one with: spacedrive location add <path>",
						)?;
					} else {
						let output_locations: Vec<OutputLocationInfo> = locations
							.into_iter()
							.map(|loc| OutputLocationInfo {
								id: loc.id,
								path: loc.path,
								indexed_files: 0, // TODO: Get actual count from daemon
							})
							.collect();

						if matches!(
							output.format(),
							crate::infrastructure::cli::output::OutputFormat::Json
						) {
							output.print(Message::LocationList {
								locations: output_locations,
							})?;
						} else {
							// For human output, use a table
							let mut table = Table::new();
							table.set_header(vec!["ID", "Path", "Files"]);

							for loc in output_locations {
								table.add_row(vec![
									loc.id.to_string(),
									loc.path.display().to_string(),
									loc.indexed_files.to_string(),
								]);
							}

							output.section().table(table).render()?;
						}
					}
				}
				Ok(DaemonResponse::Error(e)) => {
					output.error(Message::Error(format!("Failed to list locations: {}", e)))?;
				}
				Err(e) => {
					output.error(Message::Error(format!(
						"Failed to communicate with daemon: {}",
						e
					)))?;
				}
				_ => {
					output.error(Message::Error(
						"Unexpected response from daemon".to_string(),
					))?;
				}
			}
		}

		LocationCommands::Remove { identifier, yes } => {
			if !yes {
				use dialoguer::Confirm;
				let confirm = Confirm::new()
					.with_prompt(format!(
						"Are you sure you want to remove location '{}'?",
						identifier
					))
					.default(false)
					.interact()?;

				if !confirm {
					output.info("Operation cancelled")?;
					return Ok(());
				}
			}
			output.info(&format!("Removing location {}...", identifier))?;

			// Try to parse as UUID
			let id = match identifier.parse::<Uuid>() {
				Ok(id) => id,
				Err(_) => {
					output.error(Message::Error(format!(
						"Invalid location ID: {}",
						identifier
					)))?;
					return Ok(());
				}
			};

			match client
				.send_command(DaemonCommand::RemoveLocation { id })
				.await
			{
				Ok(DaemonResponse::Ok) => {
					output.success("Location removed successfully")?;
				}
				Ok(DaemonResponse::Error(e)) => {
					output.error(Message::Error(format!("Failed to remove location: {}", e)))?;
				}
				Err(e) => {
					output.error(Message::Error(format!(
						"Failed to communicate with daemon: {}",
						e
					)))?;
				}
				_ => {
					output.error(Message::Error(
						"Unexpected response from daemon".to_string(),
					))?;
				}
			}
		}

		LocationCommands::Rescan { identifier, force } => {
			output.info(&format!("Rescanning location {}...", identifier))?;

			// Try to parse as UUID
			let id = match identifier.parse::<Uuid>() {
				Ok(id) => id,
				Err(_) => {
					output.error(Message::Error(format!(
						"Invalid location ID: {}",
						identifier
					)))?;
					return Ok(());
				}
			};

			match client
				.send_command(DaemonCommand::RescanLocation { id })
				.await
			{
				Ok(DaemonResponse::Ok) => {
					output.success("Rescan started successfully")?;
					output.info("Use 'spacedrive job monitor' to track progress")?;
				}
				Ok(DaemonResponse::Error(e)) => {
					output.error(Message::Error(format!("Failed to rescan location: {}", e)))?;
				}
				Err(e) => {
					output.error(Message::Error(format!(
						"Failed to communicate with daemon: {}",
						e
					)))?;
				}
				_ => {
					output.error(Message::Error(
						"Unexpected response from daemon".to_string(),
					))?;
				}
			}
		}

		LocationCommands::Info { identifier } => {
			output.error(Message::Error(
				"Location info command not yet implemented".to_string(),
			))?;
		}
	}

	Ok(())
}
```

## src/infrastructure/cli/commands/volume.rs

```rust
//! Volume CLI commands

use crate::infrastructure::cli::daemon::{DaemonClient, DaemonCommand, DaemonResponse};
use crate::infrastructure::cli::output::{CliOutput, Message};
use crate::volume::types::VolumeFingerprint;
use clap::Subcommand;
use comfy_table::Table;
use serde::{Deserialize, Serialize};
use uuid::Uuid;

#[derive(Debug, Clone, clap::ValueEnum, Serialize, Deserialize)]
pub enum VolumeTypeFilter {
	Primary,
	UserData,
	External,
	Secondary,
	System,
	Network,
	Unknown,
}

/// Volume management commands
#[derive(Debug, Clone, Subcommand, Serialize, Deserialize)]
pub enum VolumeCommands {
	/// List all volumes
	List {
		/// Include system volumes (hidden by default)
		#[arg(long)]
		include_system: bool,

		/// Filter by volume type
		#[arg(long, value_enum)]
		type_filter: Option<VolumeTypeFilter>,

		/// Show volume type classifications
		#[arg(long)]
		show_types: bool,

		/// Include tracked volumes that are currently offline/disconnected
		#[arg(long)]
		show_offline: bool,
	},
	/// Show details for a specific volume
	Get {
		/// Volume fingerprint
		fingerprint: String,
	},
	/// Track a volume in a library
	Track {
		/// Volume fingerprint
		fingerprint: String,
		/// Optional name for the tracked volume
		#[arg(short, long)]
		name: Option<String>,
	},
	/// Untrack a volume from a library
	Untrack {
		/// Volume fingerprint
		fingerprint: String,
	},
	/// Run speed test on a volume
	SpeedTest {
		/// Volume fingerprint
		fingerprint: String,
	},
	/// Refresh volume list
	Refresh,
	/// Fix empty display names for tracked volumes
	FixNames,
}

pub async fn handle_volume_command(
	cmd: VolumeCommands,
	instance_name: Option<String>,
	mut output: CliOutput,
) -> Result<(), Box<dyn std::error::Error>> {
	let mut client = DaemonClient::new_with_instance(instance_name.clone());

	match cmd {
		VolumeCommands::List {
			include_system,
			type_filter,
			show_types,
			show_offline,
		} => {
			output.info("Fetching volumes...")?;

			match client
				.send_command(DaemonCommand::Volume(VolumeCommands::List {
					include_system,
					type_filter: type_filter.clone(),
					show_types,
					show_offline,
				}))
				.await
			{
				Ok(DaemonResponse::VolumeListWithTracking(volume_items)) => {
					if volume_items.is_empty() {
						output.info("No volumes found")?;
					} else {
						output.info(&format!("Found {} volume(s):", volume_items.len()))?;

						let mut table = Table::new();
						let mut headers = vec![
							"ID",
							"Name",
							"Mount Point",
							"File System",
							"Capacity",
							"Available",
							"Status",
							"Tracked",
						];

						if show_types {
							headers.insert(4, "Type");
						}

						table.set_header(headers);

						for volume_item in volume_items {
							let volume = &volume_item.volume;
							let is_tracked = volume_item.is_tracked;
							let tracked_name = volume_item.tracked_name.as_deref();
							let is_online = volume_item.is_online;

							// Apply filtering based on CLI flags
							let is_user_visible = volume.is_user_visible;
							let volume_type_str = format!("{:?}", volume.volume_type);

							// Skip system volumes unless --include-system is specified
							if !include_system && !is_user_visible {
								continue;
							}

							// Apply type filter if specified
							if let Some(ref filter) = type_filter {
								let filter_str = format!("{:?}", filter);
								if volume_type_str != filter_str {
									continue;
								}
							}

							let name = &volume.name;
							let mount_point = volume.mount_point.to_string_lossy();
							let file_system = format!("{:?}", volume.file_system);
							let total_capacity = volume.total_bytes_capacity;
							let available_space = volume.total_bytes_available;

							// Use is_online for tracked volumes, fall back to is_mounted for others
							let is_connected = if is_tracked {
								is_online
							} else {
								volume.is_mounted
							};

							// Get short ID from fingerprint
							let short_id = volume.fingerprint.short_id();

							// Format capacity in a human-readable way
							let capacity_str = if total_capacity > 0 {
								format_bytes(total_capacity)
							} else {
								"Unknown".to_string()
							};

							let available_str = if available_space > 0 {
								let formatted = format_bytes(available_space);
								if !is_connected {
									// For offline volumes, indicate the data is cached
									format!("{} (cached)", formatted)
								} else {
									formatted
								}
							} else {
								"Unknown".to_string()
							};

							let status = if is_connected { "Online" } else { "Offline" };

							let tracked_status = if is_tracked {
								if let Some(custom_name) = tracked_name {
									format!("Yes ({})", custom_name)
								} else {
									"Yes".to_string()
								}
							} else {
								"No".to_string()
							};

							let mut row = vec![
								short_id,
								name.to_string(),
								mount_point.to_string(),
								file_system,
							];

							if show_types {
								// Get display name for the volume type
								let type_display = match volume.volume_type {
									crate::volume::types::VolumeType::Primary => "[PRI]",
									crate::volume::types::VolumeType::UserData => "[USR]",
									crate::volume::types::VolumeType::External => "[EXT]",
									crate::volume::types::VolumeType::Secondary => "[SEC]",
									crate::volume::types::VolumeType::System => "[SYS]",
									crate::volume::types::VolumeType::Network => "[NET]",
									crate::volume::types::VolumeType::Unknown => "[UNK]",
								};
								row.push(type_display.to_string());
							}

							row.extend_from_slice(&[
								capacity_str,
								available_str,
								status.to_string(),
								tracked_status,
							]);

							table.add_row(row);
						}

						output.section().table(table).render()?;
					}
				}
				Ok(DaemonResponse::VolumeList(volumes)) => {
					// Fallback for when no current library is set
					if volumes.is_empty() {
						output.info("No volumes found")?;
					} else {
						output.info(&format!("Found {} volume(s):", volumes.len()))?;
						output.info("ðŸ’¡ Set a current library to see tracking status: spacedrive library switch <id>")?;

						let mut table = Table::new();
						table.set_header(vec![
							"Name",
							"Mount Point",
							"File System",
							"Capacity",
							"Available",
							"Status",
						]);

						for volume in volumes {
							let capacity_str = if volume.total_bytes_capacity > 0 {
								format_bytes(volume.total_bytes_capacity)
							} else {
								"Unknown".to_string()
							};

							let available_str = if volume.total_bytes_available > 0 {
								format_bytes(volume.total_bytes_available)
							} else {
								"Unknown".to_string()
							};

							let status = if volume.is_mounted {
								"Mounted"
							} else {
								"Unmounted"
							};

							table.add_row(vec![
								volume.name,
								volume.mount_point.display().to_string(),
								volume.file_system.to_string(),
								capacity_str,
								available_str,
								status.to_string(),
							]);
						}

						output.section().table(table).render()?;
					}
				}
				Ok(DaemonResponse::Error(msg)) => {
					output.error(Message::Error(msg))?;
				}
				Ok(_) => {
					output.error(Message::Error("Unexpected response".to_string()))?;
				}
				Err(e) => {
					output.error(Message::Error(format!(
						"Failed to communicate with daemon: {}",
						e
					)))?;
				}
			}
		}

		VolumeCommands::Get { fingerprint } => {
			output.info(&format!("Fetching volume {}...", fingerprint))?;

			match client
				.send_command(DaemonCommand::Volume(VolumeCommands::Get {
					fingerprint: fingerprint.clone(),
				}))
				.await
			{
				Ok(DaemonResponse::Volume(volume)) => {
					output.info(&format!("Volume: {}", volume.name))?;
					output.info(&format!("  Fingerprint: {}", volume.fingerprint.0))?;
					output.info(&format!("  Mount Point: {}", volume.mount_point.display()))?;
					output.info(&format!("  File System: {}", volume.file_system))?;
					output.info(&format!(
						"  Total Capacity: {} bytes",
						volume.total_bytes_capacity
					))?;
					output.info(&format!(
						"  Available Space: {} bytes",
						volume.total_bytes_available
					))?;
					output.info(&format!(
						"  Status: {}",
						if volume.is_mounted {
							"mounted"
						} else {
							"unmounted"
						}
					))?;
				}
				Ok(DaemonResponse::Error(msg)) => {
					output.error(Message::Error(msg))?;
				}
				Ok(_) => {
					output.error(Message::Error("Unexpected response".to_string()))?;
				}
				Err(e) => {
					output.error(Message::Error(format!(
						"Failed to communicate with daemon: {}",
						e
					)))?;
				}
			}
		}

		VolumeCommands::Track { fingerprint, name } => {
			let mut client = DaemonClient::new_with_instance(instance_name.clone());

			// Check if fingerprint looks like a short ID
			let resolved_fingerprint = if VolumeFingerprint::is_short_id(&fingerprint)
				|| VolumeFingerprint::is_medium_id(&fingerprint)
			{
				// TODO: Add short ID resolution via daemon
				output.info(&format!("ðŸ” Resolving short ID '{}'...", fingerprint))?;
				fingerprint.clone() // For now, pass through - daemon will handle resolution
			} else {
				fingerprint.clone()
			};

			match client
				.send_command(DaemonCommand::Volume(VolumeCommands::Track {
					fingerprint: resolved_fingerprint,
					name,
				}))
				.await
			{
				Ok(DaemonResponse::ActionOutput(action_output)) => {
					output.info(&format!("Successfully tracked volume {}", fingerprint))?;
					output.info(&format!("Action completed: {}", action_output))?;
				}
				Ok(DaemonResponse::Error(msg)) => {
					output.error(Message::Error(msg))?;
				}
				Ok(_) => {
					output.error(Message::Error("Unexpected response".to_string()))?;
				}
				Err(e) => {
					output.error(Message::Error(format!(
						"Failed to communicate with daemon: {}",
						e
					)))?;
				}
			}
		}

		VolumeCommands::Untrack { fingerprint } => {
			output.info(&format!("Untracking volume {}...", fingerprint))?;

			match client
				.send_command(DaemonCommand::Volume(VolumeCommands::Untrack {
					fingerprint: fingerprint.clone(),
				}))
				.await
			{
				Ok(DaemonResponse::ActionOutput(action_output)) => {
					output.info(&format!("Successfully untracked volume {}", fingerprint))?;
					output.info(&format!("Action completed: {}", action_output))?;
				}
				Ok(DaemonResponse::Error(msg)) => {
					output.error(Message::Error(msg))?;
				}
				Ok(_) => {
					output.error(Message::Error("Unexpected response".to_string()))?;
				}
				Err(e) => {
					output.error(Message::Error(format!(
						"Failed to communicate with daemon: {}",
						e
					)))?;
				}
			}
		}

		VolumeCommands::SpeedTest { fingerprint } => {
			output.info(&format!("Running speed test on volume {}...", fingerprint))?;
			output.info("This may take a moment...")?;

			match client
				.send_command(DaemonCommand::Volume(VolumeCommands::SpeedTest {
					fingerprint: fingerprint.clone(),
				}))
				.await
			{
				Ok(DaemonResponse::ActionOutput(action_output)) => {
					output.info(&format!("Speed test completed for volume {}", fingerprint))?;
					output.info(&format!("Action completed: {}", action_output))?;
				}
				Ok(DaemonResponse::Error(msg)) => {
					output.error(Message::Error(msg))?;
				}
				Ok(_) => {
					output.error(Message::Error("Unexpected response".to_string()))?;
				}
				Err(e) => {
					output.error(Message::Error(format!(
						"Failed to communicate with daemon: {}",
						e
					)))?;
				}
			}
		}

		VolumeCommands::Refresh => {
			let daemon_cmd = DaemonCommand::Volume(VolumeCommands::Refresh);
			let response = client.send_command(daemon_cmd).await?;

			match response {
				DaemonResponse::VolumeList(volumes) => {
					println!("â„¹ï¸  Refreshed {} volume(s)", volumes.len());
				}
				DaemonResponse::Error(err) => {
					return Err(format!("Failed to refresh volumes: {}", err).into());
				}
				_ => {
					return Err("Unexpected response from daemon".into());
				}
			}
		}

		VolumeCommands::FixNames => {
			let daemon_cmd = DaemonCommand::Volume(VolumeCommands::FixNames);
			let response = client.send_command(daemon_cmd).await?;

			match response {
				DaemonResponse::Ok => {
					println!("âœ… Fixed display names for tracked volumes");
				}
				DaemonResponse::Error(err) => {
					return Err(format!("Failed to fix display names: {}", err).into());
				}
				_ => {
					return Err("Unexpected response from daemon".into());
				}
			}
		}
	}

	Ok(())
}

/// Format bytes in a human-readable way
fn format_bytes(bytes: u64) -> String {
	const UNITS: &[&str] = &["B", "KB", "MB", "GB", "TB", "PB"];
	const THRESHOLD: u64 = 1024;

	if bytes == 0 {
		return "0 B".to_string();
	}

	let mut size = bytes as f64;
	let mut unit_index = 0;

	while size >= THRESHOLD as f64 && unit_index < UNITS.len() - 1 {
		size /= THRESHOLD as f64;
		unit_index += 1;
	}

	if unit_index == 0 {
		format!("{} {}", bytes, UNITS[unit_index])
	} else {
		format!("{:.1} {}", size, UNITS[unit_index])
	}
}
```

## src/infrastructure/cli/commands/file.rs

```rust
//! File operations commands
//!
//! This module handles CLI commands for file operations:
//! - Copying files using the action system
//! - Indexing operations with enhanced scope options
//! - Legacy scanning operations

use crate::infrastructure::cli::adapters::FileCopyCliArgs;
use crate::infrastructure::cli::daemon::{DaemonClient, DaemonCommand, DaemonResponse};
use crate::infrastructure::cli::output::{CliOutput, Message};
use clap::{Subcommand, ValueEnum};
use std::path::PathBuf;

// Re-export from the commands module for consistency
#[derive(Clone, Debug, ValueEnum)]
pub enum CliIndexMode {
	/// Only metadata (fast)
	Shallow,
	/// Metadata + content hashing
	Content,
	/// Full analysis including media metadata
	Deep,
}

#[derive(Clone, Debug, ValueEnum)]
pub enum CliIndexScope {
	/// Full directory tree (default)
	Full,
	/// Only direct children
	Shallow,
	/// Custom depth
	Limited,
}

#[derive(Subcommand, Clone, Debug)]
pub enum FileCommands {
	/// Copy files using the action system
	Copy(FileCopyCliArgs),

	/// Enhanced indexing with scope and persistence options
	#[command(subcommand)]
	Index(IndexCommands),

	/// Start a traditional indexing job (legacy)
	Scan {
		/// Path to index
		path: PathBuf,

		/// Indexing mode
		#[arg(short, long, value_enum, default_value = "content")]
		mode: CliIndexMode,

		/// Monitor the job in real-time
		#[arg(short = 'w', long)]
		watch: bool,
	},
}

/// Enhanced indexing commands
#[derive(Subcommand, Clone, Debug)]
pub enum IndexCommands {
	/// Browse external paths without adding to managed locations
	Browse {
		/// Path to browse
		path: PathBuf,
		/// Scope: shallow or full
		#[arg(short, long, value_enum, default_value = "shallow")]
		scope: CliIndexScope,
		/// Enable content analysis
		#[arg(short, long)]
		content: bool,
	},

	/// Re-index all locations
	All {
		/// Force re-indexing even if up-to-date
		#[arg(short, long)]
		force: bool,
		/// Monitor jobs in real-time
		#[arg(short = 'w', long)]
		watch: bool,
	},

	/// Index a specific location
	Location {
		/// Location ID or name
		location: String,
		/// Force re-indexing
		#[arg(short, long)]
		force: bool,
		/// Monitor the job in real-time
		#[arg(short = 'w', long)]
		watch: bool,
	},
}

pub async fn handle_file_command(
	cmd: FileCommands,
	instance_name: Option<String>,
	mut output: CliOutput,
) -> Result<(), Box<dyn std::error::Error>> {
	let mut client = DaemonClient::new_with_instance(instance_name.clone());

	match cmd {
		FileCommands::Copy(args) => handle_copy_command(args, &mut client, &mut output).await,
		FileCommands::Index(cmd) => handle_index_command(cmd, &mut client, &mut output).await,
		FileCommands::Scan { path, mode, watch } => {
			handle_scan_command(path, mode, watch, &mut client, &mut output).await
		}
	}
}

async fn handle_copy_command(
	args: FileCopyCliArgs,
	client: &mut DaemonClient,
	output: &mut CliOutput,
) -> Result<(), Box<dyn std::error::Error>> {
	// Convert CLI args to daemon command format
	let input = match args.validate_and_convert() {
		Ok(input) => input,
		Err(e) => {
			output.error(Message::Error(format!("Invalid copy operation: {}", e)))?;
			return Ok(());
		}
	};

	output.info(&input.summary())?;

	// Send copy command to daemon
	match client
		.send_command(DaemonCommand::Copy {
			sources: input.sources.clone(),
			destination: input.destination.clone(),
			overwrite: input.overwrite,
			verify: input.verify_checksum,
			preserve_timestamps: input.preserve_timestamps,
			move_files: input.move_files,
		})
		.await
	{
		Ok(DaemonResponse::CopyStarted {
			job_id,
			sources_count,
		}) => {
			output.success("Copy operation started successfully!")?;

			let mut section = output
				.section()
				.item("Job ID", &job_id.to_string())
				.item("Sources", &format!("{} file(s)", sources_count))
				.item("Destination", &input.destination.display().to_string());

			if input.overwrite {
				section = section.item("Mode", "Overwrite existing files");
			}
			if input.verify_checksum {
				section = section.item("Verification", "Enabled");
			}
			if input.move_files {
				section = section.item("Type", "Move (delete source after copy)");
			}

			section
				.empty_line()
				.help()
				.item("Monitor progress with: spacedrive job monitor")
				.render()?;
		}
		Ok(DaemonResponse::Ok) => {
			output.success("Copy operation completed successfully!")?;
		}
		Ok(DaemonResponse::Error(e)) => {
			output.error(Message::Error(format!("Failed to copy files: {}", e)))?;
		}
		Err(e) => {
			output.error(Message::Error(format!(
				"Failed to communicate with daemon: {}",
				e
			)))?;
		}
		_ => {
			output.error(Message::Error(
				"Unexpected response from daemon".to_string(),
			))?;
		}
	}

	Ok(())
}

async fn handle_index_command(
	cmd: IndexCommands,
	client: &mut DaemonClient,
	output: &mut CliOutput,
) -> Result<(), Box<dyn std::error::Error>> {
	match cmd {
		IndexCommands::Browse {
			path,
			scope,
			content,
		} => {
			output.info(&format!("Browsing {}...", path.display()))?;
			if content {
				output.info("Content analysis enabled")?;
			}

			let scope_str = match scope {
				CliIndexScope::Full => "full",
				CliIndexScope::Shallow => "shallow",
				CliIndexScope::Limited => "limited",
			};

			match client
				.send_command(DaemonCommand::Browse {
					path: path.clone(),
					scope: scope_str.to_string(),
					content,
				})
				.await
			{
				Ok(DaemonResponse::BrowseResults {
					path: _,
					entries,
					total_files,
					total_dirs,
				}) => {
					output.success(&format!(
						"Found {} files and {} directories",
						total_files, total_dirs
					))?;

					// Display entries
					for entry in entries {
						if entry.is_dir {
							output.info(&format!("ðŸ“ {}/", entry.name))?;
						} else {
							let size = entry
								.size
								.map(|s| format!(" ({} bytes)", s))
								.unwrap_or_default();
							output.info(&format!("ðŸ“„ {}{}", entry.name, size))?;
						}
					}
				}
				Ok(DaemonResponse::Error(e)) => {
					output.error(Message::Error(format!("Browse failed: {}", e)))?;
				}
				Err(e) => {
					output.error(Message::Error(format!(
						"Failed to communicate with daemon: {}",
						e
					)))?;
				}
				_ => {
					output.error(Message::Error(
						"Unexpected response from daemon".to_string(),
					))?;
				}
			}
		}
		IndexCommands::All { force, watch } => {
			output.info("Re-indexing all locations...")?;

			match client.send_command(DaemonCommand::IndexAll { force }).await {
				Ok(DaemonResponse::Ok) => {
					output.success("Re-indexing of all locations started successfully")?;
					if watch {
						output.info("Use 'spacedrive job monitor' to track progress")?;
					}
				}
				Ok(DaemonResponse::Error(e)) => {
					output.error(Message::Error(format!("Re-indexing failed: {}", e)))?;
				}
				Err(e) => {
					output.error(Message::Error(format!(
						"Failed to communicate with daemon: {}",
						e
					)))?;
				}
				_ => {
					output.error(Message::Error(
						"Unexpected response from daemon".to_string(),
					))?;
				}
			}
		}
		IndexCommands::Location {
			location,
			force,
			watch,
		} => {
			output.info(&format!("Indexing location {}...", location))?;

			match client
				.send_command(DaemonCommand::IndexLocation {
					location: location.clone(),
					force,
				})
				.await
			{
				Ok(DaemonResponse::Ok) => {
					output.success("Location indexing started successfully")?;
					if watch {
						output.info("Use 'spacedrive job monitor' to track progress")?;
					}
				}
				Ok(DaemonResponse::Error(e)) => {
					output.error(Message::Error(format!("Location indexing failed: {}", e)))?;
				}
				Err(e) => {
					output.error(Message::Error(format!(
						"Failed to communicate with daemon: {}",
						e
					)))?;
				}
				_ => {
					output.error(Message::Error(
						"Unexpected response from daemon".to_string(),
					))?;
				}
			}
		}
	}
	Ok(())
}

async fn handle_scan_command(
	path: PathBuf,
	mode: CliIndexMode,
	watch: bool,
	client: &mut DaemonClient,
	output: &mut CliOutput,
) -> Result<(), Box<dyn std::error::Error>> {
	output.info(&format!("Scanning {}...", path.display()))?;

	let mode_str = match mode {
		CliIndexMode::Shallow => "shallow",
		CliIndexMode::Content => "content",
		CliIndexMode::Deep => "deep",
	};

	// Scan command is no longer supported - use add location and index instead
	output.error(Message::Error(
        "The 'scan' command has been removed. Please use 'location add' followed by 'file index location' instead.".to_string()
    ))?;

	Ok(())
}
```

## src/infrastructure/cli/tui.rs

```rust
//! Terminal User Interface for Spacedrive CLI

use crate::{
    infrastructure::events::{Event, EventBus, EventFilter},
    library::{manager::LibraryManager, Library},
    location::manager::LocationManager,
};
use anyhow::Result;
use crossterm::{
    event::{self, DisableMouseCapture, EnableMouseCapture, Event as CEvent, KeyCode, KeyModifiers},
    execute,
    terminal::{disable_raw_mode, enable_raw_mode, EnterAlternateScreen, LeaveAlternateScreen},
};
use ratatui::{
    backend::CrosstermBackend,
    layout::{Alignment, Constraint, Direction, Layout, Rect},
    style::{Color, Modifier, Style},
    text::{Line, Span, Text},
    widgets::{
        Block, BorderType, Borders, Cell, Gauge, List, ListItem, Paragraph, Row, Table, Tabs,
        Wrap,
    },
    Frame, Terminal,
};
use std::{
    io,
    sync::Arc,
    time::{Duration, Instant},
};
use tokio::sync::RwLock;

/// TUI Application state
pub struct TuiApp {
    library_manager: Arc<LibraryManager>,
    event_bus: Arc<EventBus>,
    current_library: Arc<RwLock<Option<Arc<Library>>>>,
    selected_tab: usize,
    events: Vec<Event>,
    jobs: Vec<JobInfo>,
    locations: Vec<LocationInfo>,
    should_quit: bool,
    last_update: Instant,
}

#[derive(Clone)]
struct JobInfo {
    id: String,
    job_type: String,
    status: String,
    progress: f32,
    message: String,
}

#[derive(Clone)]
struct LocationInfo {
    id: String,
    name: String,
    path: String,
    status: String,
    file_count: u64,
}

impl TuiApp {
    /// Create a new TUI application
    pub fn new(
        library_manager: Arc<LibraryManager>,
        event_bus: Arc<EventBus>,
        current_library: Arc<RwLock<Option<Arc<Library>>>>,
    ) -> Self {
        Self {
            library_manager,
            event_bus,
            current_library,
            selected_tab: 0,
            events: Vec::new(),
            jobs: Vec::new(),
            locations: Vec::new(),
            should_quit: false,
            last_update: Instant::now(),
        }
    }

    /// Run the TUI application
    pub async fn run(&mut self) -> Result<()> {
        // Setup terminal
        enable_raw_mode()?;
        let mut stdout = io::stdout();
        execute!(stdout, EnterAlternateScreen, EnableMouseCapture)?;
        let backend = CrosstermBackend::new(stdout);
        let mut terminal = Terminal::new(backend)?;

        // Create event subscriber
        let mut event_subscriber = self.event_bus.subscribe();

        // Initial data load
        self.load_data().await?;

        // Main loop
        loop {
            // Draw UI
            terminal.draw(|f| self.draw(f))?;

            // Handle events with timeout
            if event::poll(Duration::from_millis(100))? {
                if let CEvent::Key(key) = event::read()? {
                    match key.code {
                        KeyCode::Char('q') => {
                            self.should_quit = true;
                        }
                        KeyCode::Char('c') if key.modifiers.contains(KeyModifiers::CONTROL) => {
                            self.should_quit = true;
                        }
                        KeyCode::Tab => {
                            self.selected_tab = (self.selected_tab + 1) % 4;
                        }
                        KeyCode::BackTab => {
                            self.selected_tab = if self.selected_tab == 0 {
                                3
                            } else {
                                self.selected_tab - 1
                            };
                        }
                        _ => {}
                    }
                }
            }

            // Process events from event bus
            while let Ok(event) = event_subscriber.try_recv() {
                self.handle_event(event).await;
            }

            // Refresh data periodically
            if self.last_update.elapsed() > Duration::from_secs(1) {
                self.update_jobs().await;
                self.last_update = Instant::now();
            }

            // Check if should quit
            if self.should_quit {
                break;
            }
        }

        // Cleanup
        disable_raw_mode()?;
        execute!(
            terminal.backend_mut(),
            LeaveAlternateScreen,
            DisableMouseCapture
        )?;
        terminal.show_cursor()?;

        Ok(())
    }

    /// Draw the UI
    fn draw(&self, f: &mut Frame<'_>) {
        let chunks = Layout::default()
            .direction(Direction::Vertical)
            .margin(1)
            .constraints(
                [
                    Constraint::Length(3),  // Header
                    Constraint::Length(3),  // Tabs
                    Constraint::Min(0),     // Content
                    Constraint::Length(3),  // Footer
                ]
                .as_ref(),
            )
            .split(f.size());

        // Draw header
        self.draw_header(f, chunks[0]);

        // Draw tabs
        self.draw_tabs(f, chunks[1]);

        // Draw content based on selected tab
        match self.selected_tab {
            0 => self.draw_overview(f, chunks[2]),
            1 => self.draw_locations(f, chunks[2]),
            2 => self.draw_jobs(f, chunks[2]),
            3 => self.draw_events(f, chunks[2]),
            _ => {}
        }

        // Draw footer
        self.draw_footer(f, chunks[3]);
    }

    /// Draw header
    fn draw_header(&self, f: &mut Frame<'_>, area: Rect) {
        let header = Paragraph::new("Spacedrive Core CLI")
            .style(
                Style::default()
                    .fg(Color::Cyan)
                    .add_modifier(Modifier::BOLD),
            )
            .alignment(Alignment::Center)
            .block(
                Block::default()
                    .borders(Borders::ALL)
                    .border_type(BorderType::Rounded),
            );
        f.render_widget(header, area);
    }

    /// Draw tabs
    fn draw_tabs(&self, f: &mut Frame<'_>, area: Rect) {
        let titles = vec!["Overview", "Locations", "Jobs", "Events"];
        let tabs = Tabs::new(titles)
            .block(Block::default().borders(Borders::ALL))
            .select(self.selected_tab)
            .style(Style::default().fg(Color::White))
            .highlight_style(
                Style::default()
                    .fg(Color::Yellow)
                    .add_modifier(Modifier::BOLD),
            );
        f.render_widget(tabs, area);
    }

    /// Draw overview tab
    fn draw_overview(&self, f: &mut Frame<'_>, area: Rect) {
        let chunks = Layout::default()
            .direction(Direction::Vertical)
            .constraints(
                [
                    Constraint::Length(5),   // Library info
                    Constraint::Length(5),   // Statistics
                    Constraint::Percentage(50), // Recent activity
                ]
                .as_ref(),
            )
            .split(area);

        // Library info
        let library_info = if let Ok(current) = self.current_library.try_read() {
            if let Some(lib) = current.as_ref() {
                format!(
                    "Current Library: {} ({})",
                    "Active Library", // Would get actual name
                    lib.id()
                )
            } else {
                "No library currently open".to_string()
            }
        } else {
            "Loading...".to_string()
        };

        let library_widget = Paragraph::new(library_info)
            .block(Block::default().title("Library").borders(Borders::ALL))
            .wrap(Wrap { trim: true });
        f.render_widget(library_widget, chunks[0]);

        // Statistics
        let stats = vec![
            Line::from(vec![
                Span::raw("Locations: "),
                Span::styled(
                    format!("{}", self.locations.len()),
                    Style::default().fg(Color::Yellow),
                ),
            ]),
            Line::from(vec![
                Span::raw("Active Jobs: "),
                Span::styled(
                    format!("{}", self.jobs.iter().filter(|j| j.status == "Running").count()),
                    Style::default().fg(Color::Green),
                ),
            ]),
            Line::from(vec![
                Span::raw("Total Events: "),
                Span::styled(
                    format!("{}", self.events.len()),
                    Style::default().fg(Color::Blue),
                ),
            ]),
        ];

        let stats_widget = Paragraph::new(stats)
            .block(Block::default().title("Statistics").borders(Borders::ALL));
        f.render_widget(stats_widget, chunks[1]);

        // Recent activity
        let recent_events: Vec<ListItem> = self
            .events
            .iter()
            .rev()
            .take(10)
            .map(|e| {
                let content = format_event_short(e);
                ListItem::new(content)
            })
            .collect();

        let activity_widget = List::new(recent_events)
            .block(Block::default().title("Recent Activity").borders(Borders::ALL))
            .style(Style::default().fg(Color::White));
        f.render_widget(activity_widget, chunks[2]);
    }

    /// Draw locations tab
    fn draw_locations(&self, f: &mut Frame<'_>, area: Rect) {
        let header_cells = ["ID", "Name", "Path", "Status", "Files"]
            .iter()
            .map(|h| Cell::from(*h).style(Style::default().fg(Color::Yellow)));
        let header = Row::new(header_cells)
            .style(Style::default().add_modifier(Modifier::BOLD))
            .height(1);

        let rows = self.locations.iter().map(|loc| {
            let cells = vec![
                Cell::from(loc.id.clone()),
                Cell::from(loc.name.clone()),
                Cell::from(loc.path.clone()),
                Cell::from(loc.status.clone()).style(match loc.status.as_str() {
                    "Indexing" => Style::default().fg(Color::Yellow),
                    "Complete" => Style::default().fg(Color::Green),
                    "Error" => Style::default().fg(Color::Red),
                    _ => Style::default(),
                }),
                Cell::from(loc.file_count.to_string()),
            ];
            Row::new(cells).height(1)
        });

        let table = Table::new(rows)
            .header(header)
            .block(Block::default().title("Locations").borders(Borders::ALL))
            .widths(&[
                Constraint::Percentage(15),
                Constraint::Percentage(20),
                Constraint::Percentage(35),
                Constraint::Percentage(15),
                Constraint::Percentage(15),
            ]);

        f.render_widget(table, area);
    }

    /// Draw jobs tab
    fn draw_jobs(&self, f: &mut Frame<'_>, area: Rect) {
        let chunks = Layout::default()
            .direction(Direction::Vertical)
            .constraints([Constraint::Min(0), Constraint::Length(3)].as_ref())
            .split(area);

        // Jobs table
        let header_cells = ["ID", "Type", "Status", "Progress", "Message"]
            .iter()
            .map(|h| Cell::from(*h).style(Style::default().fg(Color::Yellow)));
        let header = Row::new(header_cells)
            .style(Style::default().add_modifier(Modifier::BOLD))
            .height(1);

        let rows = self.jobs.iter().map(|job| {
            let cells = vec![
                Cell::from(job.id.clone()),
                Cell::from(job.job_type.clone()),
                Cell::from(job.status.clone()).style(match job.status.as_str() {
                    "Running" => Style::default().fg(Color::Yellow),
                    "Completed" => Style::default().fg(Color::Green),
                    "Failed" => Style::default().fg(Color::Red),
                    _ => Style::default(),
                }),
                Cell::from(format!("{:.1}%", job.progress * 100.0)),
                Cell::from(job.message.clone()),
            ];
            Row::new(cells).height(1)
        });

        let table = Table::new(rows)
            .header(header)
            .block(Block::default().title("Jobs").borders(Borders::ALL))
            .widths(&[
                Constraint::Percentage(15),
                Constraint::Percentage(20),
                Constraint::Percentage(15),
                Constraint::Percentage(15),
                Constraint::Percentage(35),
            ]);

        f.render_widget(table, chunks[0]);

        // Overall progress
        let running_jobs = self.jobs.iter().filter(|j| j.status == "Running").count();
        let avg_progress = if running_jobs > 0 {
            self.jobs
                .iter()
                .filter(|j| j.status == "Running")
                .map(|j| j.progress)
                .sum::<f32>()
                / running_jobs as f32
        } else {
            0.0
        };

        let progress = Gauge::default()
            .block(Block::default().title("Overall Progress").borders(Borders::ALL))
            .gauge_style(Style::default().fg(Color::Cyan))
            .percent((avg_progress * 100.0) as u16);

        f.render_widget(progress, chunks[1]);
    }

    /// Draw events tab
    fn draw_events(&self, f: &mut Frame<'_>, area: Rect) {
        let events: Vec<ListItem> = self
            .events
            .iter()
            .rev()
            .map(|e| {
                let content = format_event_detailed(e);
                ListItem::new(content)
            })
            .collect();

        let events_list = List::new(events)
            .block(Block::default().title("Events").borders(Borders::ALL))
            .style(Style::default().fg(Color::White));

        f.render_widget(events_list, area);
    }

    /// Draw footer
    fn draw_footer(&self, f: &mut Frame<'_>, area: Rect) {
        let help = Paragraph::new(
            "Tab: Switch tabs | q: Quit | â†‘â†“: Navigate | Enter: Select",
        )
        .style(Style::default().fg(Color::DarkGray))
        .alignment(Alignment::Center)
        .block(Block::default().borders(Borders::ALL));
        f.render_widget(help, area);
    }

    /// Handle incoming events
    async fn handle_event(&mut self, event: Event) {
        // Add to events list
        self.events.push(event.clone());
        if self.events.len() > 1000 {
            self.events.drain(0..500);
        }

        // Update relevant data based on event type
        match &event {
            Event::LocationAdded { .. } => {
                self.load_locations().await;
            }
            Event::LocationRemoved { .. } => {
                self.load_locations().await;
            }
            Event::JobStarted { job_id, job_type } => {
                self.jobs.push(JobInfo {
                    id: job_id.clone(),
                    job_type: job_type.clone(),
                    status: "Running".to_string(),
                    progress: 0.0,
                    message: "Started".to_string(),
                });
            }
            Event::JobProgress {
                job_id,
                progress,
                message,
            } => {
                if let Some(job) = self.jobs.iter_mut().find(|j| &j.id == job_id) {
                    job.progress = *progress as f32;
                    if let Some(msg) = message {
                        job.message = msg.clone();
                    }
                }
            }
            Event::JobCompleted { job_id, .. } => {
                if let Some(job) = self.jobs.iter_mut().find(|j| &j.id == job_id) {
                    job.status = "Completed".to_string();
                    job.progress = 1.0;
                }
            }
            Event::JobFailed { job_id, error, .. } => {
                if let Some(job) = self.jobs.iter_mut().find(|j| &j.id == job_id) {
                    job.status = "Failed".to_string();
                    job.message = error.clone();
                }
            }
            _ => {}
        }
    }

    /// Load initial data
    async fn load_data(&mut self) -> Result<()> {
        self.load_locations().await;
        Ok(())
    }

    /// Load locations
    async fn load_locations(&mut self) {
        if let Ok(current) = self.current_library.read().await {
            if let Some(library) = current.as_ref() {
                let location_manager = LocationManager::new(self.event_bus.clone());
                if let Ok(locations) = location_manager.list_locations(library).await {
                    self.locations = locations
                        .into_iter()
                        .map(|loc| LocationInfo {
                            id: loc.id.to_string(),
                            name: loc.name,
                            path: loc.path.to_string_lossy().to_string(),
                            status: "Ready".to_string(), // Would get actual status
                            file_count: 0, // Would get actual count
                        })
                        .collect();
                }
            }
        }
    }

    /// Update job information
    async fn update_jobs(&mut self) {
        // Would fetch actual job status from job manager
        // For now, just clean up completed jobs after a while
        self.jobs.retain(|j| {
            j.status == "Running" || j.status == "Failed"
        });
    }
}

/// Format event for short display
fn format_event_short(event: &Event) -> String {
    match event {
        Event::LibraryCreated { name, .. } => format!("Library '{}' created", name),
        Event::LibraryOpened { name, .. } => format!("Library '{}' opened", name),
        Event::LocationAdded { path, .. } => format!("Location '{}' added", path.display()),
        Event::IndexingStarted { .. } => "Indexing started".to_string(),
        Event::IndexingCompleted { total_files, .. } => {
            format!("Indexing completed: {} files", total_files)
        }
        Event::JobStarted { job_type, .. } => format!("{} started", job_type),
        Event::JobCompleted { job_type, .. } => format!("{} completed", job_type),
        _ => format!("{:?}", event).chars().take(50).collect(),
    }
}

/// Format event for detailed display
fn format_event_detailed(event: &Event) -> Text {
    let timestamp = chrono::Local::now().format("%H:%M:%S").to_string();
    let header = Line::from(vec![
        Span::styled(timestamp, Style::default().fg(Color::DarkGray)),
        Span::raw(" "),
    ]);

    let content = match event {
        Event::LibraryCreated { id, name, path } => {
            vec![
                header,
                Line::from(vec![
                    Span::styled("Library Created: ", Style::default().fg(Color::Green)),
                    Span::raw(name),
                ]),
                Line::from(vec![
                    Span::raw("  ID: "),
                    Span::styled(id.to_string(), Style::default().fg(Color::DarkGray)),
                ]),
                Line::from(vec![
                    Span::raw("  Path: "),
                    Span::styled(
                        path.display().to_string(),
                        Style::default().fg(Color::DarkGray),
                    ),
                ]),
            ]
        }
        Event::IndexingProgress {
            location_id,
            processed,
            total,
        } => {
            let progress = if let Some(t) = total {
                format!("{}/{}", processed, t)
            } else {
                processed.to_string()
            };
            vec![
                header,
                Line::from(vec![
                    Span::styled("Indexing Progress: ", Style::default().fg(Color::Yellow)),
                    Span::raw(progress),
                ]),
                Line::from(vec![
                    Span::raw("  Location: "),
                    Span::styled(
                        location_id.to_string(),
                        Style::default().fg(Color::DarkGray),
                    ),
                ]),
            ]
        }
        _ => vec![header, Line::from(format!("{:?}", event))],
    };

    Text::from(content)
}```

## src/infrastructure/cli/utils.rs

```rust
//! CLI utility functions

use crate::{library::Library, location::ManagedLocation};
use console::style;
use indicatif::{ProgressBar, ProgressStyle};
use std::sync::Arc;

/// Format library information for display
pub async fn format_library_info(library: &Arc<Library>) -> String {
    let mut output = String::new();
    
    output.push_str(&format!("{}\n", style("Library Information").bold().underlined()));
    output.push_str(&format!("  {} {}\n", style("Name:").bold(), style(library.name().await).cyan()));
    output.push_str(&format!("  {} {}\n", style("ID:").bold(), style(library.id()).yellow()));
    output.push_str(&format!("  {} {}\n", style("Path:").bold(), style(library.path().display()).dim()));
    
    let config = library.config().await;
    output.push_str(&format!("  {} {}\n", style("Created:").bold(), style(config.created_at.format("%Y-%m-%d %H:%M:%S")).dim()));
    output.push_str(&format!("  {} {}\n", style("Updated:").bold(), style(config.updated_at.format("%Y-%m-%d %H:%M:%S")).dim()));
    
    if let Some(desc) = &config.description {
        output.push_str(&format!("  {} {}\n", style("Description:").bold(), desc));
    }
    
    // Statistics
    output.push_str(&format!("\n{}\n", style("Statistics").bold().underlined()));
    output.push_str(&format!("  {} {}\n", style("Total files:").bold(), style(config.statistics.total_files).cyan()));
    output.push_str(&format!("  {} {}\n", style("Total size:").bold(), format_bytes(config.statistics.total_size)));
    output.push_str(&format!("  {} {}\n", style("Locations:").bold(), style(config.statistics.location_count).cyan()));
    output.push_str(&format!("  {} {}\n", style("Indexed files:").bold(), style(config.statistics.total_files).cyan()));
    
    output
}

/// Format location information for display
pub fn format_location_info(location: &ManagedLocation) -> String {
    let mut output = String::new();
    
    output.push_str(&format!("{}\n", style("Location Information").bold().underlined()));
    output.push_str(&format!("  {} {}\n", style("Name:").bold(), style(&location.name).cyan()));
    output.push_str(&format!("  {} {}\n", style("ID:").bold(), style(location.id).yellow()));
    output.push_str(&format!("  {} {}\n", style("Path:").bold(), style(location.path.display()).dim()));
    output.push_str(&format!("  {} {:?}\n", style("Index Mode:").bold(), location.index_mode));
    output.push_str(&format!("  {} {}\n", style("Watch Enabled:").bold(), 
        if location.watch_enabled { style("Yes").green() } else { style("No").red() }
    ));
    output.push_str(&format!("  {} {}\n", style("Indexing Enabled:").bold(), 
        if location.indexing_enabled { style("Yes").green() } else { style("No").red() }
    ));
    
    output
}

/// Print job information
pub fn print_job_info(
    job_id: &str,
    job_type: &str,
    status: &str,
    progress: Option<f64>,
    message: Option<&str>,
) {
    println!("{}", style("Job Information").bold().underlined());
    println!("  {} {}", style("ID:").bold(), style(job_id).yellow());
    println!("  {} {}", style("Type:").bold(), style(job_type).cyan());
    println!("  {} {}", style("Status:").bold(), format_status(status));
    
    if let Some(prog) = progress {
        println!("  {} {:.1}%", style("Progress:").bold(), prog * 100.0);
    }
    
    if let Some(msg) = message {
        println!("  {} {}", style("Message:").bold(), msg);
    }
}

/// Create and display a progress bar
pub fn print_progress_bar(current: u64, total: u64, message: &str) -> ProgressBar {
    let pb = ProgressBar::new(total);
    pb.set_style(
        ProgressStyle::default_bar()
            .template("{spinner:.green} [{bar:40.cyan/blue}] {pos}/{len} {msg}")
            .unwrap()
            .progress_chars("#>-"),
    );
    pb.set_position(current);
    pb.set_message(message.to_string());
    pb
}

/// Format status with color
fn format_status(status: &str) -> String {
    match status.to_lowercase().as_str() {
        "running" | "active" | "indexing" => format!("{}", style(status).yellow()),
        "completed" | "success" | "done" => format!("{}", style(status).green()),
        "failed" | "error" => format!("{}", style(status).red()),
        "cancelled" | "stopped" => format!("{}", style(status).magenta()),
        "pending" | "queued" => format!("{}", style(status).blue()),
        _ => status.to_string(),
    }
}

/// Format bytes into human-readable string
pub fn format_bytes(bytes: u64) -> String {
    const UNITS: &[&str] = &["B", "KB", "MB", "GB", "TB", "PB"];
    let mut size = bytes as f64;
    let mut unit_index = 0;
    
    while size >= 1024.0 && unit_index < UNITS.len() - 1 {
        size /= 1024.0;
        unit_index += 1;
    }
    
    if unit_index == 0 {
        format!("{} {}", size as u64, UNITS[unit_index])
    } else {
        format!("{:.2} {}", size, UNITS[unit_index])
    }
}

/// Format duration into human-readable string from seconds
pub fn format_duration(seconds: u64) -> String {
    if seconds < 60 {
        format!("{}s", seconds)
    } else if seconds < 3600 {
        format!("{}m {}s", seconds / 60, seconds % 60)
    } else {
        format!("{}h {}m", seconds / 3600, (seconds % 3600) / 60)
    }
}

/// Format duration from std::time::Duration
pub fn format_duration_from_std(duration: std::time::Duration) -> String {
    format_duration(duration.as_secs())
}

/// Format bytes into parts (used by networking commands)
pub fn format_bytes_parts(bytes: u64) -> (f64, &'static str) {
    const UNITS: &[&str] = &["B", "KB", "MB", "GB", "TB", "PB"];
    let mut size = bytes as f64;
    let mut unit_index = 0;
    
    while size >= 1024.0 && unit_index < UNITS.len() - 1 {
        size /= 1024.0;
        unit_index += 1;
    }
    
    (size, UNITS[unit_index])
}

/// Print a table with headers and rows
pub fn print_table(headers: Vec<&str>, rows: Vec<Vec<String>>) {
    use console::Term;
    use std::cmp::max;
    
    let term = Term::stdout();
    let term_width = term.size().1 as usize;
    
    // Calculate column widths
    let mut widths: Vec<usize> = headers.iter().map(|h| h.len()).collect();
    for row in &rows {
        for (i, cell) in row.iter().enumerate() {
            if i < widths.len() {
                widths[i] = max(widths[i], cell.len());
            }
        }
    }
    
    // Ensure we don't exceed terminal width
    let total_width: usize = widths.iter().sum::<usize>() + (widths.len() - 1) * 3;
    if total_width > term_width {
        let scale = term_width as f64 / total_width as f64;
        for width in &mut widths {
            *width = (*width as f64 * scale) as usize;
        }
    }
    
    // Print headers
    let header_row: Vec<String> = headers.iter()
        .enumerate()
        .map(|(i, h)| format!("{:width$}", h, width = widths[i]))
        .collect();
    println!("{}", style(header_row.join(" | ")).bold());
    
    // Print separator
    let separator: Vec<String> = widths.iter()
        .map(|&w| "-".repeat(w))
        .collect();
    println!("{}", style(separator.join("-+-")).dim());
    
    // Print rows
    for row in rows {
        let formatted_row: Vec<String> = row.iter()
            .enumerate()
            .map(|(i, cell)| {
                if i < widths.len() {
                    let width = widths[i];
                    if cell.len() > width {
                        format!("{:width$.width$}â€¦", cell, width = width - 1)
                    } else {
                        format!("{:width$}", cell, width = width)
                    }
                } else {
                    cell.clone()
                }
            })
            .collect();
        println!("{}", formatted_row.join(" | "));
    }
}

/// Progress style constants and builders
pub mod progress_styles {
    use indicatif::ProgressStyle;
    
    pub const BASIC_TEMPLATE: &str = "{spinner:.green} {prefix:.bold.cyan} [{bar:40.green/blue}] {percent}% | {msg}";
    pub const ENHANCED_TEMPLATE: &str = "{spinner:.green} {prefix:.bold.cyan} [{bar:40.green/blue}] {percent}% | {msg}\n     {wide_msg}";
    pub const ELAPSED_TEMPLATE: &str = "{spinner:.green} [{elapsed_precise}] [{bar:50.cyan/blue}] {percent:>3}% | {msg}";
    pub const SPINNER_TEMPLATE: &str = "{spinner:.green} {msg}";
    pub const DEFAULT_CHARS: &str = "â–ˆâ–“â–’â–‘";
    pub const SPINNER_CHARS: &str = "â ‹â ™â ¹â ¸â ¼â ´â ¦â §â ‡â ";
    
    pub fn basic_style() -> ProgressStyle {
        ProgressStyle::with_template(BASIC_TEMPLATE)
            .unwrap()
            .progress_chars(DEFAULT_CHARS)
    }
    
    pub fn enhanced_style() -> ProgressStyle {
        ProgressStyle::with_template(ENHANCED_TEMPLATE)
            .unwrap()
            .progress_chars(DEFAULT_CHARS)
    }
    
    pub fn elapsed_style() -> ProgressStyle {
        ProgressStyle::with_template(ELAPSED_TEMPLATE)
            .unwrap()
            .progress_chars(DEFAULT_CHARS)
            .tick_chars(SPINNER_CHARS)
    }
    
    pub fn spinner_style() -> ProgressStyle {
        ProgressStyle::default_spinner()
            .template(SPINNER_TEMPLATE)
            .unwrap()
    }
}

/// Create a simple spinner for long-running operations
pub fn create_spinner(message: &str) -> ProgressBar {
    let spinner = ProgressBar::new_spinner();
    spinner.set_style(progress_styles::spinner_style());
    spinner.set_message(message.to_string());
    spinner.enable_steady_tick(std::time::Duration::from_millis(100));
    spinner
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_format_bytes() {
        assert_eq!(format_bytes(0), "0 B");
        assert_eq!(format_bytes(1023), "1023 B");
        assert_eq!(format_bytes(1024), "1.00 KB");
        assert_eq!(format_bytes(1536), "1.50 KB");
        assert_eq!(format_bytes(1048576), "1.00 MB");
        assert_eq!(format_bytes(1073741824), "1.00 GB");
    }
    
    #[test]
    fn test_format_duration() {
        assert_eq!(format_duration(0), "0s");
        assert_eq!(format_duration(59), "59s");
        assert_eq!(format_duration(60), "1m 0s");
        assert_eq!(format_duration(90), "1m 30s");
        assert_eq!(format_duration(3600), "1h 0m");
        assert_eq!(format_duration(3661), "1h 1m");
    }
}```

## src/infrastructure/mod.rs

```rust
//! Infrastructure layer - external interfaces

pub mod actions;
pub mod cli;
pub mod database;
pub mod events;
pub mod jobs;
```

## src/infrastructure/actions/registry.rs

```rust
//! Action registry for automatic handler discovery

use super::{handler::ActionHandler, error::{ActionError, ActionResult}};
use inventory;
use once_cell::sync::Lazy;
use std::{collections::HashMap, sync::Arc};
use tracing::info;

/// Registration struct for action handlers
pub struct ActionRegistration {
    pub name: &'static str,
    pub create_fn: fn() -> Box<dyn ActionHandler>,
}

// Inventory for auto-registration
inventory::collect!(ActionRegistration);

/// Global action registry
pub struct ActionRegistry {
    handlers: HashMap<&'static str, Box<dyn ActionHandler>>,
}

impl ActionRegistry {
    /// Create a new registry and discover all handlers
    pub fn new() -> Self {
        let mut handlers = HashMap::new();
        
        // Collect all registered action handlers
        for registration in inventory::iter::<ActionRegistration> {
            info!("Registered action handler: {}", registration.name);
            handlers.insert(registration.name, (registration.create_fn)());
        }
        
        info!("Discovered {} action handler types", handlers.len());
        
        Self { handlers }
    }
    
    /// Get a handler for the given action kind
    pub fn get(&self, action_kind: &str) -> Option<&dyn ActionHandler> {
        self.handlers.get(action_kind).map(|h| h.as_ref())
    }
    
    /// Get all registered action kinds
    pub fn action_kinds(&self) -> Vec<&'static str> {
        self.handlers.keys().copied().collect()
    }
    
    /// Check if an action kind is registered
    pub fn has_action(&self, action_kind: &str) -> bool {
        self.handlers.contains_key(action_kind)
    }
}

/// Global registry instance
pub static REGISTRY: Lazy<ActionRegistry> = Lazy::new(ActionRegistry::new);

/// Helper macro for registering action handlers
#[macro_export]
macro_rules! register_action_handler {
    ($handler_type:ty, $action_kind:expr) => {
        inventory::submit! {
            $crate::infrastructure::actions::registry::ActionRegistration {
                name: $action_kind,
                create_fn: || Box::new(<$handler_type>::new()),
            }
        }
    };
}```

## src/infrastructure/actions/error.rs

```rust
//! Error types for the Action System

use crate::{
    infrastructure::jobs::error::JobError,
    library::LibraryError,
    shared::errors::CoreError,
};
use serde::{Deserialize, Serialize};
use thiserror::Error;
use uuid::Uuid;

/// Result type for action operations
pub type ActionResult<T> = Result<T, ActionError>;

/// Errors that can occur during action execution
#[derive(Debug, Error)]
pub enum ActionError {
    /// Action type not registered in the registry
    #[error("Action type '{0}' is not registered")]
    ActionNotRegistered(String),
    
    /// Invalid action type for the handler
    #[error("Invalid action type for this handler")]
    InvalidActionType,
    
    /// Invalid input provided to action
    #[error("Invalid input: {0}")]
    InvalidInput(String),
    
    /// Permission denied for this action
    #[error("Permission denied for action '{action}': {reason}")]
    PermissionDenied {
        action: String,
        reason: String,
    },
    
    /// Library not found
    #[error("Library {0} not found")]
    LibraryNotFound(Uuid),
    
    /// Location not found
    #[error("Location {0} not found")]
    LocationNotFound(Uuid),
    
    /// Device not found
    #[error("Device {0} not found")]
    DeviceNotFound(Uuid),
    
    /// File system error
    #[error("File system error at '{path}': {error}")]
    FileSystem {
        path: String,
        error: String,
    },
    
    /// Network error for cross-device operations
    #[error("Network error with device {device_id}: {error}")]
    Network {
        device_id: Uuid,
        error: String,
    },
    
    /// Job creation or execution error
    #[error("Job error: {0}")]
    Job(#[from] JobError),
    
    /// Database operation error
    #[error("Database error: {0}")]
    Database(String),
    
    /// Validation error
    #[error("Validation error for field '{field}': {message}")]
    Validation {
        field: String,
        message: String,
    },
    
    /// Action execution timeout
    #[error("Action execution timed out")]
    Timeout,
    
    /// Action was cancelled
    #[error("Action was cancelled")]
    Cancelled,
    
    /// Device manager error
    #[error("Device manager error: {0}")]
    DeviceManager(String),
    
    /// JSON serialization error
    #[error("JSON serialization error: {0}")]
    JsonSerialization(#[from] serde_json::Error),
    
    /// Sea-ORM database error
    #[error("Database operation failed: {0}")]
    SeaOrm(#[from] sea_orm::DbErr),
    
    /// IO error
    #[error("IO error at '{path}': {source}")]
    Io {
        path: String,
        #[source]
        source: std::io::Error,
    },
    
    /// Generic internal error
    #[error("Internal error: {0}")]
    Internal(String),
}

impl From<LibraryError> for ActionError {
    fn from(error: LibraryError) -> Self {
        match error {
            LibraryError::NotFound(_) => ActionError::Internal(error.to_string()),
            other => ActionError::Internal(other.to_string()),
        }
    }
}

impl From<CoreError> for ActionError {
    fn from(error: CoreError) -> Self {
        ActionError::Internal(error.to_string())
    }
}

impl From<std::io::Error> for ActionError {
    fn from(error: std::io::Error) -> Self {
        ActionError::Io {
            path: "unknown".to_string(),
            source: error,
        }
    }
}

/// Helper function to create IO errors with known paths
impl ActionError {
    pub fn io_error(path: impl Into<String>, error: std::io::Error) -> Self {
        Self::Io {
            path: path.into(),
            source: error,
        }
    }
    
    pub fn device_manager_error(error: impl std::fmt::Display) -> Self {
        Self::DeviceManager(error.to_string())
    }
}```

## src/infrastructure/actions/handler.rs

```rust
//! Action handler trait and related types

use super::{Action, error::ActionResult, output::ActionOutput};
use crate::context::CoreContext;
use async_trait::async_trait;
use std::sync::Arc;

/// Trait that all action handlers must implement
#[async_trait]
pub trait ActionHandler: Send + Sync {
    /// Execute the action and return output
    async fn execute(
        &self,
        context: Arc<CoreContext>,
        action: Action,
    ) -> ActionResult<ActionOutput>;
    
    /// Validate the action before execution (optional)
    async fn validate(
        &self,
        _context: Arc<CoreContext>,
        _action: &Action,
    ) -> ActionResult<()> {
        Ok(())
    }
    
    /// Check if this handler can handle the given action
    fn can_handle(&self, action: &Action) -> bool;
    
    /// Get the action kinds this handler supports
    fn supported_actions() -> &'static [&'static str]
    where
        Self: Sized;
}```

## src/infrastructure/actions/manager.rs

```rust
//! Action manager - central router for all actions

use super::{
    Action, error::{ActionError, ActionResult}, output::ActionOutput, registry::REGISTRY,
};
use crate::{
    context::CoreContext,
    infrastructure::database::entities::{audit_log, AuditLog, AuditLogActive},
    shared::types::get_current_device_id,
};
use sea_orm::{ActiveModelTrait, ColumnTrait, DatabaseConnection, EntityTrait, QueryFilter, QuerySelect, Set};
use std::sync::Arc;
use uuid::Uuid;

/// Central manager for all action execution
pub struct ActionManager {
    context: Arc<CoreContext>,
}

impl ActionManager {
    /// Create a new action manager
    pub fn new(context: Arc<CoreContext>) -> Self {
        Self { context }
    }

    /// Dispatch an action for execution
    pub async fn dispatch(
        &self,
        action: Action,
    ) -> ActionResult<ActionOutput> {
        // 1. Find the correct handler in the registry
        let handler = REGISTRY
            .get(action.kind())
            .ok_or_else(|| ActionError::ActionNotRegistered(action.kind().to_string()))?;

        // 2. Validate the action
        handler.validate(self.context.clone(), &action).await?;

        // 3. Create the initial audit log entry (if library-scoped)
        let audit_entry = if let Some(library_id) = action.library_id() {
            let entry = self.create_audit_log(library_id, &action).await?;
            Some((entry, library_id))
        } else {
            None
        };

        // 4. Execute the handler
        let result = handler.execute(self.context.clone(), action).await;

        // 5. Update the audit log with the final status (if we created one)
        if let Some((entry, library_id)) = audit_entry {
            self.finalize_audit_log(entry, &result, library_id).await?;
        }

        result
    }

    /// Create an initial audit log entry
    async fn create_audit_log(
        &self,
        library_id: Uuid,
        action: &Action,
    ) -> ActionResult<audit_log::Model> {
        let library = self.get_library(library_id).await?;
        let db = library.db().conn();
        
        let audit_entry = AuditLogActive {
            uuid: Set(Uuid::new_v4().to_string()),
            action_type: Set(action.kind().to_string()),
            actor_device_id: Set(get_current_device_id().to_string()),
            targets: Set(serde_json::to_string(&action.targets_summary()).unwrap_or_default()),
            status: Set(audit_log::ActionStatus::InProgress),
            job_id: Set(None),
            created_at: Set(chrono::Utc::now()),
            completed_at: Set(None),
            error_message: Set(None),
            result_payload: Set(None),
            ..Default::default()
        };

        audit_entry
            .insert(db)
            .await
            .map_err(ActionError::SeaOrm)
    }

    /// Finalize the audit log entry with the result
    async fn finalize_audit_log(
        &self,
        mut entry: audit_log::Model,
        result: &ActionResult<ActionOutput>,
        library_id: Uuid,
    ) -> ActionResult<()> {
        let library = self.get_library(library_id).await?;
        let db = library.db().conn();

        match result {
            Ok(_) => {
                entry.status = audit_log::ActionStatus::Completed;
                entry.completed_at = Some(chrono::Utc::now());
            }
            Err(error) => {
                entry.status = audit_log::ActionStatus::Failed;
                entry.completed_at = Some(chrono::Utc::now());
                entry.error_message = Some(error.to_string());
            }
        }

        // Convert to active model and explicitly mark changed fields
        let mut active_model: AuditLogActive = entry.into();
        
        // Explicitly mark the fields we want to update as "Set" (changed)
        match result {
            Ok(output) => {
                active_model.status = Set(audit_log::ActionStatus::Completed);
                active_model.completed_at = Set(Some(chrono::Utc::now()));
                // Extract job_id if present in certain output types
                // TODO: Update this when we have job-based actions
                active_model.result_payload = Set(Some(serde_json::to_string(output).unwrap_or_default()));
            }
            Err(error) => {
                active_model.status = Set(audit_log::ActionStatus::Failed);
                active_model.completed_at = Set(Some(chrono::Utc::now()));
                active_model.error_message = Set(Some(error.to_string()));
            }
        }
        
        active_model
            .update(db)
            .await
            .map_err(ActionError::SeaOrm)?;

        Ok(())
    }

    /// Get the library for database operations
    async fn get_library(&self, library_id: Uuid) -> ActionResult<std::sync::Arc<crate::library::Library>> {
        self.context
            .library_manager
            .get_library(library_id)
            .await
            .ok_or(ActionError::LibraryNotFound(library_id))
    }


    /// Get action history for a library
    pub async fn get_action_history(
        &self,
        library_id: Uuid,
        limit: Option<u64>,
        offset: Option<u64>,
    ) -> ActionResult<Vec<audit_log::Model>> {
        let library = self.get_library(library_id).await?;
        let db = library.db().conn();
        
        let mut query = AuditLog::find();
        
        if let Some(limit) = limit {
            query = query.limit(limit);
        }
        
        if let Some(offset) = offset {
            query = query.offset(offset);
        }

        query
            .all(db)
            .await
            .map_err(ActionError::SeaOrm)
    }

    /// Get specific action by UUID
    pub async fn get_action(
        &self,
        library_id: Uuid,
        action_uuid: Uuid,
    ) -> ActionResult<Option<audit_log::Model>> {
        let library = self.get_library(library_id).await?;
        let db = library.db().conn();
        
        AuditLog::find()
            .filter(audit_log::Column::Uuid.eq(action_uuid))
            .one(db)
            .await
            .map_err(ActionError::SeaOrm)
    }
}```

## src/infrastructure/actions/mod.rs

```rust
//! Action System - User-initiated operations with audit logging
//!
//! This module provides a centralized, robust, and extensible layer for handling
//! all user-initiated operations. It serves as the primary integration point
//! for the CLI and future APIs.

use crate::shared::types::SdPath;
use serde::{Deserialize, Serialize};
use std::path::PathBuf;
use uuid::Uuid;

pub mod builder;
pub mod error;
pub mod handler;
pub mod manager;
pub mod output;
pub mod receipt;
pub mod registry;
#[cfg(test)]
mod tests;


/// Represents a user-initiated action within Spacedrive.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum Action {
	// Global actions (no library context)
	LibraryCreate(crate::operations::libraries::create::action::LibraryCreateAction),
	LibraryDelete(crate::operations::libraries::delete::action::LibraryDeleteAction),
	
	// Library-scoped actions (require library_id)
	LibraryRename { 
		library_id: Uuid, 
		action: crate::operations::libraries::rename::action::LibraryRenameAction 
	},
	LibraryExport { 
		library_id: Uuid, 
		action: crate::operations::libraries::export::action::LibraryExportAction 
	},
	FileCopy { 
		library_id: Uuid, 
		action: crate::operations::files::copy::action::FileCopyAction 
	},
	FileDelete { 
		library_id: Uuid, 
		action: crate::operations::files::delete::action::FileDeleteAction 
	},
	FileValidate { 
		library_id: Uuid, 
		action: crate::operations::files::validation::ValidationAction 
	},
	DetectDuplicates { 
		library_id: Uuid, 
		action: crate::operations::files::duplicate_detection::DuplicateDetectionAction 
	},
	
	LocationAdd { 
		library_id: Uuid, 
		action: crate::operations::locations::add::action::LocationAddAction 
	},
	LocationRemove { 
		library_id: Uuid, 
		action: crate::operations::locations::remove::action::LocationRemoveAction 
	},
	LocationIndex { 
		library_id: Uuid, 
		action: crate::operations::locations::index::action::LocationIndexAction 
	},
	LocationRescan { 
		library_id: Uuid, 
		action: crate::operations::locations::rescan::action::LocationRescanAction 
	},
	
	Index { 
		library_id: Uuid, 
		action: crate::operations::indexing::action::IndexingAction 
	},
	
	GenerateThumbnails { 
		library_id: Uuid, 
		action: crate::operations::media::thumbnail::action::ThumbnailAction 
	},
	
	ContentAnalysis { 
		library_id: Uuid, 
		action: crate::operations::content::action::ContentAction 
	},
	
	MetadataOperation { 
		library_id: Uuid, 
		action: crate::operations::metadata::action::MetadataAction 
	},
	
	DeviceRevoke { 
		library_id: Uuid, 
		action: crate::operations::devices::revoke::action::DeviceRevokeAction 
	},
	
	VolumeTrack {
		action: crate::operations::volumes::track::action::VolumeTrackAction
	},
	VolumeUntrack {
		action: crate::operations::volumes::untrack::action::VolumeUntrackAction
	},
	VolumeSpeedTest {
		action: crate::operations::volumes::speed_test::action::VolumeSpeedTestAction
	},
}

impl Action {
	/// Returns the library ID for library-scoped actions
	pub fn library_id(&self) -> Option<Uuid> {
		match self {
			Action::LibraryCreate(_) | Action::LibraryDelete(_) => None,
			Action::LibraryRename { library_id, .. } => Some(*library_id),
			Action::LibraryExport { library_id, .. } => Some(*library_id),
			Action::FileCopy { library_id, .. } => Some(*library_id),
			Action::FileDelete { library_id, .. } => Some(*library_id),
			Action::FileValidate { library_id, .. } => Some(*library_id),
			Action::DetectDuplicates { library_id, .. } => Some(*library_id),
			Action::LocationAdd { library_id, .. } => Some(*library_id),
			Action::LocationRemove { library_id, .. } => Some(*library_id),
			Action::LocationIndex { library_id, .. } => Some(*library_id),
			Action::LocationRescan { library_id, .. } => Some(*library_id),
			Action::Index { library_id, .. } => Some(*library_id),
			Action::GenerateThumbnails { library_id, .. } => Some(*library_id),
			Action::ContentAnalysis { library_id, .. } => Some(*library_id),
			Action::MetadataOperation { library_id, .. } => Some(*library_id),
			Action::DeviceRevoke { library_id, .. } => Some(*library_id),
			Action::VolumeTrack { action } => Some(action.library_id),
			Action::VolumeUntrack { action } => Some(action.library_id),
			Action::VolumeSpeedTest { .. } => None,
		}
	}

	/// Returns a string identifier for the action type.
	pub fn kind(&self) -> &'static str {
		match self {
			Action::LibraryCreate(_) => "library.create",
			Action::LibraryDelete(_) => "library.delete",
			Action::LibraryRename { .. } => "library.rename",
			Action::LibraryExport { .. } => "library.export",
			Action::FileCopy { .. } => "file.copy",
			Action::FileDelete { .. } => "file.delete",
			Action::FileValidate { .. } => "file.validate",
			Action::DetectDuplicates { .. } => "file.detect_duplicates",
			Action::LocationAdd { .. } => "location.add",
			Action::LocationRemove { .. } => "location.remove",
			Action::LocationIndex { .. } => "location.index",
			Action::LocationRescan { .. } => "location.rescan",
			Action::Index { .. } => "indexing.index",
			Action::GenerateThumbnails { .. } => "media.thumbnail",
			Action::ContentAnalysis { .. } => "content.analyze",
			Action::MetadataOperation { .. } => "metadata.extract",
			Action::DeviceRevoke { .. } => "device.revoke",
			Action::VolumeTrack { .. } => "volume.track",
			Action::VolumeUntrack { .. } => "volume.untrack",
			Action::VolumeSpeedTest { .. } => "volume.speed_test",
		}
	}

	/// Returns a human-readable description of the action
	pub fn description(&self) -> String {
		match self {
			Action::LibraryCreate(action) => {
				format!("Create library '{}'", action.name)
			}
			Action::LibraryDelete(_action) => {
				"Delete library".to_string()
			}
			Action::LibraryRename { action, .. } => {
				format!("Rename library to '{}'", action.new_name)
			}
			Action::LibraryExport { action, .. } => {
				format!("Export library to {}", action.export_path.display())
			}
			Action::FileCopy { action, .. } => {
				format!(
					"Copy {} file(s) to {}",
					action.sources.len(),
					action.destination.display()
				)
			}
			Action::FileDelete { action, .. } => {
				format!("Delete {} file(s)", action.targets.len())
			}
			Action::FileValidate { action, .. } => {
				format!("Validate {} file(s)", action.paths.len())
			}
			Action::DetectDuplicates { action, .. } => {
				format!("Detect duplicates in {} path(s)", action.paths.len())
			}
			Action::LocationAdd { action, .. } => match &action.name {
				Some(name) => format!("Add location '{}' at {}", name, action.path.display()),
				None => format!("Add location at {}", action.path.display()),
			},
			Action::LocationRemove { action, .. } => {
				format!("Remove location {}", action.location_id)
			}
			Action::LocationIndex { action, .. } => {
				format!("Index location {} ({:?})", action.location_id, action.mode)
			}
			Action::LocationRescan { action, .. } => {
				let scan_type = if action.full_rescan { "Full" } else { "Quick" };
				format!("{} rescan location {}", scan_type, action.location_id)
			}
			Action::Index { action, .. } => {
				format!("Index {} path(s)", action.paths.len())
			}
			Action::GenerateThumbnails { action, .. } => {
				format!("Generate thumbnails for {} file(s)", action.paths.len())
			}
			Action::ContentAnalysis { action, .. } => {
				format!("Analyze content of {} file(s)", action.paths.len())
			}
			Action::MetadataOperation { action, .. } => {
				format!("Extract metadata from {} file(s)", action.paths.len())
			}
			Action::DeviceRevoke { action, .. } => {
				format!("Revoke device {}", action.device_id)
			}
			Action::VolumeTrack { action } => {
				match &action.name {
					Some(name) => format!("Track volume '{}' ({})", name, action.fingerprint),
					None => format!("Track volume {}", action.fingerprint),
				}
			}
			Action::VolumeUntrack { action } => {
				format!("Untrack volume {}", action.fingerprint)
			}
			Action::VolumeSpeedTest { action } => {
				format!("Speed test volume {}", action.fingerprint)
			}
		}
	}

	/// Returns target summary for audit logging
	pub fn targets_summary(&self) -> serde_json::Value {
		match self {
			Action::LibraryCreate(action) => serde_json::json!({
				"name": action.name,
				"path": action.path.as_ref().map(|p| p.display().to_string())
			}),
			Action::LibraryDelete(_action) => serde_json::json!({}),
			Action::LibraryRename { action, .. } => serde_json::json!({
				"new_name": action.new_name,
				"library_id": action.library_id
			}),
			Action::LibraryExport { action, .. } => serde_json::json!({
				"library_id": action.library_id,
				"export_path": action.export_path.display().to_string(),
				"include_thumbnails": action.include_thumbnails,
				"include_previews": action.include_previews
			}),
			Action::FileCopy { action, .. } => serde_json::json!({
				"sources": action.sources.iter().map(|s| s.display().to_string()).collect::<Vec<_>>(),
				"destination": action.destination.display().to_string()
			}),
			Action::FileDelete { action, .. } => serde_json::json!({
				"targets": action.targets.iter().map(|t| t.display().to_string()).collect::<Vec<_>>()
			}),
			Action::FileValidate { action, .. } => serde_json::json!({
				"paths": action.paths.iter().map(|p| p.display().to_string()).collect::<Vec<_>>()
			}),
			Action::DetectDuplicates { action, .. } => serde_json::json!({
				"paths": action.paths.iter().map(|p| p.display().to_string()).collect::<Vec<_>>()
			}),
			Action::LocationAdd { action, .. } => serde_json::json!({
				"path": action.path.display().to_string(),
				"name": action.name,
				"mode": action.mode
			}),
			Action::LocationRemove { action, .. } => serde_json::json!({
				"location_id": action.location_id
			}),
			Action::LocationIndex { action, .. } => serde_json::json!({
				"location_id": action.location_id,
				"mode": action.mode
			}),
			Action::LocationRescan { action, .. } => serde_json::json!({
				"location_id": action.location_id,
				"full_rescan": action.full_rescan
			}),
			Action::Index { action, .. } => serde_json::json!({
				"paths": action.paths.iter().map(|p| p.display().to_string()).collect::<Vec<_>>()
			}),
			Action::GenerateThumbnails { action, .. } => serde_json::json!({
				"paths": action.paths.iter().map(|p| p.display().to_string()).collect::<Vec<_>>()
			}),
			Action::ContentAnalysis { action, .. } => serde_json::json!({
				"paths": action.paths.iter().map(|p| p.display().to_string()).collect::<Vec<_>>()
			}),
			Action::MetadataOperation { action, .. } => serde_json::json!({
				"paths": action.paths.iter().map(|p| p.display().to_string()).collect::<Vec<_>>()
			}),
			Action::DeviceRevoke { action, .. } => serde_json::json!({
				"device_id": action.device_id,
				"reason": action.reason
			}),
			Action::VolumeTrack { action } => serde_json::json!({
				"fingerprint": action.fingerprint,
				"library_id": action.library_id,
				"name": action.name
			}),
			Action::VolumeUntrack { action } => serde_json::json!({
				"fingerprint": action.fingerprint,
				"library_id": action.library_id
			}),
			Action::VolumeSpeedTest { action } => serde_json::json!({
				"fingerprint": action.fingerprint
			}),
		}
	}
}
```

## src/infrastructure/actions/output.rs

```rust
//! Action execution output types

use serde::{Deserialize, Serialize};
use std::fmt;
use std::path::PathBuf;
use uuid::Uuid;
use crate::volume::VolumeFingerprint;

/// Trait for action outputs that can be serialized and displayed
pub trait ActionOutputTrait: std::fmt::Debug + Send + Sync {
    /// Serialize the output to JSON
    fn to_json(&self) -> serde_json::Value;
    
    /// Display the output as a human-readable string
    fn display_message(&self) -> String;
    
    /// Get the output type identifier
    fn output_type(&self) -> &'static str;
}

/// Serializable wrapper for action outputs
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(tag = "type", content = "data")]
pub enum ActionOutput {
    /// Simple success message
    Success { message: String },
    
    /// Volume was tracked
    VolumeTracked {
        fingerprint: VolumeFingerprint,
        library_id: Uuid,
        volume_name: String,
    },
    
    /// Volume was untracked
    VolumeUntracked {
        fingerprint: VolumeFingerprint,
        library_id: Uuid,
    },
    
    /// Volume speed test completed
    VolumeSpeedTested {
        fingerprint: VolumeFingerprint,
        read_speed_mbps: Option<u32>,
        write_speed_mbps: Option<u32>,
    },
    
    /// Generic output with custom data
    Custom {
        output_type: String,
        data: serde_json::Value,
        message: String,
    },
}

impl ActionOutput {
    /// Create output from any type implementing ActionOutputTrait
    pub fn from_trait<T: ActionOutputTrait>(output: T) -> Self {
        Self::Custom {
            output_type: output.output_type().to_string(),
            data: output.to_json(),
            message: output.display_message(),
        }
    }
    
    /// Create a simple success output
    pub fn success(message: &str) -> Self {
        Self::Success {
            message: message.to_string(),
        }
    }
}

impl Default for ActionOutput {
    fn default() -> Self {
        Self::success("Action completed successfully")
    }
}

impl fmt::Display for ActionOutput {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        match self {
            Self::Success { message } => write!(f, "{}", message),
            Self::VolumeTracked { volume_name, .. } => {
                write!(f, "Volume '{}' tracked successfully", volume_name)
            }
            Self::VolumeUntracked { fingerprint, .. } => {
                write!(f, "Volume {} untracked successfully", fingerprint)
            }
            Self::VolumeSpeedTested { 
                fingerprint, 
                read_speed_mbps, 
                write_speed_mbps 
            } => {
                match (read_speed_mbps, write_speed_mbps) {
                    (Some(read), Some(write)) => {
                        write!(f, "Volume {} speed test: {} MB/s read, {} MB/s write", 
                            fingerprint, read, write)
                    }
                    _ => write!(f, "Volume {} speed test completed", fingerprint),
                }
            }
            Self::Custom { message, .. } => write!(f, "{}", message),
        }
    }
}```

## src/infrastructure/actions/tests.rs

```rust
//! Tests for the Action System

#[cfg(test)]
mod tests {
    use super::*;
    use crate::{
        context::CoreContext,
        infrastructure::actions::{Action, registry::REGISTRY},
    };

    #[test]
    fn test_action_kind() {
        let action = Action::LibraryCreate(
            crate::operations::libraries::create::action::LibraryCreateAction {
                name: "Test Library".to_string(),
                path: None,
            }
        );
        assert_eq!(action.kind(), "library.create");
    }

    #[test]
    fn test_action_description() {
        let action = Action::LibraryCreate(
            crate::operations::libraries::create::action::LibraryCreateAction {
                name: "Test Library".to_string(),
                path: None,
            }
        );
        assert_eq!(action.description(), "Create library 'Test Library'");
    }

    #[test]
    fn test_action_targets_summary() {
        let action = Action::LibraryCreate(
            crate::operations::libraries::create::action::LibraryCreateAction {
                name: "Test Library".to_string(),
                path: Some("/path/to/library".into()),
            }
        );
        let summary = action.targets_summary();
        assert_eq!(summary["name"], "Test Library");
        assert_eq!(summary["path"], "/path/to/library");
    }

    #[test]
    fn test_registry_has_handlers() {
        // Test that the registry has been populated
        assert!(REGISTRY.has_action("library.create"));
        assert!(REGISTRY.has_action("library.delete"));
        assert!(REGISTRY.has_action("file.copy"));
        assert!(REGISTRY.has_action("file.delete"));
        assert!(REGISTRY.has_action("location.add"));
        assert!(REGISTRY.has_action("location.remove"));
        assert!(REGISTRY.has_action("location.index"));
        
        // Test that unknown actions are not registered
        assert!(!REGISTRY.has_action("unknown.action"));
    }

    #[test]
    fn test_action_registry_get_handler() {
        let handler = REGISTRY.get("library.create");
        assert!(handler.is_some());
        
        let handler = REGISTRY.get("unknown.action");
        assert!(handler.is_none());
    }
}```

## src/infrastructure/actions/receipt.rs

```rust
//! Action execution receipts

use crate::infrastructure::jobs::handle::JobHandle;
use serde::{Deserialize, Serialize};
use uuid::Uuid;

/// Receipt returned from action execution
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ActionReceipt {
    /// Unique identifier for the action execution
    pub action_id: Uuid,
    
    /// Optional job handle if the action created a background job
    #[serde(skip)]
    pub job_handle: Option<JobHandle>,
    
    /// Optional result payload (for immediate actions)
    pub result_payload: Option<serde_json::Value>,
    
    /// Whether the action completed immediately or is running in background
    pub is_immediate: bool,
}

impl ActionReceipt {
    /// Create a new receipt for an immediate action
    pub fn immediate(action_id: Uuid, result_payload: Option<serde_json::Value>) -> Self {
        Self {
            action_id,
            job_handle: None,
            result_payload,
            is_immediate: true,
        }
    }
    
    /// Create a new receipt for a job-based action
    pub fn job_based(action_id: Uuid, job_handle: JobHandle) -> Self {
        Self {
            action_id,
            job_handle: Some(job_handle),
            result_payload: None,
            is_immediate: false,
        }
    }
    
    /// Create a new receipt for a hybrid action (immediate with optional job)
    pub fn hybrid(
        action_id: Uuid,
        result_payload: Option<serde_json::Value>,
        job_handle: Option<JobHandle>,
    ) -> Self {
        let is_immediate = job_handle.is_none();
        Self {
            action_id,
            job_handle,
            result_payload,
            is_immediate,
        }
    }
}```

## src/infrastructure/actions/builder.rs

```rust
//! Builder pattern traits for actions

use std::error::Error;

/// Core trait for action builders
pub trait ActionBuilder {
    type Action;
    type Error: Error + Send + Sync + 'static;
    
    /// Validate the current builder state
    fn validate(&self) -> Result<(), Self::Error>;
    
    /// Build the final action instance
    fn build(self) -> Result<Self::Action, Self::Error>;
}

/// Trait for builders that can be constructed from CLI arguments
pub trait CliActionBuilder: ActionBuilder {
    type Args: clap::Parser;
    
    /// Create a builder from parsed CLI arguments
    fn from_cli_args(args: Self::Args) -> Self;
}

/// Errors that can occur during action building
#[derive(Debug, thiserror::Error)]
pub enum ActionBuildError {
    #[error("Validation errors: {0:?}")]
    Validation(Vec<String>),
    
    #[error("IO error: {0}")]
    Io(#[from] std::io::Error),
    
    #[error("Parse error: {0}")]
    Parse(String),
    
    #[error("Permission denied: {0}")]
    Permission(String),
    
    #[error("Invalid argument: {0}")]
    InvalidArgument(String),
    
    #[error("Required field missing: {0}")]
    RequiredField(String),
}

impl ActionBuildError {
    /// Create a validation error with a single message
    pub fn validation(message: impl Into<String>) -> Self {
        Self::Validation(vec![message.into()])
    }
    
    /// Create a validation error with multiple messages
    pub fn validations(messages: Vec<String>) -> Self {
        Self::Validation(messages)
    }
    
    /// Create a parse error
    pub fn parse(message: impl Into<String>) -> Self {
        Self::Parse(message.into())
    }
    
    /// Create a permission error
    pub fn permission(message: impl Into<String>) -> Self {
        Self::Permission(message.into())
    }
    
    /// Create an invalid argument error
    pub fn invalid_argument(message: impl Into<String>) -> Self {
        Self::InvalidArgument(message.into())
    }
    
    /// Create a required field error
    pub fn required_field(field: impl Into<String>) -> Self {
        Self::RequiredField(field.into())
    }
}```

## src/infrastructure/jobs/types.rs

```rust
//! Core types for the job system

use serde::{Deserialize, Serialize};
use std::fmt;
use uuid::Uuid;

/// Unique identifier for a job
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash, Serialize, Deserialize)]
pub struct JobId(pub Uuid);

impl JobId {
	pub fn new() -> Self {
		Self(Uuid::new_v4())
	}
}

impl fmt::Display for JobId {
	fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
		write!(f, "{}", self.0)
	}
}

impl From<Uuid> for JobId {
	fn from(uuid: Uuid) -> Self {
		Self(uuid)
	}
}

impl From<JobId> for Uuid {
	fn from(id: JobId) -> Self {
		id.0
	}
}

/// Current status of a job
#[derive(
	Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize, strum::Display, strum::EnumString,
)]
#[serde(rename_all = "snake_case")]
#[strum(serialize_all = "lowercase")]
pub enum JobStatus {
	/// Job is waiting to be executed
	Queued,
	/// Job is currently running
	Running,
	/// Job has been paused
	Paused,
	/// Job completed successfully
	Completed,
	/// Job failed with an error
	Failed,
	/// Job was cancelled
	Cancelled,
}

impl JobStatus {
	pub fn is_terminal(&self) -> bool {
		matches!(self, Self::Completed | Self::Failed | Self::Cancelled)
	}

	pub fn is_active(&self) -> bool {
		matches!(self, Self::Running | Self::Paused)
	}
}

/// Priority level for job execution
#[derive(Debug, Clone, Copy, PartialEq, Eq, PartialOrd, Ord, Serialize, Deserialize)]
pub struct JobPriority(pub i32);

impl JobPriority {
	pub const LOW: Self = Self(-1);
	pub const NORMAL: Self = Self(0);
	pub const HIGH: Self = Self(1);
	pub const CRITICAL: Self = Self(10);
}

impl Default for JobPriority {
	fn default() -> Self {
		Self::NORMAL
	}
}

/// Metrics collected during job execution
#[derive(Debug, Clone, Default, Serialize, Deserialize)]
pub struct JobMetrics {
	pub bytes_processed: u64,
	pub items_processed: u64,
	pub warnings_count: u32,
	pub non_critical_errors_count: u32,
	pub duration_ms: Option<u64>,
}

/// Schema definition for a job type
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct JobSchema {
	pub name: &'static str,
	pub resumable: bool,
	pub version: u32,
	pub description: Option<&'static str>,
}

/// Registration information for a job type
#[derive(Clone)]
pub struct JobRegistration {
	pub name: &'static str,
	pub schema_fn: fn() -> JobSchema,
	pub create_fn: fn(serde_json::Value) -> Result<Box<dyn ErasedJob>, serde_json::Error>,
	pub deserialize_fn: fn(&[u8]) -> Result<Box<dyn ErasedJob>, rmp_serde::decode::Error>,
}

/// Type-erased job for dynamic dispatch
pub trait ErasedJob: Send + Sync + std::fmt::Debug + 'static {
	fn create_executor(
		self: Box<Self>,
		job_id: JobId,
		library: std::sync::Arc<crate::library::Library>,
		job_db: std::sync::Arc<crate::infrastructure::jobs::database::JobDb>,
		status_tx: tokio::sync::watch::Sender<JobStatus>,
		progress_tx: tokio::sync::mpsc::UnboundedSender<
			crate::infrastructure::jobs::progress::Progress,
		>,
		broadcast_tx: tokio::sync::broadcast::Sender<
			crate::infrastructure::jobs::progress::Progress,
		>,
		checkpoint_handler: std::sync::Arc<
			dyn crate::infrastructure::jobs::context::CheckpointHandler,
		>,
		networking: Option<std::sync::Arc<crate::services::networking::NetworkingService>>,
		volume_manager: Option<std::sync::Arc<crate::volume::VolumeManager>>,
	) -> Box<dyn sd_task_system::Task<crate::infrastructure::jobs::error::JobError>>;

	fn serialize_state(&self) -> Result<Vec<u8>, crate::infrastructure::jobs::error::JobError>;
}

/// Information about a job (for display/querying)
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct JobInfo {
	pub id: Uuid,
	pub name: String,
	pub status: JobStatus,
	pub progress: f32,
	pub started_at: chrono::DateTime<chrono::Utc>,
	pub completed_at: Option<chrono::DateTime<chrono::Utc>>,
	pub error_message: Option<String>,
	pub parent_job_id: Option<Uuid>,
}
```

## src/infrastructure/jobs/handle.rs

```rust
//! Job handle for controlling running jobs

use super::{
    error::{JobError, JobResult},
    output::JobOutput,
    progress::Progress,
    types::{JobId, JobStatus},
};
use std::sync::Arc;
use sd_task_system::TaskHandle;
use tokio::sync::{broadcast, watch, Mutex};

/// Handle to a running job
#[derive(Debug)]
pub struct JobHandle {
    pub(crate) id: JobId,
    pub(crate) task_handle: Arc<Mutex<Option<TaskHandle<JobError>>>>,
    pub(crate) status_rx: watch::Receiver<JobStatus>,
    pub(crate) progress_rx: broadcast::Receiver<Progress>,
    pub(crate) output: Arc<Mutex<Option<JobOutput>>>,
}

impl Clone for JobHandle {
    fn clone(&self) -> Self {
        Self {
            id: self.id,
            task_handle: self.task_handle.clone(),
            status_rx: self.status_rx.clone(),
            progress_rx: self.progress_rx.resubscribe(),
            output: self.output.clone(),
        }
    }
}

impl JobHandle {
    /// Get the job ID
    pub fn id(&self) -> JobId {
        self.id
    }
    
    /// Get the current status
    pub fn status(&self) -> JobStatus {
        *self.status_rx.borrow()
    }
    
    /// Subscribe to status updates
    pub fn subscribe_status(&self) -> watch::Receiver<JobStatus> {
        self.status_rx.clone()
    }
    
    /// Subscribe to progress updates
    pub fn subscribe_progress(&self) -> broadcast::Receiver<Progress> {
        self.progress_rx.resubscribe()
    }
    
    /// Wait for the job to complete
    pub async fn wait(&self) -> JobResult<JobOutput> {
        // Wait for terminal status
        let mut status_rx = self.status_rx.clone();
        while !status_rx.borrow().is_terminal() {
            status_rx.changed().await
                .map_err(|_| JobError::Other("Status channel closed".into()))?;
        }
        
        // Check final status
        let final_status = *status_rx.borrow();
        match final_status {
            JobStatus::Completed => {
                // Get output
                let output = self.output.lock().await.clone()
                    .unwrap_or_default();
                Ok(output)
            }
            JobStatus::Failed => Err(JobError::ExecutionFailed("Job failed".into())),
            JobStatus::Cancelled => Err(JobError::Interrupted),
            _ => unreachable!("Non-terminal status after wait"),
        }
    }
    
    /// Pause the job
    pub async fn pause(&self) -> JobResult<()> {
        // For now, these operations need to be implemented through JobManager
        // since the TaskHandle is stored there, not in JobHandle
        todo!("Job control operations will be implemented through JobManager")
    }
    
    /// Resume the job
    pub async fn resume(&self) -> JobResult<()> {
        todo!("Job control operations will be implemented through JobManager")
    }
    
    /// Cancel the job
    pub async fn cancel(&self) -> JobResult<()> {
        todo!("Job control operations will be implemented through JobManager")
    }
    
    /// Force abort the job
    pub async fn force_abort(&self) -> JobResult<()> {
        todo!("Job control operations will be implemented through JobManager")
    }
}

/// Update events from a job
#[derive(Debug)]
pub enum JobUpdate {
    /// Status changed
    StatusChanged(JobStatus),
    /// Progress update
    Progress(Progress),
    /// Job completed
    Completed(JobOutput),
    /// Job failed
    Failed(JobError),
}

impl JobHandle {
    /// Subscribe to all job updates
    pub fn subscribe(&self) -> JobUpdateStream {
        JobUpdateStream {
            handle: self.clone(),
            status_rx: self.status_rx.clone(),
            progress_rx: self.progress_rx.resubscribe(),
        }
    }
}

/// Stream of job updates
pub struct JobUpdateStream {
    handle: JobHandle,
    status_rx: watch::Receiver<JobStatus>,
    progress_rx: broadcast::Receiver<Progress>,
}

impl JobUpdateStream {
    /// Get the next update
    pub async fn next(&mut self) -> Option<JobUpdate> {
        tokio::select! {
            // Status changes
            Ok(_) = self.status_rx.changed() => {
                let status = *self.status_rx.borrow();
                
                match status {
                    JobStatus::Completed => {
                        let output = self.handle.output.lock().await.clone()
                            .unwrap_or_default();
                        Some(JobUpdate::Completed(output))
                    }
                    JobStatus::Failed => {
                        Some(JobUpdate::Failed(JobError::ExecutionFailed("Job failed".into())))
                    }
                    _ => Some(JobUpdate::StatusChanged(status)),
                }
            }
            
            // Progress updates
            Ok(progress) = self.progress_rx.recv() => {
                Some(JobUpdate::Progress(progress))
            }
            
            else => None,
        }
    }
}```

## src/infrastructure/jobs/registry.rs

```rust
//! Job registry for automatic discovery

use super::{
    error::{JobError, JobResult},
    types::{ErasedJob, JobRegistration, JobSchema},
};
use inventory;
use once_cell::sync::Lazy;
use std::collections::HashMap;
use tracing::info;

// Inventory for auto-registration
inventory::collect!(JobRegistration);

/// Global job registry
pub struct JobRegistry {
    jobs: HashMap<&'static str, JobRegistration>,
}

impl JobRegistry {
    /// Create a new registry and discover all jobs
    pub fn new() -> Self {
        let mut jobs = HashMap::new();
        
        // Collect all registered jobs
        for registration in inventory::iter::<JobRegistration> {
            info!("Registered job: {}", registration.name);
            jobs.insert(registration.name, JobRegistration {
                name: registration.name,
                schema_fn: registration.schema_fn,
                create_fn: registration.create_fn,
                deserialize_fn: registration.deserialize_fn,
            });
        }
        
        info!("Discovered {} job types", jobs.len());
        
        Self { jobs }
    }
    
    /// Get all registered job names
    pub fn job_names(&self) -> Vec<&'static str> {
        self.jobs.keys().copied().collect()
    }
    
    /// Get schema for a job
    pub fn get_schema(&self, name: &str) -> Option<JobSchema> {
        self.jobs.get(name).map(|reg| (reg.schema_fn)())
    }
    
    /// Create a job instance from serialized data
    pub fn create_job(&self, name: &str, data: serde_json::Value) -> JobResult<Box<dyn ErasedJob>> {
        let registration = self.jobs.get(name)
            .ok_or_else(|| JobError::NotFound(format!("Job type '{}' not found", name)))?;
        
        (registration.create_fn)(data)
            .map_err(|e| JobError::serialization(e))
    }
    
    /// Deserialize a job instance from binary data (for resumption)
    pub fn deserialize_job(&self, name: &str, data: &[u8]) -> JobResult<Box<dyn ErasedJob>> {
        let registration = self.jobs.get(name)
            .ok_or_else(|| JobError::NotFound(format!("Job type '{}' not found", name)))?;
        
        (registration.deserialize_fn)(data)
            .map_err(|e| JobError::serialization(e))
    }
    
    /// Check if a job type is registered
    pub fn has_job(&self, name: &str) -> bool {
        self.jobs.contains_key(name)
    }
}

/// Global registry instance
pub static REGISTRY: Lazy<JobRegistry> = Lazy::new(JobRegistry::new);

/// Helper macro for registering jobs
/// This would be used by the derive macro
#[macro_export]
macro_rules! register_job {
    ($job_type:ty) => {
        inventory::submit! {
            $crate::infrastructure::jobs::types::JobRegistration {
                name: <$job_type as $crate::infrastructure::jobs::traits::Job>::NAME,
                schema_fn: <$job_type as $crate::infrastructure::jobs::traits::Job>::schema,
                create_fn: |data| {
                    let job: $job_type = serde_json::from_value(data)?;
                    Ok(Box::new($crate::infrastructure::jobs::executor::JobExecutor::new(job)))
                },
                deserialize_fn: |data| {
                    let job: $job_type = rmp_serde::from_slice(data)?;
                    Ok(Box::new($crate::infrastructure::jobs::executor::JobExecutor::new(job)))
                },
            }
        }
    };
}```

## src/infrastructure/jobs/database.rs

```rust
//! Job database schema and operations
//! This is the database for the job manager, not the global library database.
//! It is used to store the job history and checkpoints with serializable data for resuming jobs.
//! The job database is not synced between devices.
//! Jobs must be dispatched by the action system if initiated by the user.

use super::{
	error::{JobError, JobResult},
	types::{JobId, JobMetrics, JobStatus},
	progress::Progress,
};
use chrono::{DateTime, Utc};
use sea_orm::{
	entity::prelude::*,
	sea_query::{Expr, Query},
	ActiveModelTrait,
	ActiveValue::Set,
	ConnectionTrait, DatabaseConnection, DbBackend, DbErr, EntityTrait, QueryFilter, Schema,
	TransactionTrait,
};
use serde::{Deserialize, Serialize};
use serde_json::Value as JsonValue;
use std::path::Path;

pub mod jobs {
	use super::*;

	/// Job record in the database
	#[derive(Clone, Debug, PartialEq, DeriveEntityModel, Serialize, Deserialize)]
	#[sea_orm(table_name = "jobs")]
	pub struct Model {
		#[sea_orm(primary_key, auto_increment = false)]
		pub id: String,
		pub name: String,
		pub state: Vec<u8>,
		pub status: String,
		pub priority: i32,

		// Progress tracking
		pub progress_type: Option<String>,
		pub progress_data: Option<Vec<u8>>,

		// Relationships
		pub parent_job_id: Option<String>,

		// Timestamps
		pub created_at: DateTime<Utc>,
		pub started_at: Option<DateTime<Utc>>,
		pub completed_at: Option<DateTime<Utc>>,
		pub paused_at: Option<DateTime<Utc>>,

		// Error tracking
		pub error_message: Option<String>,
		pub warnings: Option<JsonValue>,
		pub non_critical_errors: Option<JsonValue>,

		// Metrics
		pub metrics: Option<Vec<u8>>,
	}

	#[derive(Copy, Clone, Debug, EnumIter, DeriveRelation)]
	pub enum Relation {}

	impl ActiveModelBehavior for ActiveModel {}
}

pub mod history {
	use super::*;

	/// Job history record
	#[derive(Clone, Debug, PartialEq, DeriveEntityModel, Serialize, Deserialize)]
	#[sea_orm(table_name = "job_history")]
	pub struct Model {
		#[sea_orm(primary_key, auto_increment = false)]
		pub id: String,
		pub name: String,
		pub status: String,
		pub started_at: DateTime<Utc>,
		pub completed_at: DateTime<Utc>,
		pub duration_ms: i64,
		pub output: Option<Vec<u8>>,
		pub metrics: Option<Vec<u8>>,
	}

	#[derive(Copy, Clone, Debug, EnumIter, DeriveRelation)]
	pub enum Relation {}

	impl ActiveModelBehavior for ActiveModel {}
}

pub mod checkpoint {
	use super::*;

	/// Job checkpoint record
	#[derive(Clone, Debug, PartialEq, DeriveEntityModel, Serialize, Deserialize)]
	#[sea_orm(table_name = "job_checkpoints")]
	pub struct Model {
		#[sea_orm(primary_key, auto_increment = false)]
		pub job_id: String,
		pub checkpoint_data: Vec<u8>,
		pub created_at: DateTime<Utc>,
	}

	#[derive(Copy, Clone, Debug, EnumIter, DeriveRelation)]
	pub enum Relation {}

	impl ActiveModelBehavior for ActiveModel {}
}

/// Initialize job database
pub async fn init_database(path: &Path) -> JobResult<DatabaseConnection> {
	// Ensure the directory exists
	tokio::fs::create_dir_all(path).await?;

	let db_path = path.join("jobs.db");
	let db_url = format!("sqlite://{}?mode=rwc", db_path.display());

	let db = sea_orm::Database::connect(&db_url).await?;

	// Create tables
	create_tables(&db).await?;

	Ok(db)
}

/// Create job tables
async fn create_tables(db: &DatabaseConnection) -> JobResult<()> {
	let schema = Schema::new(DbBackend::Sqlite);

	// Create jobs table if not exists
	let mut jobs_statement = schema.create_table_from_entity(jobs::Entity);
	jobs_statement.if_not_exists();
	db.execute(db.get_database_backend().build(&jobs_statement))
		.await?;

	// Create history table if not exists
	let mut history_statement = schema.create_table_from_entity(history::Entity);
	history_statement.if_not_exists();
	db.execute(db.get_database_backend().build(&history_statement))
		.await?;

	// Create checkpoint table if not exists
	let mut checkpoint_statement = schema.create_table_from_entity(checkpoint::Entity);
	checkpoint_statement.if_not_exists();
	db.execute(db.get_database_backend().build(&checkpoint_statement))
		.await?;

	Ok(())
}

/// Job database operations
pub struct JobDb {
	conn: DatabaseConnection,
}

impl JobDb {
	pub fn new(conn: DatabaseConnection) -> Self {
		Self { conn }
	}

	pub fn conn(&self) -> &DatabaseConnection {
		&self.conn
	}

	/// Get all queued jobs
	pub async fn get_queued_jobs(&self) -> JobResult<Vec<jobs::Model>> {
		jobs::Entity::find()
			.filter(jobs::Column::Status.eq(JobStatus::Queued.to_string()))
			.all(&self.conn)
			.await
			.map_err(Into::into)
	}

	/// Get a job by ID
	pub async fn get_job(&self, id: JobId) -> JobResult<Option<jobs::Model>> {
		jobs::Entity::find_by_id(id.to_string())
			.one(&self.conn)
			.await
			.map_err(Into::into)
	}

	/// Update job status
	pub async fn update_status(&self, id: JobId, status: JobStatus) -> JobResult<()> {
		let mut job = jobs::ActiveModel {
			id: Set(id.to_string()),
			status: Set(status.to_string()),
			..Default::default()
		};

		// Update timestamps based on status
		match status {
			JobStatus::Running => {
				job.started_at = Set(Some(Utc::now()));
			}
			JobStatus::Paused => {
				job.paused_at = Set(Some(Utc::now()));
			}
			JobStatus::Completed | JobStatus::Failed | JobStatus::Cancelled => {
				job.completed_at = Set(Some(Utc::now()));
			}
			_ => {}
		}

		job.update(&self.conn).await?;
		Ok(())
	}

	/// Update job progress in database
	pub async fn update_progress(&self, job_id: JobId, progress: &Progress) -> JobResult<()> {
		let progress_data = rmp_serde::to_vec(progress)
			.map_err(|e| JobError::serialization(e))?;
		
		let mut job = jobs::ActiveModel {
			id: Set(job_id.to_string()),
			progress_data: Set(Some(progress_data)),
			..Default::default()
		};
		
		job.update(&self.conn).await?;
		Ok(())
	}

	/// Update job status and optionally progress atomically
	pub async fn update_status_and_progress(
		&self,
		job_id: JobId,
		status: JobStatus,
		progress: Option<&Progress>,
		error_message: Option<String>,
	) -> JobResult<()> {
		// Use update query builder for partial updates
		let mut update = jobs::Entity::update_many()
			.filter(jobs::Column::Id.eq(job_id.to_string()))
			.col_expr(jobs::Column::Status, Expr::value(status.to_string()));
		
		// Update progress if provided
		if let Some(prog) = progress {
			let progress_data = rmp_serde::to_vec(prog)
				.map_err(|e| JobError::serialization(e))?;
			update = update.col_expr(jobs::Column::ProgressData, Expr::value(progress_data));
		}
		
		// Update error message if provided
		if let Some(err_msg) = error_message {
			update = update.col_expr(jobs::Column::ErrorMessage, Expr::value(err_msg));
		}
		
		// Update timestamps based on status
		let now = Utc::now();
		match status {
			JobStatus::Running => {
				update = update.col_expr(jobs::Column::StartedAt, Expr::value(now));
			}
			JobStatus::Paused => {
				update = update.col_expr(jobs::Column::PausedAt, Expr::value(now));
			}
			JobStatus::Completed | JobStatus::Failed | JobStatus::Cancelled => {
				update = update.col_expr(jobs::Column::CompletedAt, Expr::value(now));
			}
			_ => {}
		}
		
		let result = update.exec(&self.conn).await?;
		
		if result.rows_affected == 0 {
			return Err(JobError::NotFound(format!("Job {} not found or update failed", job_id)));
		}
		
		Ok(())
	}

	/// Clean up old job history
	pub async fn cleanup_history(&self, older_than: DateTime<Utc>) -> JobResult<u64> {
		let result = history::Entity::delete_many()
			.filter(history::Column::CompletedAt.lt(older_than))
			.exec(&self.conn)
			.await?;

		Ok(result.rows_affected)
	}
}
```

## src/infrastructure/jobs/error.rs

```rust
//! Error types for the job system

use std::fmt;
use thiserror::Error;

/// Result type for job operations
pub type JobResult<T = ()> = Result<T, JobError>;

/// Errors that can occur during job execution
#[derive(Debug, Error)]
pub enum JobError {
    /// Job was interrupted (paused or cancelled)
    #[error("Job was interrupted")]
    Interrupted,
    
    /// Job execution failed
    #[error("Job execution failed: {0}")]
    ExecutionFailed(String),
    
    /// Database operation failed
    #[error("Database error: {0}")]
    Database(#[from] sea_orm::DbErr),
    
    /// Serialization/deserialization error
    #[error("Serialization error: {0}")]
    Serialization(String),
    
    /// Job not found
    #[error("Job not found: {0}")]
    NotFound(String),
    
    /// Invalid job state
    #[error("Invalid job state: {0}")]
    InvalidState(String),
    
    /// Task system error
    #[error("Task system error: {0}")]
    TaskSystem(String),
    
    /// I/O error
    #[error("I/O error: {0}")]
    Io(#[from] std::io::Error),
    
    /// Other errors
    #[error("{0}")]
    Other(Box<dyn std::error::Error + Send + Sync>),
}

impl From<String> for JobError {
    fn from(msg: String) -> Self {
        Self::ExecutionFailed(msg)
    }
}

impl JobError {
    /// Create an execution failed error
    pub fn execution<T: fmt::Display>(msg: T) -> Self {
        Self::ExecutionFailed(msg.to_string())
    }
    
    /// Create a serialization error
    pub fn serialization<T: fmt::Display>(msg: T) -> Self {
        Self::Serialization(msg.to_string())
    }
    
    /// Create an invalid state error
    pub fn invalid_state<T: fmt::Display>(msg: T) -> Self {
        Self::InvalidState(msg.to_string())
    }
    
    /// Create a task system error
    pub fn task_system<T: fmt::Display>(msg: T) -> Self {
        Self::TaskSystem(msg.to_string())
    }
    
    /// Check if this error is due to interruption
    pub fn is_interrupted(&self) -> bool {
        matches!(self, Self::Interrupted)
    }
}

// JobError automatically implements RunError via blanket implementation```

## src/infrastructure/jobs/manager.rs

```rust
//! Job manager for scheduling and executing jobs
//! The job manager has its own database in the library directory, not the global library database.

use super::{
	context::CheckpointHandler,
	database::{self, JobDb},
	error::{JobError, JobResult},
	executor::JobExecutor,
	handle::JobHandle,
	progress::Progress,
	registry::REGISTRY,
	traits::{Job, JobHandler},
	types::{JobId, JobInfo, JobPriority, JobStatus},
};
use crate::{
	context::CoreContext,
	infrastructure::events::{Event, EventBus},
	library::Library,
};
use async_trait::async_trait;
use chrono::Utc;
use sd_task_system::{TaskDispatcher, TaskHandle, TaskSystem};
use sea_orm::{ActiveModelTrait, ActiveValue::Set, DatabaseConnection, EntityTrait};
use std::{collections::HashMap, path::PathBuf, sync::Arc};
use tokio::sync::{broadcast, mpsc, watch, Mutex, RwLock};
use tracing::{debug, error, info, warn};

/// Manages job execution for a library
pub struct JobManager {
	db: Arc<JobDb>,
	dispatcher: Arc<TaskSystem<JobError>>,
	running_jobs: Arc<RwLock<HashMap<JobId, RunningJob>>>,
	shutdown_tx: watch::Sender<bool>,
	context: Arc<CoreContext>,
	library_id: uuid::Uuid,
}

struct RunningJob {
	handle: JobHandle,
	task_handle: TaskHandle<JobError>,
	status_tx: watch::Sender<JobStatus>,
	latest_progress: Arc<Mutex<Option<Progress>>>,
}

impl JobManager {
	/// Create a new job manager
	pub async fn new(
		data_dir: PathBuf,
		context: Arc<CoreContext>,
		library_id: uuid::Uuid,
	) -> JobResult<Self> {
		// Initialize job database
		let job_db_path = data_dir.join("jobs.db");
		let db = database::init_database(&job_db_path).await?;

		// Create task system
		let dispatcher = TaskSystem::new();

		let (shutdown_tx, _) = watch::channel(false);

		let manager = Self {
			db: Arc::new(JobDb::new(db)),
			dispatcher: Arc::new(dispatcher),
			running_jobs: Arc::new(RwLock::new(HashMap::new())),
			shutdown_tx,
			context,
			library_id,
		};

		Ok(manager)
	}

	/// Initialize job manager (resume interrupted jobs)
	pub async fn initialize(&self) -> JobResult<()> {
		if let Err(e) = self.resume_interrupted_jobs().await {
			error!("Failed to resume interrupted jobs: {}", e);
		}
		Ok(())
	}

	/// Dispatch a job for execution
	pub async fn dispatch<J>(&self, job: J) -> JobResult<JobHandle>
	where
		J: Job + JobHandler,
	{
		self.dispatch_with_priority(job, JobPriority::NORMAL).await
	}

	/// Dispatch a job by name and parameters (useful for APIs)
	pub async fn dispatch_by_name(
		&self,
		job_name: &str,
		params: serde_json::Value,
	) -> JobResult<JobHandle> {
		self.dispatch_by_name_with_priority(job_name, params, JobPriority::NORMAL)
			.await
	}

	/// Dispatch a job by name with specific priority
	pub async fn dispatch_by_name_with_priority(
		&self,
		job_name: &str,
		params: serde_json::Value,
		priority: JobPriority,
	) -> JobResult<JobHandle> {
		// Check if job type is registered
		if !REGISTRY.has_job(job_name) {
			return Err(JobError::NotFound(format!(
				"Job type '{}' not registered",
				job_name
			)));
		}

		// Create job instance
		let erased_job = REGISTRY.create_job(job_name, params)?;

		let job_id = JobId::new();
		info!("Dispatching job {} ({}): {}", job_id, job_name, job_name);

		// Serialize job state for database
		let state = erased_job.serialize_state()?;

		// Create database record
		let job_model = database::jobs::ActiveModel {
			id: Set(job_id.to_string()),
			name: Set(job_name.to_string()),
			state: Set(state),
			status: Set(JobStatus::Queued.to_string()),
			priority: Set(priority.0),
			progress_type: Set(None),
			progress_data: Set(None),
			parent_job_id: Set(None),
			created_at: Set(Utc::now()),
			started_at: Set(None),
			completed_at: Set(None),
			paused_at: Set(None),
			error_message: Set(None),
			warnings: Set(None),
			non_critical_errors: Set(None),
			metrics: Set(None),
		};

		job_model.insert(self.db.conn()).await?;

		// Create channels
		let (status_tx, status_rx) = watch::channel(JobStatus::Queued);
		let (progress_tx, progress_rx) = mpsc::unbounded_channel();
		let (broadcast_tx, broadcast_rx) = broadcast::channel(100);

		// Create storage for latest progress
		let latest_progress = Arc::new(Mutex::new(None));

		// Create progress forwarding task
		let broadcast_tx_clone = broadcast_tx.clone();
		let latest_progress_clone = latest_progress.clone();
		let event_bus = self.context.events.clone();
		let job_id_clone = job_id.clone();
		let job_type_str = job_name.to_string();
		tokio::spawn(async move {
			let mut progress_rx: mpsc::UnboundedReceiver<Progress> = progress_rx;
			while let Some(progress) = progress_rx.recv().await {
				*latest_progress_clone.lock().await = Some(progress.clone());
				let _ = broadcast_tx_clone.send(progress.clone());

				// Emit enhanced progress event
				use crate::infrastructure::events::Event;

				// Extract generic progress data if available
				let generic_progress = match &progress {
					Progress::Structured(value) => {
						// Try to deserialize CopyProgress and convert to GenericProgress
						if let Ok(copy_progress) = serde_json::from_value::<
							crate::operations::files::copy::CopyProgress,
						>(value.clone())
						{
							use crate::infrastructure::jobs::generic_progress::ToGenericProgress;
							Some(serde_json::to_value(copy_progress.to_generic_progress()).ok())
						} else {
							None
						}
					}
					Progress::Generic(gp) => Some(serde_json::to_value(gp).ok()),
					_ => None,
				}
				.flatten();

				event_bus.emit(Event::JobProgress {
					job_id: job_id_clone.to_string(),
					job_type: job_type_str.to_string(),
					progress: progress.as_percentage().unwrap_or(0.0) as f64,
					message: Some(progress.to_string()),
					generic_progress,
				});
			}
		});

		// Get library from context using stored library_id
		let library = self
			.context
			.library_manager
			.get_library(self.library_id)
			.await
			.ok_or_else(|| {
				JobError::invalid_state(&format!("Library {} not found", self.library_id))
			})?;

		// Get services from context
		let networking = self.context.get_networking().await;
		let volume_manager = Some(self.context.volume_manager.clone());

		// Create executor using the erased job
		let executor = erased_job.create_executor(
			job_id,
			library,
			self.db.clone(),
			status_tx.clone(),
			progress_tx,
			broadcast_tx,
			Arc::new(DbCheckpointHandler {
				db: self.db.clone(),
			}),
			networking,
			volume_manager,
		);

		// Create handle
		let handle = JobHandle {
			id: job_id,
			task_handle: Arc::new(Mutex::new(None)),
			status_rx,
			progress_rx: broadcast_rx,
			output: Arc::new(Mutex::new(None)),
		};

		// Dispatch to task system
		let task_handle = self
			.dispatcher
			.get_dispatcher()
			.dispatch_boxed(executor)
			.await;

		match task_handle {
			Ok(handle_result) => {
				// Track running job
				self.running_jobs.write().await.insert(
					job_id,
					RunningJob {
						handle: handle.clone(),
						task_handle: handle_result,
						status_tx: status_tx.clone(),
						latest_progress,
					},
				);

				// Spawn a task to monitor job completion and clean up
				let running_jobs = self.running_jobs.clone();
				let job_id_clone = job_id.clone();
				let event_bus = self.context.events.clone();
				let job_type_str = job_name.to_string();
				tokio::spawn(async move {
					let mut status_rx = status_tx.subscribe();
					while status_rx.changed().await.is_ok() {
						let status = *status_rx.borrow();
						match status {
							JobStatus::Completed => {
								// Emit completion event
								event_bus.emit(Event::JobCompleted {
									job_id: job_id_clone.to_string(),
									job_type: job_type_str.clone(),
								});
								// Remove from running jobs
								running_jobs.write().await.remove(&job_id_clone);
								info!(
									"Job {} completed and removed from running jobs",
									job_id_clone
								);
								break;
							}
							JobStatus::Failed => {
								// Emit failure event
								event_bus.emit(Event::JobFailed {
									job_id: job_id_clone.to_string(),
									job_type: job_type_str.clone(),
									error: "Job failed".to_string(),
								});
								// Remove from running jobs
								running_jobs.write().await.remove(&job_id_clone);
								info!("Job {} failed and removed from running jobs", job_id_clone);
								break;
							}
							JobStatus::Cancelled => {
								// Emit cancellation event
								event_bus.emit(Event::JobCancelled {
									job_id: job_id_clone.to_string(),
									job_type: job_type_str.clone(),
								});
								// Remove from running jobs
								running_jobs.write().await.remove(&job_id_clone);
								info!(
									"Job {} cancelled and removed from running jobs",
									job_id_clone
								);
								break;
							}
							_ => {} // Continue monitoring for other status changes
						}
					}
				});

				Ok(handle)
			}
			Err(e) => Err(JobError::task_system(format!("{:?}", e))),
		}
	}

	/// Dispatch a job with specific priority
	pub async fn dispatch_with_priority<J>(
		&self,
		job: J,
		priority: JobPriority,
	) -> JobResult<JobHandle>
	where
		J: Job + JobHandler,
	{
		let job_id = JobId::new();
		info!("Dispatching job {}: {}", job_id, J::NAME);

		// Serialize job state
		let state =
			rmp_serde::to_vec(&job).map_err(|e| JobError::serialization(format!("{}", e)))?;

		// Create database record
		let job_model = database::jobs::ActiveModel {
			id: Set(job_id.to_string()),
			name: Set(J::NAME.to_string()),
			state: Set(state),
			status: Set(JobStatus::Queued.to_string()),
			priority: Set(priority.0),
			progress_type: Set(None),
			progress_data: Set(None),
			parent_job_id: Set(None),
			created_at: Set(Utc::now()),
			started_at: Set(None),
			completed_at: Set(None),
			paused_at: Set(None),
			error_message: Set(None),
			warnings: Set(None),
			non_critical_errors: Set(None),
			metrics: Set(None),
		};

		job_model.insert(self.db.conn()).await?;

		// Create channels
		let (status_tx, status_rx) = watch::channel(JobStatus::Queued);
		let (progress_tx, progress_rx) = mpsc::unbounded_channel();
		let (broadcast_tx, broadcast_rx) = broadcast::channel(100);

		// Create storage for latest progress
		let latest_progress = Arc::new(Mutex::new(None));

		// Create progress forwarding task with batching and throttling
		let broadcast_tx_clone = broadcast_tx.clone();
		let latest_progress_clone = latest_progress.clone();
		let event_bus = self.context.events.clone();
		let job_id_clone = job_id.clone();
		let job_type_str = J::NAME;
		let job_db_clone = self.db.clone();

		tokio::spawn(async move {
			let mut progress_rx: mpsc::UnboundedReceiver<Progress> = progress_rx;
			let mut last_db_update = std::time::Instant::now();
			const DB_UPDATE_INTERVAL: std::time::Duration = std::time::Duration::from_secs(2);

			while let Some(progress) = progress_rx.recv().await {
				// Store latest progress
				*latest_progress_clone.lock().await = Some(progress.clone());

				// Forward progress from mpsc to broadcast
				// Ignore errors if no one is listening
				let _ = broadcast_tx_clone.send(progress.clone());

				// Persist progress to database with throttling
				if last_db_update.elapsed() >= DB_UPDATE_INTERVAL {
					if let Err(e) = job_db_clone.update_progress(job_id_clone, &progress).await {
						debug!("Failed to persist job progress to database: {}", e);
					}
					last_db_update = std::time::Instant::now();
				}

				// Emit enhanced progress event
				use crate::infrastructure::events::Event;

				// Extract generic progress data if available
				let generic_progress = match &progress {
					Progress::Structured(value) => {
						// Try to deserialize CopyProgress and convert to GenericProgress
						if let Ok(copy_progress) = serde_json::from_value::<
							crate::operations::files::copy::CopyProgress,
						>(value.clone())
						{
							use crate::infrastructure::jobs::generic_progress::ToGenericProgress;
							Some(serde_json::to_value(copy_progress.to_generic_progress()).ok())
						} else {
							None
						}
					}
					Progress::Generic(gp) => Some(serde_json::to_value(gp).ok()),
					_ => None,
				}
				.flatten();

				event_bus.emit(Event::JobProgress {
					job_id: job_id_clone.to_string(),
					job_type: job_type_str.to_string(),
					progress: progress.as_percentage().unwrap_or(0.0) as f64,
					message: Some(progress.to_string()),
					generic_progress,
				});
			}

			// Final progress update when channel closes
			if let Some(final_progress) = &*latest_progress_clone.lock().await {
				if let Err(e) = job_db_clone
					.update_progress(job_id_clone, final_progress)
					.await
				{
					debug!("Failed to persist final job progress to database: {}", e);
				}
			}
		});

		// Get library from context using stored library_id
		let library = self
			.context
			.library_manager
			.get_library(self.library_id)
			.await
			.ok_or_else(|| {
				JobError::invalid_state(&format!("Library {} not found", self.library_id))
			})?;

		// Get services from context
		let networking = self.context.get_networking().await;
		let volume_manager = Some(self.context.volume_manager.clone());

		// Create executor
		let executor = JobExecutor::new(
			job,
			job_id,
			library,
			self.db.clone(),
			status_tx.clone(),
			progress_tx,
			broadcast_tx,
			Arc::new(DbCheckpointHandler {
				db: self.db.clone(),
			}),
			networking,
			volume_manager,
		);

		// Clone status_rx for cleanup task
		let status_rx_cleanup = status_rx.clone();

		// Create handle
		let handle = JobHandle {
			id: job_id,
			task_handle: Arc::new(Mutex::new(None)),
			status_rx,
			progress_rx: broadcast_rx,
			output: Arc::new(Mutex::new(None)),
		};

		// Dispatch to task system
		let task_handle = self.dispatcher.dispatch(executor).await;

		match task_handle {
			Ok(handle_result) => {
				// We don't store the task handle in JobHandle anymore
				// since it's already stored in RunningJob

				// Track running job
				self.running_jobs.write().await.insert(
					job_id,
					RunningJob {
						handle: handle.clone(),
						task_handle: handle_result,
						status_tx: status_tx.clone(),
						latest_progress: latest_progress.clone(),
					},
				);

				// Spawn a task to monitor job completion and clean up
				let running_jobs = self.running_jobs.clone();
				let job_id_clone = job_id.clone();
				let event_bus = self.context.events.clone();
				let job_type_str = J::NAME;
				tokio::spawn(async move {
					info!("Started cleanup monitor for job {}", job_id_clone);
					let mut status_monitor = status_rx_cleanup;
					while status_monitor.changed().await.is_ok() {
						let status = *status_monitor.borrow();
						info!("Job {} status changed to: {:?}", job_id_clone, status);
						match status {
							JobStatus::Completed => {
								// Emit completion event
								event_bus.emit(Event::JobCompleted {
									job_id: job_id_clone.to_string(),
									job_type: job_type_str.to_string(),
								});
								// Remove from running jobs
								running_jobs.write().await.remove(&job_id_clone);
								info!(
									"Job {} completed and removed from running jobs",
									job_id_clone
								);
								break;
							}
							JobStatus::Failed => {
								// Emit failure event
								event_bus.emit(Event::JobFailed {
									job_id: job_id_clone.to_string(),
									job_type: job_type_str.to_string(),
									error: "Job failed".to_string(),
								});
								// Remove from running jobs
								running_jobs.write().await.remove(&job_id_clone);
								info!("Job {} failed and removed from running jobs", job_id_clone);
								break;
							}
							JobStatus::Cancelled => {
								// Emit cancellation event
								event_bus.emit(Event::JobCancelled {
									job_id: job_id_clone.to_string(),
									job_type: job_type_str.to_string(),
								});
								// Remove from running jobs
								running_jobs.write().await.remove(&job_id_clone);
								info!(
									"Job {} cancelled and removed from running jobs",
									job_id_clone
								);
								break;
							}
							_ => {} // Continue monitoring for other status changes
						}
					}
				});

				Ok(handle)
			}
			Err(e) => Err(JobError::task_system(format!("{:?}", e))),
		}
	}

	/// Get a handle to a running job
	pub async fn get_job(&self, id: JobId) -> Option<JobHandle> {
		self.running_jobs
			.read()
			.await
			.get(&id)
			.map(|j| j.handle.clone())
	}

	/// List all available job types
	pub fn list_job_types(&self) -> Vec<&'static str> {
		REGISTRY.job_names()
	}

	/// Get schema for a job type
	pub fn get_job_schema(&self, job_name: &str) -> Option<super::types::JobSchema> {
		REGISTRY.get_schema(job_name)
	}

	/// List currently running jobs from memory (for live monitoring)
	pub async fn list_running_jobs(&self) -> Vec<JobInfo> {
		let running_jobs = self.running_jobs.read().await;
		let mut job_infos = Vec::new();

		for (job_id, running_job) in running_jobs.iter() {
			let handle = &running_job.handle;
			let status = handle.status();

			// Only include active jobs (running or paused)
			if status.is_active() {
				// Get latest progress
				let progress_percentage =
					if let Some(progress) = running_job.latest_progress.lock().await.as_ref() {
						progress.as_percentage().unwrap_or(0.0)
					} else {
						0.0
					};

				// Create JobInfo from in-memory state
				let job_info = JobInfo {
					id: job_id.0,
					name: format!("Job {}", job_id), // Use job ID as name for now
					status,
					progress: progress_percentage,
					started_at: chrono::Utc::now(), // TODO: Get actual start time
					completed_at: None,
					error_message: None,
					parent_job_id: None,
				};

				job_infos.push(job_info);
			}
		}

		job_infos
	}

	/// List all jobs with a specific status (unified query)
	pub async fn list_jobs(&self, status: Option<JobStatus>) -> JobResult<Vec<JobInfo>> {
		use sea_orm::QueryFilter;

		// First, get running jobs from memory for accurate real-time status
		let mut all_jobs = Vec::new();
		let running_jobs_map = self.running_jobs.read().await;

		// Collect job IDs that are in memory
		let mut in_memory_ids = std::collections::HashSet::new();

		for (job_id, running_job) in running_jobs_map.iter() {
			let handle = &running_job.handle;
			let current_status = handle.status();

			in_memory_ids.insert(job_id.0.to_string());

			// Check if status matches filter
			if let Some(filter_status) = status {
				if current_status != filter_status {
					continue;
				}
			}

			// Get latest progress from memory
			let progress_percentage =
				if let Some(progress) = running_job.latest_progress.lock().await.as_ref() {
					progress.as_percentage().unwrap_or(0.0)
				} else {
					0.0
				};

			// Get job name from database for complete info
			let job_name = match database::jobs::Entity::find_by_id(job_id.0.to_string())
				.one(self.db.conn())
				.await?
			{
				Some(db_job) => db_job.name,
				None => format!("Job {}", job_id.0),
			};

			all_jobs.push(JobInfo {
				id: job_id.0,
				name: job_name,
				status: current_status,
				progress: progress_percentage,
				started_at: chrono::Utc::now(), // TODO: Get from DB
				completed_at: None,
				error_message: None,
				parent_job_id: None,
			});
		}
		drop(running_jobs_map);

		// Now query database for jobs not in memory
		let mut query = database::jobs::Entity::find();

		if let Some(status) = status {
			use sea_orm::ColumnTrait;
			query = query.filter(database::jobs::Column::Status.eq(status.to_string()));
		}

		let db_jobs = query.all(self.db.conn()).await?;

		// Add database jobs that aren't in memory
		for j in db_jobs {
			// Skip if already in memory (memory takes precedence)
			if in_memory_ids.contains(&j.id) {
				continue;
			}

			let id = match j.id.parse::<Uuid>() {
				Ok(id) => id,
				Err(_) => continue,
			};

			let status = match j.status.as_str() {
				"queued" => JobStatus::Queued,
				"running" => JobStatus::Running,
				"paused" => JobStatus::Paused,
				"completed" => JobStatus::Completed,
				"failed" => JobStatus::Failed,
				"cancelled" => JobStatus::Cancelled,
				_ => continue,
			};

			// Parse progress from database
			let progress = if let Some(progress_data) = &j.progress_data {
				rmp_serde::from_slice::<Progress>(progress_data)
					.ok()
					.and_then(|p| p.as_percentage())
					.unwrap_or(0.0)
			} else {
				0.0
			};

			all_jobs.push(JobInfo {
				id,
				name: j.name,
				status,
				progress,
				started_at: j.started_at.unwrap_or(j.created_at),
				completed_at: j.completed_at,
				error_message: j.error_message,
				parent_job_id: j.parent_job_id.and_then(|s| s.parse::<Uuid>().ok()),
			});
		}

		Ok(all_jobs)
	}

	/// Get detailed information about a specific job
	pub async fn get_job_info(&self, id: Uuid) -> JobResult<Option<JobInfo>> {
		let job_id = JobId(id);

		if let Some(running_job) = self.running_jobs.read().await.get(&job_id) {
			let handle = &running_job.handle;
			let status = handle.status();

			// Get latest progress from memory
			let progress = if let Some(progress) = running_job.latest_progress.lock().await.as_ref()
			{
				progress.as_percentage().unwrap_or(0.0)
			} else {
				0.0
			};

			// For running jobs, we also need the job name from database
			let job_name = match database::jobs::Entity::find_by_id(id.to_string())
				.one(self.db.conn())
				.await?
			{
				Some(db_job) => db_job.name,
				None => format!("Job {}", id), // Fallback if not in DB
			};

			return Ok(Some(JobInfo {
				id,
				name: job_name,
				status,
				progress,
				started_at: chrono::Utc::now(), // TODO: Get actual start time from DB
				completed_at: None,             // Running jobs aren't completed yet
				error_message: None,            // TODO: Get from handle if failed
				parent_job_id: None,            // TODO: Get from DB if needed
			}));
		}

		let job = database::jobs::Entity::find_by_id(id.to_string())
			.one(self.db.conn())
			.await?;

		Ok(job.and_then(|j| {
			let id = j.id.parse::<Uuid>().ok()?;
			let status = match j.status.as_str() {
				"queued" => JobStatus::Queued,
				"running" => JobStatus::Running,
				"paused" => JobStatus::Paused,
				"completed" => JobStatus::Completed,
				"failed" => JobStatus::Failed,
				"cancelled" => JobStatus::Cancelled,
				_ => return None,
			};

			let progress = if let Some(progress_data) = &j.progress_data {
				rmp_serde::from_slice::<Progress>(progress_data)
					.ok()
					.and_then(|p| p.as_percentage())
					.unwrap_or(0.0)
			} else {
				0.0
			};

			Some(JobInfo {
				id,
				name: j.name,
				status,
				progress,
				started_at: j.started_at.unwrap_or(j.created_at),
				completed_at: j.completed_at,
				error_message: j.error_message,
				parent_job_id: j.parent_job_id.and_then(|s| s.parse::<Uuid>().ok()),
			})
		}))
	}

	/// Resume interrupted jobs from the last run
	async fn resume_interrupted_jobs(&self) -> JobResult<()> {
		info!("Checking for interrupted jobs to resume");

		use sea_orm::{ColumnTrait, QueryFilter};
		let interrupted = database::jobs::Entity::find()
			.filter(database::jobs::Column::Status.is_in([
				JobStatus::Running.to_string(),
				JobStatus::Paused.to_string(),
			]))
			.all(self.db.conn())
			.await?;

		for job_record in interrupted {
			if let Ok(job_id) = job_record.id.parse::<Uuid>().map(JobId) {
				info!("Resuming job {}: {}", job_id, job_record.name);

				// Deserialize job from binary data
				match REGISTRY.deserialize_job(&job_record.name, &job_record.state) {
					Ok(erased_job) => {
						// Create channels for the resumed job
						let (status_tx, status_rx) = watch::channel(JobStatus::Paused);
						let (progress_tx, progress_rx) = mpsc::unbounded_channel();
						let (broadcast_tx, broadcast_rx) = broadcast::channel(100);

						let latest_progress = Arc::new(Mutex::new(None));

						// Create progress forwarding task
						let broadcast_tx_clone = broadcast_tx.clone();
						let latest_progress_clone = latest_progress.clone();
						tokio::spawn(async move {
							let mut progress_rx: mpsc::UnboundedReceiver<Progress> = progress_rx;
							while let Some(progress) = progress_rx.recv().await {
								*latest_progress_clone.lock().await = Some(progress.clone());
								let _ = broadcast_tx_clone.send(progress);
							}
						});

						// Get library from context using stored library_id
						let library = self
							.context
							.library_manager
							.get_library(self.library_id)
							.await
							.ok_or_else(|| {
								JobError::invalid_state(&format!(
									"Library {} not found",
									self.library_id
								))
							})?;

						// Get services from context
						let networking = self.context.get_networking().await;
						let volume_manager = Some(self.context.volume_manager.clone());

						// Create executor using the erased job
						let executor = erased_job.create_executor(
							job_id,
							library,
							self.db.clone(),
							status_tx.clone(),
							progress_tx,
							broadcast_tx,
							Arc::new(DbCheckpointHandler {
								db: self.db.clone(),
							}),
							networking,
							volume_manager,
						);

						// Create handle
						let handle = JobHandle {
							id: job_id,
							task_handle: Arc::new(Mutex::new(None)),
							status_rx,
							progress_rx: broadcast_rx,
							output: Arc::new(Mutex::new(None)),
						};

						// Dispatch to task system
						match self
							.dispatcher
							.get_dispatcher()
							.dispatch_boxed(executor)
							.await
						{
							Ok(task_handle) => {
								// Track running job
								self.running_jobs.write().await.insert(
									job_id,
									RunningJob {
										handle: handle.clone(),
										task_handle,
										status_tx: status_tx.clone(),
										latest_progress,
									},
								);

								// Spawn a task to monitor resumed job completion and clean up
								let running_jobs = self.running_jobs.clone();
								let job_id_clone = job_id.clone();
								let event_bus = self.context.events.clone();
								let job_type_str = job_record.name.clone();
								tokio::spawn(async move {
									let mut status_rx = status_tx.subscribe();
									while status_rx.changed().await.is_ok() {
										let status = *status_rx.borrow();
										match status {
											JobStatus::Completed => {
												// Emit completion event
												event_bus.emit(Event::JobCompleted {
													job_id: job_id_clone.to_string(),
													job_type: job_type_str.clone(),
												});
												// Remove from running jobs
												running_jobs.write().await.remove(&job_id_clone);
												info!("Resumed job {} completed and removed from running jobs", job_id_clone);
												break;
											}
											JobStatus::Failed => {
												// Emit failure event
												event_bus.emit(Event::JobFailed {
													job_id: job_id_clone.to_string(),
													job_type: job_type_str.clone(),
													error: "Job failed".to_string(),
												});
												// Remove from running jobs
												running_jobs.write().await.remove(&job_id_clone);
												info!("Resumed job {} failed and removed from running jobs", job_id_clone);
												break;
											}
											JobStatus::Cancelled => {
												// Emit cancellation event
												event_bus.emit(Event::JobCancelled {
													job_id: job_id_clone.to_string(),
													job_type: job_type_str.clone(),
												});
												// Remove from running jobs
												running_jobs.write().await.remove(&job_id_clone);
												info!("Resumed job {} cancelled and removed from running jobs", job_id_clone);
												break;
											}
											_ => {} // Continue monitoring for other status changes
										}
									}
								});

								info!("Successfully resumed job {}: {}", job_id, job_record.name);
							}
							Err(e) => {
								error!("Failed to dispatch resumed job {}: {:?}", job_id, e);
							}
						}
					}
					Err(e) => {
						error!("Failed to create job {} for resumption: {}", job_id, e);
					}
				}
			}
		}

		Ok(())
	}

	/// Pause a running job
	pub async fn pause_job(&self, job_id: JobId) -> JobResult<()> {
		let mut running_jobs = self.running_jobs.write().await;
		
		if let Some(running_job) = running_jobs.get_mut(&job_id) {
			// Check if job is in a pausable state
			let current_status = running_job.handle.status();
			if current_status != JobStatus::Running {
				return Err(JobError::invalid_state(&format!(
					"Cannot pause job in {:?} state",
					current_status
				)));
			}

			// Update status to Paused
			running_job.status_tx.send(JobStatus::Paused)
				.map_err(|e| JobError::Other(format!("Failed to update status: {}", e).into()))?;
			
			// The task will check status and interrupt itself when it sees Paused status
			// This happens through the executor checking the status channel
			
			// Update database
			use sea_orm::{ActiveModelTrait, ActiveValue::Set};
			let mut job_model = database::jobs::ActiveModel {
				id: Set(job_id.to_string()),
				status: Set(JobStatus::Paused.to_string()),
				paused_at: Set(Some(Utc::now())),
				..Default::default()
			};
			job_model.update(self.db.conn()).await?;
			
			// Emit pause event
			self.context.events.emit(Event::JobPaused {
				job_id: job_id.to_string(),
			});
			
			info!("Job {} paused successfully", job_id);
			Ok(())
		} else {
			Err(JobError::NotFound(format!("Job {} not found", job_id)))
		}
	}

	/// Resume a paused job
	pub async fn resume_job(&self, job_id: JobId) -> JobResult<()> {
		// First check if job exists in running jobs
		let job_info = {
			let running_jobs = self.running_jobs.read().await;
			if let Some(running_job) = running_jobs.get(&job_id) {
				// Check if job is paused
				let current_status = running_job.handle.status();
				if current_status != JobStatus::Paused {
					return Err(JobError::invalid_state(&format!(
						"Cannot resume job in {:?} state",
						current_status
					)));
				}
				None // Job is already in memory, just needs status update
			} else {
				// Job might be in database but not in memory
				drop(running_jobs);
				
				// Load job from database
				let job_record = database::jobs::Entity::find_by_id(job_id.to_string())
					.one(self.db.conn())
					.await?
					.ok_or_else(|| JobError::NotFound(format!("Job {} not found", job_id)))?;
				
				// Check if job is paused
				if job_record.status != JobStatus::Paused.to_string() {
					return Err(JobError::invalid_state(&format!(
						"Cannot resume job in {} state",
						job_record.status
					)));
				}
				
				Some((job_record.name.clone(), job_record.state.clone()))
			}
		};

		// If job was not in memory, recreate and dispatch it
		if let Some((job_name, job_state)) = job_info {
			// Deserialize job from binary data
			let erased_job = REGISTRY.deserialize_job(&job_name, &job_state)?;
			
			// Update database status to Running
			use sea_orm::{ActiveModelTrait, ActiveValue::Set};
			let mut job_model = database::jobs::ActiveModel {
				id: Set(job_id.to_string()),
				status: Set(JobStatus::Running.to_string()),
				paused_at: Set(None),
				..Default::default()
			};
			job_model.update(self.db.conn()).await?;
			
			// Create channels
			let (status_tx, status_rx) = watch::channel(JobStatus::Running);
			let (progress_tx, progress_rx) = mpsc::unbounded_channel();
			let (broadcast_tx, broadcast_rx) = broadcast::channel(100);
			
			let latest_progress = Arc::new(Mutex::new(None));
			
			// Create progress forwarding task
			let broadcast_tx_clone = broadcast_tx.clone();
			let latest_progress_clone = latest_progress.clone();
			let event_bus = self.context.events.clone();
			let job_id_clone = job_id.clone();
			let job_type_str = job_name.clone();
			tokio::spawn(async move {
				let mut progress_rx: mpsc::UnboundedReceiver<Progress> = progress_rx;
				while let Some(progress) = progress_rx.recv().await {
					*latest_progress_clone.lock().await = Some(progress.clone());
					let _ = broadcast_tx_clone.send(progress.clone());
					
					// Emit progress event
					event_bus.emit(Event::JobProgress {
						job_id: job_id_clone.to_string(),
						job_type: job_type_str.to_string(),
						progress: progress.as_percentage().unwrap_or(0.0) as f64,
						message: Some(progress.to_string()),
						generic_progress: None,
					});
				}
			});
			
			// Get library from context
			let library = self
				.context
				.library_manager
				.get_library(self.library_id)
				.await
				.ok_or_else(|| {
					JobError::invalid_state(&format!("Library {} not found", self.library_id))
				})?;
			
			// Get services from context
			let networking = self.context.get_networking().await;
			let volume_manager = Some(self.context.volume_manager.clone());
			
			// Create executor
			let executor = erased_job.create_executor(
				job_id,
				library,
				self.db.clone(),
				status_tx.clone(),
				progress_tx,
				broadcast_tx,
				Arc::new(DbCheckpointHandler {
					db: self.db.clone(),
				}),
				networking,
				volume_manager,
			);
			
			// Create handle
			let handle = JobHandle {
				id: job_id,
				task_handle: Arc::new(Mutex::new(None)),
				status_rx,
				progress_rx: broadcast_rx,
				output: Arc::new(Mutex::new(None)),
			};
			
			// Dispatch to task system
			let task_handle = self
				.dispatcher
				.get_dispatcher()
				.dispatch_boxed(executor)
				.await
				.map_err(|e| JobError::task_system(format!("Failed to dispatch: {:?}", e)))?;
			
			// Track running job
			self.running_jobs.write().await.insert(
				job_id,
				RunningJob {
					handle: handle.clone(),
					task_handle,
					status_tx: status_tx.clone(),
					latest_progress,
				},
			);
			
			// Spawn cleanup monitor
			let running_jobs = self.running_jobs.clone();
			let job_id_clone = job_id.clone();
			let event_bus = self.context.events.clone();
			let job_type_str = job_name.clone();
			tokio::spawn(async move {
				let mut status_rx = status_tx.subscribe();
				while status_rx.changed().await.is_ok() {
					let status = *status_rx.borrow();
					match status {
						JobStatus::Completed => {
							event_bus.emit(Event::JobCompleted {
								job_id: job_id_clone.to_string(),
								job_type: job_type_str.clone(),
							});
							running_jobs.write().await.remove(&job_id_clone);
							info!("Resumed job {} completed", job_id_clone);
							break;
						}
						JobStatus::Failed => {
							event_bus.emit(Event::JobFailed {
								job_id: job_id_clone.to_string(),
								job_type: job_type_str.clone(),
								error: "Job failed".to_string(),
							});
							running_jobs.write().await.remove(&job_id_clone);
							info!("Resumed job {} failed", job_id_clone);
							break;
						}
						JobStatus::Cancelled => {
							event_bus.emit(Event::JobCancelled {
								job_id: job_id_clone.to_string(),
								job_type: job_type_str.clone(),
							});
							running_jobs.write().await.remove(&job_id_clone);
							info!("Resumed job {} cancelled", job_id_clone);
							break;
						}
						_ => {}
					}
				}
			});
			
			// Emit resume event
			self.context.events.emit(Event::JobResumed {
				job_id: job_id.to_string(),
			});
			
			info!("Job {} resumed from database", job_id);
		} else {
			// Job is already in memory, just update status
			let mut running_jobs = self.running_jobs.write().await;
			if let Some(running_job) = running_jobs.get_mut(&job_id) {
				// Update status to Running
				running_job.status_tx.send(JobStatus::Running)
					.map_err(|e| JobError::Other(format!("Failed to update status: {}", e).into()))?;
				
				// Update database
				use sea_orm::{ActiveModelTrait, ActiveValue::Set};
				let mut job_model = database::jobs::ActiveModel {
					id: Set(job_id.to_string()),
					status: Set(JobStatus::Running.to_string()),
					paused_at: Set(None),
					..Default::default()
				};
				job_model.update(self.db.conn()).await?;
				
				// Emit resume event
				self.context.events.emit(Event::JobResumed {
					job_id: job_id.to_string(),
				});
				
				info!("Job {} resumed", job_id);
			}
		}
		
		Ok(())
	}

	/// Shutdown the job manager
	pub async fn shutdown(&self) -> JobResult<()> {
		info!("Shutting down job manager");

		// First, pause all running jobs
		let job_ids: Vec<JobId> = self.running_jobs.read().await.keys().copied().collect();
		
		info!("Pausing {} running jobs before shutdown", job_ids.len());
		for job_id in &job_ids {
			// Check if job is still running before pausing
			if let Some(running_job) = self.running_jobs.read().await.get(job_id) {
				let status = running_job.handle.status();
				if status == JobStatus::Running {
					info!("Pausing job {} for shutdown", job_id);
					if let Err(e) = self.pause_job(*job_id).await {
						warn!("Failed to pause job {} during shutdown: {}", job_id, e);
						// Continue with shutdown even if pause fails
					}
				}
			}
		}

		// Signal shutdown
		let _ = self.shutdown_tx.send(true);

		// Wait for all jobs to finish pausing
		let start_time = tokio::time::Instant::now();
		let timeout = std::time::Duration::from_secs(10);
		
		loop {
			let running_count = self.running_jobs.read().await.len();
			if running_count == 0 {
				info!("All jobs have stopped");
				break;
			}
			
			if start_time.elapsed() > timeout {
				warn!("Timeout waiting for {} jobs to stop", running_count);
				break;
			}
			
			tokio::time::sleep(std::time::Duration::from_millis(100)).await;
		}

		Ok(())
	}
}

/// Checkpoint handler that uses the job database
struct DbCheckpointHandler {
	db: Arc<JobDb>,
}

#[async_trait]
impl CheckpointHandler for DbCheckpointHandler {
	async fn save_checkpoint(&self, job_id: JobId, data: Option<Vec<u8>>) -> JobResult<()> {
		use database::checkpoint;

		let checkpoint = checkpoint::ActiveModel {
			job_id: Set(job_id.to_string()),
			checkpoint_data: Set(data.unwrap_or_default()),
			created_at: Set(Utc::now()),
		};

		// Insert or update
		match checkpoint.clone().insert(self.db.conn()).await {
			Ok(model) => model,
			Err(_) => checkpoint.update(self.db.conn()).await?,
		};

		Ok(())
	}

	async fn load_checkpoint(&self, job_id: JobId) -> JobResult<Option<Vec<u8>>> {
		use database::checkpoint;

		let checkpoint = checkpoint::Entity::find_by_id(job_id.to_string())
			.one(self.db.conn())
			.await?;

		Ok(checkpoint.map(|c| c.checkpoint_data))
	}

	async fn delete_checkpoint(&self, job_id: JobId) -> JobResult<()> {
		use database::checkpoint;

		checkpoint::Entity::delete_by_id(job_id.to_string())
			.exec(self.db.conn())
			.await?;

		Ok(())
	}
}

use uuid::Uuid;
```

## src/infrastructure/jobs/mod.rs

```rust
//! Job system for Spacedrive
//! 
//! Provides a minimal-boilerplate job execution framework built on top of the task-system.

pub mod context;
pub mod database;
pub mod error;
pub mod executor;
pub mod generic_progress;
pub mod handle;
pub mod manager;
pub mod output;
pub mod progress;
pub mod registry;
pub mod traits;
pub mod types;

// Re-export commonly used types
pub mod prelude {
    pub use super::{
        context::JobContext,
        error::{JobError, JobResult},
        generic_progress::{GenericProgress, ToGenericProgress},
        handle::JobHandle,
        output::JobOutput,
        progress::{JobProgress, Progress},
        traits::{Job, JobHandler},
        types::{JobId, JobStatus, JobInfo},
    };
    
    // Re-export derive macros
    pub use spacedrive_jobs_derive::Job;
}

pub use manager::JobManager;
pub use registry::JobRegistry;
pub use types::{JobInfo, JobStatus};```

## src/infrastructure/jobs/executor.rs

```rust
//! Job executor that wraps jobs for task system integration

use super::{
	context::{CheckpointHandler, JobContext},
	database::{self, JobDb},
	error::{JobError, JobResult},
	handle::JobHandle,
	output::JobOutput,
	progress::Progress,
	traits::{Job, JobHandler},
	types::{ErasedJob, JobId, JobMetrics, JobStatus},
};
use crate::library::Library;
use async_trait::async_trait;
use sd_task_system::{ExecStatus, Interrupter, Task, TaskId};
use std::sync::Arc;
use tokio::sync::{broadcast, mpsc, watch, Mutex};
use tracing::{debug, error, info};

/// Executor that wraps a job for task system execution
pub struct JobExecutor<J: JobHandler> {
	job: J,
	state: JobExecutorState,
}

pub struct JobExecutorState {
	pub job_id: JobId,
	pub library: Arc<Library>,
	pub job_db: Arc<JobDb>,
	pub status_tx: watch::Sender<super::types::JobStatus>,
	pub progress_tx: mpsc::UnboundedSender<Progress>,
	pub broadcast_tx: broadcast::Sender<Progress>,
	pub checkpoint_handler: Arc<dyn CheckpointHandler>,
	pub metrics: JobMetrics,
	pub output: Option<JobOutput>,
	pub networking: Option<Arc<crate::services::networking::NetworkingService>>,
	pub volume_manager: Option<Arc<crate::volume::VolumeManager>>,
	pub latest_progress: Arc<Mutex<Option<Progress>>>,
}

impl<J: JobHandler> JobExecutor<J> {
	pub fn new(
		job: J,
		job_id: JobId,
		library: Arc<Library>,
		job_db: Arc<JobDb>,
		status_tx: watch::Sender<super::types::JobStatus>,
		progress_tx: mpsc::UnboundedSender<Progress>,
		broadcast_tx: broadcast::Sender<Progress>,
		checkpoint_handler: Arc<dyn CheckpointHandler>,
		networking: Option<Arc<crate::services::networking::NetworkingService>>,
		volume_manager: Option<Arc<crate::volume::VolumeManager>>,
	) -> Self {
		Self {
			job,
			state: JobExecutorState {
				job_id,
				library,
				job_db,
				status_tx,
				progress_tx,
				broadcast_tx,
				checkpoint_handler,
				metrics: Default::default(),
				output: None,
				networking,
				volume_manager,
				latest_progress: Arc::new(Mutex::new(None)),
			},
		}
	}

	/// Update job status in the database
	async fn update_job_status_in_db(&self, status: super::types::JobStatus) -> JobResult<()> {
		use super::database;
		use chrono::Utc;
		use sea_orm::{ActiveModelTrait, ActiveValue::Set};

		let mut job = database::jobs::ActiveModel {
			id: Set(self.state.job_id.to_string()),
			status: Set(status.to_string()),
			..Default::default()
		};

		// Update timestamps based on status
		match status {
			super::types::JobStatus::Running => {
				job.started_at = Set(Some(Utc::now()));
			}
			super::types::JobStatus::Paused => {
				job.paused_at = Set(Some(Utc::now()));
			}
			super::types::JobStatus::Completed
			| super::types::JobStatus::Failed
			| super::types::JobStatus::Cancelled => {
				job.completed_at = Set(Some(Utc::now()));
			}
			_ => {}
		}

		job.update(self.state.job_db.conn()).await?;
		Ok(())
	}
}

#[async_trait]
impl<J: JobHandler> Task<JobError> for JobExecutor<J> {
	fn id(&self) -> TaskId {
		TaskId::from(self.state.job_id.0)
	}

	fn with_priority(&self) -> bool {
		// TODO: Get from job priority
		false
	}

	async fn run(&mut self, interrupter: &Interrupter) -> Result<ExecStatus, JobError> {
		info!("Starting job {}: {}", self.state.job_id, J::NAME);

		// Update status to running
		let _ = self.state.status_tx.send(super::types::JobStatus::Running);

		// Also persist status to database
		if let Err(e) = self
			.update_job_status_in_db(super::types::JobStatus::Running)
			.await
		{
			error!("Failed to update job status in database: {}", e);
		}

		// Create job context
		let ctx = JobContext {
			id: self.state.job_id,
			library: self.state.library.clone(),
			interrupter: interrupter,
			progress_tx: self.state.progress_tx.clone(),
			metrics: Arc::new(Mutex::new(self.state.metrics.clone())),
			checkpoint_handler: self.state.checkpoint_handler.clone(),
			child_handles: Arc::new(Mutex::new(Vec::new())),
			networking: self.state.networking.clone(),
			volume_manager: self.state.volume_manager.clone(),
		};

		// Progress forwarding is handled by JobManager

		// Check if we're resuming
		// TODO: Implement proper resume detection
		debug!("Starting job {}", self.state.job_id);

		// Store metrics reference for later update
		let metrics_ref = ctx.metrics.clone();

		// Run the job
		let result = match self.job.run(ctx).await {
			Ok(output) => {
				self.state.output = Some(output.into());

				// Update metrics
				self.state.metrics = metrics_ref.lock().await.clone();

				// Update status with final progress
				info!("Job {} sending Completed status", self.state.job_id);
				if let Err(e) = self.state.status_tx.send(JobStatus::Completed) {
					error!(
						"Failed to send Completed status for job {}: {:?}",
						self.state.job_id, e
					);
				}

				// Persist final status and progress to database atomically
				let final_progress = if let Some(ref output) = self.state.output {
					output.as_progress()
				} else {
					Some(Progress::percentage(1.0))
				};

				if let Err(e) = self
					.state
					.job_db
					.update_status_and_progress(
						self.state.job_id,
						JobStatus::Completed,
						final_progress.as_ref(),
						None,
					)
					.await
				{
					error!("Failed to update job completion status in database: {}", e);
				}

				info!(
					"Job {} completed successfully - status sent and DB updated",
					self.state.job_id
				);
				Ok(ExecStatus::Done(sd_task_system::TaskOutput::Empty))
			}
			Err(e) => {
				if e.is_interrupted() {
					debug!("Job {} interrupted", self.state.job_id);

					// Check current status to determine if this is a pause or cancel
					let current_status = *self.state.status_tx.borrow();
					
					if current_status == JobStatus::Paused {
						// Job was paused, don't update status (already set by pause_job)
						debug!("Job {} paused", self.state.job_id);
						
						// Save job state for resume
						use sea_orm::{ActiveModelTrait, ActiveValue::Set};
						let job_state = rmp_serde::to_vec(&self.job)
							.map_err(|e| JobError::serialization(format!("{}", e)))?;
						
						let mut job_model = super::database::jobs::ActiveModel {
							id: Set(self.state.job_id.to_string()),
							state: Set(job_state),
							..Default::default()
						};
						
						if let Err(e) = job_model.update(self.state.job_db.conn()).await {
							error!("Failed to save paused job state: {}", e);
						}
						
						Ok(ExecStatus::Canceled)
					} else {
						// Job was cancelled
						let _ = self.state.status_tx.send(JobStatus::Cancelled);

						// Persist cancellation status with latest progress
						let latest_progress = self.state.latest_progress.lock().await.clone();
						if let Err(e) = self
							.state
							.job_db
							.update_status_and_progress(
								self.state.job_id,
								JobStatus::Cancelled,
								latest_progress.as_ref(),
								None,
							)
							.await
						{
							error!(
								"Failed to update job cancellation status in database: {}",
								e
							);
						}

						Ok(ExecStatus::Canceled)
					}
				} else {
					error!("Job {} failed: {}", self.state.job_id, e);

					// Update status with current progress
					let _ = self.state.status_tx.send(JobStatus::Failed);

					// Persist failure status with latest progress and error message
					let latest_progress = self.state.latest_progress.lock().await.clone();
					if let Err(e_db) = self
						.state
						.job_db
						.update_status_and_progress(
							self.state.job_id,
							JobStatus::Failed,
							latest_progress.as_ref(),
							Some(e.to_string()),
						)
						.await
					{
						error!("Failed to update job failure status in database: {}", e_db);
					}

					Err(e)
				}
			}
		};

		// Clean up checkpoint if job completed
		if matches!(result, Ok(ExecStatus::Done(_))) {
			let _ = self
				.state
				.checkpoint_handler
				.delete_checkpoint(self.state.job_id)
				.await;
		}

		result
	}
}
```

## src/infrastructure/jobs/output.rs

```rust
//! Job output types

use serde::{Deserialize, Serialize};
use std::fmt;
use super::progress::Progress;

/// Output from a completed job
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(tag = "type", content = "data")]
pub enum JobOutput {
    /// Job completed successfully with no specific output
    Success,
    
    /// File copy job output
    FileCopy {
        copied_count: usize,
        total_bytes: u64,
    },
    
    /// Indexer job output
    Indexed {
        total_files: u64,
        total_dirs: u64,
        total_bytes: u64,
    },
    
    /// Thumbnail generation output
    ThumbnailsGenerated {
        generated_count: usize,
        failed_count: usize,
    },
    
    /// Thumbnail generation output (detailed)
    ThumbnailGeneration {
        generated_count: u64,
        skipped_count: u64,
        error_count: u64,
        total_size_bytes: u64,
    },
    
    /// File move/rename operation output
    FileMove {
        moved_count: usize,
        failed_count: usize,
        total_bytes: u64,
    },
    
    /// File delete operation output
    FileDelete {
        deleted_count: usize,
        failed_count: usize,
        total_bytes: u64,
    },
    
    /// Duplicate detection output
    DuplicateDetection {
        duplicate_groups: usize,
        total_duplicates: usize,
        potential_savings: u64,
    },
    
    /// File validation output
    FileValidation {
        validated_count: usize,
        issues_found: usize,
        total_bytes_validated: u64,
    },
    
    /// Generic output with custom data
    Custom(serde_json::Value),
}

impl JobOutput {
    /// Create a custom output
    pub fn custom<T: Serialize>(data: T) -> Self {
        Self::Custom(serde_json::to_value(data).unwrap_or(serde_json::Value::Null))
    }
    
    /// Get indexed output if this is an indexed job
    pub fn as_indexed(&self) -> Option<IndexedOutput> {
        match self {
            Self::Indexed { total_files, total_dirs, total_bytes } => {
                Some(IndexedOutput {
                    total_files: *total_files,
                    total_dirs: *total_dirs,
                    total_bytes: *total_bytes,
                })
            }
            _ => None,
        }
    }
    
    /// Convert output to a progress representation (for final progress)
    pub fn as_progress(&self) -> Option<Progress> {
        match self {
            Self::Success => Some(Progress::percentage(1.0)),
            Self::FileCopy { copied_count, total_bytes } => {
                Some(Progress::generic(
                    crate::infrastructure::jobs::generic_progress::GenericProgress::new(
                        1.0,
                        "Completed",
                        format!("Copied {} files", copied_count)
                    ).with_bytes(*total_bytes, *total_bytes)
                ))
            }
            Self::Indexed { total_files, total_dirs, total_bytes } => {
                Some(Progress::generic(
                    crate::infrastructure::jobs::generic_progress::GenericProgress::new(
                        1.0,
                        "Completed",
                        format!("Indexed {} files, {} directories", total_files, total_dirs)
                    ).with_bytes(*total_bytes, *total_bytes)
                ))
            }
            Self::ThumbnailGeneration { generated_count, .. } => {
                Some(Progress::generic(
                    crate::infrastructure::jobs::generic_progress::GenericProgress::new(
                        1.0,
                        "Completed",
                        format!("Generated {} thumbnails", generated_count)
                    )
                ))
            }
            Self::FileMove { moved_count, .. } => {
                Some(Progress::percentage(1.0))
            }
            Self::FileDelete { deleted_count, .. } => {
                Some(Progress::percentage(1.0))
            }
            Self::DuplicateDetection { duplicate_groups, .. } => {
                Some(Progress::percentage(1.0))
            }
            Self::FileValidation { validated_count, .. } => {
                Some(Progress::percentage(1.0))
            }
            _ => Some(Progress::percentage(1.0)),
        }
    }
}

/// Typed output for indexed jobs
#[derive(Debug, Clone)]
pub struct IndexedOutput {
    pub total_files: u64,
    pub total_dirs: u64,
    pub total_bytes: u64,
}

impl Default for JobOutput {
    fn default() -> Self {
        Self::Success
    }
}

impl fmt::Display for JobOutput {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        match self {
            Self::Success => write!(f, "Success"),
            Self::FileCopy { copied_count, total_bytes } => {
                write!(f, "Copied {} files ({} bytes)", copied_count, total_bytes)
            }
            Self::Indexed { total_files, total_dirs, total_bytes } => {
                write!(f, "Indexed {} files, {} directories ({} bytes)", 
                    total_files, total_dirs, total_bytes)
            }
            Self::ThumbnailsGenerated { generated_count, failed_count } => {
                write!(f, "Generated {} thumbnails ({} failed)", 
                    generated_count, failed_count)
            }
            Self::ThumbnailGeneration { generated_count, skipped_count, error_count, total_size_bytes } => {
                write!(f, "Generated {} thumbnails ({} skipped, {} errors, {} bytes)", 
                    generated_count, skipped_count, error_count, total_size_bytes)
            }
            Self::FileMove { moved_count, failed_count, total_bytes } => {
                write!(f, "Moved {} files ({} failed, {} bytes)", 
                    moved_count, failed_count, total_bytes)
            }
            Self::FileDelete { deleted_count, failed_count, total_bytes } => {
                write!(f, "Deleted {} files ({} failed, {} bytes)", 
                    deleted_count, failed_count, total_bytes)
            }
            Self::DuplicateDetection { duplicate_groups, total_duplicates, potential_savings } => {
                write!(f, "Found {} duplicate groups ({} duplicates, {} bytes savings)", 
                    duplicate_groups, total_duplicates, potential_savings)
            }
            Self::FileValidation { validated_count, issues_found, total_bytes_validated } => {
                write!(f, "Validated {} files ({} issues, {} bytes)", 
                    validated_count, issues_found, total_bytes_validated)
            }
            Self::Custom(_) => write!(f, "Custom output"),
        }
    }
}```

## src/infrastructure/jobs/progress.rs

```rust
//! Progress reporting for jobs

use crate::infrastructure::jobs::generic_progress::GenericProgress;
use serde::{Deserialize, Serialize};
use std::fmt;

/// Progress information for a job
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(tag = "type", content = "data")]
pub enum Progress {
    /// Simple count-based progress
    Count { current: usize, total: usize },
    
    /// Percentage-based progress (0.0 to 1.0)
    Percentage(f32),
    
    /// Indeterminate progress with a message
    Indeterminate(String),
    
    /// Bytes-based progress
    Bytes { current: u64, total: u64 },
    
    /// Custom structured progress
    Structured(serde_json::Value),
    
    /// Generic progress (recommended for all jobs)
    Generic(GenericProgress),
}

impl Progress {
    /// Create count-based progress
    pub fn count(current: usize, total: usize) -> Self {
        Self::Count { current, total }
    }
    
    /// Create percentage progress
    pub fn percentage(value: f32) -> Self {
        Self::Percentage(value.clamp(0.0, 1.0))
    }
    
    /// Create indeterminate progress
    pub fn indeterminate(message: impl Into<String>) -> Self {
        Self::Indeterminate(message.into())
    }
    
    /// Create bytes-based progress
    pub fn bytes(current: u64, total: u64) -> Self {
        Self::Bytes { current, total }
    }
    
    /// Create structured progress
    pub fn structured<T: Serialize>(data: T) -> Self {
        Self::Structured(serde_json::to_value(data).unwrap_or(serde_json::Value::Null))
    }
    
    /// Create generic progress
    pub fn generic(progress: GenericProgress) -> Self {
        Self::Generic(progress)
    }
    
    /// Get progress as a percentage (0.0 to 1.0)
    pub fn as_percentage(&self) -> Option<f32> {
        match self {
            Self::Count { current, total } if *total > 0 => {
                Some(*current as f32 / *total as f32)
            }
            Self::Percentage(p) => Some(*p),
            Self::Bytes { current, total } if *total > 0 => {
                Some(*current as f32 / *total as f32)
            }
            Self::Generic(progress) => Some(progress.as_percentage()),
            _ => None,
        }
    }
    
    /// Check if progress is determinate
    pub fn is_determinate(&self) -> bool {
        !matches!(self, Self::Indeterminate(_))
    }
}

impl fmt::Display for Progress {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        match self {
            Self::Count { current, total } => write!(f, "{}/{}", current, total),
            Self::Percentage(p) => write!(f, "{:.1}%", p * 100.0),
            Self::Indeterminate(msg) => write!(f, "{}", msg),
            Self::Bytes { current, total } => {
                write!(f, "{}/{}", format_bytes(*current), format_bytes(*total))
            }
            Self::Structured(_) => write!(f, "[structured progress]"),
            Self::Generic(progress) => write!(f, "{}", progress.format_progress()),
        }
    }
}

/// Trait for custom progress types
pub trait JobProgress: Serialize + Send + Sync + 'static {
    /// Convert to generic Progress
    fn to_progress(&self) -> Progress {
        Progress::structured(self)
    }
}

// Helper function to format bytes
fn format_bytes(bytes: u64) -> String {
    const UNITS: &[&str] = &["B", "KB", "MB", "GB", "TB"];
    let mut size = bytes as f64;
    let mut unit_idx = 0;
    
    while size >= 1024.0 && unit_idx < UNITS.len() - 1 {
        size /= 1024.0;
        unit_idx += 1;
    }
    
    if unit_idx == 0 {
        format!("{} {}", size as u64, UNITS[unit_idx])
    } else {
        format!("{:.2} {}", size, UNITS[unit_idx])
    }
}```

## src/infrastructure/jobs/traits.rs

```rust
//! Core traits for defining jobs

use super::{
    context::JobContext,
    error::JobResult,
    output::JobOutput,
    types::{JobSchema, ErasedJob},
};
use async_trait::async_trait;
use serde::{de::DeserializeOwned, Serialize};

/// Main trait for defining a job
pub trait Job: Serialize + DeserializeOwned + Send + Sync + 'static {
    /// Job name - must be unique
    const NAME: &'static str;
    
    /// Whether this job can be resumed after interruption
    const RESUMABLE: bool = true;
    
    /// Schema version for migrations
    const VERSION: u32 = 1;
    
    /// Optional description
    const DESCRIPTION: Option<&'static str> = None;
    
    /// Get the job schema
    fn schema() -> JobSchema {
        JobSchema {
            name: Self::NAME,
            resumable: Self::RESUMABLE,
            version: Self::VERSION,
            description: Self::DESCRIPTION,
        }
    }
}

/// Handler trait that defines job execution logic
#[async_trait]
pub trait JobHandler: Job {
    /// Output type for this job
    type Output: Into<JobOutput> + Send;
    
    /// Run the job
    async fn run(&mut self, ctx: JobContext<'_>) -> JobResult<Self::Output>;
    
    /// Called when job is paused (optional)
    async fn on_pause(&mut self, _ctx: &JobContext<'_>) -> JobResult {
        Ok(())
    }
    
    /// Called when job is resumed (optional)
    async fn on_resume(&mut self, _ctx: &JobContext<'_>) -> JobResult {
        Ok(())
    }
    
    /// Called when job is cancelled (optional)
    async fn on_cancel(&mut self, _ctx: &JobContext<'_>) -> JobResult {
        Ok(())
    }
}

/// Trait for jobs that can be serialized
pub trait SerializableJob: Job {
    /// Serialize job state
    fn serialize_state(&self) -> JobResult<Vec<u8>> {
        rmp_serde::to_vec(self)
            .map_err(|e| super::error::JobError::serialization(format!("{}", e)))
    }
    
    /// Deserialize job state
    fn deserialize_state(data: &[u8]) -> JobResult<Self> {
        rmp_serde::from_slice(data)
            .map_err(|e| super::error::JobError::serialization(format!("{}", e)))
    }
}

// Blanket implementation for all Jobs
impl<T: Job> SerializableJob for T {}

/// Progress reporter trait for jobs with custom progress
pub trait ProgressReporter {
    /// Progress type for this job
    type Progress: super::progress::JobProgress;
}

/// Resource requirements for a job
pub trait ResourceRequirements {
    /// Maximum number of concurrent instances
    fn max_concurrent() -> Option<usize> {
        None
    }
    
    /// Required resources
    fn required_resources() -> Vec<ResourceRequirement> {
        vec![]
    }
}

/// A required resource
#[derive(Debug, Clone)]
pub enum ResourceRequirement {
    /// Named resource (e.g., "gpu")
    Named(&'static str),
    /// Disk space in bytes
    DiskSpace(u64),
    /// Memory in bytes
    Memory(u64),
}

/// Job dependencies
pub trait JobDependencies {
    /// Jobs that must complete before this one
    fn dependencies() -> &'static [&'static str] {
        &[]
    }
    
    /// Jobs that should run after this one
    fn run_after() -> &'static [&'static str] {
        &[]
    }
}```

## src/infrastructure/jobs/generic_progress.rs

```rust
//! Generic progress system for job monitoring
//! 
//! This module provides a unified progress structure that jobs can convert
//! their domain-specific progress into, making progress data compatible
//! with the job monitoring system while preserving rich information.

use crate::shared::types::SdPath;
use serde::{Deserialize, Serialize};
use std::time::Duration;

/// Generic progress information that all job types can convert into
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct GenericProgress {
    /// Current progress as a percentage (0.0 to 1.0)
    pub percentage: f32,
    
    /// Current phase or stage name (e.g., "Discovery", "Processing", "Finalizing")
    pub phase: String,
    
    /// Current path being processed (if applicable)
    pub current_path: Option<SdPath>,
    
    /// Human-readable message describing current activity
    pub message: String,
    
    /// Completion metrics
    pub completion: ProgressCompletion,
    
    /// Performance metrics
    pub performance: PerformanceMetrics,
    
    /// Extended metadata specific to job type
    pub metadata: serde_json::Value,
}

/// Progress completion information
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ProgressCompletion {
    /// Items completed (files, entries, operations, etc.)
    pub completed: u64,
    
    /// Total items to complete
    pub total: u64,
    
    /// Bytes processed (if applicable)
    pub bytes_completed: Option<u64>,
    
    /// Total bytes to process (if applicable)
    pub total_bytes: Option<u64>,
}

/// Performance and timing metrics
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PerformanceMetrics {
    /// Processing rate (items per second)
    pub rate: f32,
    
    /// Estimated time remaining
    pub estimated_remaining: Option<Duration>,
    
    /// Time elapsed since start
    pub elapsed: Option<Duration>,
    
    /// Number of errors encountered
    pub error_count: u64,
    
    /// Number of warnings
    pub warning_count: u64,
}

impl GenericProgress {
    /// Create a new generic progress instance
    pub fn new(
        percentage: f32,
        phase: impl Into<String>,
        message: impl Into<String>,
    ) -> Self {
        Self {
            percentage: percentage.clamp(0.0, 1.0),
            phase: phase.into(),
            current_path: None,
            message: message.into(),
            completion: ProgressCompletion {
                completed: 0,
                total: 0,
                bytes_completed: None,
                total_bytes: None,
            },
            performance: PerformanceMetrics {
                rate: 0.0,
                estimated_remaining: None,
                elapsed: None,
                error_count: 0,
                warning_count: 0,
            },
            metadata: serde_json::Value::Null,
        }
    }
    
    /// Set the current path being processed
    pub fn with_current_path(mut self, path: SdPath) -> Self {
        self.current_path = Some(path);
        self
    }
    
    /// Set completion metrics
    pub fn with_completion(mut self, completed: u64, total: u64) -> Self {
        self.completion.completed = completed;
        self.completion.total = total;
        // Auto-calculate percentage if not already set appropriately
        if total > 0 {
            self.percentage = (completed as f32 / total as f32).clamp(0.0, 1.0);
        }
        self
    }
    
    /// Set byte metrics
    pub fn with_bytes(mut self, bytes_completed: u64, total_bytes: u64) -> Self {
        self.completion.bytes_completed = Some(bytes_completed);
        self.completion.total_bytes = Some(total_bytes);
        self
    }
    
    /// Set performance metrics
    pub fn with_performance(
        mut self,
        rate: f32,
        estimated_remaining: Option<Duration>,
        elapsed: Option<Duration>,
    ) -> Self {
        self.performance.rate = rate;
        self.performance.estimated_remaining = estimated_remaining;
        self.performance.elapsed = elapsed;
        self
    }
    
    /// Set error and warning counts
    pub fn with_errors(mut self, error_count: u64, warning_count: u64) -> Self {
        self.performance.error_count = error_count;
        self.performance.warning_count = warning_count;
        self
    }
    
    /// Set job-specific metadata
    pub fn with_metadata<T: Serialize>(mut self, metadata: T) -> Self {
        self.metadata = serde_json::to_value(metadata).unwrap_or(serde_json::Value::Null);
        self
    }
    
    /// Get a simple percentage (0.0 to 1.0) for basic progress bars
    pub fn as_percentage(&self) -> f32 {
        self.percentage
    }
    
    /// Get a formatted progress string for display
    pub fn format_progress(&self) -> String {
        format!("{} - {:.1}%", self.message, self.percentage * 100.0)
    }
    
    /// Get completion ratio as a formatted string
    pub fn format_completion(&self) -> String {
        if self.completion.total > 0 {
            format!("{}/{}", self.completion.completed, self.completion.total)
        } else {
            "Processing...".to_string()
        }
    }
    
    /// Get bytes progress as a formatted string
    pub fn format_bytes(&self) -> Option<String> {
        match (self.completion.bytes_completed, self.completion.total_bytes) {
            (Some(completed), Some(total)) => {
                Some(format!("{}/{}", format_bytes(completed), format_bytes(total)))
            }
            _ => None,
        }
    }
    
    /// Get processing rate as a formatted string
    pub fn format_rate(&self) -> String {
        if self.performance.rate > 0.0 {
            format!("{:.1} items/sec", self.performance.rate)
        } else {
            "Calculating...".to_string()
        }
    }
    
    /// Get estimated remaining time as a formatted string
    pub fn format_eta(&self) -> String {
        match self.performance.estimated_remaining {
            Some(duration) => format_duration(duration),
            None => "Unknown".to_string(),
        }
    }
}

/// Trait for converting job-specific progress into generic progress
pub trait ToGenericProgress {
    /// Convert this progress type into a GenericProgress
    fn to_generic_progress(&self) -> GenericProgress;
}

// Helper function to format bytes
fn format_bytes(bytes: u64) -> String {
    const UNITS: &[&str] = &["B", "KB", "MB", "GB", "TB"];
    let mut size = bytes as f64;
    let mut unit_idx = 0;
    
    while size >= 1024.0 && unit_idx < UNITS.len() - 1 {
        size /= 1024.0;
        unit_idx += 1;
    }
    
    if unit_idx == 0 {
        format!("{} {}", size as u64, UNITS[unit_idx])
    } else {
        format!("{:.2} {}", size, UNITS[unit_idx])
    }
}

// Helper function to format duration
fn format_duration(duration: Duration) -> String {
    let secs = duration.as_secs();
    let hours = secs / 3600;
    let mins = (secs % 3600) / 60;
    let secs = secs % 60;
    
    if hours > 0 {
        format!("{}h {}m {}s", hours, mins, secs)
    } else if mins > 0 {
        format!("{}m {}s", mins, secs)
    } else {
        format!("{}s", secs)
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_generic_progress_creation() {
        let progress = GenericProgress::new(0.5, "Processing", "Processing files")
            .with_completion(50, 100)
            .with_performance(10.5, Some(Duration::from_secs(30)), Some(Duration::from_secs(60)));
        
        assert_eq!(progress.percentage, 0.5);
        assert_eq!(progress.phase, "Processing");
        assert_eq!(progress.completion.completed, 50);
        assert_eq!(progress.completion.total, 100);
    }
    
    #[test]
    fn test_auto_percentage_calculation() {
        let progress = GenericProgress::new(0.0, "Test", "Testing")
            .with_completion(25, 100);
        
        assert_eq!(progress.percentage, 0.25);
    }
    
    #[test]
    fn test_formatting() {
        let progress = GenericProgress::new(0.75, "Test", "Testing files")
            .with_completion(75, 100)
            .with_bytes(1024 * 1024 * 500, 1024 * 1024 * 1000); // 500MB / 1000MB
        
        assert_eq!(progress.format_progress(), "Testing files - 75.0%");
        assert_eq!(progress.format_completion(), "75/100");
        assert_eq!(progress.format_bytes(), Some("500.00 MB/1000.00 MB".to_string()));
    }
}```

## src/infrastructure/jobs/context.rs

```rust
//! Job execution context

use super::{
    error::{JobError, JobResult},
    handle::JobHandle,
    progress::Progress,
    types::{JobId, JobMetrics},
};
use crate::{library::Library, services::networking::NetworkingService};
use sea_orm::DatabaseConnection;
use serde::{de::DeserializeOwned, Serialize};
use std::sync::Arc;
use sd_task_system::Interrupter;
use tokio::sync::{mpsc, Mutex, RwLock};
use tracing::{debug, warn};

/// Context provided to jobs during execution
pub struct JobContext<'a> {
    pub(crate) id: JobId,
    pub(crate) library: Arc<Library>,
    pub(crate) interrupter: &'a Interrupter,
    pub(crate) progress_tx: mpsc::UnboundedSender<Progress>,
    pub(crate) metrics: Arc<Mutex<JobMetrics>>,
    pub(crate) checkpoint_handler: Arc<dyn CheckpointHandler>,
    pub(crate) child_handles: Arc<Mutex<Vec<JobHandle>>>,
    pub(crate) networking: Option<Arc<NetworkingService>>,
    pub(crate) volume_manager: Option<Arc<crate::volume::VolumeManager>>,
}

impl<'a> JobContext<'a> {
    /// Get the job ID
    pub fn id(&self) -> JobId {
        self.id
    }
    
    /// Get the library this job is running in
    pub fn library(&self) -> &Library {
        &self.library
    }
    
    /// Get the library database connection
    pub fn library_db(&self) -> &DatabaseConnection {
        self.library.db().conn()
    }
    
    /// Get networking service if available
    pub fn networking_service(&self) -> Option<Arc<NetworkingService>> {
        self.networking.clone()
    }
    
    /// Get volume manager if available
    pub fn volume_manager(&self) -> Option<Arc<crate::volume::VolumeManager>> {
        self.volume_manager.clone()
    }
    
    /// Report progress
    pub fn progress(&self, progress: Progress) {
        if let Err(e) = self.progress_tx.send(progress) {
            warn!("Failed to send progress update: {}", e);
        }
    }
    
    /// Add a warning message
    pub fn add_warning(&self, warning: impl Into<String>) {
        self.progress(Progress::indeterminate(format!("âš ï¸ {}", warning.into())));
    }
    
    /// Add a non-critical error
    pub fn add_non_critical_error(&self, error: impl Into<JobError>) {
        let error_msg = error.into().to_string();
        self.progress(Progress::indeterminate(format!("âŒ {}", error_msg)));
        
        // Increment error count
        if let Ok(mut metrics) = self.metrics.try_lock() {
            metrics.non_critical_errors_count += 1;
        }
    }
    
    /// Get current metrics
    pub async fn metrics(&self) -> JobMetrics {
        self.metrics.lock().await.clone()
    }
    
    /// Increment bytes processed
    pub async fn increment_bytes(&self, bytes: u64) {
        self.metrics.lock().await.bytes_processed += bytes;
    }
    
    /// Increment items processed
    pub async fn increment_items(&self, count: u64) {
        self.metrics.lock().await.items_processed += count;
    }
    
    /// Check if the job should be interrupted
    pub async fn check_interrupt(&self) -> JobResult<()> {
        if let Some(_kind) = self.interrupter.try_check_interrupt() {
            return Err(JobError::Interrupted);
        }
        Ok(())
    }
    
    /// Create a checkpoint (job can be resumed from here)
    pub async fn checkpoint(&self) -> JobResult<()> {
        self.check_interrupt().await?;
        self.checkpoint_handler.save_checkpoint(self.id, None).await
    }
    
    /// Create a checkpoint with custom state
    pub async fn checkpoint_with_state<S: Serialize>(&self, state: &S) -> JobResult<()> {
        self.check_interrupt().await?;
        let data = rmp_serde::to_vec(state)
            .map_err(|e| JobError::serialization(e))?;
        self.checkpoint_handler.save_checkpoint(self.id, Some(data)).await
    }
    
    /// Load saved state
    pub async fn load_state<S: DeserializeOwned>(&self) -> JobResult<Option<S>> {
        match self.checkpoint_handler.load_checkpoint(self.id).await? {
            Some(data) => {
                let state = rmp_serde::from_slice(&data)
                    .map_err(|e| JobError::serialization(e))?;
                Ok(Some(state))
            }
            None => Ok(None),
        }
    }
    
    /// Save state (without creating a checkpoint)
    pub async fn save_state<S: Serialize>(&self, state: &S) -> JobResult<()> {
        let data = rmp_serde::to_vec(state)
            .map_err(|e| JobError::serialization(e))?;
        self.checkpoint_handler.save_checkpoint(self.id, Some(data)).await
    }
    
    /// Spawn a child job
    pub async fn spawn_child<J>(&self, job: J) -> JobResult<JobHandle>
    where
        J: super::traits::Job + super::traits::JobHandler,
    {
        // This will be implemented by JobManager
        // For now, return a placeholder
        todo!("Child job spawning will be implemented with JobManager")
    }
    
    /// Wait for all child jobs to complete
    pub async fn wait_for_children(&self) -> JobResult<()> {
        let handles = self.child_handles.lock().await.clone();
        
        for handle in handles {
            handle.wait().await?;
        }
        
        Ok(())
    }
    
    /// Log a message
    pub fn log(&self, message: impl Into<String>) {
        debug!(job_id = %self.id, "{}", message.into());
    }
}

/// Handler for checkpoint operations
#[async_trait::async_trait]
pub trait CheckpointHandler: Send + Sync {
    /// Save a checkpoint
    async fn save_checkpoint(&self, job_id: JobId, data: Option<Vec<u8>>) -> JobResult<()>;
    
    /// Load a checkpoint
    async fn load_checkpoint(&self, job_id: JobId) -> JobResult<Option<Vec<u8>>>;
    
    /// Delete a checkpoint
    async fn delete_checkpoint(&self, job_id: JobId) -> JobResult<()>;
}```

## src/infrastructure/events/mod.rs

```rust
//! Event bus for decoupled communication

use serde::{Deserialize, Serialize};
use std::path::PathBuf;
use tokio::sync::broadcast;
use tracing::{debug, warn};
use uuid::Uuid;

/// Core events that can be emitted throughout the system
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum Event {
    // Core lifecycle events
    CoreStarted,
    CoreShutdown,

    // Library events
    LibraryCreated { id: Uuid, name: String, path: PathBuf },
    LibraryOpened { id: Uuid, name: String, path: PathBuf },
    LibraryClosed { id: Uuid, name: String },
    LibraryDeleted { id: Uuid },

    // Entry events (file/directory operations)
    EntryCreated { library_id: Uuid, entry_id: Uuid },
    EntryModified { library_id: Uuid, entry_id: Uuid },
    EntryDeleted { library_id: Uuid, entry_id: Uuid },
    EntryMoved { 
        library_id: Uuid, 
        entry_id: Uuid, 
        old_path: String, 
        new_path: String 
    },

    // Volume events
    VolumeAdded(crate::volume::Volume),
    VolumeRemoved { 
        fingerprint: crate::volume::VolumeFingerprint 
    },
    VolumeUpdated {
        fingerprint: crate::volume::VolumeFingerprint,
        old_info: crate::volume::VolumeInfo,
        new_info: crate::volume::VolumeInfo,
    },
    VolumeSpeedTested {
        fingerprint: crate::volume::VolumeFingerprint,
        read_speed_mbps: u64,
        write_speed_mbps: u64,
    },
    VolumeMountChanged {
        fingerprint: crate::volume::VolumeFingerprint,
        is_mounted: bool,
    },
    VolumeError {
        fingerprint: crate::volume::VolumeFingerprint,
        error: String,
    },

    // Job events
    JobQueued { job_id: String, job_type: String },
    JobStarted { job_id: String, job_type: String },
    JobProgress { 
        job_id: String, 
        job_type: String,
        progress: f64, 
        message: Option<String>,
        // Enhanced progress data - serialized GenericProgress
        generic_progress: Option<serde_json::Value>,
    },
    JobCompleted { job_id: String, job_type: String },
    JobFailed { 
        job_id: String, 
        job_type: String, 
        error: String 
    },
    JobCancelled { job_id: String, job_type: String },
    JobPaused { job_id: String },
    JobResumed { job_id: String },

    // Indexing events
    IndexingStarted { location_id: Uuid },
    IndexingProgress { 
        location_id: Uuid, 
        processed: u64, 
        total: Option<u64> 
    },
    IndexingCompleted { 
        location_id: Uuid, 
        total_files: u64, 
        total_dirs: u64 
    },
    IndexingFailed { location_id: Uuid, error: String },

    // Device events
    DeviceConnected { device_id: Uuid, device_name: String },
    DeviceDisconnected { device_id: Uuid },

    // Legacy events (for compatibility)
    LocationAdded {
        library_id: Uuid,
        location_id: Uuid,
        path: PathBuf,
    },
    LocationRemoved {
        library_id: Uuid,
        location_id: Uuid,
    },
    FilesIndexed {
        library_id: Uuid,
        location_id: Uuid,
        count: usize,
    },
    ThumbnailsGenerated {
        library_id: Uuid,
        count: usize,
    },
    FileOperationCompleted {
        library_id: Uuid,
        operation: FileOperation,
        affected_files: usize,
    },
    FilesModified {
        library_id: Uuid,
        paths: Vec<PathBuf>,
    },

    // Custom events for extensibility
    Custom { 
        event_type: String, 
        data: serde_json::Value 
    },
}

/// Types of file operations
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum FileOperation {
    Copy,
    Move,
    Delete,
    Rename,
}

/// Event bus for broadcasting events
#[derive(Debug, Clone)]
pub struct EventBus {
    sender: broadcast::Sender<Event>,
}

impl EventBus {
    /// Create a new event bus with specified capacity
    pub fn new(capacity: usize) -> Self {
        let (sender, _) = broadcast::channel(capacity);
        Self { sender }
    }
    
    /// Emit an event to all subscribers
    pub fn emit(&self, event: Event) {
        match self.sender.send(event.clone()) {
            Ok(subscriber_count) => {
                debug!("Event emitted to {} subscribers", subscriber_count);
            }
            Err(_) => {
                // No subscribers - this is fine, just debug log it
                debug!("Event emitted but no subscribers: {:?}", event);
            }
        }
    }
    
    /// Subscribe to events
    pub fn subscribe(&self) -> EventSubscriber {
        EventSubscriber {
            receiver: self.sender.subscribe(),
        }
    }
    
    /// Get the number of active subscribers
    pub fn subscriber_count(&self) -> usize {
        self.sender.receiver_count()
    }
}

impl Default for EventBus {
    fn default() -> Self {
        Self::new(1024)
    }
}

/// Event subscriber for receiving events
#[derive(Debug)]
pub struct EventSubscriber {
    receiver: broadcast::Receiver<Event>,
}

impl EventSubscriber {
    /// Receive the next event (blocking)
    pub async fn recv(&mut self) -> Result<Event, broadcast::error::RecvError> {
        self.receiver.recv().await
    }

    /// Try to receive an event without blocking
    pub fn try_recv(&mut self) -> Result<Event, broadcast::error::TryRecvError> {
        self.receiver.try_recv()
    }

    /// Filter events by type using a closure
    pub async fn recv_filtered<F>(&mut self, filter: F) -> Result<Event, broadcast::error::RecvError>
    where
        F: Fn(&Event) -> bool,
    {
        loop {
            let event = self.recv().await?;
            if filter(&event) {
                return Ok(event);
            }
        }
    }
}

/// Helper trait for event filtering
pub trait EventFilter {
    fn is_library_event(&self) -> bool;
    fn is_volume_event(&self) -> bool;
    fn is_job_event(&self) -> bool;
    fn is_for_library(&self, library_id: Uuid) -> bool;
}

impl EventFilter for Event {
    fn is_library_event(&self) -> bool {
        matches!(
            self,
            Event::LibraryCreated { .. }
                | Event::LibraryOpened { .. }
                | Event::LibraryClosed { .. }
                | Event::LibraryDeleted { .. }
                | Event::EntryCreated { .. }
                | Event::EntryModified { .. }
                | Event::EntryDeleted { .. }
                | Event::EntryMoved { .. }
        )
    }

    fn is_volume_event(&self) -> bool {
        matches!(
            self,
            Event::VolumeAdded(_)
                | Event::VolumeRemoved { .. }
                | Event::VolumeUpdated { .. }
                | Event::VolumeSpeedTested { .. }
                | Event::VolumeMountChanged { .. }
                | Event::VolumeError { .. }
        )
    }

    fn is_job_event(&self) -> bool {
        matches!(
            self,
            Event::JobQueued { .. }
                | Event::JobStarted { .. }
                | Event::JobProgress { .. }
                | Event::JobCompleted { .. }
                | Event::JobFailed { .. }
                | Event::JobCancelled { .. }
        )
    }

    fn is_for_library(&self, library_id: Uuid) -> bool {
        match self {
            Event::LibraryCreated { id, .. }
            | Event::LibraryOpened { id, .. }
            | Event::LibraryClosed { id, .. }
            | Event::LibraryDeleted { id } => *id == library_id,
            Event::EntryCreated { library_id: lid, .. }
            | Event::EntryModified { library_id: lid, .. }
            | Event::EntryDeleted { library_id: lid, .. }
            | Event::EntryMoved { library_id: lid, .. } => *lid == library_id,
            Event::LocationAdded { library_id: lid, .. }
            | Event::LocationRemoved { library_id: lid, .. }
            | Event::FilesIndexed { library_id: lid, .. }
            | Event::ThumbnailsGenerated { library_id: lid, .. }
            | Event::FileOperationCompleted { library_id: lid, .. }
            | Event::FilesModified { library_id: lid, .. } => *lid == library_id,
            _ => false,
        }
    }
}```

## src/domain/device.rs

```rust
//! Unified device model - no more node/device/instance confusion
//!
//! A Device represents a machine running Spacedrive. This unifies the old
//! concepts of Node, Device, and Instance into one clear model.

use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use uuid::Uuid;

/// A device running Spacedrive
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Device {
    /// Unique identifier for this device
    pub id: Uuid,

    /// Human-readable name
    pub name: String,

    /// Operating system
    pub os: OperatingSystem,

    /// Hardware model (e.g., "MacBook Pro", "iPhone 15")
    pub hardware_model: Option<String>,

    /// Network addresses for P2P connections
    pub network_addresses: Vec<String>,

    /// Whether this device is currently online
    pub is_online: bool,

    /// Sync leadership status per library
    pub sync_leadership: HashMap<Uuid, SyncRole>,

    /// Last time this device was seen
    pub last_seen_at: DateTime<Utc>,

    /// When this device was first added
    pub created_at: DateTime<Utc>,

    /// When this device info was last updated
    pub updated_at: DateTime<Utc>,
}

/// Sync role for a device in a specific library
#[derive(Debug, Clone, Copy, Serialize, Deserialize, PartialEq)]
pub enum SyncRole {
    /// This device maintains the sync log for the library
    Leader,

    /// This device syncs from the leader
    Follower,

    /// This device doesn't participate in sync for this library
    Inactive,
}

/// Operating system types
#[derive(Debug, Clone, Copy, Serialize, Deserialize, PartialEq)]
pub enum OperatingSystem {
    MacOS,
    Windows,
    Linux,
    IOs,
    Android,
    Other,
}

impl Device {
    /// Create a new device
    pub fn new(name: String) -> Self {
        let now = Utc::now();
        Self {
            id: Uuid::new_v4(),
            name,
            os: detect_operating_system(),
            hardware_model: detect_hardware_model(),
            network_addresses: Vec::new(),
            is_online: true,
            sync_leadership: HashMap::new(),
            last_seen_at: now,
            created_at: now,
            updated_at: now,
        }
    }

    /// Create the current device
    pub fn current() -> Self {
        Self::new(get_device_name())
    }

    /// Update network addresses
    pub fn update_network_addresses(&mut self, addresses: Vec<String>) {
        self.network_addresses = addresses;
        self.updated_at = Utc::now();
    }

    /// Mark device as online
    pub fn mark_online(&mut self) {
        self.is_online = true;
        self.last_seen_at = Utc::now();
        self.updated_at = Utc::now();
    }

    /// Mark device as offline
    pub fn mark_offline(&mut self) {
        self.is_online = false;
        self.updated_at = Utc::now();
    }

    /// Check if this is the current device
    pub fn is_current(&self) -> bool {
        self.id == crate::shared::types::get_current_device_id()
    }

    /// Set sync role for a library
    pub fn set_sync_role(&mut self, library_id: Uuid, role: SyncRole) {
        self.sync_leadership.insert(library_id, role);
        self.updated_at = Utc::now();
    }

    /// Get sync role for a library
    pub fn sync_role(&self, library_id: &Uuid) -> SyncRole {
        self.sync_leadership.get(library_id).copied().unwrap_or(SyncRole::Inactive)
    }

    /// Check if this device is the sync leader for a library
    pub fn is_sync_leader(&self, library_id: &Uuid) -> bool {
        matches!(self.sync_role(library_id), SyncRole::Leader)
    }

    /// Get all libraries where this device is the leader
    pub fn leader_libraries(&self) -> Vec<Uuid> {
        self.sync_leadership
            .iter()
            .filter_map(|(lib_id, role)| {
                if *role == SyncRole::Leader {
                    Some(*lib_id)
                } else {
                    None
                }
            })
            .collect()
    }
}

/// Get the device name from the system
fn get_device_name() -> String {
    #[cfg(target_os = "macos")]
    {
        return whoami::devicename();
    }

    #[cfg(any(target_os = "windows", target_os = "linux"))]
    {
        if let Ok(name) = hostname::get() {
            if let Ok(name_str) = name.into_string() {
                return name_str;
            }
        }
    }

    "Unknown Device".to_string()
}

/// Detect the operating system
fn detect_operating_system() -> OperatingSystem {
    #[cfg(target_os = "macos")]
    return OperatingSystem::MacOS;

    #[cfg(target_os = "windows")]
    return OperatingSystem::Windows;

    #[cfg(target_os = "linux")]
    return OperatingSystem::Linux;

    #[cfg(target_os = "ios")]
    return OperatingSystem::IOs;

    #[cfg(target_os = "android")]
    return OperatingSystem::Android;

    #[cfg(not(any(
        target_os = "macos",
        target_os = "windows",
        target_os = "linux",
        target_os = "ios",
        target_os = "android"
    )))]
    return OperatingSystem::Other;
}

/// Get hardware model information
fn detect_hardware_model() -> Option<String> {
    // This would use platform-specific APIs
    // For now, return None
    None
}

impl std::fmt::Display for OperatingSystem {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            OperatingSystem::MacOS => write!(f, "macOS"),
            OperatingSystem::Windows => write!(f, "Windows"),
            OperatingSystem::Linux => write!(f, "Linux"),
            OperatingSystem::IOs => write!(f, "iOS"),
            OperatingSystem::Android => write!(f, "Android"),
            OperatingSystem::Other => write!(f, "Other"),
        }
    }
}

// Conversion implementations for database entities
use crate::infrastructure::database::entities;
use sea_orm::ActiveValue;

impl From<Device> for entities::device::ActiveModel {
    fn from(device: Device) -> Self {
        use sea_orm::ActiveValue::*;

        entities::device::ActiveModel {
            id: NotSet, // Auto-increment
            uuid: Set(device.id),
            name: Set(device.name),
            os: Set(device.os.to_string()),
            os_version: Set(None), // TODO: Add to domain model if needed
            hardware_model: Set(device.hardware_model),
            network_addresses: Set(serde_json::json!(device.network_addresses)),
            is_online: Set(device.is_online),
            last_seen_at: Set(device.last_seen_at),
            capabilities: Set(serde_json::json!({
                "indexing": true,
                "p2p": true,
                "volume_detection": true
            })),
            sync_leadership: Set(serde_json::json!(device.sync_leadership)),
            created_at: Set(device.created_at),
            updated_at: Set(device.updated_at),
        }
    }
}

impl TryFrom<entities::device::Model> for Device {
    type Error = serde_json::Error;

    fn try_from(model: entities::device::Model) -> Result<Self, Self::Error> {
        let network_addresses: Vec<String> = serde_json::from_value(model.network_addresses)?;
        let sync_leadership: HashMap<Uuid, SyncRole> = serde_json::from_value(model.sync_leadership)?;

        Ok(Device {
            id: model.uuid,
            name: model.name,
            os: parse_operating_system(&model.os),
            hardware_model: model.hardware_model,
            network_addresses,
            is_online: model.is_online,
            sync_leadership,
            last_seen_at: model.last_seen_at,
            created_at: model.created_at,
            updated_at: model.updated_at,
        })
    }
}

/// Parse OS string to enum
fn parse_operating_system(os_str: &str) -> OperatingSystem {
    match os_str {
        "macOS" => OperatingSystem::MacOS,
        "Windows" => OperatingSystem::Windows,
        "Linux" => OperatingSystem::Linux,
        "iOS" => OperatingSystem::IOs,
        "Android" => OperatingSystem::Android,
        _ => OperatingSystem::Other,
    }
}```

## src/domain/content_identity.rs

```rust
//! Content domain types - for content identification and media metadata
//!
//! This module contains domain types used by the content identification system.
//! The actual ContentIdentity persistence is handled by database entities.

use chrono::{DateTime, Utc};
use int_enum::IntEnum;
use serde::{Deserialize, Serialize};
use serde_json::Value as JsonValue;

/// Type of content
#[derive(Debug, Clone, Copy, Serialize, Deserialize, PartialEq, Eq, IntEnum)]
#[serde(rename_all = "snake_case")]
#[repr(i32)]
pub enum ContentKind {
	Unknown = 0,
	Image = 1,
	Video = 2,
	Audio = 3,
	Document = 4,
	Archive = 5,
	Code = 6,
	Text = 7,
	Database = 8,
	Book = 9,
	Font = 10,
	Mesh = 11,
	Config = 12,
	Encrypted = 13,
	Key = 14,
	Executable = 15,
	Binary = 16,
}

impl std::fmt::Display for ContentKind {
	fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
		let s = match self {
			ContentKind::Unknown => "unknown",
			ContentKind::Image => "image",
			ContentKind::Video => "video",
			ContentKind::Audio => "audio",
			ContentKind::Document => "document",
			ContentKind::Archive => "archive",
			ContentKind::Code => "code",
			ContentKind::Text => "text",
			ContentKind::Database => "database",
			ContentKind::Book => "book",
			ContentKind::Font => "font",
			ContentKind::Mesh => "mesh",
			ContentKind::Config => "config",
			ContentKind::Encrypted => "encrypted",
			ContentKind::Key => "key",
			ContentKind::Executable => "executable",
			ContentKind::Binary => "binary",
		};
		write!(f, "{}", s)
	}
}

/// Media-specific metadata
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MediaData {
	/// Width in pixels (for images/video)
	pub width: Option<u32>,

	/// Height in pixels (for images/video)
	pub height: Option<u32>,

	/// Duration in seconds (for audio/video)
	pub duration: Option<f64>,

	/// Bitrate in bits per second
	pub bitrate: Option<u32>,

	/// Frame rate (for video)
	pub fps: Option<f32>,

	/// EXIF data (for images)
	pub exif: Option<ExifData>,

	/// Additional metadata as JSON
	pub extra: JsonValue,
}

/// EXIF metadata for images
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ExifData {
	/// Camera make
	pub make: Option<String>,

	/// Camera model
	pub model: Option<String>,

	/// Date taken
	pub date_taken: Option<DateTime<Utc>>,

	/// GPS coordinates
	pub gps: Option<GpsCoordinates>,

	/// ISO speed
	pub iso: Option<u32>,

	/// Aperture (f-stop)
	pub aperture: Option<f32>,

	/// Shutter speed in seconds
	pub shutter_speed: Option<f32>,

	/// Focal length in mm
	pub focal_length: Option<f32>,
}

/// GPS coordinates
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct GpsCoordinates {
	pub latitude: f64,
	pub longitude: f64,
	pub altitude: Option<f32>,
}

impl ContentKind {
	/// Determine content kind from MIME type
	pub fn from_mime_type(mime_type: &str) -> Self {
		match mime_type.split('/').next() {
			Some("image") => ContentKind::Image,
			Some("video") => ContentKind::Video,
			Some("audio") => ContentKind::Audio,
			Some("text") => ContentKind::Text,
			_ if mime_type.contains("pdf") => ContentKind::Document,
			_ if mime_type.contains("zip") || mime_type.contains("tar") => ContentKind::Archive,
			_ => ContentKind::Unknown,
		}
	}

	/// Get content kind from file type
	pub fn from_file_type(file_type: &crate::file_type::FileType) -> Self {
		file_type.category
	}
}

/// Current CAS algorithm version
pub const CURRENT_CAS_VERSION: u8 = 2;

/// Size threshold for sampling vs full hashing (10MB)
pub const SMALL_FILE_THRESHOLD: u64 = 10 * 1024 * 1024;

/// Content hash generator for content identification
pub struct ContentHashGenerator;

impl ContentHashGenerator {
	/// Generate a content hash for a file
	/// Uses sampling for large files, full hash for small files
	pub async fn generate_content_hash(path: &std::path::Path) -> Result<String, ContentHashError> {
		let metadata = tokio::fs::metadata(path).await?;
		let file_size = metadata.len();

		if file_size <= SMALL_FILE_THRESHOLD {
			// Small file: hash entire content
			Self::generate_full_hash(path).await
		} else {
			// Large file: use sampling algorithm
			Self::generate_sampled_hash(path, file_size).await
		}
	}

	/// Generate full SHA-256 hash for small files
	pub async fn generate_full_hash(path: &std::path::Path) -> Result<String, ContentHashError> {
		use sha2::{Digest, Sha256};

		let content = tokio::fs::read(path).await?;
		let mut hasher = Sha256::new();
		hasher.update(&content);
		let hash = hasher.finalize();

		Ok(format!("v{}_full:{:x}", CURRENT_CAS_VERSION, hash))
	}

	/// Generate sampled hash for large files
	/// Samples from beginning, middle, and end of file
	async fn generate_sampled_hash(
		path: &std::path::Path,
		file_size: u64,
	) -> Result<String, ContentHashError> {
		use sha2::{Digest, Sha256};
		use tokio::io::{AsyncReadExt, AsyncSeekExt};

		const SAMPLE_SIZE: u64 = 8192; // 8KB samples
		const NUM_SAMPLES: u64 = 3;

		let mut file = tokio::fs::File::open(path).await?;
		let mut hasher = Sha256::new();

		// Include file size in hash to distinguish files of different sizes
		hasher.update(&file_size.to_le_bytes());

		// Sample from beginning
		let mut buffer = vec![0u8; SAMPLE_SIZE as usize];
		file.seek(std::io::SeekFrom::Start(0)).await?;
		let bytes_read = file.read(&mut buffer).await?;
		hasher.update(&buffer[..bytes_read]);

		// Sample from middle (if file is large enough)
		if file_size > SAMPLE_SIZE * 2 {
			let middle_pos = file_size / 2 - SAMPLE_SIZE / 2;
			file.seek(std::io::SeekFrom::Start(middle_pos)).await?;
			let bytes_read = file.read(&mut buffer).await?;
			hasher.update(&buffer[..bytes_read]);
		}

		// Sample from end (if file is large enough)
		if file_size > SAMPLE_SIZE * NUM_SAMPLES {
			let end_pos = file_size.saturating_sub(SAMPLE_SIZE);
			file.seek(std::io::SeekFrom::Start(end_pos)).await?;
			let bytes_read = file.read(&mut buffer).await?;
			hasher.update(&buffer[..bytes_read]);
		}

		let hash = hasher.finalize();
		Ok(format!("v{}_sampled:{:x}", CURRENT_CAS_VERSION, hash))
	}

	/// Generate content hash from raw content (for in-memory data)
	pub fn generate_from_content(content: &[u8]) -> String {
		use sha2::{Digest, Sha256};

		let mut hasher = Sha256::new();
		hasher.update(content);
		let hash = hasher.finalize();

		format!("v{}_content:{:x}", CURRENT_CAS_VERSION, hash)
	}

	/// Verify a content hash matches the current content of a file
	pub async fn verify_content_hash(
		path: &std::path::Path,
		expected_hash: &str,
	) -> Result<bool, ContentHashError> {
		let current_hash = Self::generate_content_hash(path).await?;
		Ok(current_hash == expected_hash)
	}
}

/// Errors that can occur during content hash generation
#[derive(Debug, thiserror::Error)]
pub enum ContentHashError {
	#[error("IO error: {0}")]
	Io(#[from] std::io::Error),

	#[error("Invalid file path")]
	InvalidPath,

	#[error("File too large to process")]
	FileTooLarge,
}
```

## src/domain/mod.rs

```rust
//! Core domain models - the heart of Spacedrive's VDFS
//! 
//! These models implement the new file data model design where:
//! - Entry represents any file/directory
//! - UserMetadata is always present (enabling immediate tagging)
//! - ContentIdentity is optional (for deduplication)

pub mod content_identity;
pub mod device;
pub mod entry;
pub mod location;
pub mod user_metadata;
pub mod volume;

// Re-export commonly used types
pub use content_identity::{ContentKind, MediaData, ContentHashGenerator, ContentHashError};
pub use device::{Device, OperatingSystem};
pub use entry::{Entry, EntryKind, SdPathSerialized};
pub use location::{Location, IndexMode, ScanState};
pub use user_metadata::{UserMetadata, Tag, Label};
pub use volume::{Volume as DomainVolume, VolumeType, MountType as DomainMountType, DiskType as DomainDiskType, FileSystem as DomainFileSystem};```

## src/domain/entry.rs

```rust
//! Entry - the core file/directory representation in Spacedrive
//!
//! An Entry represents any filesystem item (file, directory, symlink) that
//! Spacedrive knows about. It's the foundation of the VDFS.

use crate::shared::types::SdPath;
use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use uuid::Uuid;

/// Represents any filesystem entry (file or directory) in the VDFS
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Entry {
    /// Unique identifier for this entry
    pub id: Uuid,

    /// The virtual path including device context
    pub sd_path: SdPathSerialized,

    /// File/directory name
    pub name: String,

    /// Type of entry
    pub kind: EntryKind,

    /// Size in bytes (None for directories)
    pub size: Option<u64>,

    /// Filesystem timestamps
    pub created_at: Option<DateTime<Utc>>,
    pub modified_at: Option<DateTime<Utc>>,
    pub accessed_at: Option<DateTime<Utc>>,

    /// Platform-specific identifiers
    pub inode: Option<u64>,      // Unix/macOS
    pub file_id: Option<u64>,     // Windows

    /// Parent directory entry ID
    pub parent_id: Option<Uuid>,

    /// Location this entry belongs to (if indexed)
    pub location_id: Option<Uuid>,

    /// User metadata (ALWAYS exists - key innovation!)
    pub metadata_id: Uuid,

    /// Content identity for deduplication (optional)
    pub content_id: Option<Uuid>,

    /// Tracking information
    pub first_seen_at: DateTime<Utc>,
    pub last_indexed_at: Option<DateTime<Utc>>,
}

/// Type of filesystem entry
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub enum EntryKind {
    /// Regular file
    File {
        /// File extension (without dot)
        extension: Option<String>
    },

    /// Directory
    Directory,

    /// Symbolic link
    Symlink {
        /// Target path
        target: String
    },
}

/// How SdPath is stored in the database
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SdPathSerialized {
    /// Device where this entry exists
    pub device_id: Uuid,

    /// Normalized path on that device
    pub path: String,
}

impl SdPathSerialized {
    /// Create from an SdPath
    pub fn from_sdpath(sdpath: &SdPath) -> Self {
        Self {
            device_id: sdpath.device_id,
            path: sdpath.path.to_string_lossy().to_string(),
        }
    }

    /// Convert back to SdPath
    pub fn to_sdpath(&self) -> SdPath {
        SdPath {
            device_id: self.device_id,
            path: self.path.clone().into(),
        }
    }
}

impl Entry {
    /// Create a new Entry from filesystem metadata
    pub fn new(sd_path: SdPath, metadata: std::fs::Metadata) -> Self {
        let name = sd_path
            .path
            .file_name()
            .and_then(|n| n.to_str())
            .unwrap_or("unknown")
            .to_string();

        let kind = if metadata.is_dir() {
            EntryKind::Directory
        } else if metadata.is_symlink() {
            EntryKind::Symlink {
                target: String::new(), // Would need to read link
            }
        } else {
            let extension = sd_path
                .path
                .extension()
                .and_then(|e| e.to_str())
                .map(|e| e.to_string());
            EntryKind::File { extension }
        };

        let size = if metadata.is_file() {
            Some(metadata.len())
        } else {
            None
        };

        Self {
            id: Uuid::new_v4(),
            sd_path: SdPathSerialized::from_sdpath(&sd_path),
            name,
            kind,
            size,
            created_at: metadata.created().ok().map(|t| t.into()),
            modified_at: metadata.modified().ok().map(|t| t.into()),
            accessed_at: metadata.accessed().ok().map(|t| t.into()),
            inode: None, // Platform-specific, would need conditional compilation
            file_id: None,
            parent_id: None,
            location_id: None,
            metadata_id: Uuid::new_v4(), // Will create UserMetadata with this ID
            content_id: None,
            first_seen_at: Utc::now(),
            last_indexed_at: None,
        }
    }

    /// Check if this is a file
    pub fn is_file(&self) -> bool {
        matches!(self.kind, EntryKind::File { .. })
    }

    /// Check if this is a directory
    pub fn is_directory(&self) -> bool {
        matches!(self.kind, EntryKind::Directory)
    }

    /// Get the file extension if this is a file
    pub fn extension(&self) -> Option<&str> {
        match &self.kind {
            EntryKind::File { extension } => extension.as_deref(),
            _ => None,
        }
    }

    /// Get the SdPath for this entry
    pub fn sd_path(&self) -> SdPath {
        self.sd_path.to_sdpath()
    }
}```

## src/domain/location.rs

```rust
//! Location - an indexed directory within a library
//!
//! Locations are directories that Spacedrive actively monitors and indexes.
//! They can be on any device and are addressed using SdPath.

use crate::domain::entry::SdPathSerialized;
use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use std::time::Duration;
use uuid::Uuid;

/// An indexed directory that Spacedrive monitors
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Location {
	/// Unique identifier
	pub id: Uuid,

	/// Library this location belongs to
	pub library_id: Uuid,

	/// Root path of this location (includes device!)
	pub sd_path: SdPathSerialized,

	/// Human-friendly name
	pub name: String,

	/// Indexing configuration
	pub index_mode: IndexMode,

	/// How often to rescan (None = manual only)
	pub scan_interval: Option<Duration>,

	/// Statistics
	pub total_size: u64,
	pub file_count: u64,
	pub directory_count: u64,

	/// Current state
	pub scan_state: ScanState,

	/// Timestamps
	pub created_at: DateTime<Utc>,
	pub updated_at: DateTime<Utc>,
	pub last_scan_at: Option<DateTime<Utc>>,

	/// Whether this location is currently available
	pub is_available: bool,

	/// Hidden glob patterns (e.g., [".*", "node_modules"])
	pub ignore_patterns: Vec<String>,
}

/// How deeply to index files in this location
#[derive(Debug, Clone, Copy, Serialize, Deserialize, PartialEq)]
pub enum IndexMode {
	/// Just filesystem metadata (name, size, dates)
	Shallow,

	/// Generate content IDs for deduplication
	Content,

	/// Full indexing - content IDs, text extraction, thumbnails
	Deep,
}

/// Current scanning state of a location
#[derive(Debug, Clone, Copy, Serialize, Deserialize, PartialEq)]
pub enum ScanState {
	/// Not currently being scanned
	Idle,

	/// Currently scanning
	Scanning {
		/// Progress percentage (0-100)
		progress: u8,
	},

	/// Scan completed successfully
	Completed,

	/// Scan failed with error
	Failed,

	/// Scan was paused
	Paused,
}

impl Location {
	/// Create a new location
	pub fn new(
		library_id: Uuid,
		name: String,
		sd_path: SdPathSerialized,
		index_mode: IndexMode,
	) -> Self {
		let now = Utc::now();
		Self {
			id: Uuid::new_v4(),
			library_id,
			sd_path,
			name,
			index_mode,
			scan_interval: None,
			total_size: 0,
			file_count: 0,
			directory_count: 0,
			scan_state: ScanState::Idle,
			created_at: now,
			updated_at: now,
			last_scan_at: None,
			is_available: true,
			ignore_patterns: vec![
				".*".to_string(),           // Hidden files
				"*.tmp".to_string(),        // Temporary files
				"node_modules".to_string(), // Node.js
				"__pycache__".to_string(),  // Python
				".git".to_string(),         // Git
			],
		}
	}

	/// Check if this location is currently being scanned
	pub fn is_scanning(&self) -> bool {
		matches!(self.scan_state, ScanState::Scanning { .. })
	}

	/// Check if this location needs scanning based on interval
	pub fn needs_scan(&self) -> bool {
		if !self.is_available {
			return false;
		}

		match (self.scan_interval, self.last_scan_at) {
			(Some(interval), Some(last_scan)) => {
				let next_scan = last_scan + chrono::Duration::from_std(interval).unwrap();
				Utc::now() >= next_scan
			}
			(Some(_), None) => true, // Never scanned but has interval
			(None, _) => false,      // Manual scan only
		}
	}

	/// Update scan progress
	pub fn set_scan_progress(&mut self, progress: u8) {
		self.scan_state = ScanState::Scanning {
			progress: progress.min(100),
		};
		self.updated_at = Utc::now();
	}

	/// Mark scan as completed
	pub fn complete_scan(&mut self, file_count: u64, directory_count: u64, total_size: u64) {
		self.scan_state = ScanState::Completed;
		self.file_count = file_count;
		self.directory_count = directory_count;
		self.total_size = total_size;
		self.last_scan_at = Some(Utc::now());
		self.updated_at = Utc::now();
	}

	/// Mark scan as failed
	pub fn fail_scan(&mut self) {
		self.scan_state = ScanState::Failed;
		self.updated_at = Utc::now();
	}

	/// Check if a path should be ignored
	pub fn should_ignore(&self, path: &str) -> bool {
		self.ignore_patterns.iter().any(|pattern| {
			// Simple glob matching (could use glob crate for full support)
			if pattern.starts_with("*.") {
				path.ends_with(&pattern[1..])
			} else if pattern.starts_with('.') {
				path.split('/').any(|part| part == pattern)
			} else {
				path.contains(pattern)
			}
		})
	}
}

impl Default for IndexMode {
	fn default() -> Self {
		IndexMode::Content
	}
}

#[cfg(test)]
mod tests {
	use super::*;
	use crate::shared::types::SdPath;

	#[test]
	fn test_location_creation() {
		let sd_path = SdPathSerialized::from_sdpath(&SdPath::local("/Users/test/Documents"));
		let location = Location::new(
			Uuid::new_v4(),
			"My Documents".to_string(),
			sd_path,
			IndexMode::Deep,
		);

		assert_eq!(location.name, "My Documents");
		assert_eq!(location.index_mode, IndexMode::Deep);
		assert!(location.is_available);
		assert!(!location.is_scanning());
	}

	#[test]
	fn test_ignore_patterns() {
		let sd_path = SdPathSerialized::from_sdpath(&SdPath::local("/test"));
		let location = Location::new(
			Uuid::new_v4(),
			"Test".to_string(),
			sd_path,
			IndexMode::Shallow,
		);

		assert!(location.should_ignore(".hidden_file"));
		assert!(location.should_ignore("file.tmp"));
		assert!(location.should_ignore("/path/to/node_modules/file.js"));
		assert!(!location.should_ignore("normal_file.txt"));
	}
}
```

## src/domain/volume.rs

```rust
//! Volume domain model - persistent storage for tracked volumes
//!
//! This represents volumes that are tracked in the database, allowing Spacedrive
//! to remember volumes across sessions and track their metadata/statistics.

use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use std::path::PathBuf;
use uuid::Uuid;

/// A tracked volume in the database
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Volume {
    /// Unique identifier
    pub id: Uuid,

    /// Library this volume belongs to (None for system-wide volumes)
    pub library_id: Option<Uuid>,

    /// Device this volume is attached to
    pub device_id: Uuid,

    /// Volume fingerprint for identification
    pub fingerprint: String,

    /// Human-readable name
    pub name: String,

    /// Current mount point (can change)
    pub mount_point: PathBuf,

    /// Additional mount points for the same volume
    pub mount_points: Vec<PathBuf>,

    /// Volume type/category
    pub volume_type: VolumeType,

    /// Mount type classification
    pub mount_type: MountType,

    /// Disk type (SSD, HDD, etc.)
    pub disk_type: DiskType,

    /// Filesystem type
    pub file_system: FileSystem,

    /// Total capacity in bytes
    pub total_capacity: u64,

    /// Currently available space in bytes
    pub available_space: u64,

    /// Whether volume is read-only
    pub is_read_only: bool,

    /// Whether volume is currently mounted/available
    pub is_mounted: bool,

    /// Whether this volume is being tracked by Spacedrive
    pub is_tracked: bool,

    /// Hardware identifier (device path, UUID, etc.)
    pub hardware_id: Option<String>,

    /// Performance metrics
    pub read_speed_mbps: Option<u64>,
    pub write_speed_mbps: Option<u64>,

    /// Timestamps
    pub created_at: DateTime<Utc>,
    pub updated_at: DateTime<Utc>,
    pub last_seen_at: DateTime<Utc>,

    /// Statistics
    pub total_files: Option<u64>,
    pub total_directories: Option<u64>,
    pub last_stats_update: Option<DateTime<Utc>>,

    /// User preferences
    pub display_name: Option<String>,
    pub is_favorite: bool,
    pub color: Option<String>,
    pub icon: Option<String>,

    /// Error state
    pub error_message: Option<String>,
}

/// Volume type classification
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub enum VolumeType {
    /// Primary system drive
    System,

    /// Internal storage (additional drives)
    Internal,

    /// External storage (USB, external drives)
    External,

    /// Network storage (NFS, SMB, etc.)
    Network,

    /// Cloud storage mounts
    Cloud,

    /// Virtual/temporary storage
    Virtual,

    /// Unknown type
    Unknown,
}

/// Mount type classification
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub enum MountType {
    /// System mount (root, boot, etc.)
    System,

    /// External device mount
    External,

    /// Network mount
    Network,

    /// User mount
    User,
}

/// Disk type classification
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub enum DiskType {
    /// Solid State Drive
    SSD,

    /// Hard Disk Drive
    HDD,

    /// Network storage
    Network,

    /// Virtual/RAM disk
    Virtual,

    /// Unknown type
    Unknown,
}

/// Filesystem type
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub enum FileSystem {
    /// Apple File System
    APFS,

    /// NT File System (Windows)
    NTFS,

    /// Fourth Extended Filesystem (Linux)
    Ext4,

    /// B-tree Filesystem (Linux)
    Btrfs,

    /// ZFS
    ZFS,

    /// Resilient File System (Windows)
    ReFS,

    /// File Allocation Table 32
    FAT32,

    /// Extended File Allocation Table
    ExFAT,

    /// Hierarchical File System Plus (macOS legacy)
    HFSPlus,

    /// Network File System
    NFS,

    /// Server Message Block
    SMB,

    /// Other filesystem
    Other(String),
}

impl Volume {
    /// Create a new tracked volume
    pub fn new(
        device_id: Uuid,
        fingerprint: String,
        name: String,
        mount_point: PathBuf,
    ) -> Self {
        let now = Utc::now();
        Self {
            id: Uuid::new_v4(),
            library_id: None,
            device_id,
            fingerprint,
            name: name.clone(),
            mount_point,
            mount_points: Vec::new(),
            volume_type: VolumeType::Unknown,
            mount_type: MountType::System,
            disk_type: DiskType::Unknown,
            file_system: FileSystem::Other("Unknown".to_string()),
            total_capacity: 0,
            available_space: 0,
            is_read_only: false,
            is_mounted: true,
            is_tracked: false,
            hardware_id: None,
            read_speed_mbps: None,
            write_speed_mbps: None,
            created_at: now,
            updated_at: now,
            last_seen_at: now,
            total_files: None,
            total_directories: None,
            last_stats_update: None,
            display_name: Some(name),
            is_favorite: false,
            color: None,
            icon: None,
            error_message: None,
        }
    }

    /// Create from runtime volume detection
    pub fn from_runtime_volume(runtime_vol: &crate::volume::Volume, device_id: Uuid) -> Self {
        let now = Utc::now();
        Self {
            id: Uuid::new_v4(),
            library_id: None,
            device_id,
            fingerprint: runtime_vol.fingerprint.to_string(),
            name: runtime_vol.name.clone(),
            mount_point: runtime_vol.mount_point.clone(),
            mount_points: runtime_vol.mount_points.clone(),
            volume_type: VolumeType::from_mount_type(&runtime_vol.mount_type),
            mount_type: MountType::from_runtime_mount_type(&runtime_vol.mount_type),
            disk_type: DiskType::from_runtime_disk_type(&runtime_vol.disk_type),
            file_system: FileSystem::from_runtime_filesystem(&runtime_vol.file_system),
            total_capacity: runtime_vol.total_bytes_capacity,
            available_space: runtime_vol.total_bytes_available,
            is_read_only: runtime_vol.read_only,
            is_mounted: runtime_vol.is_mounted,
            is_tracked: false,
            hardware_id: runtime_vol.hardware_id.clone(),
            read_speed_mbps: runtime_vol.read_speed_mbps,
            write_speed_mbps: runtime_vol.write_speed_mbps,
            created_at: now,
            updated_at: now,
            last_seen_at: now,
            total_files: None,
            total_directories: None,
            last_stats_update: None,
            display_name: Some(runtime_vol.name.clone()),
            is_favorite: false,
            color: None,
            icon: None,
            error_message: None,
        }
    }

    /// Update from runtime volume
    pub fn update_from_runtime(&mut self, runtime_vol: &crate::volume::Volume) {
        self.mount_point = runtime_vol.mount_point.clone();
        self.mount_points = runtime_vol.mount_points.clone();
        self.total_capacity = runtime_vol.total_bytes_capacity;
        self.available_space = runtime_vol.total_bytes_available;
        self.is_read_only = runtime_vol.read_only;
        self.is_mounted = runtime_vol.is_mounted;
        self.hardware_id = runtime_vol.hardware_id.clone();
        self.read_speed_mbps = runtime_vol.read_speed_mbps;
        self.write_speed_mbps = runtime_vol.write_speed_mbps;
        self.updated_at = Utc::now();
        self.last_seen_at = Utc::now();
        self.error_message = None;
    }

    /// Mark volume as tracked
    pub fn track(&mut self, library_id: Option<Uuid>) {
        self.is_tracked = true;
        self.library_id = library_id;
        self.updated_at = Utc::now();
    }

    /// Mark volume as untracked
    pub fn untrack(&mut self) {
        self.is_tracked = false;
        self.library_id = None;
        self.updated_at = Utc::now();
    }

    /// Set display preferences
    pub fn set_display_preferences(
        &mut self,
        display_name: Option<String>,
        color: Option<String>,
        icon: Option<String>,
    ) {
        self.display_name = display_name;
        self.color = color;
        self.icon = icon;
        self.updated_at = Utc::now();
    }

    /// Mark as favorite
    pub fn set_favorite(&mut self, is_favorite: bool) {
        self.is_favorite = is_favorite;
        self.updated_at = Utc::now();
    }

    /// Update statistics
    pub fn update_statistics(&mut self, total_files: u64, total_directories: u64) {
        self.total_files = Some(total_files);
        self.total_directories = Some(total_directories);
        self.last_stats_update = Some(Utc::now());
        self.updated_at = Utc::now();
    }

    /// Set error state
    pub fn set_error(&mut self, error: String) {
        self.error_message = Some(error);
        self.is_mounted = false;
        self.updated_at = Utc::now();
    }

    /// Clear error state
    pub fn clear_error(&mut self) {
        self.error_message = None;
        self.updated_at = Utc::now();
    }

    /// Get display name (fallback to name)
    pub fn display_name(&self) -> &str {
        self.display_name.as_ref().unwrap_or(&self.name)
    }

    /// Check if volume supports copy-on-write
    pub fn supports_cow(&self) -> bool {
        matches!(
            self.file_system,
            FileSystem::APFS | FileSystem::Btrfs | FileSystem::ZFS | FileSystem::ReFS
        )
    }

    /// Get capacity utilization percentage
    pub fn utilization_percentage(&self) -> f64 {
        if self.total_capacity == 0 {
            return 0.0;
        }
        let used = self.total_capacity.saturating_sub(self.available_space);
        (used as f64 / self.total_capacity as f64) * 100.0
    }

    /// Check if volume needs space warning
    pub fn needs_space_warning(&self, threshold_percent: f64) -> bool {
        self.utilization_percentage() > threshold_percent
    }
}

impl VolumeType {
    pub fn from_mount_type(mount_type: &crate::volume::MountType) -> Self {
        match mount_type {
            crate::volume::MountType::System => VolumeType::System,
            crate::volume::MountType::External => VolumeType::External,
            crate::volume::MountType::Network => VolumeType::Network,
            crate::volume::MountType::Virtual => VolumeType::Virtual,
        }
    }
}

impl MountType {
    pub fn from_runtime_mount_type(mount_type: &crate::volume::MountType) -> Self {
        match mount_type {
            crate::volume::MountType::System => MountType::System,
            crate::volume::MountType::External => MountType::External,
            crate::volume::MountType::Network => MountType::Network,
            crate::volume::MountType::Virtual => MountType::User, // Map Virtual to User as closest equivalent
        }
    }
}

impl DiskType {
    pub fn from_runtime_disk_type(disk_type: &crate::volume::DiskType) -> Self {
        match disk_type {
            crate::volume::DiskType::SSD => DiskType::SSD,
            crate::volume::DiskType::HDD => DiskType::HDD,
            // Map network and virtual to Unknown since they don't exist in the volume types
            // crate::volume::DiskType::Network => DiskType::Unknown,
            // crate::volume::DiskType::Virtual => DiskType::Unknown,
            crate::volume::DiskType::Unknown => DiskType::Unknown,
        }
    }
}

impl FileSystem {
    pub fn from_runtime_filesystem(fs: &crate::volume::FileSystem) -> Self {
        match fs {
            crate::volume::FileSystem::APFS => FileSystem::APFS,
            crate::volume::FileSystem::NTFS => FileSystem::NTFS,
            crate::volume::FileSystem::EXT4 => FileSystem::Ext4,
            crate::volume::FileSystem::Btrfs => FileSystem::Btrfs,
            crate::volume::FileSystem::ZFS => FileSystem::ZFS,
            crate::volume::FileSystem::ReFS => FileSystem::ReFS,
            crate::volume::FileSystem::FAT32 => FileSystem::FAT32,
            crate::volume::FileSystem::ExFAT => FileSystem::ExFAT,
            crate::volume::FileSystem::Other(name) => FileSystem::Other(name.clone()),
        }
    }

    /// Convert to string for storage
    pub fn to_string(&self) -> String {
        match self {
            FileSystem::APFS => "APFS".to_string(),
            FileSystem::NTFS => "NTFS".to_string(),
            FileSystem::Ext4 => "ext4".to_string(),
            FileSystem::Btrfs => "btrfs".to_string(),
            FileSystem::ZFS => "ZFS".to_string(),
            FileSystem::ReFS => "ReFS".to_string(),
            FileSystem::FAT32 => "FAT32".to_string(),
            FileSystem::ExFAT => "exFAT".to_string(),
            FileSystem::HFSPlus => "HFS+".to_string(),
            FileSystem::NFS => "NFS".to_string(),
            FileSystem::SMB => "SMB".to_string(),
            FileSystem::Other(name) => name.clone(),
        }
    }

    /// Create from string
    pub fn from_string(s: &str) -> Self {
        match s.to_uppercase().as_str() {
            "APFS" => FileSystem::APFS,
            "NTFS" => FileSystem::NTFS,
            "EXT4" => FileSystem::Ext4,
            "BTRFS" => FileSystem::Btrfs,
            "ZFS" => FileSystem::ZFS,
            "REFS" => FileSystem::ReFS,
            "FAT32" => FileSystem::FAT32,
            "EXFAT" => FileSystem::ExFAT,
            "HFS+" => FileSystem::HFSPlus,
            "NFS" => FileSystem::NFS,
            "SMB" => FileSystem::SMB,
            _ => FileSystem::Other(s.to_string()),
        }
    }
}

impl std::fmt::Display for VolumeType {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            VolumeType::System => write!(f, "System"),
            VolumeType::Internal => write!(f, "Internal"),
            VolumeType::External => write!(f, "External"),
            VolumeType::Network => write!(f, "Network"),
            VolumeType::Cloud => write!(f, "Cloud"),
            VolumeType::Virtual => write!(f, "Virtual"),
            VolumeType::Unknown => write!(f, "Unknown"),
        }
    }
}

impl std::fmt::Display for FileSystem {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        write!(f, "{}", self.to_string())
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::path::PathBuf;

    #[test]
    fn test_volume_creation() {
        let volume = Volume::new(
            Uuid::new_v4(),
            "test-fingerprint".to_string(),
            "Test Volume".to_string(),
            PathBuf::from("/mnt/test"),
        );

        assert_eq!(volume.name, "Test Volume");
        assert_eq!(volume.fingerprint, "test-fingerprint");
        assert_eq!(volume.display_name(), "Test Volume");
        assert!(!volume.is_tracked);
        assert!(!volume.is_favorite);
    }

    #[test]
    fn test_volume_tracking() {
        let mut volume = Volume::new(
            Uuid::new_v4(),
            "test".to_string(),
            "Test".to_string(),
            PathBuf::from("/test"),
        );

        let library_id = Uuid::new_v4();
        volume.track(Some(library_id));

        assert!(volume.is_tracked);
        assert_eq!(volume.library_id, Some(library_id));

        volume.untrack();
        assert!(!volume.is_tracked);
        assert_eq!(volume.library_id, None);
    }

    #[test]
    fn test_filesystem_conversion() {
        assert_eq!(FileSystem::from_string("APFS"), FileSystem::APFS);
        assert_eq!(FileSystem::from_string("ext4"), FileSystem::Ext4);
        assert_eq!(FileSystem::from_string("unknown"), FileSystem::Other("unknown".to_string()));

        assert_eq!(FileSystem::APFS.to_string(), "APFS");
        assert_eq!(FileSystem::Ext4.to_string(), "ext4");
    }

    #[test]
    fn test_utilization_calculation() {
        let mut volume = Volume::new(
            Uuid::new_v4(),
            "test".to_string(),
            "Test".to_string(),
            PathBuf::from("/test"),
        );

        volume.total_capacity = 1000;
        volume.available_space = 300;

        assert!((volume.utilization_percentage() - 70.0).abs() < f64::EPSILON);
        assert!(volume.needs_space_warning(60.0));
        assert!(!volume.needs_space_warning(80.0));
    }

    #[test]
    fn test_cow_support() {
        let mut volume = Volume::new(
            Uuid::new_v4(),
            "test".to_string(),
            "Test".to_string(),
            PathBuf::from("/test"),
        );

        volume.file_system = FileSystem::APFS;
        assert!(volume.supports_cow());

        volume.file_system = FileSystem::NTFS;
        assert!(!volume.supports_cow());

        volume.file_system = FileSystem::Btrfs;
        assert!(volume.supports_cow());
    }
}```

## src/domain/user_metadata.rs

```rust
//! User metadata - tags, labels, notes, and custom fields
//!
//! This is the key innovation: EVERY Entry has UserMetadata, even if empty.
//! This means any file can be tagged immediately without content indexing.

use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use serde_json::Value as JsonValue;
use uuid::Uuid;

/// User-applied metadata for any Entry
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct UserMetadata {
    /// Unique identifier (matches Entry.metadata_id)
    pub id: Uuid,

    /// User-applied tags
    pub tags: Vec<Tag>,

    /// Labels for categorization
    pub labels: Vec<Label>,

    /// Free-form notes
    pub notes: Option<String>,

    /// Whether this entry is marked as favorite
    pub favorite: bool,

    /// Whether this entry should be hidden
    pub hidden: bool,

    /// Custom fields for future extensibility
    pub custom_fields: JsonValue,

    /// When this metadata was created
    pub created_at: DateTime<Utc>,

    /// When this metadata was last updated
    pub updated_at: DateTime<Utc>,
}

/// A user-defined tag
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub struct Tag {
    /// Unique tag ID
    pub id: Uuid,

    /// Tag name
    pub name: String,

    /// Optional color (hex format)
    pub color: Option<String>,

    /// Optional emoji/icon
    pub icon: Option<String>,
}

/// A label for categorization
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub struct Label {
    /// Unique label ID
    pub id: Uuid,

    /// Label name
    pub name: String,

    /// Label color (hex format)
    pub color: String,
}

impl UserMetadata {
    /// Create new empty metadata
    pub fn new(id: Uuid) -> Self {
        let now = Utc::now();
        Self {
            id,
            tags: Vec::new(),
            labels: Vec::new(),
            notes: None,
            favorite: false,
            hidden: false,
            custom_fields: JsonValue::Object(serde_json::Map::new()),
            created_at: now,
            updated_at: now,
        }
    }

    /// Add a tag
    pub fn add_tag(&mut self, tag: Tag) {
        if !self.tags.iter().any(|t| t.id == tag.id) {
            self.tags.push(tag);
            self.updated_at = Utc::now();
        }
    }

    /// Remove a tag
    pub fn remove_tag(&mut self, tag_id: Uuid) {
        if let Some(pos) = self.tags.iter().position(|t| t.id == tag_id) {
            self.tags.remove(pos);
            self.updated_at = Utc::now();
        }
    }

    /// Add a label
    pub fn add_label(&mut self, label: Label) {
        if !self.labels.iter().any(|l| l.id == label.id) {
            self.labels.push(label);
            self.updated_at = Utc::now();
        }
    }

    /// Remove a label
    pub fn remove_label(&mut self, label_id: Uuid) {
        if let Some(pos) = self.labels.iter().position(|l| l.id == label_id) {
            self.labels.remove(pos);
            self.updated_at = Utc::now();
        }
    }

    /// Set notes
    pub fn set_notes(&mut self, notes: Option<String>) {
        self.notes = notes;
        self.updated_at = Utc::now();
    }

    /// Toggle favorite status
    pub fn toggle_favorite(&mut self) {
        self.favorite = !self.favorite;
        self.updated_at = Utc::now();
    }

    /// Set hidden status
    pub fn set_hidden(&mut self, hidden: bool) {
        self.hidden = hidden;
        self.updated_at = Utc::now();
    }

    /// Check if metadata has any user-applied data
    pub fn is_empty(&self) -> bool {
        self.tags.is_empty()
            && self.labels.is_empty()
            && self.notes.is_none()
            && !self.favorite
            && !self.hidden
            && self.custom_fields == JsonValue::Object(serde_json::Map::new())
    }
}

impl Default for UserMetadata {
    fn default() -> Self {
        Self::new(Uuid::new_v4())
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_empty_metadata() {
        let metadata = UserMetadata::new(Uuid::new_v4());
        assert!(metadata.is_empty());
        assert_eq!(metadata.tags.len(), 0);
        assert_eq!(metadata.labels.len(), 0);
        assert!(!metadata.favorite);
        assert!(!metadata.hidden);
    }

    #[test]
    fn test_add_tag() {
        let mut metadata = UserMetadata::new(Uuid::new_v4());
        let tag = Tag {
            id: Uuid::new_v4(),
            name: "Important".to_string(),
            color: Some("#FF0000".to_string()),
            icon: Some("â­".to_string()),
        };

        metadata.add_tag(tag.clone());
        assert_eq!(metadata.tags.len(), 1);
        assert!(!metadata.is_empty());

        // Adding same tag again shouldn't duplicate
        metadata.add_tag(tag);
        assert_eq!(metadata.tags.len(), 1);
    }
}```

## src/volume/types.rs

```rust
//! Volume type definitions

use serde::{Deserialize, Serialize};
use std::fmt;
use std::path::PathBuf;
use uuid::Uuid;

/// Spacedrive volume identifier file content
/// This file is created in the root of writable volumes for persistent identification
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SpacedriveVolumeId {
	/// Unique identifier for this volume
	pub id: Uuid,
	/// When this identifier was created
	pub created: chrono::DateTime<chrono::Utc>,
	/// Name of the device that created this identifier
	pub device_name: Option<String>,
	/// Original volume name when identifier was created
	pub volume_name: String,
	/// Device ID that created this identifier
	pub device_id: Uuid,
}

/// Unique fingerprint for a storage volume
#[derive(Serialize, Deserialize, Debug, Clone, PartialEq, Eq, Hash)]
pub struct VolumeFingerprint(pub String);

impl VolumeFingerprint {
	/// Create a new volume fingerprint from volume properties
	/// Uses intrinsic volume characteristics for cross-device portable identification
	pub fn new(name: &str, total_bytes: u64, file_system: &str) -> Self {
		let mut hasher = blake3::Hasher::new();
		hasher.update(b"content_based:");
		hasher.update(name.as_bytes());
		hasher.update(&total_bytes.to_be_bytes());
		hasher.update(file_system.as_bytes());
		hasher.update(&(name.len() as u64).to_be_bytes());

		Self(hasher.finalize().to_hex().to_string())
	}

	/// Create a fingerprint from a Spacedrive identifier UUID (preferred method)
	/// This provides stable identification across devices, renames and remounts
	pub fn from_spacedrive_id(spacedrive_id: Uuid) -> Self {
		let mut hasher = blake3::Hasher::new();
		hasher.update(b"spacedrive_id:");
		hasher.update(spacedrive_id.as_bytes());

		Self(hasher.finalize().to_hex().to_string())
	}

	/// Generate 8-character short ID for CLI display and commands
	pub fn short_id(&self) -> String {
		self.0.chars().take(8).collect()
	}

	/// Generate 12-character medium ID for disambiguation
	pub fn medium_id(&self) -> String {
		self.0.chars().take(12).collect()
	}

	/// Create fingerprint from hex string
	pub fn from_hex(hex: impl Into<String>) -> Self {
		Self(hex.into())
	}

	/// Create fingerprint from string (alias for from_hex)
	pub fn from_string(s: &str) -> Result<Self, crate::volume::VolumeError> {
		Ok(Self(s.to_string()))
	}

	/// Check if a string could be a short ID (8 chars, hex)
	pub fn is_short_id(s: &str) -> bool {
		s.len() == 8 && s.chars().all(|c| c.is_ascii_hexdigit())
	}

	/// Check if a string could be a medium ID (12 chars, hex)
	pub fn is_medium_id(s: &str) -> bool {
		s.len() == 12 && s.chars().all(|c| c.is_ascii_hexdigit())
	}

	/// Check if this fingerprint matches a short or medium ID
	pub fn matches_short_id(&self, short_id: &str) -> bool {
		if Self::is_short_id(short_id) {
			self.short_id() == short_id
		} else if Self::is_medium_id(short_id) {
			self.medium_id() == short_id
		} else {
			false
		}
	}
}

impl fmt::Display for VolumeFingerprint {
	fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
		write!(f, "{}", self.0)
	}
}

/// Classification of volume types for UX and auto-tracking decisions
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
pub enum VolumeType {
	/// Primary system drive containing OS and user data
	/// Examples: C:\ on Windows, / on Linux, Macintosh HD on macOS
	Primary,

	/// Dedicated user data volumes (separate from OS)
	/// Examples: /System/Volumes/Data on macOS, separate /home on Linux
	UserData,

	/// External or removable storage devices
	/// Examples: USB drives, external HDDs, /Volumes/* on macOS
	External,

	/// Secondary internal storage (additional drives/partitions)
	/// Examples: D:, E: drives on Windows, additional mounted drives
	Secondary,

	/// System/OS internal volumes (hidden from normal view)
	/// Examples: /System/Volumes/* on macOS, Recovery partitions
	System,

	/// Network attached storage
	/// Examples: SMB mounts, NFS, cloud storage
	Network,

	/// Unknown or unclassified volumes
	Unknown,
}

impl VolumeType {
	/// Should this volume type be auto-tracked by default?
	pub fn auto_track_by_default(&self) -> bool {
		match self {
			VolumeType::Primary
			| VolumeType::UserData
			| VolumeType::External
			| VolumeType::Secondary
			| VolumeType::Network => true,
			VolumeType::System | VolumeType::Unknown => false,
		}
	}

	/// Should this volume be shown in the default UI view?
	pub fn show_by_default(&self) -> bool {
		!matches!(self, VolumeType::System | VolumeType::Unknown)
	}

	/// User-friendly display name for the volume type
	pub fn display_name(&self) -> &'static str {
		match self {
			VolumeType::Primary => "Primary Drive",
			VolumeType::UserData => "User Data",
			VolumeType::External => "External Drive",
			VolumeType::Secondary => "Secondary Drive",
			VolumeType::System => "System Volume",
			VolumeType::Network => "Network Drive",
			VolumeType::Unknown => "Unknown",
		}
	}

	/// Icon/indicator for CLI display
	pub fn icon(&self) -> &'static str {
		match self {
			VolumeType::Primary => "[PRI]",
			VolumeType::UserData => "[USR]",
			VolumeType::External => "[EXT]",
			VolumeType::Secondary => "[SEC]",
			VolumeType::System => "[SYS]",
			VolumeType::Network => "[NET]",
			VolumeType::Unknown => "[UNK]",
		}
	}
}

/// Events emitted by the Volume Manager when volume state changes
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum VolumeEvent {
	/// Emitted when a new volume is discovered
	VolumeAdded(Volume),
	/// Emitted when a volume is removed/unmounted
	VolumeRemoved { fingerprint: VolumeFingerprint },
	/// Emitted when a volume's properties are updated
	VolumeUpdated {
		fingerprint: VolumeFingerprint,
		old: VolumeInfo,
		new: VolumeInfo,
	},
	/// Emitted when a volume's speed test completes
	VolumeSpeedTested {
		fingerprint: VolumeFingerprint,
		read_speed_mbps: u64,
		write_speed_mbps: u64,
	},
	/// Emitted when a volume's mount status changes
	VolumeMountChanged {
		fingerprint: VolumeFingerprint,
		is_mounted: bool,
	},
	/// Emitted when a volume encounters an error
	VolumeError {
		fingerprint: VolumeFingerprint,
		error: String,
	},
}

/// Represents a physical or virtual storage volume in the system
#[derive(Serialize, Deserialize, Debug, Clone)]
pub struct Volume {
	/// Unique fingerprint for this volume
	pub fingerprint: VolumeFingerprint,

	/// Device this volume belongs to
	pub device_id: uuid::Uuid,

	/// Human-readable volume name
	pub name: String,
	/// Type of mount (system, external, etc)
	pub mount_type: MountType,
	/// Classification of this volume for UX decisions
	pub volume_type: VolumeType,
	/// Primary path where the volume is mounted
	pub mount_point: PathBuf,
	/// Additional mount points (for APFS volumes, etc.)
	pub mount_points: Vec<PathBuf>,
	/// Whether the volume is currently mounted
	pub is_mounted: bool,

	/// Type of storage device (SSD, HDD, etc)
	pub disk_type: DiskType,
	/// Filesystem type (NTFS, EXT4, etc)
	pub file_system: FileSystem,
	/// Whether the volume is mounted read-only
	pub read_only: bool,

	/// Hardware identifier (platform-specific)
	pub hardware_id: Option<String>,
	/// Current error status if any
	pub error_status: Option<String>,

	// Storage information
	/// Total storage capacity in bytes
	pub total_bytes_capacity: u64,
	/// Available storage space in bytes
	pub total_bytes_available: u64,

	// Performance metrics (populated by speed tests)
	/// Read speed in megabytes per second
	pub read_speed_mbps: Option<u64>,
	/// Write speed in megabytes per second
	pub write_speed_mbps: Option<u64>,

	/// Whether this volume should be visible in default views
	pub is_user_visible: bool,

	/// Whether this volume should be auto-tracked
	pub auto_track_eligible: bool,

	/// When this volume information was last updated
	pub last_updated: chrono::DateTime<chrono::Utc>,
}

/// Summary information about a volume (for updates and caching)
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct VolumeInfo {
	pub is_mounted: bool,
	pub total_bytes_available: u64,
	pub read_speed_mbps: Option<u64>,
	pub write_speed_mbps: Option<u64>,
	pub error_status: Option<String>,
}

/// Information about a tracked volume in the database
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TrackedVolume {
	pub id: i32,
	pub uuid: uuid::Uuid,
	pub device_id: uuid::Uuid,
	pub fingerprint: VolumeFingerprint,
	pub display_name: Option<String>,
	pub tracked_at: chrono::DateTime<chrono::Utc>,
	pub last_seen_at: chrono::DateTime<chrono::Utc>,
	pub is_online: bool,
	pub total_capacity: Option<u64>,
	pub available_capacity: Option<u64>,
	pub read_speed_mbps: Option<u32>,
	pub write_speed_mbps: Option<u32>,
	pub last_speed_test_at: Option<chrono::DateTime<chrono::Utc>>,
	pub file_system: Option<String>,
	pub mount_point: Option<String>,
	pub is_removable: Option<bool>,
	pub is_network_drive: Option<bool>,
	pub device_model: Option<String>,
	pub volume_type: String,
	pub is_user_visible: Option<bool>,
	pub auto_track_eligible: Option<bool>,
}

impl From<&Volume> for VolumeInfo {
	fn from(volume: &Volume) -> Self {
		Self {
			is_mounted: volume.is_mounted,
			total_bytes_available: volume.total_bytes_available,
			read_speed_mbps: volume.read_speed_mbps,
			write_speed_mbps: volume.write_speed_mbps,
			error_status: volume.error_status.clone(),
		}
	}
}

impl TrackedVolume {
	/// Convert a TrackedVolume back to a Volume for display purposes
	/// This is used for offline volumes that aren't currently detected
	pub fn to_offline_volume(&self) -> Volume {
		use std::path::PathBuf;

		Volume {
			fingerprint: self.fingerprint.clone(),
			device_id: self.device_id,
			name: self
				.display_name
				.clone()
				.unwrap_or_else(|| "Unknown".to_string()),
			mount_type: crate::volume::types::MountType::External, // Default for tracked volumes
			volume_type: match self.volume_type.as_str() {
				"Primary" => VolumeType::Primary,
				"UserData" => VolumeType::UserData,
				"External" => VolumeType::External,
				"Secondary" => VolumeType::Secondary,
				"System" => VolumeType::System,
				"Network" => VolumeType::Network,
				_ => VolumeType::Unknown,
			},
			mount_point: PathBuf::from(
				self.mount_point
					.clone()
					.unwrap_or_else(|| "Not connected".to_string()),
			),
			mount_points: vec![], // Not available for offline volumes
			disk_type: crate::volume::types::DiskType::Unknown,
			file_system: crate::volume::types::FileSystem::from_string(
				&self
					.file_system
					.clone()
					.unwrap_or_else(|| "Unknown".to_string()),
			),
			total_bytes_capacity: self.total_capacity.unwrap_or(0),
			total_bytes_available: self.available_capacity.unwrap_or(0),
			read_only: false, // Assume not read-only for tracked volumes
			hardware_id: self.device_model.clone(),
			is_mounted: false, // Offline volumes are not mounted
			error_status: None,
			read_speed_mbps: self.read_speed_mbps.map(|s| s as u64),
			write_speed_mbps: self.write_speed_mbps.map(|s| s as u64),
			last_updated: self.last_seen_at,
			is_user_visible: self.is_user_visible.unwrap_or(true),
			auto_track_eligible: self.auto_track_eligible.unwrap_or(false),
		}
	}
}

impl Volume {
	/// Create a new Volume instance
	pub fn new(
		device_id: uuid::Uuid,
		name: String,
		mount_type: MountType,
		volume_type: VolumeType,
		mount_point: PathBuf,
		additional_mount_points: Vec<PathBuf>,
		disk_type: DiskType,
		file_system: FileSystem,
		total_bytes_capacity: u64,
		total_bytes_available: u64,
		read_only: bool,
		hardware_id: Option<String>,
		fingerprint: VolumeFingerprint, // Accept pre-computed fingerprint
	) -> Self {
		Self {
			fingerprint,
			device_id,
			name,
			mount_type,
			volume_type,
			mount_point,
			mount_points: additional_mount_points,
			is_mounted: true,
			disk_type,
			file_system,
			total_bytes_capacity,
			total_bytes_available,
			read_only,
			hardware_id,
			error_status: None,
			read_speed_mbps: None,
			write_speed_mbps: None,
			auto_track_eligible: volume_type.auto_track_by_default(),
			is_user_visible: volume_type.show_by_default(),
			last_updated: chrono::Utc::now(),
		}
	}

	/// Update volume information
	pub fn update_info(&mut self, info: VolumeInfo) {
		self.is_mounted = info.is_mounted;
		self.total_bytes_available = info.total_bytes_available;
		self.read_speed_mbps = info.read_speed_mbps;
		self.write_speed_mbps = info.write_speed_mbps;
		self.error_status = info.error_status;
		self.last_updated = chrono::Utc::now();
	}

	/// Check if this volume supports fast copy operations (CoW)
	pub fn supports_fast_copy(&self) -> bool {
		matches!(
			self.file_system,
			FileSystem::APFS | FileSystem::Btrfs | FileSystem::ZFS | FileSystem::ReFS
		)
	}

	/// Get the optimal chunk size for copying to/from this volume
	pub fn optimal_chunk_size(&self) -> usize {
		match self.disk_type {
			DiskType::SSD => 1024 * 1024,   // 1MB for SSDs
			DiskType::HDD => 256 * 1024,    // 256KB for HDDs
			DiskType::Unknown => 64 * 1024, // 64KB default
		}
	}

	/// Estimate copy speed between this and another volume
	pub fn estimate_copy_speed(&self, other: &Volume) -> Option<u64> {
		let self_read = self.read_speed_mbps?;
		let other_write = other.write_speed_mbps?;

		// Bottleneck is the slower of read or write speed
		Some(self_read.min(other_write))
	}

	/// Check if a path is contained within this volume
	pub fn contains_path(&self, path: &PathBuf) -> bool {
		// Check primary mount point
		if path.starts_with(&self.mount_point) {
			return true;
		}

		// Check additional mount points
		for mount_point in &self.mount_points {
			if path.starts_with(mount_point) {
				return true;
			}
		}

		false
	}
}

/// Represents the type of physical storage device
#[derive(Serialize, Deserialize, Debug, Clone, PartialEq, Eq, Hash)]
pub enum DiskType {
	/// Solid State Drive
	SSD,
	/// Hard Disk Drive
	HDD,
	/// Unknown or virtual disk type
	Unknown,
}

impl fmt::Display for DiskType {
	fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
		match self {
			DiskType::SSD => write!(f, "SSD"),
			DiskType::HDD => write!(f, "HDD"),
			DiskType::Unknown => write!(f, "Unknown"),
		}
	}
}

impl DiskType {
	pub fn from_string(disk_type: &str) -> Self {
		match disk_type.to_uppercase().as_str() {
			"SSD" => Self::SSD,
			"HDD" => Self::HDD,
			_ => Self::Unknown,
		}
	}
}

/// Represents the filesystem type of the volume
#[derive(Serialize, Deserialize, Debug, Clone, PartialEq, Eq, Hash)]
pub enum FileSystem {
	/// Windows NTFS filesystem
	NTFS,
	/// FAT32 filesystem
	FAT32,
	/// Linux EXT4 filesystem
	EXT4,
	/// Apple APFS filesystem
	APFS,
	/// ExFAT filesystem
	ExFAT,
	/// Btrfs filesystem (Linux)
	Btrfs,
	/// ZFS filesystem
	ZFS,
	/// Windows ReFS filesystem
	ReFS,
	/// Other/unknown filesystem type
	Other(String),
}

impl fmt::Display for FileSystem {
	fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
		match self {
			FileSystem::NTFS => write!(f, "NTFS"),
			FileSystem::FAT32 => write!(f, "FAT32"),
			FileSystem::EXT4 => write!(f, "EXT4"),
			FileSystem::APFS => write!(f, "APFS"),
			FileSystem::ExFAT => write!(f, "ExFAT"),
			FileSystem::Btrfs => write!(f, "Btrfs"),
			FileSystem::ZFS => write!(f, "ZFS"),
			FileSystem::ReFS => write!(f, "ReFS"),
			FileSystem::Other(name) => write!(f, "{}", name),
		}
	}
}

impl FileSystem {
	pub fn from_string(fs: &str) -> Self {
		match fs.to_uppercase().as_str() {
			"NTFS" => Self::NTFS,
			"FAT32" => Self::FAT32,
			"EXT4" => Self::EXT4,
			"APFS" => Self::APFS,
			"EXFAT" => Self::ExFAT,
			"BTRFS" => Self::Btrfs,
			"ZFS" => Self::ZFS,
			"REFS" => Self::ReFS,
			other => Self::Other(other.to_string()),
		}
	}

	/// Check if this filesystem supports reflinks/clones
	pub fn supports_reflink(&self) -> bool {
		matches!(self, Self::APFS | Self::Btrfs | Self::ZFS | Self::ReFS)
	}

	/// Check if this filesystem supports sendfile optimization
	pub fn supports_sendfile(&self) -> bool {
		matches!(self, Self::EXT4 | Self::Btrfs | Self::ZFS | Self::NTFS)
	}
}

/// Represents how the volume is mounted in the system
#[derive(Serialize, Deserialize, Debug, Clone, PartialEq, Eq, Hash)]
pub enum MountType {
	/// System/boot volume
	System,
	/// External/removable volume
	External,
	/// Network-attached volume
	Network,
	/// Virtual/container volume
	Virtual,
}

impl fmt::Display for MountType {
	fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
		match self {
			MountType::System => write!(f, "System"),
			MountType::External => write!(f, "External"),
			MountType::Network => write!(f, "Network"),
			MountType::Virtual => write!(f, "Virtual"),
		}
	}
}

impl MountType {
	pub fn from_string(mount_type: &str) -> Self {
		match mount_type.to_uppercase().as_str() {
			"SYSTEM" => Self::System,
			"EXTERNAL" => Self::External,
			"NETWORK" => Self::Network,
			"VIRTUAL" => Self::Virtual,
			_ => Self::System,
		}
	}
}

/// Configuration for volume detection and monitoring
#[derive(Debug, Clone)]
pub struct VolumeDetectionConfig {
	/// Whether to include system volumes
	pub include_system: bool,
	/// Whether to include virtual volumes
	pub include_virtual: bool,
	/// Whether to run speed tests on discovery
	pub run_speed_test: bool,
	/// How often to refresh volume information (in seconds)
	pub refresh_interval_secs: u64,
}

impl Default for VolumeDetectionConfig {
	fn default() -> Self {
		Self {
			include_system: true,
			include_virtual: false,
			run_speed_test: false, // Expensive operation, off by default
			refresh_interval_secs: 30,
		}
	}
}

#[cfg(test)]
mod tests {
	use super::*;

	#[test]
	fn test_volume_fingerprint() {
		let volume = Volume::new(
			uuid::Uuid::new_v4(),
			"Test Volume".to_string(),
			MountType::External,
			VolumeType::External,
			PathBuf::from("/mnt/test"),
			vec![],
			DiskType::SSD,
			FileSystem::EXT4,
			1000000000,
			500000000,
			false,
			Some("test-hw-id".to_string()),
			VolumeFingerprint::new("Test", 500000000, "ext4"),
		);

		// Test basic fingerprint creation
		let fingerprint = VolumeFingerprint::new(
			"Test Volume",
			1000000000, // 1GB
			"ext4",
		);
		assert!(!fingerprint.0.is_empty());

		// Test Spacedrive ID fingerprint
		let spacedrive_id = Uuid::new_v4();
		let spacedrive_fingerprint = VolumeFingerprint::from_spacedrive_id(spacedrive_id);
		assert!(!spacedrive_fingerprint.0.is_empty());
		assert_ne!(fingerprint, spacedrive_fingerprint);
	}

	#[test]
	fn test_volume_contains_path() {
		let volume = Volume::new(
			uuid::Uuid::new_v4(),
			"Test".to_string(),
			MountType::System,
			VolumeType::System,
			PathBuf::from("/home"),
			vec![PathBuf::from("/home"), PathBuf::from("/mnt/home")],
			DiskType::SSD,
			FileSystem::EXT4,
			1000000,
			500000,
			false,
			None,
			VolumeFingerprint::new(uuid::Uuid::new_v4(), "Test"),
		);

		assert!(volume.contains_path(&PathBuf::from("/home/user/file.txt")));
		assert!(volume.contains_path(&PathBuf::from("/mnt/home/user/file.txt")));
		assert!(!volume.contains_path(&PathBuf::from("/var/log/file.txt")));
	}

	#[test]
	fn test_filesystem_capabilities() {
		assert!(FileSystem::APFS.supports_reflink());
		assert!(FileSystem::Btrfs.supports_reflink());
		assert!(!FileSystem::FAT32.supports_reflink());

		assert!(FileSystem::EXT4.supports_sendfile());
		assert!(!FileSystem::FAT32.supports_sendfile());
	}
}
```

## src/volume/speed.rs

```rust
//! Volume speed testing functionality

use crate::volume::{
	error::{VolumeError, VolumeResult},
	types::{MountType, Volume},
};
use std::time::Instant;
use tokio::{
	fs::{File, OpenOptions},
	io::{AsyncReadExt, AsyncWriteExt},
	time::{timeout, Duration},
};
use tracing::{debug, instrument, warn};

/// Configuration for speed tests
#[derive(Debug, Clone)]
pub struct SpeedTestConfig {
	/// Size of the test file in megabytes
	pub file_size_mb: usize,
	/// Timeout for the test in seconds
	pub timeout_secs: u64,
	/// Number of test iterations for averaging
	pub iterations: usize,
}

impl Default for SpeedTestConfig {
	fn default() -> Self {
		Self {
			file_size_mb: 10,
			timeout_secs: 30,
			iterations: 1,
		}
	}
}

/// Result of a speed test
#[derive(Debug, Clone)]
pub struct SpeedTestResult {
	/// Write speed in MB/s
	pub write_speed_mbps: f64,
	/// Read speed in MB/s
	pub read_speed_mbps: f64,
	/// Total time taken for the test
	pub duration_secs: f64,
}

/// Run a speed test on the given volume
#[instrument(skip(volume), fields(volume_name = %volume.name))]
pub async fn run_speed_test(volume: &Volume) -> VolumeResult<(u64, u64)> {
	run_speed_test_with_config(volume, SpeedTestConfig::default()).await
}

/// Run a speed test with custom configuration
#[instrument(skip(volume, config), fields(volume_name = %volume.name))]
pub async fn run_speed_test_with_config(
	volume: &Volume,
	config: SpeedTestConfig,
) -> VolumeResult<(u64, u64)> {
	if !volume.is_mounted {
		return Err(VolumeError::NotMounted(volume.name.clone()));
	}

	if volume.read_only {
		return Err(VolumeError::ReadOnly(volume.name.clone()));
	}

	debug!("Starting speed test with config: {:?}", config);

	let test_location = TestLocation::new(&volume.mount_point, &volume.mount_type).await?;
	let result = perform_speed_test(&test_location, &config).await?;

	// Cleanup
	test_location.cleanup().await?;

	debug!(
		"Speed test completed: {:.2} MB/s write, {:.2} MB/s read",
		result.write_speed_mbps, result.read_speed_mbps
	);

	Ok((
		result.read_speed_mbps as u64,
		result.write_speed_mbps as u64,
	))
}

/// Helper for managing test files and directories
struct TestLocation {
	test_file: std::path::PathBuf,
	created_dir: Option<std::path::PathBuf>,
}

impl TestLocation {
	/// Create a new test location
	async fn new(volume_path: &std::path::Path, mount_type: &MountType) -> VolumeResult<Self> {
		let (dir, created_dir) = get_writable_directory(volume_path, mount_type).await?;
		let test_file = dir.join("spacedrive_speed_test.tmp");

		Ok(Self {
			test_file,
			created_dir,
		})
	}

	/// Clean up test files and directories
	async fn cleanup(&self) -> VolumeResult<()> {
		// Remove test file
		if self.test_file.exists() {
			if let Err(e) = tokio::fs::remove_file(&self.test_file).await {
				warn!("Failed to remove test file: {}", e);
			}
		}

		// Remove created directory if we created it
		if let Some(ref dir) = self.created_dir {
			if let Err(e) = tokio::fs::remove_dir_all(dir).await {
				warn!("Failed to remove test directory: {}", e);
			}
		}

		Ok(())
	}
}

/// Perform the actual speed test
async fn perform_speed_test(
	location: &TestLocation,
	config: &SpeedTestConfig,
) -> VolumeResult<SpeedTestResult> {
	let test_data = generate_test_data(config.file_size_mb);
	let timeout_duration = Duration::from_secs(config.timeout_secs);

	let mut write_speeds = Vec::new();
	let mut read_speeds = Vec::new();
	let overall_start = Instant::now();

	for iteration in 0..config.iterations {
		debug!(
			"Speed test iteration {}/{}",
			iteration + 1,
			config.iterations
		);

		// Write test
		let write_speed = timeout(
			timeout_duration,
			perform_write_test(&location.test_file, &test_data),
		)
		.await
		.map_err(|_| VolumeError::Timeout)??;

		write_speeds.push(write_speed);

		// Read test
		let read_speed = timeout(
			timeout_duration,
			perform_read_test(&location.test_file, test_data.len()),
		)
		.await
		.map_err(|_| VolumeError::Timeout)??;

		read_speeds.push(read_speed);

		// Clean up test file between iterations
		if iteration < config.iterations - 1 {
			let _ = tokio::fs::remove_file(&location.test_file).await;
		}
	}

	let avg_write_speed = write_speeds.iter().sum::<f64>() / write_speeds.len() as f64;
	let avg_read_speed = read_speeds.iter().sum::<f64>() / read_speeds.len() as f64;

	Ok(SpeedTestResult {
		write_speed_mbps: avg_write_speed,
		read_speed_mbps: avg_read_speed,
		duration_secs: overall_start.elapsed().as_secs_f64(),
	})
}

/// Generate test data for speed testing
fn generate_test_data(size_mb: usize) -> Vec<u8> {
	let size_bytes = size_mb * 1024 * 1024;

	// Use a pattern instead of zeros to avoid compression optimizations
	let pattern = b"SpacedriveSpeedTest0123456789ABCDEF";
	let mut data = Vec::with_capacity(size_bytes);

	for i in 0..size_bytes {
		data.push(pattern[i % pattern.len()]);
	}

	data
}

/// Perform write speed test
async fn perform_write_test(file_path: &std::path::Path, data: &[u8]) -> VolumeResult<f64> {
	let start = Instant::now();

	let mut file = OpenOptions::new()
		.write(true)
		.create(true)
		.truncate(true)
		.open(file_path)
		.await?;

	file.write_all(data).await?;
	file.sync_all().await?; // Ensure data is written to disk

	let duration = start.elapsed();
	let speed_mbps = (data.len() as f64 / 1024.0 / 1024.0) / duration.as_secs_f64();

	Ok(speed_mbps)
}

/// Perform read speed test
async fn perform_read_test(file_path: &std::path::Path, expected_size: usize) -> VolumeResult<f64> {
	let start = Instant::now();

	let mut file = File::open(file_path).await?;
	let mut buffer = Vec::with_capacity(expected_size);
	file.read_to_end(&mut buffer).await?;

	let duration = start.elapsed();
	let speed_mbps = (buffer.len() as f64 / 1024.0 / 1024.0) / duration.as_secs_f64();

	Ok(speed_mbps)
}

/// Get a writable directory within the volume
async fn get_writable_directory(
	volume_path: &std::path::Path,
	mount_type: &MountType,
) -> VolumeResult<(std::path::PathBuf, Option<std::path::PathBuf>)> {
	match mount_type {
		MountType::System => {
			// For system volumes, prefer using temp directory
			let temp_dir = std::env::temp_dir();
			Ok((temp_dir, None))
		}
		_ => {
			// For external volumes, try to write in the root or create a temp directory
			let candidates = [
				volume_path.join("tmp"),
				volume_path.join(".spacedrive_temp"),
				volume_path.to_path_buf(),
			];

			for candidate in &candidates {
				// Try to create the directory
				if let Ok(()) = tokio::fs::create_dir_all(candidate).await {
					// Test if we can write to it
					let test_file = candidate.join("test_write_permissions");
					if tokio::fs::write(&test_file, b"test").await.is_ok() {
						let _ = tokio::fs::remove_file(&test_file).await;

						// If we created a directory specifically for this test, mark it for cleanup
						let created_dir = if candidate
							.file_name()
							.map_or(false, |name| name == "tmp" || name == ".spacedrive_temp")
						{
							Some(candidate.clone())
						} else {
							None
						};

						return Ok((candidate.clone(), created_dir));
					}
				}
			}

			Err(VolumeError::PermissionDenied(format!(
				"No writable directory found in volume: {}",
				volume_path.display()
			)))
		}
	}
}

#[cfg(test)]
mod tests {
	use super::*;
	use crate::volume::{
		types::{DiskType, FileSystem},
		VolumeFingerprint,
	};
	use tempfile::TempDir;

	#[tokio::test]
	async fn test_speed_test_config() {
		let config = SpeedTestConfig::default();
		assert_eq!(config.file_size_mb, 10);
		assert_eq!(config.timeout_secs, 30);
		assert_eq!(config.iterations, 1);
	}

	#[tokio::test]
	async fn test_generate_test_data() {
		let data = generate_test_data(1); // 1MB
		assert_eq!(data.len(), 1024 * 1024);

		// Verify pattern is not all zeros
		assert!(data.iter().any(|&b| b != 0));
	}

	#[tokio::test]
	async fn test_writable_directory_external() {
		let temp_dir = TempDir::new().unwrap();
		let volume_path = temp_dir.path();

		let (writable_dir, created_dir) = get_writable_directory(volume_path, &MountType::External)
			.await
			.unwrap();

		assert!(writable_dir.exists());

		// Cleanup if we created a directory
		if let Some(dir) = created_dir {
			let _ = tokio::fs::remove_dir_all(dir).await;
		}
	}

	#[tokio::test]
	async fn test_writable_directory_system() {
		let (writable_dir, created_dir) =
			get_writable_directory(&std::path::PathBuf::from("/"), &MountType::System)
				.await
				.unwrap();

		assert!(writable_dir.exists());
		assert!(created_dir.is_none()); // Should use system temp, not create new dir
	}

	#[tokio::test]
	async fn test_full_speed_test() {
		let temp_dir = TempDir::new().unwrap();

		let volume = Volume::new(
			uuid::Uuid::new_v4(), // Test device ID
			"Test Volume".to_string(),
			MountType::External,
			temp_dir.path().to_path_buf(),
			vec![],
			DiskType::Unknown,
			FileSystem::Other("test".to_string()),
			1000000000, // 1GB capacity
			500000000,  // 500MB available
			false,      // Not read-only
			None,
			VolumeFingerprint::new(
				uuid::Uuid::new_v4(),
				&temp_dir.path().to_path_buf(),
				"Test Volume",
			),
		);

		let config = SpeedTestConfig {
			file_size_mb: 1, // Small test file
			timeout_secs: 10,
			iterations: 1,
		};

		let result = run_speed_test_with_config(&volume, config).await;
		assert!(result.is_ok());

		let (read_speed, write_speed) = result.unwrap();
		assert!(read_speed > 0);
		assert!(write_speed > 0);
	}
}
```

## src/volume/error.rs

```rust
//! Volume-related error types

use thiserror::Error;

/// Errors that can occur during volume operations
#[derive(Error, Debug)]
pub enum VolumeError {
    /// IO error during volume operations
    #[error("IO error: {0}")]
    Io(#[from] std::io::Error),

    /// Platform-specific error
    #[error("Platform error: {0}")]
    Platform(String),

    /// Volume not found
    #[error("Volume not found: {0}")]
    NotFound(String),

    /// Volume is not mounted
    #[error("Volume is not mounted: {0}")]
    NotMounted(String),

    /// Volume is read-only
    #[error("Volume is read-only: {0}")]
    ReadOnly(String),

    /// Insufficient space on volume
    #[error("Insufficient space on volume: required {required}, available {available}")]
    InsufficientSpace { required: u64, available: u64 },

    /// Speed test was cancelled or failed
    #[error("Speed test cancelled or failed")]
    SpeedTestFailed,

    /// Volume detection failed
    #[error("Volume detection failed: {0}")]
    DetectionFailed(String),

    /// Permission denied
    #[error("Permission denied: {0}")]
    PermissionDenied(String),

    /// Operation timed out
    #[error("Operation timed out")]
    Timeout,

    /// Invalid volume data
    #[error("Invalid volume data: {0}")]
    InvalidData(String),

    /// Database operation failed
    #[error("Database error: {0}")]
    Database(String),

    /// Volume is already tracked
    #[error("Volume is already tracked: {0}")]
    AlreadyTracked(String),

    /// Volume is not tracked
    #[error("Volume is not tracked: {0}")]
    NotTracked(String),
}

impl VolumeError {
    /// Create a platform-specific error
    pub fn platform(msg: impl Into<String>) -> Self {
        Self::Platform(msg.into())
    }

    /// Create a detection failed error
    pub fn detection_failed(msg: impl Into<String>) -> Self {
        Self::DetectionFailed(msg.into())
    }

    /// Create an insufficient space error
    pub fn insufficient_space(required: u64, available: u64) -> Self {
        Self::InsufficientSpace { required, available }
    }
}

/// Result type for volume operations
pub type VolumeResult<T> = Result<T, VolumeError>;```

## src/volume/classification.rs

```rust
//! Volume classification system for platform-aware volume type detection

use crate::volume::types::{FileSystem, VolumeType};
use std::path::Path;

/// Information needed for volume classification
#[derive(Debug, Clone)]
pub struct VolumeDetectionInfo {
	pub mount_point: std::path::PathBuf,
	pub file_system: FileSystem,
	pub total_bytes_capacity: u64,
	pub is_removable: Option<bool>,
	pub is_network_drive: Option<bool>,
	pub device_model: Option<String>,
}

/// Trait for platform-specific volume classification
pub trait VolumeClassifier {
	fn classify(&self, volume_info: &VolumeDetectionInfo) -> VolumeType;
}

/// macOS volume classifier
pub struct MacOSClassifier;

impl VolumeClassifier for MacOSClassifier {
	fn classify(&self, info: &VolumeDetectionInfo) -> VolumeType {
		let mount_str = info.mount_point.to_string_lossy();

		match mount_str.as_ref() {
			// Primary system drive
			"/" => VolumeType::Primary,

			// User data volume (modern macOS separates this)
			path if path.starts_with("/System/Volumes/Data") => VolumeType::UserData,

			// System internal volumes
			path if path.starts_with("/System/Volumes/") => VolumeType::System,

			// macOS autofs system
			path if mount_str.contains("auto_home")
				|| info.file_system == FileSystem::Other("autofs".to_string()) =>
			{
				VolumeType::System
			}

			// External drives
			path if path.starts_with("/Volumes/") => {
				if info.is_removable.unwrap_or(false) {
					VolumeType::External
				} else {
					// Could be user-created APFS volume
					VolumeType::Secondary
				}
			}

			// Network mounts
			path if path.starts_with("/Network/") => VolumeType::Network,

			_ => VolumeType::Unknown,
		}
	}
}

/// Windows volume classifier
pub struct WindowsClassifier;

impl VolumeClassifier for WindowsClassifier {
	fn classify(&self, info: &VolumeDetectionInfo) -> VolumeType {
		let mount_str = info.mount_point.to_string_lossy();

		match mount_str.as_ref() {
			// Primary system drive (usually C:)
			"C:\\" => VolumeType::Primary,

			// Recovery and EFI partitions
			path if path.contains("Recovery")
				|| path.contains("EFI")
				|| (info.file_system == FileSystem::FAT32
					&& info.total_bytes_capacity < 1_000_000_000) =>
			{
				VolumeType::System
			}

			// Other drive letters
			path if path.len() == 3 && path.ends_with(":\\") => {
				if info.is_removable.unwrap_or(false) {
					VolumeType::External
				} else {
					VolumeType::Secondary
				}
			}

			// Network drives
			path if path.starts_with("\\\\") => VolumeType::Network,

			_ => VolumeType::Unknown,
		}
	}
}

/// Linux volume classifier
pub struct LinuxClassifier;

impl VolumeClassifier for LinuxClassifier {
	fn classify(&self, info: &VolumeDetectionInfo) -> VolumeType {
		let mount_str = info.mount_point.to_string_lossy();

		match mount_str.as_ref() {
			// Root filesystem
			"/" => VolumeType::Primary,

			// User data partition
			"/home" => VolumeType::UserData,

			// System/virtual filesystems
			path if path.starts_with("/proc")
				|| path.starts_with("/sys")
				|| path.starts_with("/dev")
				|| path.starts_with("/boot") =>
			{
				VolumeType::System
			}

			// External/removable media
			path if path.starts_with("/media/")
				|| path.starts_with("/mnt/")
				|| info.is_removable.unwrap_or(false) =>
			{
				VolumeType::External
			}

			// Network mounts
			path if info.file_system == FileSystem::Other("nfs".to_string())
				|| info.file_system == FileSystem::Other("cifs".to_string()) =>
			{
				VolumeType::Network
			}

			_ => VolumeType::Secondary,
		}
	}
}

/// Fallback classifier for unknown platforms
pub struct UnknownClassifier;

impl VolumeClassifier for UnknownClassifier {
	fn classify(&self, info: &VolumeDetectionInfo) -> VolumeType {
		// Basic classification based on common patterns
		if info.is_removable.unwrap_or(false) {
			VolumeType::External
		} else if info.is_network_drive.unwrap_or(false) {
			VolumeType::Network
		} else {
			VolumeType::Unknown
		}
	}
}

/// Get the appropriate classifier for the current platform
pub fn get_classifier() -> Box<dyn VolumeClassifier> {
	#[cfg(target_os = "macos")]
	return Box::new(MacOSClassifier);

	#[cfg(target_os = "windows")]
	return Box::new(WindowsClassifier);

	#[cfg(target_os = "linux")]
	return Box::new(LinuxClassifier);

	#[cfg(not(any(target_os = "macos", target_os = "windows", target_os = "linux")))]
	return Box::new(UnknownClassifier);
}
```

## src/volume/manager.rs

```rust
//! Volume Manager - Central management for all volume operations

use crate::infrastructure::database::entities;
use crate::infrastructure::events::{Event, EventBus};
use crate::library::LibraryManager;
use crate::volume::{
	error::{VolumeError, VolumeResult},
	os_detection,
	types::{
		SpacedriveVolumeId, TrackedVolume, Volume, VolumeDetectionConfig, VolumeFingerprint,
		VolumeInfo,
	},
};
use crate::Core;
use notify::{RecommendedWatcher, RecursiveMode, Watcher};
use sea_orm::{ActiveModelTrait, ColumnTrait, EntityTrait, PaginatorTrait, QueryFilter, Set};
use std::collections::HashMap;
use std::path::{Path, PathBuf};
use std::sync::{Arc, Weak};
use std::time::Duration as StdDuration;
use tokio::{fs, sync::RwLock, time::Duration};
use tracing::{debug, error, info, instrument, warn};
use uuid::Uuid;

/// Filename for Spacedrive volume identifier files
const SPACEDRIVE_VOLUME_ID_FILE: &str = ".spacedrive-volume-id";

/// Get platform-specific directories to watch for volume mount changes
fn get_volume_watch_paths() -> Vec<PathBuf> {
	let mut paths = Vec::new();

	#[cfg(target_os = "macos")]
	{
		paths.push(PathBuf::from("/Volumes"));
		// Note: System volumes like / and /System/Volumes/Data are typically stable
	}

	#[cfg(target_os = "linux")]
	{
		paths.push(PathBuf::from("/media"));
		paths.push(PathBuf::from("/mnt"));
		// Note: Could also watch /proc/mounts but that's more complex
	}

	#[cfg(target_os = "windows")]
	{
		// Windows drive letters are harder to watch - we'll rely on polling for now
		// Could potentially use WMI events in the future
	}

	// Filter to only existing directories
	paths.into_iter().filter(|p| p.exists()).collect()
}

/// Central manager for volume detection, monitoring, and operations
pub struct VolumeManager {
	/// Device ID for this manager
	device_id: uuid::Uuid,

	/// Currently known volumes, indexed by fingerprint
	volumes: Arc<RwLock<HashMap<VolumeFingerprint, Volume>>>,

	/// Cache mapping paths to volume fingerprints for fast lookup
	path_cache: Arc<RwLock<HashMap<PathBuf, VolumeFingerprint>>>,

	/// Configuration for volume detection
	config: VolumeDetectionConfig,

	/// Event bus for emitting volume events
	events: Arc<EventBus>,

	/// Whether the manager is currently running monitoring
	is_monitoring: Arc<RwLock<bool>>,

	/// File system watcher for real-time volume change detection
	volume_watcher: Arc<RwLock<Option<RecommendedWatcher>>>,

	/// Weak reference to library manager for database operations
	library_manager: RwLock<Option<Weak<LibraryManager>>>,
}

impl VolumeManager {
	/// Create a new VolumeManager instance
	pub fn new(
		device_id: uuid::Uuid,
		config: VolumeDetectionConfig,
		events: Arc<EventBus>,
	) -> Self {
		Self {
			device_id,
			volumes: Arc::new(RwLock::new(HashMap::new())),
			path_cache: Arc::new(RwLock::new(HashMap::new())),
			config,
			events,
			is_monitoring: Arc::new(RwLock::new(false)),
			volume_watcher: Arc::new(RwLock::new(None)),
			library_manager: RwLock::new(None),
		}
	}

	/// Set the library manager reference
	pub async fn set_library_manager(&self, library_manager: Weak<LibraryManager>) {
		*self.library_manager.write().await = Some(library_manager);
	}

	/// Initialize the volume manager and perform initial detection
	#[instrument(skip(self))]
	pub async fn initialize(&self) -> VolumeResult<()> {
		info!("Initializing volume manager");

		// Perform initial volume detection
		self.refresh_volumes().await?;

		// Start monitoring if configured
		if self.config.refresh_interval_secs > 0 {
			self.start_monitoring().await;
		}

		info!(
			"Volume manager initialized with {} volumes",
			self.volumes.read().await.len()
		);

		Ok(())
	}

	/// Start background monitoring of volume changes
	pub async fn start_monitoring(&self) {
		if *self.is_monitoring.read().await {
			warn!("Volume monitoring already started");
			return;
		}

		*self.is_monitoring.write().await = true;

		// Start file system watcher for real-time detection
		self.start_volume_watcher().await;

		// Continue with existing timer-based monitoring as fallback
		let volumes = self.volumes.clone();
		let path_cache = self.path_cache.clone();
		let events = self.events.clone();
		let config = self.config.clone();
		let is_monitoring = self.is_monitoring.clone();
		let device_id = self.device_id;

		tokio::spawn(async move {
			info!(
				"Starting volume monitoring (refresh every {}s)",
				config.refresh_interval_secs
			);

			let mut interval =
				tokio::time::interval(Duration::from_secs(config.refresh_interval_secs));

			while *is_monitoring.read().await {
				interval.tick().await;

				if let Err(e) = Self::refresh_volumes_internal(
					device_id,
					&volumes,
					&path_cache,
					&events,
					&config,
				)
				.await
				{
					error!("Error during volume refresh: {}", e);
				}
			}

			info!("Volume monitoring stopped");
		});
	}

	/// Start file system watcher for real-time volume change detection
	async fn start_volume_watcher(&self) {
		let watch_paths = get_volume_watch_paths();
		if watch_paths.is_empty() {
			debug!("No volume watch paths available on this platform, using timer-based monitoring only");
			return;
		}

		let volumes = self.volumes.clone();
		let path_cache = self.path_cache.clone();
		let events = self.events.clone();
		let config = self.config.clone();
		let device_id = self.device_id;
		let is_monitoring = self.is_monitoring.clone();

		// Create the watcher
		let (tx, mut rx) = tokio::sync::mpsc::channel(100);

		let watcher = notify::recommended_watcher(move |result: notify::Result<notify::Event>| {
			match result {
				Ok(event) => {
					// Send the event to our async handler
					if let Err(_) = tx.blocking_send(event) {
						// Channel closed, watcher is stopping
					}
				}
				Err(e) => {
					error!("Volume watcher error: {}", e);
				}
			}
		});

		match watcher {
			Ok(mut watcher) => {
				// Watch the volume directories
				for path in &watch_paths {
					if let Err(e) = watcher.watch(path, RecursiveMode::NonRecursive) {
						warn!("Failed to watch {}: {}", path.display(), e);
					} else {
						info!("Watching {} for volume changes", path.display());
					}
				}

				// Store the watcher
				*self.volume_watcher.write().await = Some(watcher);

				// Handle events
				tokio::spawn(async move {
					while let Some(event) = rx.recv().await {
						if !*is_monitoring.read().await {
							break;
						}

						// Check if this is a mount/unmount event
						if event.kind.is_create() || event.kind.is_remove() {
							debug!("Volume change detected: {:?}", event);

							// Debounce rapid events (reduced from 500ms for faster response)
							tokio::time::sleep(Duration::from_millis(200)).await;

							let start_time = std::time::Instant::now();

							// Trigger volume refresh
							match Self::refresh_volumes_internal(
								device_id,
								&volumes,
								&path_cache,
								&events,
								&config,
							)
							.await
							{
								Ok(()) => {
									let elapsed = start_time.elapsed();
									info!(
										"Event-triggered volume refresh completed in {:?}",
										elapsed
									);
								}
								Err(e) => {
									error!("Error during event-triggered volume refresh: {}", e);
								}
							}
						}
					}
					debug!("Volume watcher event handler stopped");
				});
			}
			Err(e) => {
				warn!("Failed to create volume watcher: {}", e);
			}
		}
	}

	/// Stop background monitoring
	pub async fn stop_monitoring(&self) {
		*self.is_monitoring.write().await = false;

		// Stop the file system watcher
		if let Some(_watcher) = self.volume_watcher.write().await.take() {
			debug!("Volume watcher stopped");
		}

		info!("Volume monitoring stopped");
	}

	/// Refresh all volumes and detect changes
	#[instrument(skip(self))]
	pub async fn refresh_volumes(&self) -> VolumeResult<()> {
		Self::refresh_volumes_internal(
			self.device_id,
			&self.volumes,
			&self.path_cache,
			&self.events,
			&self.config,
		)
		.await
	}

	/// Internal implementation of volume refresh
	async fn refresh_volumes_internal(
		device_id: uuid::Uuid,
		volumes: &Arc<RwLock<HashMap<VolumeFingerprint, Volume>>>,
		path_cache: &Arc<RwLock<HashMap<PathBuf, VolumeFingerprint>>>,
		events: &Arc<EventBus>,
		config: &VolumeDetectionConfig,
	) -> VolumeResult<()> {
		debug!("Refreshing volumes for device {}", device_id);

		// Detect current volumes
		let detected_volumes = os_detection::detect_volumes(device_id, config).await?;
		let mut current_volumes = volumes.write().await;
		let mut cache = path_cache.write().await;

		// Track which volumes we've seen in this refresh
		let mut seen_fingerprints = std::collections::HashSet::new();

		// Process detected volumes
		for detected in detected_volumes {
			let fingerprint = detected.fingerprint.clone();
			seen_fingerprints.insert(fingerprint.clone());

			match current_volumes.get(&fingerprint) {
				Some(existing) => {
					// Volume exists - check for changes
					let old_info = VolumeInfo::from(existing);
					let new_info = VolumeInfo::from(&detected);

					if old_info.is_mounted != new_info.is_mounted
						|| old_info.total_bytes_available != new_info.total_bytes_available
						|| old_info.error_status != new_info.error_status
					{
						// Update the volume
						let mut updated_volume = detected.clone();
						updated_volume.update_info(new_info.clone());
						current_volumes.insert(fingerprint.clone(), updated_volume);

						// Emit update event
						events.emit(Event::VolumeUpdated {
							fingerprint: fingerprint.clone(),
							old_info: old_info.clone(),
							new_info: new_info.clone(),
						});

						// Emit mount status change if applicable
						if old_info.is_mounted != new_info.is_mounted {
							events.emit(Event::VolumeMountChanged {
								fingerprint: fingerprint.clone(),
								is_mounted: new_info.is_mounted,
							});
						}
					}
				}
				None => {
					// New volume discovered
					info!("New volume discovered: {}", detected.name);

					// Update cache for all mount points
					cache.insert(detected.mount_point.clone(), fingerprint.clone());
					for mount_point in &detected.mount_points {
						cache.insert(mount_point.clone(), fingerprint.clone());
					}

					current_volumes.insert(fingerprint.clone(), detected.clone());

					// Emit volume added event
					events.emit(Event::VolumeAdded(detected));
				}
			}
		}

		// Check for removed volumes
		let removed_fingerprints: Vec<_> = current_volumes
			.keys()
			.filter(|fp| !seen_fingerprints.contains(fp))
			.cloned()
			.collect();

		for fingerprint in removed_fingerprints {
			if let Some(removed_volume) = current_volumes.remove(&fingerprint) {
				info!("Volume removed: {}", removed_volume.name);

				// Clean up cache entries
				cache.retain(|_, fp| fp != &fingerprint);

				// Emit volume removed event
				events.emit(Event::VolumeRemoved { fingerprint });
			}
		}

		// Update offline status for tracked volumes that are no longer detected
		// Note: This requires library context, so we'll add this to a separate method
		// that gets called from places where we have library access
		debug!(
			"Volume refresh completed. Detected {} volumes",
			seen_fingerprints.len()
		);

		Ok(())
	}

	/// Mark tracked volumes as offline if they're no longer detected
	/// This should be called after refresh_volumes_internal when we have library access
	pub async fn update_offline_volumes(
		&self,
		library: &crate::library::Library,
	) -> VolumeResult<()> {
		let db = library.db().conn();
		let current_volumes = self.volumes.read().await;
		let detected_fingerprints: std::collections::HashSet<_> =
			current_volumes.keys().cloned().collect();

		// Get all tracked volumes for this device
		let tracked_volumes = entities::volume::Entity::find()
			.filter(entities::volume::Column::DeviceId.eq(self.device_id))
			.all(db)
			.await
			.map_err(|e| VolumeError::Database(e.to_string()))?;

		let mut updated_count = 0;

		for tracked_volume in tracked_volumes {
			let fingerprint = VolumeFingerprint(tracked_volume.fingerprint.clone());
			let is_currently_detected = detected_fingerprints.contains(&fingerprint);

			// Update online status if it has changed
			if tracked_volume.is_online != is_currently_detected {
				let mut active_model: entities::volume::ActiveModel = tracked_volume.into();
				active_model.is_online = Set(is_currently_detected);
				active_model.last_seen_at = Set(chrono::Utc::now());

				active_model
					.update(db)
					.await
					.map_err(|e| VolumeError::Database(e.to_string()))?;

				updated_count += 1;

				if is_currently_detected {
					debug!("Marked volume {} as online", fingerprint.0);
				} else {
					debug!("Marked volume {} as offline", fingerprint.0);
				}
			}
		}

		if updated_count > 0 {
			debug!(
				"Updated online status for {} tracked volumes",
				updated_count
			);
		}

		Ok(())
	}

	/// Get volume information for a specific path
	#[instrument(skip(self))]
	pub async fn volume_for_path(&self, path: &Path) -> Option<Volume> {
		// Canonicalize the path to handle relative paths properly
		let canonical_path = match path.canonicalize() {
			Ok(p) => p,
			Err(e) => {
				debug!("Failed to canonicalize path {}: {}", path.display(), e);
				// If canonicalization fails, try with the original path
				path.to_path_buf()
			}
		};

		// Check cache first (use canonical path for cache key)
		{
			let cache = self.path_cache.read().await;
			if let Some(fingerprint) = cache.get(&canonical_path) {
				let volumes = self.volumes.read().await;
				if let Some(volume) = volumes.get(fingerprint) {
					return Some(volume.clone());
				}
			}
		}

		// Search through all volumes using canonical path
		let volumes = self.volumes.read().await;
		for volume in volumes.values() {
			if volume.contains_path(&canonical_path) {
				// Cache the result using canonical path
				let mut cache = self.path_cache.write().await;
				cache.insert(canonical_path.clone(), volume.fingerprint.clone());
				return Some(volume.clone());
			}
		}

		debug!("No volume found for path: {}", canonical_path.display());
		None
	}

	/// Get all currently known volumes
	pub async fn get_all_volumes(&self) -> Vec<Volume> {
		self.volumes.read().await.values().cloned().collect()
	}

	/// Get a specific volume by fingerprint
	pub async fn get_volume(&self, fingerprint: &VolumeFingerprint) -> Option<Volume> {
		self.volumes.read().await.get(fingerprint).cloned()
	}

	/// Check if two paths are on the same volume
	pub async fn same_volume(&self, path1: &Path, path2: &Path) -> bool {
		let vol1 = self.volume_for_path(path1).await;
		let vol2 = self.volume_for_path(path2).await;

		match (vol1, vol2) {
			(Some(v1), Some(v2)) => v1.fingerprint == v2.fingerprint,
			_ => false,
		}
	}

	/// Find volumes with available space
	pub async fn volumes_with_space(&self, required_bytes: u64) -> Vec<Volume> {
		self.volumes
			.read()
			.await
			.values()
			.filter(|vol| vol.total_bytes_available >= required_bytes)
			.cloned()
			.collect()
	}

	/// Get volume statistics
	pub async fn get_statistics(&self) -> VolumeStatistics {
		let volumes = self.volumes.read().await;

		let total_volumes = volumes.len();
		let mounted_volumes = volumes.values().filter(|v| v.is_mounted).count();
		let total_capacity: u64 = volumes.values().map(|v| v.total_bytes_capacity).sum();
		let total_available: u64 = volumes.values().map(|v| v.total_bytes_available).sum();

		let mut by_type = HashMap::new();
		let mut by_filesystem = HashMap::new();

		for volume in volumes.values() {
			*by_type.entry(volume.disk_type.clone()).or_insert(0) += 1;
			*by_filesystem.entry(volume.file_system.clone()).or_insert(0) += 1;
		}

		VolumeStatistics {
			total_volumes,
			mounted_volumes,
			total_capacity,
			total_available,
			by_type,
			by_filesystem,
		}
	}

	/// Run speed test on a specific volume
	#[instrument(skip(self))]
	pub async fn run_speed_test(&self, fingerprint: &VolumeFingerprint) -> VolumeResult<()> {
		let mut volumes = self.volumes.write().await;

		if let Some(volume) = volumes.get_mut(fingerprint) {
			info!("Running speed test on volume: {}", volume.name);

			match crate::volume::speed::run_speed_test(volume).await {
				Ok((read_speed, write_speed)) => {
					volume.read_speed_mbps = Some(read_speed);
					volume.write_speed_mbps = Some(write_speed);

					// Emit speed test event
					self.events.emit(Event::VolumeSpeedTested {
						fingerprint: fingerprint.clone(),
						read_speed_mbps: read_speed,
						write_speed_mbps: write_speed,
					});

					info!(
						"Speed test completed: {}MB/s read, {}MB/s write",
						read_speed, write_speed
					);

					Ok(())
				}
				Err(e) => {
					error!("Speed test failed for volume {}: {}", volume.name, e);

					// Emit error event
					self.events.emit(Event::VolumeError {
						fingerprint: fingerprint.clone(),
						error: format!("Speed test failed: {}", e),
					});

					Err(e)
				}
			}
		} else {
			Err(VolumeError::NotFound(fingerprint.to_string()))
		}
	}

	/// Clear the path cache (useful after major volume changes)
	pub async fn clear_cache(&self) {
		self.path_cache.write().await.clear();
		debug!("Volume path cache cleared");
	}

	/// Track a volume in the specified library
	pub async fn track_volume(
		&self,
		library: &crate::library::Library,
		fingerprint: &VolumeFingerprint,
		display_name: Option<String>,
	) -> VolumeResult<entities::volume::Model> {
		// Find the volume in our current detected volumes
		let volume = {
			let volumes = self.volumes.read().await;
			volumes
				.get(fingerprint)
				.cloned()
				.ok_or_else(|| VolumeError::NotFound(fingerprint.to_string()))?
		};

		// Try to create/read identifier file for this volume
		if let Some(spacedrive_id) = self.manage_spacedrive_identifier(&volume).await {
			info!(
				"Created/found Spacedrive ID {} for manually tracked volume {}",
				spacedrive_id, volume.name
			);

			// Check if we should upgrade to Spacedrive ID-based fingerprint
			let spacedrive_fingerprint = VolumeFingerprint::from_spacedrive_id(spacedrive_id);
			if spacedrive_fingerprint != volume.fingerprint {
				info!(
					"Upgrading fingerprint for volume {} from content-based to Spacedrive ID-based",
					volume.name
				);
				// Note: In a full implementation, we'd want to update the volume's fingerprint
				// and potentially migrate database records. For now, we'll log this.
			}
		}

		// Check if volume is already tracked
		if let Some(existing) = entities::volume::Entity::find()
			.filter(entities::volume::Column::Fingerprint.eq(fingerprint.0.clone()))
			.filter(entities::volume::Column::DeviceId.eq(volume.device_id))
			.one(library.db().conn())
			.await
			.map_err(|e| VolumeError::Database(e.to_string()))?
		{
			warn!(
				"Volume {} is already tracked in library {}",
				volume.name,
				library.name().await
			);
			return Ok(existing);
		}

		// Determine removability and network status
		let is_removable = matches!(volume.mount_type, crate::volume::types::MountType::External);
		let is_network_drive =
			matches!(volume.mount_type, crate::volume::types::MountType::Network);

		// Create tracking record
		let active_model = entities::volume::ActiveModel {
			uuid: Set(Uuid::new_v4()),
			device_id: Set(volume.device_id), // Use Uuid directly
			fingerprint: Set(fingerprint.0.clone()),
			display_name: Set(display_name.clone()),
			tracked_at: Set(chrono::Utc::now()),
			last_seen_at: Set(chrono::Utc::now()),
			is_online: Set(volume.is_mounted),
			total_capacity: Set(Some(volume.total_bytes_capacity as i64)),
			available_capacity: Set(Some(volume.total_bytes_available as i64)),
			read_speed_mbps: Set(volume.read_speed_mbps.map(|s| s as i32)),
			write_speed_mbps: Set(volume.write_speed_mbps.map(|s| s as i32)),
			last_speed_test_at: Set(None),
			file_system: Set(Some(volume.file_system.to_string())),
			mount_point: Set(Some(volume.mount_point.to_string_lossy().to_string())),
			is_removable: Set(Some(is_removable)),
			is_network_drive: Set(Some(is_network_drive)),
			device_model: Set(volume.hardware_id.clone()),
			// Save volume classification fields
			volume_type: Set(Some(format!("{:?}", volume.volume_type))),
			is_user_visible: Set(Some(volume.is_user_visible)),
			auto_track_eligible: Set(Some(volume.auto_track_eligible)),
			..Default::default()
		};

		let model = active_model
			.insert(library.db().conn())
			.await
			.map_err(|e| VolumeError::Database(e.to_string()))?;

		info!(
			"Tracked volume '{}' for library '{}'",
			display_name.as_ref().unwrap_or(&volume.name),
			library.name().await
		);

		// Emit tracking event
		self.events.emit(Event::Custom {
			event_type: "VolumeTracked".to_string(),
			data: serde_json::json!({
				"library_id": library.id(),
				"volume_fingerprint": fingerprint.to_string(),
				"display_name": display_name,
			}),
		});

		Ok(model)
	}

	/// Untrack a volume from the database
	pub async fn untrack_volume(
		&self,
		library: &crate::library::Library,
		fingerprint: &VolumeFingerprint,
	) -> VolumeResult<()> {
		let db = library.db().conn();

		let result = entities::volume::Entity::delete_many()
			.filter(entities::volume::Column::Fingerprint.eq(fingerprint.0.clone()))
			.exec(db)
			.await
			.map_err(|e| VolumeError::Database(e.to_string()))?;

		if result.rows_affected == 0 {
			return Err(VolumeError::NotTracked(fingerprint.to_string()));
		}

		info!(
			"Untracked volume '{}' from library '{}'",
			fingerprint.to_string(),
			library.name().await
		);

		// Emit untracking event
		self.events.emit(Event::Custom {
			event_type: "VolumeUntracked".to_string(),
			data: serde_json::json!({
				"library_id": library.id(),
				"volume_fingerprint": fingerprint.to_string(),
			}),
		});

		Ok(())
	}

	/// Get tracked volumes for a library
	pub async fn get_tracked_volumes(
		&self,
		library: &crate::library::Library,
	) -> VolumeResult<Vec<TrackedVolume>> {
		let db = library.db().conn();

		let volumes = entities::volume::Entity::find()
			.all(db)
			.await
			.map_err(|e| VolumeError::Database(e.to_string()))?;

		let tracked_volumes: Vec<TrackedVolume> = volumes
			.into_iter()
			.map(|model| model.to_tracked_volume())
			.collect();

		debug!(
			"Found {} tracked volumes for library '{}'",
			tracked_volumes.len(),
			library.name().await
		);

		Ok(tracked_volumes)
	}

	/// Check if a volume is tracked in a specific library
	pub async fn is_volume_tracked(
		&self,
		library: &crate::library::Library,
		fingerprint: &VolumeFingerprint,
	) -> VolumeResult<bool> {
		let db = library.db().conn();

		// Get the volume to find its device_id
		let volume = self
			.get_volume(fingerprint)
			.await
			.ok_or_else(|| VolumeError::NotFound(fingerprint.to_string()))?;

		let count = entities::volume::Entity::find()
			.filter(entities::volume::Column::DeviceId.eq(volume.device_id))
			.filter(entities::volume::Column::Fingerprint.eq(fingerprint.0.clone()))
			.count(db)
			.await
			.map_err(|e| VolumeError::Database(e.to_string()))?;

		Ok(count > 0)
	}

	/// Update tracked volume state during refresh
	pub async fn update_tracked_volume_state(
		&self,
		library: &crate::library::Library,
		fingerprint: &VolumeFingerprint,
		volume: &Volume,
	) -> VolumeResult<()> {
		let db = library.db().conn();

		let mut active_model: entities::volume::ActiveModel = entities::volume::Entity::find()
			.filter(entities::volume::Column::DeviceId.eq(volume.device_id))
			.filter(entities::volume::Column::Fingerprint.eq(fingerprint.0.clone()))
			.one(db)
			.await
			.map_err(|e| VolumeError::Database(e.to_string()))?
			.ok_or_else(|| VolumeError::NotTracked(fingerprint.to_string()))?
			.into();

		active_model.last_seen_at = Set(chrono::Utc::now());
		active_model.is_online = Set(volume.is_mounted);
		active_model.total_capacity = Set(Some(volume.total_bytes_capacity as i64));
		active_model.available_capacity = Set(Some(volume.total_bytes_available as i64));

		active_model
			.update(db)
			.await
			.map_err(|e| VolumeError::Database(e.to_string()))?;

		Ok(())
	}

	/// Update display names for tracked volumes that have empty display names
	pub async fn update_empty_display_names(
		&self,
		library: &crate::library::Library,
	) -> VolumeResult<usize> {
		let db = library.db().conn();

		// Find tracked volumes with empty display names
		let volumes_to_update = entities::volume::Entity::find()
			.filter(
				entities::volume::Column::DisplayName
					.is_null()
					.or(entities::volume::Column::DisplayName.eq("")),
			)
			.all(db)
			.await
			.map_err(|e| VolumeError::Database(e.to_string()))?;

		let mut updated_count = 0;

		for tracked_volume in volumes_to_update {
			let fingerprint = VolumeFingerprint(tracked_volume.fingerprint.clone());

			// Get the current volume info to get the name
			if let Some(volume) = self.get_volume(&fingerprint).await {
				let volume_name = volume.name.clone();
				let mut active_model: entities::volume::ActiveModel = tracked_volume.into();
				active_model.display_name = Set(Some(volume.name));

				match active_model.update(db).await {
					Ok(_) => {
						updated_count += 1;
						info!("Updated display name for volume: {}", volume_name);
					}
					Err(e) => {
						warn!(
							"Failed to update display name for volume {}: {}",
							fingerprint.0, e
						);
					}
				}
			}
		}

		info!(
			"Updated display names for {} volumes in library '{}'",
			updated_count,
			library.name().await
		);
		Ok(updated_count)
	}

	/// Get all system volumes (boot/OS volumes)
	pub async fn get_system_volumes(&self) -> Vec<Volume> {
		self.volumes
			.read()
			.await
			.values()
			.filter(|v| matches!(v.mount_type, crate::volume::types::MountType::System))
			.cloned()
			.collect()
	}

	/// Auto-track eligible volumes (Primary and External types)
	pub async fn auto_track_user_volumes(
		&self,
		library: &crate::library::Library,
	) -> VolumeResult<Vec<entities::volume::Model>> {
		let eligible_volumes: Vec<_> = self
			.volumes
			.read()
			.await
			.values()
			.filter(|v| v.auto_track_eligible)
			.cloned()
			.collect();

		let mut tracked_volumes = Vec::new();

		for volume in eligible_volumes {
			// Try to create/read identifier file for better fingerprinting
			if let Some(spacedrive_id) = self.manage_spacedrive_identifier(&volume).await {
				info!(
					"Using Spacedrive ID {} for volume {} fingerprinting",
					spacedrive_id, volume.name
				);
				// We could update the fingerprint here, but for now we'll keep using the existing one
				// to maintain compatibility with already tracked volumes
			}

			if !self.is_volume_tracked(library, &volume.fingerprint).await? {
				match self
					.track_volume(library, &volume.fingerprint, Some(volume.name.clone()))
					.await
				{
					Ok(tracked_volume) => {
						info!("Auto-tracked volume: {}", volume.name);
						tracked_volumes.push(tracked_volume);
					}
					Err(e) => {
						warn!("Failed to auto-track volume {}: {}", volume.name, e);
					}
				}
			}
		}

		Ok(tracked_volumes)
	}

	/// Automatically track system volumes for a library (legacy - use auto_track_user_volumes instead)
	pub async fn auto_track_system_volumes(
		&self,
		library: &crate::library::Library,
	) -> VolumeResult<Vec<entities::volume::Model>> {
		// Use the new filtered auto-tracking
		self.auto_track_user_volumes(library).await
	}

	/// Save speed test results to all libraries where this volume is tracked
	pub async fn save_speed_test_results(
		&self,
		fingerprint: &VolumeFingerprint,
		read_speed_mbps: u64,
		write_speed_mbps: u64,
		libraries: &[Arc<crate::library::Library>],
	) -> VolumeResult<()> {
		for library in libraries {
			// Check if this volume is tracked in this library
			if self.is_volume_tracked(library, fingerprint).await? {
				let db = library.db().conn();

				// Get the volume to find its device_id
				let volume = self
					.get_volume(fingerprint)
					.await
					.ok_or_else(|| VolumeError::NotFound(fingerprint.to_string()))?;

				// Update the tracked volume record with speed test results
				let now = chrono::Utc::now();

				let update_result = entities::volume::Entity::update_many()
					.filter(entities::volume::Column::DeviceId.eq(volume.device_id))
					.filter(entities::volume::Column::Fingerprint.eq(fingerprint.0.clone()))
					.set(entities::volume::ActiveModel {
						read_speed_mbps: Set(Some(read_speed_mbps as i32)),
						write_speed_mbps: Set(Some(write_speed_mbps as i32)),
						last_speed_test_at: Set(Some(now)),
						..Default::default()
					})
					.exec(db)
					.await
					.map_err(|e| VolumeError::Database(e.to_string()))?;

				if update_result.rows_affected > 0 {
					info!(
						"Saved speed test results for volume {} in library {}: {}MB/s read, {}MB/s write",
						fingerprint.0,
						library.name().await,
						read_speed_mbps,
						write_speed_mbps
					);
				}
			}
		}

		Ok(())
	}

	/// Get volume by short ID
	pub async fn get_volume_by_short_id(&self, short_id: &str) -> Option<Volume> {
		if !VolumeFingerprint::is_short_id(short_id) && !VolumeFingerprint::is_medium_id(short_id) {
			return None;
		}

		let volumes = self.volumes.read().await;
		for volume in volumes.values() {
			if volume.fingerprint.matches_short_id(short_id) {
				return Some(volume.clone());
			}
		}
		None
	}

	/// Get volumes that match a partial name (for smart name matching)
	pub async fn get_volumes_by_name(&self, name: &str) -> Vec<Volume> {
		let volumes = self.volumes.read().await;
		let name_lower = name.to_lowercase();

		volumes
			.values()
			.filter(|volume| volume.name.to_lowercase().contains(&name_lower))
			.cloned()
			.collect()
	}

	/// Create or read Spacedrive identifier file for a volume
	/// Returns the UUID from the identifier file if successfully created/read
	async fn manage_spacedrive_identifier(&self, volume: &Volume) -> Option<Uuid> {
		let id_file_path = volume.mount_point.join(SPACEDRIVE_VOLUME_ID_FILE);

		// Try to read existing identifier file
		if let Ok(content) = fs::read_to_string(&id_file_path).await {
			if let Ok(spacedrive_id) = serde_json::from_str::<SpacedriveVolumeId>(&content) {
				debug!(
					"Found existing Spacedrive ID {} for volume {}",
					spacedrive_id.id, volume.name
				);
				return Some(spacedrive_id.id);
			}
		}

		// Try to create new identifier file if volume is writable
		if !volume.read_only && volume.mount_point.exists() {
			let spacedrive_id = SpacedriveVolumeId {
				id: Uuid::new_v4(),
				created: chrono::Utc::now(),
				device_name: None, // TODO: Get from DeviceManager when available
				volume_name: volume.name.clone(),
				device_id: volume.device_id,
			};

			if let Ok(content) = serde_json::to_string_pretty(&spacedrive_id) {
				match fs::write(&id_file_path, content).await {
					Ok(()) => {
						info!(
							"Created Spacedrive ID {} for volume {} at {}",
							spacedrive_id.id,
							volume.name,
							id_file_path.display()
						);
						return Some(spacedrive_id.id);
					}
					Err(e) => {
						debug!(
							"Failed to write Spacedrive ID file to {}: {}",
							id_file_path.display(),
							e
						);
					}
				}
			}
		}

		debug!(
			"Could not create or read Spacedrive identifier for volume {} (read_only: {}, exists: {})",
			volume.name,
			volume.read_only,
			volume.mount_point.exists()
		);
		None
	}

	/// Read Spacedrive identifier file from a volume if it exists
	pub async fn read_spacedrive_identifier(
		&self,
		mount_point: &Path,
	) -> Option<SpacedriveVolumeId> {
		let id_file_path = mount_point.join(SPACEDRIVE_VOLUME_ID_FILE);

		if let Ok(content) = fs::read_to_string(&id_file_path).await {
			if let Ok(spacedrive_id) = serde_json::from_str::<SpacedriveVolumeId>(&content) {
				return Some(spacedrive_id);
			}
		}

		None
	}
}

/// Statistics about detected volumes
#[derive(Debug, Clone)]
pub struct VolumeStatistics {
	pub total_volumes: usize,
	pub mounted_volumes: usize,
	pub total_capacity: u64,
	pub total_available: u64,
	pub by_type: HashMap<crate::volume::types::DiskType, usize>,
	pub by_filesystem: HashMap<crate::volume::types::FileSystem, usize>,
}

impl Drop for VolumeManager {
	fn drop(&mut self) {
		// Ensure monitoring is stopped when manager is dropped
		let is_monitoring = self.is_monitoring.clone();
		tokio::spawn(async move {
			*is_monitoring.write().await = false;
		});
	}
}

#[cfg(test)]
mod tests {
	use super::*;

	fn create_test_events() -> Arc<EventBus> {
		Arc::new(EventBus::default())
	}

	#[tokio::test]
	async fn test_volume_manager_creation() {
		let config = VolumeDetectionConfig::default();
		let events = create_test_events();
		let manager = VolumeManager::new(Uuid::new_v4(), config, events);

		let stats = manager.get_statistics().await;
		assert_eq!(stats.total_volumes, 0);
	}

	#[tokio::test]
	async fn test_volume_path_lookup() {
		let config = VolumeDetectionConfig::default();
		let events = create_test_events();
		let manager = VolumeManager::new(Uuid::new_v4(), config, events);

		// Initially no volumes
		let volume = manager
			.volume_for_path(&PathBuf::from("/nonexistent"))
			.await;
		assert!(volume.is_none());
	}

	#[tokio::test]
	async fn test_same_volume_check() {
		let config = VolumeDetectionConfig::default();
		let events = create_test_events();
		let manager = VolumeManager::new(Uuid::new_v4(), config, events);

		// Both paths don't exist, so should return false
		let same = manager
			.same_volume(&PathBuf::from("/path1"), &PathBuf::from("/path2"))
			.await;
		assert!(!same);
	}
}
```

## src/volume/mod.rs

```rust
//! Volume management for Spacedrive Core v2
//!
//! This module provides functionality for detecting, monitoring, and managing storage volumes
//! across different platforms. It's designed to integrate with the copy system for optimal
//! file operation routing.

pub mod classification;
pub mod error;
pub mod manager;
pub mod os_detection;
pub mod speed;
pub mod types;

pub use error::VolumeError;
pub use manager::VolumeManager;
pub use types::{
	DiskType, FileSystem, MountType, Volume, VolumeDetectionConfig, VolumeEvent, VolumeFingerprint,
	VolumeInfo,
};

// Re-export platform-specific detection
pub use os_detection::detect_volumes;

/// Extension trait for Volume operations
pub trait VolumeExt {
	/// Checks if volume is mounted and accessible
	async fn is_available(&self) -> bool;

	/// Checks if volume has enough free space
	fn has_space(&self, required_bytes: u64) -> bool;

	/// Check if path is on this volume
	fn contains_path(&self, path: &std::path::Path) -> bool;
}

impl VolumeExt for Volume {
	async fn is_available(&self) -> bool {
		self.is_mounted && tokio::fs::metadata(&self.mount_point).await.is_ok()
	}

	fn has_space(&self, required_bytes: u64) -> bool {
		self.total_bytes_available >= required_bytes
	}

	fn contains_path(&self, path: &std::path::Path) -> bool {
		// Check primary mount point
		if path.starts_with(&self.mount_point) {
			return true;
		}

		// Check additional mount points (for APFS volumes)
		self.mount_points.iter().any(|mp| path.starts_with(mp))
	}
}

/// Utilities for volume operations
pub mod util {
	use super::*;
	use std::path::Path;

	/// Check if a path is on the specified volume
	pub fn is_path_on_volume(path: &Path, volume: &Volume) -> bool {
		volume.contains_path(&path.to_path_buf())
	}

	/// Calculate relative path from volume mount point
	pub fn relative_path_on_volume(path: &Path, volume: &Volume) -> Option<std::path::PathBuf> {
		// Try primary mount point first
		if let Ok(relative) = path.strip_prefix(&volume.mount_point) {
			return Some(relative.to_path_buf());
		}

		// Try additional mount points
		for mount_point in &volume.mount_points {
			if let Ok(relative) = path.strip_prefix(mount_point) {
				return Some(relative.to_path_buf());
			}
		}

		None
	}

	/// Find the volume that contains the given path
	pub fn find_volume_for_path<'a>(
		path: &Path,
		volumes: impl Iterator<Item = &'a Volume>,
	) -> Option<&'a Volume> {
		volumes
			.filter(|vol| vol.contains_path(&path.to_path_buf()))
			.max_by_key(|vol| vol.mount_point.as_os_str().len()) // Prefer most specific mount
	}
}
```

## src/volume/os_detection.rs

```rust
//! Platform-specific volume detection

use crate::volume::{
	classification::{get_classifier, VolumeDetectionInfo},
	error::{VolumeError, VolumeResult},
	types::{
		DiskType, FileSystem, MountType, SpacedriveVolumeId, Volume, VolumeDetectionConfig,
		VolumeFingerprint,
	},
};
use std::collections::HashMap;
use std::path::PathBuf;
use tokio::{fs, process::Command, task};
use tracing::{debug, instrument, warn};
use uuid::Uuid;

/// Filename for Spacedrive volume identifier files
const SPACEDRIVE_VOLUME_ID_FILE: &str = ".spacedrive-volume-id";

/// Classify a volume using the platform-specific classifier
fn classify_volume(
	mount_point: &PathBuf,
	file_system: &FileSystem,
	name: &str,
) -> crate::volume::types::VolumeType {
	let classifier = get_classifier();
	let detection_info = VolumeDetectionInfo {
		mount_point: mount_point.clone(),
		file_system: file_system.clone(),
		total_bytes_capacity: 0, // We don't have this info yet in some contexts
		is_removable: None,      // Would need additional detection
		is_network_drive: None,  // Would need additional detection
		device_model: None,      // Would need additional detection
	};

	classifier.classify(&detection_info)
}

/// Detect all volumes on the system
#[instrument(skip(config))]
pub async fn detect_volumes(
	device_id: uuid::Uuid,
	config: &VolumeDetectionConfig,
) -> VolumeResult<Vec<Volume>> {
	debug!("Starting volume detection for device {}", device_id);

	#[cfg(target_os = "macos")]
	let volumes = macos::detect_volumes(device_id, config).await?;

	#[cfg(target_os = "linux")]
	let volumes = linux::detect_volumes(device_id, config).await?;

	#[cfg(target_os = "windows")]
	let volumes = windows::detect_volumes(device_id, config).await?;

	#[cfg(not(any(target_os = "macos", target_os = "linux", target_os = "windows")))]
	let volumes = Vec::new();

	debug!(
		"Detected {} volumes for device {}",
		volumes.len(),
		device_id
	);
	Ok(volumes)
}

#[cfg(target_os = "macos")]
mod macos {
	use super::*;
	use std::process::Command;

	pub async fn detect_volumes(
		device_id: uuid::Uuid,
		config: &VolumeDetectionConfig,
	) -> VolumeResult<Vec<Volume>> {
		// Clone config for move into task
		let config = config.clone();

		// Run in blocking task since Command is sync
		task::spawn_blocking(move || {
			let mut volumes = Vec::new();

			// Use diskutil to get volume information
			let output = Command::new("diskutil")
				.args(["list", "-plist"])
				.output()
				.map_err(|e| VolumeError::platform(format!("Failed to run diskutil: {}", e)))?;

			if !output.status.success() {
				return Err(VolumeError::platform(format!(
					"diskutil failed with status: {}",
					output.status
				)));
			}

			// For now, use a simpler approach with df command to get mounted volumes
			let df_output = Command::new("df")
				.args(["-H"])
				.output()
				.map_err(|e| VolumeError::platform(format!("Failed to run df: {}", e)))?;

			if !df_output.status.success() {
				return Err(VolumeError::platform(
					"Failed to get volume information".to_string(),
				));
			}

			let df_stdout = String::from_utf8_lossy(&df_output.stdout);
			for line in df_stdout.lines().skip(1) {
				// Skip header
				let fields: Vec<&str> = line.split_whitespace().collect();
				if fields.len() >= 9 {
					let filesystem = fields[0];
					let total_str = fields[1];
					let used_str = fields[2];
					let available_str = fields[3];
					// Skip percentage field (fields[4])
					// Skip inode fields (fields[5], fields[6], fields[7])
					// Mount point might have spaces, so join remaining fields
					let mount_point = fields[8..].join(" ");

					// Skip certain filesystems
					if should_skip_filesystem(filesystem) {
						debug!("Skipping {} filesystem: {}", filesystem, mount_point);
						continue;
					}

					// Parse sizes (in bytes)
					let total_bytes = parse_size_string(total_str).unwrap_or(0);
					let available_bytes = parse_size_string(available_str).unwrap_or(0);

					let mount_path = PathBuf::from(&mount_point);
					let name = extract_volume_name(&mount_path);

					let mount_type = if mount_point.starts_with("/Volumes/") {
						MountType::External
					} else if mount_point.starts_with("/System/") {
						MountType::System
					} else if filesystem.contains("://") {
						MountType::Network
					} else {
						MountType::System
					};

					let disk_type = detect_disk_type(&mount_path)?;
					let file_system = detect_filesystem(&mount_path)?;

					let volume = Volume::new(
						device_id,
						name.clone(),
						mount_type,
						classify_volume(&mount_path, &file_system, &name),
						mount_path,
						vec![], // Additional mount points would need diskutil parsing
						disk_type,
						file_system.clone(),
						total_bytes,
						available_bytes,
						false,                        // Read-only detection would need additional checks
						Some(filesystem.to_string()), // Use filesystem as hardware ID
						VolumeFingerprint::new(&name, total_bytes, &file_system.to_string()), // Content-based fingerprint using intrinsic properties
					);
					volumes.push(volume);
				}
			}
			Ok(volumes)
		})
		.await
		.map_err(|e| VolumeError::platform(format!("Task join error: {}", e)))?
	}

	// Helper function to check if filesystem should be skipped
	fn should_skip_filesystem(filesystem: &str) -> bool {
		matches!(
			filesystem,
			"devfs" | "tmpfs" | "proc" | "sysfs" | "fdescfs" | "kernfs"
		)
	}

	// Helper function to extract volume name from mount path
	fn extract_volume_name(mount_path: &PathBuf) -> String {
		if let Some(name) = mount_path.file_name() {
			name.to_string_lossy().to_string()
		} else if mount_path.to_string_lossy() == "/" {
			"Macintosh HD".to_string()
		} else {
			mount_path.to_string_lossy().to_string()
		}
	}

	fn parse_df_line(
		line: &str,
		device_id: uuid::Uuid,
		config: &VolumeDetectionConfig,
	) -> VolumeResult<Option<Volume>> {
		let parts: Vec<&str> = line.split_whitespace().collect();
		if parts.len() < 9 {
			return Ok(None);
		}

		let filesystem = parts[0];

		// Handle special case where autofs filesystem has name and target split across columns
		if filesystem == "map" && parts.len() > 1 && parts[1].contains("auto") {
			debug!("Skipping autofs filesystem: map {}", parts[1]);
			return Ok(None);
		}

		let size_str = parts[1];
		let used_str = parts[2];
		let available_str = parts[3];
		let mount_point = parts[8];

		// Skip autofs and other special filesystems
		if filesystem.starts_with("map") || filesystem.contains("auto_") {
			debug!("Skipping autofs filesystem: {}", filesystem);
			return Ok(None);
		}

		// Skip system filesystems unless requested
		if !config.include_system && is_system_filesystem(filesystem) {
			return Ok(None);
		}

		// Skip virtual filesystems unless requested
		if !config.include_virtual && is_virtual_filesystem(filesystem) {
			return Ok(None);
		}

		let mount_path = PathBuf::from(mount_point);

		// Parse sizes (df output like "931Gi", "465Gi", etc.)
		let total_bytes = parse_size_string(size_str)?;
		let available_bytes = parse_size_string(available_str)?;

		let name = if mount_point == "/" {
			"Macintosh HD".to_string()
		} else {
			mount_path
				.file_name()
				.unwrap_or_default()
				.to_string_lossy()
				.to_string()
		};

		let mount_type = if mount_point == "/" {
			MountType::System
		} else if mount_point.starts_with("/Volumes/") {
			MountType::External
		} else if filesystem.starts_with("//") {
			MountType::Network
		} else {
			MountType::System
		};

		let disk_type = detect_disk_type(&mount_path)?;
		let file_system = detect_filesystem(&mount_path)?;

		let volume = Volume::new(
			device_id,
			name.clone(),
			mount_type,
			classify_volume(&mount_path, &file_system, &name),
			mount_path.clone(),
			vec![], // Additional mount points would need diskutil parsing
			disk_type,
			file_system.clone(),
			total_bytes,
			available_bytes,
			false,                        // Read-only detection would need additional checks
			Some(filesystem.to_string()), // Use filesystem as hardware ID
			VolumeFingerprint::new(&name, total_bytes, &file_system.to_string()), // Content-based fingerprint using intrinsic properties
		);

		Ok(Some(volume))
	}

	fn detect_disk_type(mount_point: &PathBuf) -> VolumeResult<DiskType> {
		// Try to detect SSD vs HDD using system_profiler or diskutil
		let output = Command::new("diskutil")
			.args(["info", mount_point.to_str().unwrap_or("/")])
			.output();

		match output {
			Ok(output) if output.status.success() => {
				let info = String::from_utf8_lossy(&output.stdout);
				if info.contains("Solid State") {
					Ok(DiskType::SSD)
				} else if info.contains("Rotational") {
					Ok(DiskType::HDD)
				} else {
					Ok(DiskType::Unknown)
				}
			}
			_ => Ok(DiskType::Unknown),
		}
	}

	fn detect_filesystem(mount_point: &PathBuf) -> VolumeResult<FileSystem> {
		let output = Command::new("diskutil")
			.args(["info", mount_point.to_str().unwrap_or("/")])
			.output();

		match output {
			Ok(output) if output.status.success() => {
				let info = String::from_utf8_lossy(&output.stdout);
				if info.contains("APFS") {
					Ok(FileSystem::APFS)
				} else if info.contains("HFS+") {
					Ok(FileSystem::Other("HFS+".to_string()))
				} else if info.contains("ExFAT") {
					Ok(FileSystem::ExFAT)
				} else if info.contains("FAT32") {
					Ok(FileSystem::FAT32)
				} else {
					Ok(FileSystem::Other("Unknown".to_string()))
				}
			}
			_ => Ok(FileSystem::Other("Unknown".to_string())),
		}
	}
}

#[cfg(target_os = "linux")]
mod linux {
	use super::*;
	use std::process::Command;

	pub async fn detect_volumes(
		device_id: uuid::Uuid,
		config: &VolumeDetectionConfig,
	) -> VolumeResult<Vec<Volume>> {
		task::spawn_blocking(move || {
			let mut volumes = Vec::new();

			// Use df to get mounted filesystems
			let output = Command::new("df")
				.args(["-h", "-T"]) // -T shows filesystem type
				.output()
				.map_err(|e| VolumeError::platform(format!("Failed to run df: {}", e)))?;

			if !output.status.success() {
				return Err(VolumeError::platform("df command failed"));
			}

			let df_text = String::from_utf8_lossy(&output.stdout);

			for line in df_text.lines().skip(1) {
				// Skip header
				if let Some(volume) = parse_df_line(line, device_id, &config)? {
					volumes.push(volume);
				}
			}

			Ok(volumes)
		})
		.await
		.map_err(|e| VolumeError::platform(format!("Task join error: {}", e)))?
	}

	fn parse_df_line(
		line: &str,
		device_id: uuid::Uuid,
		config: &VolumeDetectionConfig,
	) -> VolumeResult<Option<Volume>> {
		let parts: Vec<&str> = line.split_whitespace().collect();
		if parts.len() < 7 {
			return Ok(None);
		}

		let filesystem_device = parts[0];
		let filesystem_type = parts[1];
		let size_str = parts[2];
		let used_str = parts[3];
		let available_str = parts[4];
		let mount_point = parts[6];

		// Skip system filesystems unless requested
		if !config.include_system && is_system_filesystem(filesystem_device) {
			return Ok(None);
		}

		// Skip virtual filesystems unless requested
		if !config.include_virtual && is_virtual_filesystem(filesystem_type) {
			return Ok(None);
		}

		let mount_path = PathBuf::from(mount_point);

		let total_bytes = parse_size_string(size_str)?;
		let available_bytes = parse_size_string(available_str)?;

		let name = if mount_point == "/" {
			"Root".to_string()
		} else {
			mount_path
				.file_name()
				.unwrap_or_default()
				.to_string_lossy()
				.to_string()
		};

		let mount_type = determine_mount_type(mount_point, filesystem_device);
		let disk_type = detect_disk_type_linux(filesystem_device)?;
		let file_system = FileSystem::from_string(filesystem_type);

		let volume = Volume::new(
			device_id,
			name,
			mount_type,
			mount_path,
			vec![],
			disk_type,
			file_system,
			total_bytes,
			available_bytes,
			false, // Would need additional check for read-only
			Some(filesystem_device.to_string()),
		);

		Ok(Some(volume))
	}

	fn detect_disk_type_linux(device: &str) -> VolumeResult<DiskType> {
		// Try to detect using /sys/block/*/queue/rotational
		if let Some(device_name) = device.strip_prefix("/dev/") {
			let base_device = device_name.trim_end_matches(char::is_numeric);
			let rotational_path = format!("/sys/block/{}/queue/rotational", base_device);

			if let Ok(contents) = std::fs::read_to_string(rotational_path) {
				return match contents.trim() {
					"0" => Ok(DiskType::SSD),
					"1" => Ok(DiskType::HDD),
					_ => Ok(DiskType::Unknown),
				};
			}
		}

		Ok(DiskType::Unknown)
	}

	fn determine_mount_type(mount_point: &str, device: &str) -> MountType {
		if mount_point == "/" || mount_point.starts_with("/boot") {
			MountType::System
		} else if device.starts_with("//") || device.contains("nfs") {
			MountType::Network
		} else if mount_point.starts_with("/media/") || mount_point.starts_with("/mnt/") {
			MountType::External
		} else {
			MountType::System
		}
	}
}

#[cfg(target_os = "windows")]
mod windows {
	use super::*;
	use std::process::Command;

	pub async fn detect_volumes(
		device_id: uuid::Uuid,
		_config: &VolumeDetectionConfig,
	) -> VolumeResult<Vec<Volume>> {
		task::spawn_blocking(|| {
            // Use PowerShell to get volume information
            let output = Command::new("powershell")
                .args([
                    "-Command",
                    "Get-Volume | Select-Object DriveLetter,FileSystemLabel,Size,SizeRemaining,FileSystem | ConvertTo-Json"
                ])
                .output()
                .map_err(|e| VolumeError::platform(format!("Failed to run PowerShell: {}", e)))?;

            if !output.status.success() {
                return Err(VolumeError::platform("PowerShell command failed"));
            }

            // For now, return empty until we implement full Windows support
            warn!("Windows volume detection not fully implemented yet");
            Ok(Vec::new())
        })
        .await
        .map_err(|e| VolumeError::platform(format!("Task join error: {}", e)))?
	}
}

#[cfg(not(any(target_os = "macos", target_os = "linux", target_os = "windows")))]
mod unsupported {
	use super::*;

	pub async fn detect_volumes(
		device_id: uuid::Uuid,
		_config: &VolumeDetectionConfig,
	) -> VolumeResult<Vec<Volume>> {
		warn!("Volume detection not supported on this platform");
		Ok(Vec::new())
	}
}

// Common utility functions
fn is_system_filesystem(filesystem: &str) -> bool {
	matches!(
		filesystem,
		"/" | "/dev" | "/proc" | "/sys" | "/tmp" | "/var/tmp"
	)
}

fn is_virtual_filesystem(filesystem: &str) -> bool {
	let fs_lower = filesystem.to_lowercase();
	matches!(
		fs_lower.as_str(),
		"devfs" | "sysfs" | "proc" | "tmpfs" | "ramfs" | "devtmpfs" | "overlay" | "fuse"
	) || fs_lower.starts_with("map ")
		|| fs_lower.contains("auto_")
}

fn parse_size_string(size_str: &str) -> VolumeResult<u64> {
	if size_str == "-" {
		return Ok(0);
	}

	// Skip invalid size strings that don't look like numbers
	if size_str.is_empty() || size_str.chars().all(char::is_alphabetic) {
		return Ok(0);
	}

	let size_str = size_str.replace(",", ""); // Remove commas
	let (number_part, unit) = if let Some(pos) = size_str.find(char::is_alphabetic) {
		(&size_str[..pos], &size_str[pos..])
	} else {
		(size_str.as_str(), "")
	};

	let number: f64 = number_part
		.parse()
		.map_err(|_| VolumeError::InvalidData(format!("Invalid size: {}", size_str)))?;

	let multiplier = match unit.to_uppercase().as_str() {
		"" | "B" => 1,
		"K" | "KB" | "KI" => 1024,
		"M" | "MB" | "MI" => 1024 * 1024,
		"G" | "GB" | "GI" => 1024 * 1024 * 1024,
		"T" | "TB" | "TI" => 1024u64.pow(4),
		"P" | "PB" | "PI" => 1024u64.pow(5),
		_ => {
			warn!("Unknown size unit: {}", unit);
			1
		}
	};

	Ok((number * multiplier as f64) as u64)
}

#[cfg(test)]
mod tests {
	use super::*;

	#[test]
	fn test_parse_size_string() {
		assert_eq!(parse_size_string("1024").unwrap(), 1024);
		assert_eq!(parse_size_string("1K").unwrap(), 1024);
		assert_eq!(parse_size_string("1M").unwrap(), 1024 * 1024);
		assert_eq!(parse_size_string("1G").unwrap(), 1024 * 1024 * 1024);
		assert_eq!(
			parse_size_string("1.5G").unwrap(),
			(1.5 * 1024.0 * 1024.0 * 1024.0) as u64
		);
		assert_eq!(parse_size_string("-").unwrap(), 0);
	}

	#[test]
	fn test_filesystem_detection() {
		assert!(is_virtual_filesystem("tmpfs"));
		assert!(is_virtual_filesystem("proc"));
		assert!(!is_virtual_filesystem("ext4"));

		assert!(is_system_filesystem("/"));
		assert!(is_system_filesystem("/proc"));
		assert!(!is_system_filesystem("/home"));
	}
}
```

## src/services/device.rs

```rust
//! Device management service
//! 
//! Provides access to device connection information and networking functionality

use crate::{context::CoreContext, services::networking};
use anyhow::Result;
use std::sync::Arc;
use uuid::Uuid;

/// Service for managing device connections and information
pub struct DeviceService {
    context: Arc<CoreContext>,
}

impl DeviceService {
    /// Create a new device service
    pub fn new(context: Arc<CoreContext>) -> Self {
        Self { context }
    }

    /// Get list of connected device IDs
    pub async fn get_connected_devices(&self) -> Result<Vec<Uuid>> {
        if let Some(networking) = self.context.get_networking().await {
            let service = &*networking;
            let devices = service.get_connected_devices().await;
            Ok(devices.into_iter().map(|d| d.device_id).collect())
        } else {
            Ok(Vec::new())
        }
    }

    /// Get detailed information about connected devices
    pub async fn get_connected_devices_info(&self) -> Result<Vec<networking::DeviceInfo>> {
        if let Some(networking) = self.context.get_networking().await {
            let service = &*networking;
            let devices = service.get_connected_devices().await;
            Ok(devices)
        } else {
            Ok(Vec::new())
        }
    }
}```

## src/services/location_watcher/event_handler.rs

```rust
//! Event handling for file system changes

use crate::infrastructure::events::Event;
use notify::{Event as NotifyEvent, EventKind};
use serde::{Deserialize, Serialize};
use std::path::PathBuf;
use std::time::SystemTime;
use uuid::Uuid;

/// Wrapper for file system events with additional metadata
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct WatcherEvent {
    /// The file system event kind
    pub kind: WatcherEventKind,
    /// Paths affected by the event
    pub paths: Vec<PathBuf>,
    /// Timestamp when the event was received
    pub timestamp: SystemTime,
    /// Additional attributes from the file system
    pub attrs: Vec<String>,
}

/// Types of file system events we handle
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum WatcherEventKind {
    /// File or directory was created
    Create,
    /// File or directory was modified
    Modify,
    /// File or directory was removed
    Remove,
    /// File or directory was renamed/moved (from, to)
    Rename { from: PathBuf, to: PathBuf },
    /// Catch-all for other events
    Other(String),
}

impl WatcherEvent {
    /// Convert from notify's Event to our WatcherEvent
    pub fn from_notify_event(event: NotifyEvent) -> Self {
        let kind = match event.kind {
            EventKind::Create(_) => WatcherEventKind::Create,
            EventKind::Modify(_) => WatcherEventKind::Modify,
            EventKind::Remove(_) => WatcherEventKind::Remove,
            other => WatcherEventKind::Other(format!("{:?}", other)),
        };

        let attrs = vec![format!("{:?}", event.attrs)];

        Self {
            kind,
            paths: event.paths,
            timestamp: SystemTime::now(),
            attrs,
        }
    }

    /// Convert to core Event for the event bus
    pub fn to_core_event(&self, library_id: Uuid, entry_id: Option<Uuid>) -> Option<Event> {
        match &self.kind {
            WatcherEventKind::Create => {
                entry_id.map(|id| Event::EntryCreated { library_id, entry_id: id })
            }
            WatcherEventKind::Modify => {
                entry_id.map(|id| Event::EntryModified { library_id, entry_id: id })
            }
            WatcherEventKind::Remove => {
                entry_id.map(|id| Event::EntryDeleted { library_id, entry_id: id })
            }
            WatcherEventKind::Rename { from, to } => {
                entry_id.map(|id| Event::EntryMoved {
                    library_id,
                    entry_id: id,
                    old_path: from.to_string_lossy().to_string(),
                    new_path: to.to_string_lossy().to_string(),
                })
            }
            WatcherEventKind::Other(_) => None,
        }
    }

    /// Check if this event should be processed (filter out temporary files, etc.)
    pub fn should_process(&self) -> bool {
        for path in &self.paths {
            let path_str = path.to_string_lossy();
            
            // Skip temporary files
            if path_str.contains(".tmp") 
                || path_str.contains(".temp")
                || path_str.contains("~")
                || path_str.ends_with(".swp")
                || path_str.contains(".DS_Store")
                || path_str.contains("Thumbs.db") {
                return false;
            }
            
            // Skip hidden files starting with dot (except .gitignore, etc.)
            if let Some(file_name) = path.file_name() {
                let name = file_name.to_string_lossy();
                if name.starts_with('.') && !is_important_dotfile(&name) {
                    return false;
                }
            }
        }
        
        true
    }

    /// Get the primary path for this event
    pub fn primary_path(&self) -> Option<&PathBuf> {
        self.paths.first()
    }
}

/// Check if a dotfile is important enough to track
fn is_important_dotfile(name: &str) -> bool {
    matches!(name, 
        ".gitignore" | ".gitkeep" | ".gitattributes" |
        ".editorconfig" | ".env" | ".env.local" |
        ".nvmrc" | ".node-version" | ".python-version" |
        ".dockerignore" | ".eslintrc" | ".prettierrc"
    )
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::path::Path;

    #[test]
    fn test_should_process_filtering() {
        // Should process normal files
        let event = WatcherEvent {
            kind: WatcherEventKind::Create,
            paths: vec![PathBuf::from("/test/file.txt")],
            timestamp: SystemTime::now(),
            attrs: vec![],
        };
        assert!(event.should_process());

        // Should skip temporary files
        let event = WatcherEvent {
            kind: WatcherEventKind::Create,
            paths: vec![PathBuf::from("/test/file.tmp")],
            timestamp: SystemTime::now(),
            attrs: vec![],
        };
        assert!(!event.should_process());

        // Should skip .DS_Store
        let event = WatcherEvent {
            kind: WatcherEventKind::Create,
            paths: vec![PathBuf::from("/test/.DS_Store")],
            timestamp: SystemTime::now(),
            attrs: vec![],
        };
        assert!(!event.should_process());

        // Should process important dotfiles
        let event = WatcherEvent {
            kind: WatcherEventKind::Create,
            paths: vec![PathBuf::from("/test/.gitignore")],
            timestamp: SystemTime::now(),
            attrs: vec![],
        };
        assert!(event.should_process());
    }

    #[test]
    fn test_primary_path() {
        let event = WatcherEvent {
            kind: WatcherEventKind::Create,
            paths: vec![
                PathBuf::from("/test/file1.txt"),
                PathBuf::from("/test/file2.txt"),
            ],
            timestamp: SystemTime::now(),
            attrs: vec![],
        };
        
        assert_eq!(event.primary_path(), Some(&PathBuf::from("/test/file1.txt")));
    }
}```

## src/services/location_watcher/platform/macos.rs

```rust
//! macOS-specific file system event handling
//!
//! On macOS, we use the FSEvents backend of notify-rs and Rename events are complex.
//! There are just ModifyKind::Name(RenameMode::Any) events and nothing else.
//! This means we have to link the old path with the new path to know which file was renamed.
//!
//! Renames aren't always file name changes - the path can be modified when files are moved.
//! When a file is moved inside the same location, we receive 2 events: old and new path.
//! When moved to another location, we only receive the old path event (handle as deletion).
//! When moved from elsewhere to our location, we receive new path rename event (handle as creation).

use super::EventHandler;
use crate::infrastructure::events::Event;
use crate::services::location_watcher::event_handler::WatcherEventKind;
use crate::services::location_watcher::{WatchedLocation, WatcherEvent};
use anyhow::Result;
use notify::{
	event::{CreateKind, DataChange, MetadataKind, ModifyKind, RenameMode},
	EventKind,
};
use std::collections::HashMap;
use std::path::PathBuf;
use std::sync::Arc;
use std::time::{Duration, Instant, SystemTime};
use tokio::sync::RwLock;
use tracing::{debug, error, trace, warn};
use uuid::Uuid;

/// Simplified inode type for macOS
type INode = u64;

/// Tuple of instant and path for rename tracking
type InstantAndPath = (Instant, PathBuf);

/// Constants for timing
const HUNDRED_MILLIS: Duration = Duration::from_millis(100);
const ONE_SECOND: Duration = Duration::from_secs(1);

/// macOS-specific event handler that handles FSEvents complexities
pub struct MacOSHandler {
	/// Last time we performed eviction checks
	last_events_eviction_check: Arc<RwLock<Instant>>,

	/// Latest created directory to handle Finder's duplicate events
	latest_created_dir: Arc<RwLock<Option<PathBuf>>>,

	/// Old paths map for rename tracking (inode -> (instant, path))
	old_paths_map: Arc<RwLock<HashMap<INode, InstantAndPath>>>,

	/// New paths map for rename tracking (inode -> (instant, path))
	new_paths_map: Arc<RwLock<HashMap<INode, InstantAndPath>>>,

	/// Files pending update after create/modify events
	files_to_update: Arc<RwLock<HashMap<PathBuf, Instant>>>,

	/// Files that need updating after multiple rapid changes
	reincident_to_update_files: Arc<RwLock<HashMap<PathBuf, Instant>>>,

	/// Directories that need size recalculation
	to_recalculate_size: Arc<RwLock<HashMap<PathBuf, Instant>>>,
}

impl MacOSHandler {
	pub fn new() -> Self {
		Self {
			last_events_eviction_check: Arc::new(RwLock::new(Instant::now())),
			latest_created_dir: Arc::new(RwLock::new(None)),
			old_paths_map: Arc::new(RwLock::new(HashMap::new())),
			new_paths_map: Arc::new(RwLock::new(HashMap::new())),
			files_to_update: Arc::new(RwLock::new(HashMap::new())),
			reincident_to_update_files: Arc::new(RwLock::new(HashMap::new())),
			to_recalculate_size: Arc::new(RwLock::new(HashMap::new())),
		}
	}

	/// Extract inode from file metadata (simplified for now)
	async fn get_inode_from_path(&self, path: &PathBuf) -> Option<INode> {
		match tokio::fs::metadata(path).await {
			Ok(metadata) => {
				// On Unix systems, we can extract the inode
				#[cfg(unix)]
				{
					use std::os::unix::fs::MetadataExt;
					Some(metadata.ino())
				}
				#[cfg(not(unix))]
				{
					// Fallback: use a hash of the path
					use std::collections::hash_map::DefaultHasher;
					use std::hash::{Hash, Hasher};
					let mut hasher = DefaultHasher::new();
					path.hash(&mut hasher);
					Some(hasher.finish())
				}
			}
			Err(_) => None,
		}
	}

	/// Convert notify event to our internal event representation
	fn convert_notify_event(&self, notify_event: notify::Event) -> WatcherEvent {
		let kind = match notify_event.kind {
			EventKind::Create(CreateKind::Folder) => WatcherEventKind::Create,
			EventKind::Create(CreateKind::File) => WatcherEventKind::Create,
			EventKind::Modify(ModifyKind::Data(DataChange::Content)) => WatcherEventKind::Modify,
			EventKind::Modify(ModifyKind::Metadata(
				MetadataKind::WriteTime | MetadataKind::Extended,
			)) => WatcherEventKind::Modify,
			EventKind::Modify(ModifyKind::Name(RenameMode::Any)) => {
				WatcherEventKind::Other("rename".to_string())
			}
			EventKind::Remove(_) => WatcherEventKind::Remove,
			other => WatcherEventKind::Other(format!("{:?}", other)),
		};

		WatcherEvent {
			kind,
			paths: notify_event.paths,
			timestamp: std::time::SystemTime::now(),
			attrs: vec![format!("{:?}", notify_event.attrs)],
		}
	}

	/// Handle a single rename event (the core complexity of macOS watching)
	async fn handle_single_rename_event(
		&self,
		path: PathBuf,
		watched_locations: &Arc<RwLock<HashMap<Uuid, WatchedLocation>>>,
	) -> Result<Vec<Event>> {
		let mut events = Vec::new();

		match tokio::fs::metadata(&path).await {
			Ok(metadata) => {
				// File exists - this could be the "new" part of a rename or a creation
				trace!("Rename event: path exists {}", path.display());

				if let Some(inode) = self.get_inode_from_path(&path).await {
					// Check if this matches an old path we're tracking
					let mut old_paths = self.old_paths_map.write().await;
					if let Some((_, old_path)) = old_paths.remove(&inode) {
						// We found a match! This is a real rename operation
						trace!(
							"Detected rename: {} -> {}",
							old_path.display(),
							path.display()
						);

						// Find the matching location and generate rename event
						let locations = watched_locations.read().await;
						for location in locations.values() {
							if path.starts_with(&location.path) {
								let entry_id = Uuid::new_v4(); // TODO: Look up actual entry
								events.push(Event::EntryMoved {
									library_id: location.library_id,
									entry_id,
									old_path: old_path.to_string_lossy().to_string(),
									new_path: path.to_string_lossy().to_string(),
								});
								break;
							}
						}
					} else {
						// No matching old path - store as new path for potential future match
						trace!("Storing new path for rename: {}", path.display());
						let mut new_paths = self.new_paths_map.write().await;
						new_paths.insert(inode, (Instant::now(), path));
					}
				}
			}
			Err(e) if e.kind() == std::io::ErrorKind::NotFound => {
				// File doesn't exist - this could be the "old" part of a rename or a deletion
				trace!("Rename event: path doesn't exist {}", path.display());

				if let Some(inode) = self.get_inode_from_path(&path).await {
					// Check if this matches a new path we're tracking
					let mut new_paths = self.new_paths_map.write().await;
					if let Some((_, new_path)) = new_paths.remove(&inode) {
						// We found a match! This is a real rename operation
						trace!(
							"Detected rename: {} -> {}",
							path.display(),
							new_path.display()
						);

						// Find the matching location and generate rename event
						let locations = watched_locations.read().await;
						for location in locations.values() {
							if new_path.starts_with(&location.path) {
								let entry_id = Uuid::new_v4(); // TODO: Look up actual entry
								events.push(Event::EntryMoved {
									library_id: location.library_id,
									entry_id,
									old_path: path.to_string_lossy().to_string(),
									new_path: new_path.to_string_lossy().to_string(),
								});
								break;
							}
						}
					} else {
						// No matching new path - store as old path for potential future match
						trace!("Storing old path for rename: {}", path.display());
						let mut old_paths = self.old_paths_map.write().await;
						old_paths.insert(inode, (Instant::now(), path));
					}
				}
			}
			Err(e) => {
				error!(
					"Error accessing path during rename: {}: {}",
					path.display(),
					e
				);
			}
		}

		Ok(events)
	}

	/// Handle eviction of files that need updating
	async fn handle_to_update_eviction(
		&self,
		watched_locations: &Arc<RwLock<HashMap<Uuid, WatchedLocation>>>,
	) -> Result<Vec<Event>> {
		let mut events = Vec::new();
		let mut files_to_update = self.files_to_update.write().await;
		let mut reincident_files = self.reincident_to_update_files.write().await;
		let mut to_recalc_size = self.to_recalculate_size.write().await;

		// Process files that have been waiting for updates
		let mut files_to_keep = HashMap::new();
		for (path, created_at) in files_to_update.drain() {
			if created_at.elapsed() < HUNDRED_MILLIS * 5 {
				files_to_keep.insert(path, created_at);
			} else {
				// File has been stable long enough, generate update event
				if let Some(parent) = path.parent() {
					to_recalc_size.insert(parent.to_path_buf(), Instant::now());
				}

				reincident_files.remove(&path);

				// Generate modify event
				let locations = watched_locations.read().await;
				for location in locations.values() {
					if path.starts_with(&location.path) {
						let entry_id = Uuid::new_v4(); // TODO: Look up actual entry
						events.push(Event::EntryModified {
							library_id: location.library_id,
							entry_id,
						});
						break;
					}
				}
			}
		}
		*files_to_update = files_to_keep;

		// Process reincident files with longer timeout
		let mut reincident_to_keep = HashMap::new();
		for (path, created_at) in reincident_files.drain() {
			if created_at.elapsed() < ONE_SECOND * 10 {
				reincident_to_keep.insert(path, created_at);
			} else {
				if let Some(parent) = path.parent() {
					to_recalc_size.insert(parent.to_path_buf(), Instant::now());
				}

				files_to_update.remove(&path);

				// Generate modify event
				let locations = watched_locations.read().await;
				for location in locations.values() {
					if path.starts_with(&location.path) {
						let entry_id = Uuid::new_v4(); // TODO: Look up actual entry
						events.push(Event::EntryModified {
							library_id: location.library_id,
							entry_id,
						});
						break;
					}
				}
			}
		}
		*reincident_files = reincident_to_keep;

		Ok(events)
	}

	/// Handle creation events from rename eviction
	async fn handle_rename_create_eviction(
		&self,
		watched_locations: &Arc<RwLock<HashMap<Uuid, WatchedLocation>>>,
	) -> Result<Vec<Event>> {
		let mut events = Vec::new();
		let mut new_paths = self.new_paths_map.write().await;
		let files_to_update = self.files_to_update.read().await;

		let mut paths_to_keep = HashMap::new();
		for (inode, (instant, path)) in new_paths.drain() {
			if instant.elapsed() > HUNDRED_MILLIS {
				if !files_to_update.contains_key(&path) {
					// Path has timed out and isn't already being updated
					match tokio::fs::metadata(&path).await {
						Ok(metadata) => {
							let locations = watched_locations.read().await;
							for location in locations.values() {
								if path.starts_with(&location.path) {
									let entry_id = Uuid::new_v4(); // TODO: Look up or create actual entry
									events.push(Event::EntryCreated {
										library_id: location.library_id,
										entry_id,
									});

									if let Some(parent) = path.parent() {
										let mut to_recalc = self.to_recalculate_size.write().await;
										to_recalc.insert(parent.to_path_buf(), Instant::now());
									}
									break;
								}
							}
						}
						Err(_) => {
							// File no longer exists, ignore
						}
					}
				}
			} else {
				paths_to_keep.insert(inode, (instant, path));
			}
		}
		*new_paths = paths_to_keep;

		Ok(events)
	}

	/// Handle removal events from rename eviction
	async fn handle_rename_remove_eviction(
		&self,
		watched_locations: &Arc<RwLock<HashMap<Uuid, WatchedLocation>>>,
	) -> Result<Vec<Event>> {
		let mut events = Vec::new();
		let mut old_paths = self.old_paths_map.write().await;

		let mut paths_to_keep = HashMap::new();
		for (inode, (instant, path)) in old_paths.drain() {
			if instant.elapsed() > HUNDRED_MILLIS {
				// Path has timed out, treat as removal
				let locations = watched_locations.read().await;
				for location in locations.values() {
					if path.starts_with(&location.path) {
						let entry_id = Uuid::new_v4(); // TODO: Look up actual entry
						events.push(Event::EntryDeleted {
							library_id: location.library_id,
							entry_id,
						});

						if let Some(parent) = path.parent() {
							let mut to_recalc = self.to_recalculate_size.write().await;
							to_recalc.insert(parent.to_path_buf(), Instant::now());
						}
						break;
					}
				}
			} else {
				paths_to_keep.insert(inode, (instant, path));
			}
		}
		*old_paths = paths_to_keep;

		Ok(events)
	}
}

#[async_trait::async_trait]
impl EventHandler for MacOSHandler {
	async fn process_event(
		&self,
		event: WatcherEvent,
		watched_locations: &Arc<RwLock<HashMap<Uuid, WatchedLocation>>>,
	) -> Result<Vec<Event>> {
		if !event.should_process() {
			return Ok(vec![]);
		}

		let mut events = Vec::new();
		let path = match event.paths.first() {
			Some(path) => path.clone(),
			None => return Ok(vec![]),
		};

		// Handle different event types like the original implementation
		match &event.kind {
			WatcherEventKind::Create => {
				// Check for duplicate directory creation events (macOS Finder issue)
				if tokio::fs::metadata(&path)
					.await
					.map_or(false, |m| m.is_dir())
				{
					let mut latest_created = self.latest_created_dir.write().await;
					if let Some(ref latest) = *latest_created {
						if path == *latest {
							// Duplicate event, ignore
							return Ok(vec![]);
						}
					}
					*latest_created = Some(path.clone());
				}

				// Generate creation event
				let locations = watched_locations.read().await;
				for location in locations.values() {
					if location.enabled && path.starts_with(&location.path) {
						let entry_id = Uuid::new_v4(); // TODO: Look up or create actual entry
						events.push(Event::EntryCreated {
							library_id: location.library_id,
							entry_id,
						});

						// Schedule parent for size recalculation
						if let Some(parent) = path.parent() {
							let mut to_recalc = self.to_recalculate_size.write().await;
							to_recalc.insert(parent.to_path_buf(), Instant::now());
						}
						break;
					}
				}
			}

			WatcherEventKind::Modify => {
				// Mark file for future update (with debouncing)
				let mut files_to_update = self.files_to_update.write().await;
				let mut reincident_files = self.reincident_to_update_files.write().await;

				if files_to_update.contains_key(&path) {
					if let Some(old_instant) = files_to_update.insert(path.clone(), Instant::now())
					{
						reincident_files.entry(path).or_insert(old_instant);
					}
				} else {
					files_to_update.insert(path, Instant::now());
				}
			}

			WatcherEventKind::Remove => {
				// Generate removal event and schedule parent for size recalculation
				let locations = watched_locations.read().await;
				for location in locations.values() {
					if location.enabled && path.starts_with(&location.path) {
						let entry_id = Uuid::new_v4(); // TODO: Look up actual entry
						events.push(Event::EntryDeleted {
							library_id: location.library_id,
							entry_id,
						});

						if let Some(parent) = path.parent() {
							let mut to_recalc = self.to_recalculate_size.write().await;
							to_recalc.insert(parent.to_path_buf(), Instant::now());
						}
						break;
					}
				}
			}

			WatcherEventKind::Other(event_type) if event_type == "rename" => {
				// Handle macOS rename events (the complex part)
				let rename_events = self
					.handle_single_rename_event(path, watched_locations)
					.await?;
				events.extend(rename_events);
			}

			_ => {
				trace!("Unhandled macOS event type: {:?}", event.kind);
			}
		}

		Ok(events)
	}

	async fn tick(&self) -> Result<()> {
		let mut last_check = self.last_events_eviction_check.write().await;

		if last_check.elapsed() > HUNDRED_MILLIS {
			*last_check = Instant::now();
		}

		Ok(())
	}
}

/// Additional methods for macOS handler beyond the EventHandler trait
impl MacOSHandler {
	/// Tick with access to watched locations for event processing
	pub async fn tick_with_locations(
		&self,
		watched_locations: &Arc<RwLock<HashMap<Uuid, WatchedLocation>>>,
	) -> Result<Vec<Event>> {
		let mut all_events = Vec::new();
		let mut last_check = self.last_events_eviction_check.write().await;

		if last_check.elapsed() > HUNDRED_MILLIS {
			// Handle file update evictions
			let update_events = self.handle_to_update_eviction(watched_locations).await?;
			all_events.extend(update_events);

			// Handle rename create evictions
			let create_events = self
				.handle_rename_create_eviction(watched_locations)
				.await?;
			all_events.extend(create_events);

			// Handle rename remove evictions
			let remove_events = self
				.handle_rename_remove_eviction(watched_locations)
				.await?;
			all_events.extend(remove_events);

			// Handle size recalculation
			// TODO: Implement directory size recalculation like original

			*last_check = Instant::now();
		}

		Ok(all_events)
	}
}

// #[cfg(test)]
// mod tests {
//     use super::*;
//     use std::collections::HashMap;

//     #[tokio::test]
//     async fn test_macos_handler_creation() {
//         let handler = MacOSHandler::new();
//         assert_eq!(handler.debounce_duration, Duration::from_millis(100));
//     }

//     #[tokio::test]
//     async fn test_debounce_logic() {
//         let handler = MacOSHandler::new();
//         let path = PathBuf::from("/test/file.txt");

//         // First event should not be debounced
//         assert!(!handler.should_debounce(&path, "create").await);

//         // Second identical event should be debounced
//         assert!(handler.should_debounce(&path, "create").await);

//         // Different event type should not be debounced
//         assert!(!handler.should_debounce(&path, "modify").await);
//     }

//     #[tokio::test]
//     async fn test_tick_cleanup() {
//         let handler = MacOSHandler::new();

//         // Add some test data
//         {
//             let mut rename_map = handler.rename_map.write().await;
//             rename_map.insert(123, (PathBuf::from("/old"), SystemTime::now() - Duration::from_secs(10)));
//             rename_map.insert(456, (PathBuf::from("/recent"), SystemTime::now()));
//         }

//         // Run tick to clean up old entries
//         handler.tick().await.unwrap();

//         // Check that old entry was removed
//         let rename_map = handler.rename_map.read().await;
//         assert_eq!(rename_map.len(), 1);
//         assert!(rename_map.contains_key(&456));
//     }
// }
```

## src/services/location_watcher/platform/mod.rs

```rust
//! Platform-specific event handling

use crate::infrastructure::events::Event;
use crate::services::location_watcher::{WatchedLocation, WatcherEvent};
use anyhow::Result;
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;
use uuid::Uuid;

#[cfg(target_os = "linux")]
mod linux;
#[cfg(target_os = "macos")]
mod macos;
#[cfg(target_os = "windows")]
mod windows;

/// Platform-specific event handler
pub struct PlatformHandler {
    #[cfg(target_os = "linux")]
    pub inner: linux::LinuxHandler,
    #[cfg(target_os = "macos")]
    pub inner: macos::MacOSHandler,
    #[cfg(target_os = "windows")]
    pub inner: windows::WindowsHandler,
    #[cfg(not(any(target_os = "linux", target_os = "macos", target_os = "windows")))]
    pub inner: DefaultHandler,
}

impl PlatformHandler {
    /// Create a new platform handler
    pub fn new() -> Self {
        Self {
            #[cfg(target_os = "linux")]
            inner: linux::LinuxHandler::new(),
            #[cfg(target_os = "macos")]
            inner: macos::MacOSHandler::new(),
            #[cfg(target_os = "windows")]
            inner: windows::WindowsHandler::new(),
            #[cfg(not(any(target_os = "linux", target_os = "macos", target_os = "windows")))]
            inner: DefaultHandler::new(),
        }
    }

    /// Process a file system event
    pub async fn process_event(
        &self,
        event: WatcherEvent,
        watched_locations: &Arc<RwLock<HashMap<Uuid, WatchedLocation>>>,
    ) -> Result<Vec<Event>> {
        self.inner.process_event(event, watched_locations).await
    }

    /// Periodic tick for cleanup and debouncing
    pub async fn tick(&self) -> Result<()> {
        self.inner.tick().await
    }
}

/// Trait for platform-specific handlers
#[async_trait::async_trait]
pub trait EventHandler: Send + Sync {
    /// Process a file system event and return core events
    async fn process_event(
        &self,
        event: WatcherEvent,
        watched_locations: &Arc<RwLock<HashMap<Uuid, WatchedLocation>>>,
    ) -> Result<Vec<Event>>;

    /// Periodic cleanup and processing
    async fn tick(&self) -> Result<()>;
}

/// Default handler for unsupported platforms
#[cfg(not(any(target_os = "linux", target_os = "macos", target_os = "windows")))]
pub struct DefaultHandler;

#[cfg(not(any(target_os = "linux", target_os = "macos", target_os = "windows")))]
impl DefaultHandler {
    pub fn new() -> Self {
        Self
    }
}

#[cfg(not(any(target_os = "linux", target_os = "macos", target_os = "windows")))]
#[async_trait::async_trait]
impl EventHandler for DefaultHandler {
    async fn process_event(
        &self,
        event: WatcherEvent,
        watched_locations: &Arc<RwLock<HashMap<Uuid, WatchedLocation>>>,
    ) -> Result<Vec<Event>> {
        // Basic event processing without platform-specific optimizations
        if !event.should_process() {
            return Ok(vec![]);
        }

        let locations = watched_locations.read().await;
        let mut events = Vec::new();

        for location in locations.values() {
            if !location.enabled {
                continue;
            }

            for path in &event.paths {
                if path.starts_with(&location.path) {
                    // Generate a placeholder entry ID for now
                    // In a real implementation, this would look up or create an entry
                    let entry_id = Uuid::new_v4();

                    if let Some(core_event) = event.to_core_event(location.library_id, Some(entry_id)) {
                        events.push(core_event);
                    }
                    break;
                }
            }
        }

        Ok(events)
    }

    async fn tick(&self) -> Result<()> {
        // Nothing to do for default handler
        Ok(())
    }
}```

## src/services/location_watcher/platform/windows.rs

```rust
//! Windows-specific file system event handling

use super::EventHandler;
use crate::infrastructure::events::Event;
use crate::services::location_watcher::{WatchedLocation, WatcherEvent};
use crate::services::location_watcher::event_handler::WatcherEventKind;
use anyhow::Result;
use std::collections::HashMap;
use std::path::PathBuf;
use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::sync::RwLock;
use tracing::{debug, trace, warn};
use uuid::Uuid;

/// Windows-specific event handler that handles Windows filesystem quirks
pub struct WindowsHandler {
    /// Recently processed events for debouncing
    recent_events: Arc<RwLock<HashMap<PathBuf, Instant>>>,
    /// Files pending deletion (Windows has delayed deletion)
    pending_deletions: Arc<RwLock<HashMap<PathBuf, Instant>>>,
    /// Debounce duration
    debounce_duration: Duration,
}

impl WindowsHandler {
    pub fn new() -> Self {
        Self {
            recent_events: Arc::new(RwLock::new(HashMap::new())),
            pending_deletions: Arc::new(RwLock::new(HashMap::new())),
            debounce_duration: Duration::from_millis(200), // Windows needs more debouncing
        }
    }

    /// Check if event should be debounced
    async fn should_debounce(&self, path: &PathBuf) -> bool {
        let mut recent = self.recent_events.write().await;
        let now = Instant::now();

        if let Some(&last_seen) = recent.get(path) {
            if now.duration_since(last_seen) < self.debounce_duration {
                return true;
            }
        }

        recent.insert(path.clone(), now);

        // Cleanup old entries
        recent.retain(|_, &mut last_seen| {
            now.duration_since(last_seen) < Duration::from_secs(2)
        });

        false
    }

    /// Handle Windows-specific delayed deletion detection
    async fn handle_delayed_deletion(&self, path: &PathBuf) -> bool {
        // On Windows, files can appear to be deleted but still be accessible
        // for a short time due to file locking, antivirus, etc.
        match tokio::fs::metadata(path).await {
            Ok(_) => {
                // File still exists, might be a false deletion event
                let mut pending = self.pending_deletions.write().await;
                pending.insert(path.clone(), Instant::now());
                false // Don't process deletion yet
            }
            Err(_) => {
                // File actually doesn't exist
                let mut pending = self.pending_deletions.write().await;
                pending.remove(path);
                true // Process deletion
            }
        }
    }

    /// Process pending deletions that have timed out
    async fn process_pending_deletions(
        &self,
        watched_locations: &Arc<RwLock<HashMap<Uuid, WatchedLocation>>>,
    ) -> Result<Vec<Event>> {
        let mut events = Vec::new();
        let mut pending = self.pending_deletions.write().await;
        let now = Instant::now();

        let mut to_remove = Vec::new();
        for (path, timestamp) in pending.iter() {
            if now.duration_since(*timestamp) > Duration::from_millis(500) {
                // Check if file still doesn't exist
                if tokio::fs::metadata(path).await.is_err() {
                    // File is definitely gone, emit deletion event
                    let locations = watched_locations.read().await;
                    for location in locations.values() {
                        if location.enabled && path.starts_with(&location.path) {
                            let entry_id = Uuid::new_v4(); // TODO: Look up actual entry
                            events.push(Event::EntryDeleted {
                                library_id: location.library_id,
                                entry_id,
                            });
                            break;
                        }
                    }
                }
                to_remove.push(path.clone());
            }
        }

        for path in to_remove {
            pending.remove(&path);
        }

        Ok(events)
    }

    /// Handle Windows-specific temporary file patterns
    fn is_windows_temp_file(&self, path: &PathBuf) -> bool {
        let path_str = path.to_string_lossy().to_lowercase();

        // Windows common temporary file patterns
        path_str.contains("~$") || // Office temp files
        path_str.ends_with(".tmp") ||
        path_str.ends_with(".temp") ||
        path_str.contains(".crdownload") || // Chrome downloads
        path_str.contains(".part") || // Firefox downloads
        path_str.contains("thumbs.db") ||
        path_str.contains("desktop.ini") ||
        path_str.contains("$recycle.bin")
    }
}

#[async_trait::async_trait]
impl EventHandler for WindowsHandler {
    async fn process_event(
        &self,
        event: WatcherEvent,
        watched_locations: &Arc<RwLock<HashMap<Uuid, WatchedLocation>>>,
    ) -> Result<Vec<Event>> {
        if !event.should_process() {
            return Ok(vec![]);
        }

        let mut events = Vec::new();

        for path in &event.paths {
            // Skip Windows-specific temporary files
            if self.is_windows_temp_file(path) {
                trace!("Skipping Windows temp file: {}", path.display());
                continue;
            }

            // Check for debouncing
            if self.should_debounce(path).await {
                debug!("Debounced event for: {}", path.display());
                continue;
            }

            let locations = watched_locations.read().await;
            for location in locations.values() {
                if !location.enabled || !path.starts_with(&location.path) {
                    continue;
                }

                let entry_id = Uuid::new_v4(); // TODO: Look up or create actual entry

                match &event.kind {
                    WatcherEventKind::Create => {
                        events.push(Event::EntryCreated {
                            library_id: location.library_id,
                            entry_id,
                        });
                        trace!("Windows: Created {}", path.display());
                    }
                    WatcherEventKind::Modify => {
                        events.push(Event::EntryModified {
                            library_id: location.library_id,
                            entry_id,
                        });
                        trace!("Windows: Modified {}", path.display());
                    }
                    WatcherEventKind::Remove => {
                        // Handle Windows delayed deletion
                        if self.handle_delayed_deletion(path).await {
                            events.push(Event::EntryDeleted {
                                library_id: location.library_id,
                                entry_id,
                            });
                            trace!("Windows: Removed {}", path.display());
                        } else {
                            trace!("Windows: Pending deletion for {}", path.display());
                        }
                    }
                    WatcherEventKind::Rename { from, to } => {
                        events.push(Event::EntryMoved {
                            library_id: location.library_id,
                            entry_id,
                            old_path: from.to_string_lossy().to_string(),
                            new_path: to.to_string_lossy().to_string(),
                        });
                        trace!("Windows: Renamed {} -> {}", from.display(), to.display());
                    }
                    _ => {
                        trace!("Windows: Unhandled event type for {}", path.display());
                    }
                }
                break;
            }
        }

        Ok(events)
    }

    async fn tick(&self) -> Result<()> {
        // Clean up recent events
        let mut recent = self.recent_events.write().await;
        let now = Instant::now();
        recent.retain(|_, &mut last_seen| {
            now.duration_since(last_seen) < Duration::from_secs(5)
        });

        Ok(())
    }
}

/// Additional method for Windows handler to process pending deletions
impl WindowsHandler {
    pub async fn tick_with_locations(
        &self,
        watched_locations: &Arc<RwLock<HashMap<Uuid, WatchedLocation>>>,
    ) -> Result<Vec<Event>> {
        // Process any pending deletions
        self.process_pending_deletions(watched_locations).await
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::time::SystemTime;

    #[tokio::test]
    async fn test_windows_handler_creation() {
        let handler = WindowsHandler::new();
        assert_eq!(handler.debounce_duration, Duration::from_millis(200));
    }

    #[tokio::test]
    async fn test_windows_temp_file_detection() {
        let handler = WindowsHandler::new();

        // Should detect Windows temp files
        assert!(handler.is_windows_temp_file(&PathBuf::from(r"C:\temp\~$document.docx")));
        assert!(handler.is_windows_temp_file(&PathBuf::from(r"C:\temp\file.tmp")));
        assert!(handler.is_windows_temp_file(&PathBuf::from(r"C:\temp\Thumbs.db")));
        assert!(handler.is_windows_temp_file(&PathBuf::from(r"C:\temp\desktop.ini")));

        // Should not detect normal files
        assert!(!handler.is_windows_temp_file(&PathBuf::from(r"C:\temp\document.docx")));
        assert!(!handler.is_windows_temp_file(&PathBuf::from(r"C:\temp\image.jpg")));
    }

    #[tokio::test]
    async fn test_debounce_logic() {
        let handler = WindowsHandler::new();
        let path = PathBuf::from(r"C:\test\file.txt");

        // First event should not be debounced
        assert!(!handler.should_debounce(&path).await);

        // Second immediate event should be debounced
        assert!(handler.should_debounce(&path).await);
    }
}```

## src/services/location_watcher/platform/linux.rs

```rust
//! Linux-specific file system event handling using inotify

use super::EventHandler;
use crate::infrastructure::events::Event;
use crate::services::location_watcher::{WatchedLocation, WatcherEvent};
use crate::services::location_watcher::event_handler::WatcherEventKind;
use anyhow::Result;
use std::collections::HashMap;
use std::path::PathBuf;
use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::sync::RwLock;
use tracing::{debug, trace};
use uuid::Uuid;

/// Linux-specific event handler that uses inotify
pub struct LinuxHandler {
    /// Recently processed events for debouncing
    recent_events: Arc<RwLock<HashMap<PathBuf, Instant>>>,
    /// Debounce duration
    debounce_duration: Duration,
}

impl LinuxHandler {
    pub fn new() -> Self {
        Self {
            recent_events: Arc::new(RwLock::new(HashMap::new())),
            debounce_duration: Duration::from_millis(50), // Linux inotify is generally faster
        }
    }

    /// Check if event should be debounced
    async fn should_debounce(&self, path: &PathBuf) -> bool {
        let mut recent = self.recent_events.write().await;
        let now = Instant::now();

        // Check if we've seen this path recently
        if let Some(&last_seen) = recent.get(path) {
            if now.duration_since(last_seen) < self.debounce_duration {
                return true;
            }
        }

        recent.insert(path.clone(), now);

        // Cleanup old entries
        recent.retain(|_, &mut last_seen| {
            now.duration_since(last_seen) < Duration::from_secs(1)
        });

        false
    }

    /// Handle Linux-specific rename detection
    /// Linux inotify provides cleaner rename events compared to macOS
    async fn handle_rename_events(
        &self,
        event: &WatcherEvent,
        watched_locations: &Arc<RwLock<HashMap<Uuid, WatchedLocation>>>,
    ) -> Result<Vec<Event>> {
        let mut events = Vec::new();

        if let WatcherEventKind::Rename { from, to } = &event.kind {
            let locations = watched_locations.read().await;
            for location in locations.values() {
                if location.enabled && (from.starts_with(&location.path) || to.starts_with(&location.path)) {
                    let entry_id = Uuid::new_v4(); // TODO: Look up actual entry
                    events.push(Event::EntryMoved {
                        library_id: location.library_id,
                        entry_id,
                        old_path: from.to_string_lossy().to_string(),
                        new_path: to.to_string_lossy().to_string(),
                    });
                    break;
                }
            }
        }

        Ok(events)
    }
}

#[async_trait::async_trait]
impl EventHandler for LinuxHandler {
    async fn process_event(
        &self,
        event: WatcherEvent,
        watched_locations: &Arc<RwLock<HashMap<Uuid, WatchedLocation>>>,
    ) -> Result<Vec<Event>> {
        if !event.should_process() {
            return Ok(vec![]);
        }

        let mut events = Vec::new();

        // Handle rename events specially
        if let WatcherEventKind::Rename { .. } = &event.kind {
            return self.handle_rename_events(&event, watched_locations).await;
        }

        // Process other event types
        for path in &event.paths {
            // Check for debouncing
            if self.should_debounce(path).await {
                debug!("Debounced event for: {}", path.display());
                continue;
            }

            let locations = watched_locations.read().await;
            for location in locations.values() {
                if !location.enabled || !path.starts_with(&location.path) {
                    continue;
                }

                let entry_id = Uuid::new_v4(); // TODO: Look up or create actual entry

                match &event.kind {
                    WatcherEventKind::Create => {
                        events.push(Event::EntryCreated {
                            library_id: location.library_id,
                            entry_id,
                        });
                        trace!("Linux: Created {}", path.display());
                    }
                    WatcherEventKind::Modify => {
                        events.push(Event::EntryModified {
                            library_id: location.library_id,
                            entry_id,
                        });
                        trace!("Linux: Modified {}", path.display());
                    }
                    WatcherEventKind::Remove => {
                        events.push(Event::EntryDeleted {
                            library_id: location.library_id,
                            entry_id,
                        });
                        trace!("Linux: Removed {}", path.display());
                    }
                    _ => {
                        trace!("Linux: Unhandled event type for {}", path.display());
                    }
                }
                break;
            }
        }

        Ok(events)
    }

    async fn tick(&self) -> Result<()> {
        // Linux inotify is generally more reliable and doesn't need as much cleanup
        let mut recent = self.recent_events.write().await;
        let now = Instant::now();

        // Clean up old debounce entries
        recent.retain(|_, &mut last_seen| {
            now.duration_since(last_seen) < Duration::from_secs(5)
        });

        Ok(())
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::time::SystemTime;

    #[tokio::test]
    async fn test_linux_handler_creation() {
        let handler = LinuxHandler::new();
        assert_eq!(handler.debounce_duration, Duration::from_millis(50));
    }

    #[tokio::test]
    async fn test_debounce_logic() {
        let handler = LinuxHandler::new();
        let path = PathBuf::from("/test/file.txt");

        // First event should not be debounced
        assert!(!handler.should_debounce(&path).await);

        // Second immediate event should be debounced
        assert!(handler.should_debounce(&path).await);

        // Wait for debounce period and try again
        tokio::time::sleep(Duration::from_millis(60)).await;
        assert!(!handler.should_debounce(&path).await);
    }

    #[tokio::test]
    async fn test_rename_event_handling() {
        let handler = LinuxHandler::new();
        let watched_locations = Arc::new(RwLock::new(HashMap::new()));

        let event = WatcherEvent {
            kind: WatcherEventKind::Rename {
                from: PathBuf::from("/test/old.txt"),
                to: PathBuf::from("/test/new.txt"),
            },
            paths: vec![PathBuf::from("/test/old.txt"), PathBuf::from("/test/new.txt")],
            timestamp: SystemTime::now(),
            attrs: vec![],
        };

        let events = handler.handle_rename_events(&event, &watched_locations).await.unwrap();
        // Should be empty since no locations are configured
        assert_eq!(events.len(), 0);
    }
}```

## src/services/location_watcher/mod.rs

```rust
//! Location Watcher Service - Monitors file system changes in indexed locations

use crate::infrastructure::events::{Event, EventBus};
use crate::services::Service;
use anyhow::Result;
use notify::{Config, RecommendedWatcher, RecursiveMode, Watcher as NotifyWatcher};
use std::collections::HashMap;
use std::path::{Path, PathBuf};
use std::sync::Arc;
use std::time::Duration;
use tokio::sync::{mpsc, RwLock};
use tracing::{debug, error, info, warn};
use uuid::Uuid;

mod event_handler;
mod platform;
pub mod utils;

pub use event_handler::WatcherEvent;
pub use platform::PlatformHandler;

/// Configuration for the location watcher
#[derive(Debug, Clone)]
pub struct LocationWatcherConfig {
    /// Debounce duration for file system events
    pub debounce_duration: Duration,
    /// Maximum number of events to buffer
    pub event_buffer_size: usize,
    /// Whether to enable detailed debug logging
    pub debug_mode: bool,
}

impl Default for LocationWatcherConfig {
    fn default() -> Self {
        Self {
            debounce_duration: Duration::from_millis(100),
            event_buffer_size: 1000,
            debug_mode: false,
        }
    }
}

/// Location watcher service that monitors file system changes
pub struct LocationWatcher {
    /// Watcher configuration
    config: LocationWatcherConfig,
    /// Event bus for emitting events
    events: Arc<EventBus>,
    /// Currently watched locations
    watched_locations: Arc<RwLock<HashMap<Uuid, WatchedLocation>>>,
    /// File system watcher
    watcher: Arc<RwLock<Option<RecommendedWatcher>>>,
    /// Whether the service is running
    is_running: Arc<RwLock<bool>>,
    /// Platform-specific event handler
    platform_handler: Arc<PlatformHandler>,
}

/// Information about a watched location
#[derive(Debug, Clone)]
pub struct WatchedLocation {
    /// Location UUID
    pub id: Uuid,
    /// Library UUID this location belongs to
    pub library_id: Uuid,
    /// Path being watched
    pub path: PathBuf,
    /// Whether watching is enabled for this location
    pub enabled: bool,
}

impl LocationWatcher {
    /// Create a new location watcher
    pub fn new(config: LocationWatcherConfig, events: Arc<EventBus>) -> Self {
        let platform_handler = Arc::new(PlatformHandler::new());

        Self {
            config,
            events,
            watched_locations: Arc::new(RwLock::new(HashMap::new())),
            watcher: Arc::new(RwLock::new(None)),
            is_running: Arc::new(RwLock::new(false)),
            platform_handler,
        }
    }

    /// Add a location to watch
    pub async fn add_location(&self, location: WatchedLocation) -> Result<()> {
        if !location.enabled {
            debug!("Location {} is disabled, not adding to watcher", location.id);
            return Ok(());
        }

        let mut locations = self.watched_locations.write().await;

        if locations.contains_key(&location.id) {
            warn!("Location {} is already being watched", location.id);
            return Ok(());
        }

        // Add to file system watcher if running
        if *self.is_running.read().await {
            if let Some(watcher) = self.watcher.write().await.as_mut() {
                watcher.watch(&location.path, RecursiveMode::Recursive)?;
                info!("Started watching location: {}", location.path.display());
            }
        }

        locations.insert(location.id, location);
        Ok(())
    }

    /// Remove a location from watching
    pub async fn remove_location(&self, location_id: Uuid) -> Result<()> {
        let mut locations = self.watched_locations.write().await;

        if let Some(location) = locations.remove(&location_id) {
            // Remove from file system watcher if running
            if *self.is_running.read().await {
                if let Some(watcher) = self.watcher.write().await.as_mut() {
                    watcher.unwatch(&location.path)?;
                    info!("Stopped watching location: {}", location.path.display());
                }
            }
        }

        Ok(())
    }

    /// Update a location's settings
    pub async fn update_location(&self, location_id: Uuid, enabled: bool) -> Result<()> {
        let mut locations = self.watched_locations.write().await;

        if let Some(location) = locations.get_mut(&location_id) {
            let was_enabled = location.enabled;
            location.enabled = enabled;

            if *self.is_running.read().await {
                if let Some(watcher) = self.watcher.write().await.as_mut() {
                    match (was_enabled, enabled) {
                        (false, true) => {
                            // Enable watching
                            watcher.watch(&location.path, RecursiveMode::Recursive)?;
                            info!("Enabled watching for location: {}", location.path.display());
                        }
                        (true, false) => {
                            // Disable watching
                            watcher.unwatch(&location.path)?;
                            info!("Disabled watching for location: {}", location.path.display());
                        }
                        _ => {} // No change needed
                    }
                }
            }
        }

        Ok(())
    }

    /// Get all watched locations
    pub async fn get_watched_locations(&self) -> Vec<WatchedLocation> {
        self.watched_locations.read().await.values().cloned().collect()
    }

    /// Start the event processing loop
    async fn start_event_loop(&self) -> Result<()> {
        let events = self.events.clone();
        let platform_handler = self.platform_handler.clone();
        let watched_locations = self.watched_locations.clone();
        let is_running = self.is_running.clone();
        let debug_mode = self.config.debug_mode;

        let (tx, mut rx) = mpsc::channel(self.config.event_buffer_size);

        // Create file system watcher
        let mut watcher = notify::recommended_watcher(move |res| {
            match res {
                Ok(event) => {
                    if debug_mode {
                        debug!("Raw file system event: {:?}", event);
                    }

                    // Convert notify event to our WatcherEvent
                    let watcher_event = WatcherEvent::from_notify_event(event);

                    if let Err(e) = tx.try_send(watcher_event) {
                        warn!("Failed to send watcher event: {}", e);
                    }
                }
                Err(e) => {
                    error!("File system watcher error: {}", e);
                }
            }
        })?;

        // Configure watcher
        watcher.configure(Config::default().with_poll_interval(Duration::from_millis(500)))?;

        // Watch all enabled locations
        let locations = watched_locations.read().await;
        for location in locations.values() {
            if location.enabled {
                watcher.watch(&location.path, RecursiveMode::Recursive)?;
                info!("Started watching location: {}", location.path.display());
            }
        }
        drop(locations);

        // Store watcher
        *self.watcher.write().await = Some(watcher);

        // Start event processing loop
        tokio::spawn(async move {
            while *is_running.read().await {
                tokio::select! {
                    Some(event) = rx.recv() => {
                        // Process the event through platform handler
                        match platform_handler.process_event(event, &watched_locations).await {
                            Ok(processed_events) => {
                                for processed_event in processed_events {
                                    events.emit(processed_event);
                                }
                            }
                            Err(e) => {
                                error!("Error processing watcher event: {}", e);
                            }
                        }
                    }
                    _ = tokio::time::sleep(Duration::from_millis(100)) => {
                        // Periodic tick for debouncing and cleanup
                        if let Err(e) = platform_handler.tick().await {
                            error!("Error during platform handler tick: {}", e);
                        }

                        // Handle platform-specific tick events that might generate additional events
                        #[cfg(target_os = "macos")]
                        {
                            if let Ok(tick_events) = platform_handler.inner.tick_with_locations(&watched_locations).await {
                                for tick_event in tick_events {
                                    events.emit(tick_event);
                                }
                            }
                        }

                        #[cfg(target_os = "windows")]
                        {
                            if let Ok(tick_events) = platform_handler.inner.tick_with_locations(&watched_locations).await {
                                for tick_event in tick_events {
                                    events.emit(tick_event);
                                }
                            }
                        }
                    }
                }
            }

            info!("Location watcher event loop stopped");
        });

        Ok(())
    }
}

#[async_trait::async_trait]
impl Service for LocationWatcher {
    async fn start(&self) -> Result<()> {
        if *self.is_running.read().await {
            warn!("Location watcher is already running");
            return Ok(());
        }

        info!("Starting location watcher service");

        *self.is_running.write().await = true;

        self.start_event_loop().await?;

        info!("Location watcher service started");
        Ok(())
    }

    async fn stop(&self) -> Result<()> {
        if !*self.is_running.read().await {
            return Ok(());
        }

        info!("Stopping location watcher service");

        *self.is_running.write().await = false;

        // Clean up watcher
        *self.watcher.write().await = None;

        info!("Location watcher service stopped");
        Ok(())
    }

    fn is_running(&self) -> bool {
        // Use try_read to avoid blocking
        self.is_running.try_read().map_or(false, |guard| *guard)
    }

    fn name(&self) -> &'static str {
        "location_watcher"
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use tempfile::TempDir;

    fn create_test_events() -> Arc<EventBus> {
        Arc::new(EventBus::default())
    }

    #[tokio::test]
    async fn test_location_watcher_creation() {
        let config = LocationWatcherConfig::default();
        let events = create_test_events();
        let watcher = LocationWatcher::new(config, events);

        assert!(!watcher.is_running());
        assert_eq!(watcher.name(), "location_watcher");
    }

    #[tokio::test]
    async fn test_add_remove_location() {
        let config = LocationWatcherConfig::default();
        let events = create_test_events();
        let watcher = LocationWatcher::new(config, events);

        let temp_dir = TempDir::new().unwrap();
        let location = WatchedLocation {
            id: Uuid::new_v4(),
            library_id: Uuid::new_v4(),
            path: temp_dir.path().to_path_buf(),
            enabled: true,
        };

        let location_id = location.id;

        // Add location
        watcher.add_location(location).await.unwrap();

        let locations = watcher.get_watched_locations().await;
        assert_eq!(locations.len(), 1);
        assert_eq!(locations[0].id, location_id);

        // Remove location
        watcher.remove_location(location_id).await.unwrap();

        let locations = watcher.get_watched_locations().await;
        assert_eq!(locations.len(), 0);
    }
}```

## src/services/location_watcher/utils.rs

```rust
//! Utility functions for file system watching

use std::path::{Path, PathBuf};
use tracing::debug;

/// Check if a path should be ignored by the watcher
pub fn should_ignore_path(path: &Path) -> bool {
    let path_str = path.to_string_lossy();
    
    // Skip system directories
    if path_str.contains("/.git/") ||
       path_str.contains("/.svn/") ||
       path_str.contains("/.hg/") ||
       path_str.contains("/node_modules/") ||
       path_str.contains("/.vscode/") ||
       path_str.contains("/.idea/") ||
       path_str.contains("/target/") ||
       path_str.contains("/build/") ||
       path_str.contains("/dist/") {
        return true;
    }
    
    // Skip system files
    if let Some(file_name) = path.file_name() {
        let name = file_name.to_string_lossy();
        if name == ".DS_Store" ||
           name == "Thumbs.db" ||
           name == "desktop.ini" ||
           name.starts_with("._") ||
           name.starts_with("~$") {
            return true;
        }
    }
    
    false
}

/// Extract the relative path from a location root
pub fn extract_relative_path(location_root: &Path, full_path: &Path) -> Option<PathBuf> {
    full_path.strip_prefix(location_root)
        .ok()
        .map(|p| p.to_path_buf())
}

/// Check if a path is a subdirectory of another path
pub fn is_subdirectory(parent: &Path, child: &Path) -> bool {
    child.starts_with(parent) && child != parent
}

/// Normalize path separators for cross-platform compatibility
pub fn normalize_path(path: &Path) -> PathBuf {
    // Convert all separators to forward slashes for internal storage
    let path_str = path.to_string_lossy();
    let normalized = path_str.replace('\\', "/");
    PathBuf::from(normalized)
}

/// Check if a file extension indicates it should be watched
pub fn should_watch_extension(path: &Path) -> bool {
    if let Some(extension) = path.extension() {
        let ext = extension.to_string_lossy().to_lowercase();
        
        // Skip some binary and cache files
        !matches!(ext.as_str(),
            "tmp" | "temp" | "cache" | "log" | "lock" | "pid" |
            "swap" | "swp" | "bak" | "old" | "orig"
        )
    } else {
        true // Files without extensions are usually important
    }
}

/// Get a human-readable description of a file system event
pub fn describe_event(event_kind: &str, path: &Path) -> String {
    match event_kind {
        "create" => format!("Created: {}", path.display()),
        "modify" => format!("Modified: {}", path.display()),
        "remove" => format!("Removed: {}", path.display()),
        "rename" => format!("Renamed: {}", path.display()),
        _ => format!("{}: {}", event_kind, path.display()),
    }
}

/// Calculate a simple hash for a path (for inode fallback on non-Unix systems)
pub fn path_hash(path: &Path) -> u64 {
    use std::collections::hash_map::DefaultHasher;
    use std::hash::{Hash, Hasher};
    
    let mut hasher = DefaultHasher::new();
    path.hash(&mut hasher);
    hasher.finish()
}

/// Check if a directory is empty
pub async fn is_directory_empty(path: &Path) -> bool {
    match tokio::fs::read_dir(path).await {
        Ok(mut entries) => entries.next_entry().await.map_or(true, |e| e.is_none()),
        Err(_) => true, // If we can't read it, consider it empty
    }
}

/// Get file size safely
pub async fn get_file_size(path: &Path) -> u64 {
    tokio::fs::metadata(path)
        .await
        .map(|m| m.len())
        .unwrap_or(0)
}

/// Check if a path is likely a temporary file based on naming patterns
pub fn is_likely_temporary(path: &Path) -> bool {
    let path_str = path.to_string_lossy().to_lowercase();
    
    // Common temporary file patterns across platforms
    path_str.contains(".tmp") ||
    path_str.contains(".temp") ||
    path_str.contains(".partial") ||
    path_str.contains(".part") ||
    path_str.contains(".crdownload") ||
    path_str.contains(".download") ||
    path_str.ends_with("~") ||
    path_str.starts_with(".#") ||
    path_str.contains(".swp") ||
    path_str.contains(".swo")
}

/// Debounce helper that tracks the last time a path was seen
pub struct PathDebouncer {
    last_seen: std::collections::HashMap<PathBuf, std::time::Instant>,
    debounce_duration: std::time::Duration,
}

impl PathDebouncer {
    pub fn new(debounce_duration: std::time::Duration) -> Self {
        Self {
            last_seen: std::collections::HashMap::new(),
            debounce_duration,
        }
    }
    
    /// Check if a path should be debounced (returns true if should skip)
    pub fn should_debounce(&mut self, path: &Path) -> bool {
        let now = std::time::Instant::now();
        let path_buf = path.to_path_buf();
        
        if let Some(&last_time) = self.last_seen.get(&path_buf) {
            if now.duration_since(last_time) < self.debounce_duration {
                return true;
            }
        }
        
        self.last_seen.insert(path_buf, now);
        false
    }
    
    /// Clean up old entries to prevent memory leaks
    pub fn cleanup_old_entries(&mut self) {
        let cutoff = std::time::Instant::now() - std::time::Duration::from_secs(60);
        self.last_seen.retain(|_, &mut last_time| last_time > cutoff);
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::time::Duration;

    #[test]
    fn test_should_ignore_path() {
        assert!(should_ignore_path(Path::new("/project/.git/config")));
        assert!(should_ignore_path(Path::new("/project/node_modules/package")));
        assert!(should_ignore_path(Path::new("/project/.DS_Store")));
        assert!(!should_ignore_path(Path::new("/project/src/main.rs")));
    }

    #[test]
    fn test_extract_relative_path() {
        let root = Path::new("/home/user/project");
        let full = Path::new("/home/user/project/src/main.rs");
        let relative = extract_relative_path(root, full).unwrap();
        assert_eq!(relative, Path::new("src/main.rs"));
    }

    #[test]
    fn test_is_subdirectory() {
        let parent = Path::new("/home/user");
        let child = Path::new("/home/user/project");
        let other = Path::new("/home/other");
        
        assert!(is_subdirectory(parent, child));
        assert!(!is_subdirectory(parent, other));
        assert!(!is_subdirectory(parent, parent)); // Same path
    }

    #[test]
    fn test_should_watch_extension() {
        assert!(should_watch_extension(Path::new("file.txt")));
        assert!(should_watch_extension(Path::new("file.rs")));
        assert!(!should_watch_extension(Path::new("file.tmp")));
        assert!(!should_watch_extension(Path::new("file.cache")));
        assert!(should_watch_extension(Path::new("README"))); // No extension
    }

    #[test]
    fn test_is_likely_temporary() {
        assert!(is_likely_temporary(Path::new("file.tmp")));
        assert!(is_likely_temporary(Path::new("download.part")));
        assert!(is_likely_temporary(Path::new("document.docx.crdownload")));
        assert!(is_likely_temporary(Path::new("file~")));
        assert!(!is_likely_temporary(Path::new("important.txt")));
    }

    #[test]
    fn test_path_debouncer() {
        let mut debouncer = PathDebouncer::new(Duration::from_millis(100));
        let path = Path::new("/test/file.txt");
        
        // First call should not debounce
        assert!(!debouncer.should_debounce(path));
        
        // Immediate second call should debounce
        assert!(debouncer.should_debounce(path));
        
        // Different path should not debounce
        assert!(!debouncer.should_debounce(Path::new("/test/other.txt")));
    }
}```

## src/services/networking/core/mod.rs

```rust
//! Core networking engine with Iroh P2P

pub mod event_loop;

use crate::device::DeviceManager;
use crate::services::networking::{
	device::{DeviceInfo, DeviceRegistry},
	protocols::{pairing::PairingProtocolHandler, ProtocolRegistry},
	utils::{logging::NetworkLogger, NetworkIdentity},
	NetworkingError, Result,
};
use iroh::net::endpoint::Connection;
use iroh::net::key::NodeId;
use iroh::net::{Endpoint, NodeAddr};
use iroh_net::discovery::local_swarm_discovery::LocalSwarmDiscovery;
use iroh_net::discovery::{ConcurrentDiscovery, Discovery};
use std::sync::Arc;
use tokio::sync::{mpsc, RwLock};
use uuid::Uuid;

pub use event_loop::{EventLoopCommand, NetworkingEventLoop};

/// Protocol ALPN identifiers
pub const PAIRING_ALPN: &[u8] = b"spacedrive/pairing/1";
pub const FILE_TRANSFER_ALPN: &[u8] = b"spacedrive/filetransfer/1";
pub const MESSAGING_ALPN: &[u8] = b"spacedrive/messaging/1";

/// Central networking event types
#[derive(Debug, Clone)]
pub enum NetworkEvent {
	// Discovery events
	PeerDiscovered {
		node_id: NodeId,
		node_addr: NodeAddr,
	},
	PeerDisconnected {
		node_id: NodeId,
	},

	// Pairing events
	PairingRequest {
		session_id: Uuid,
		device_info: DeviceInfo,
		node_id: NodeId,
	},
	PairingSessionDiscovered {
		session_id: Uuid,
		node_id: NodeId,
		node_addr: NodeAddr,
		device_info: DeviceInfo,
	},
	PairingCompleted {
		device_id: Uuid,
		device_info: DeviceInfo,
	},
	PairingFailed {
		session_id: Uuid,
		reason: String,
	},

	// Connection events
	ConnectionEstablished {
		device_id: Uuid,
		node_id: NodeId,
	},
	ConnectionLost {
		device_id: Uuid,
		node_id: NodeId,
	},
	MessageReceived {
		from: Uuid,
		protocol: String,
		data: Vec<u8>,
	},
}

/// Main networking service using Iroh
pub struct NetworkingService {
	/// Iroh endpoint for all networking
	endpoint: Option<Endpoint>,

	/// Our network identity
	identity: NetworkIdentity,

	/// Our Iroh node ID
	node_id: NodeId,

	/// Discovery service for finding peers
	discovery: Option<Box<dyn Discovery>>,

	/// Shutdown sender for stopping the event loop
	shutdown_sender: Arc<RwLock<Option<mpsc::UnboundedSender<()>>>>,

	/// Command sender for sending commands to the event loop
	command_sender: Option<mpsc::UnboundedSender<event_loop::EventLoopCommand>>,

	/// Registry for protocol handlers
	protocol_registry: Arc<RwLock<ProtocolRegistry>>,

	/// Registry for device state and connections
	device_registry: Arc<RwLock<DeviceRegistry>>,

	/// Event sender for broadcasting network events
	event_sender: mpsc::UnboundedSender<NetworkEvent>,

	/// Event receiver for subscribers
	event_receiver: Arc<RwLock<Option<mpsc::UnboundedReceiver<NetworkEvent>>>>,

	/// Active connections tracker
	active_connections: Arc<RwLock<std::collections::HashMap<NodeId, Connection>>>,

	/// Logger for networking operations
	logger: Arc<dyn NetworkLogger>,
}

impl NetworkingService {
	/// Create a new networking service
	pub async fn new(
		device_manager: Arc<DeviceManager>,
		library_key_manager: Arc<crate::keys::library_key_manager::LibraryKeyManager>,
		data_dir: impl AsRef<std::path::Path>,
		logger: Arc<dyn NetworkLogger>,
	) -> Result<Self> {
		// Generate network identity from master key
		let device_key = device_manager
			.master_key()
			.map_err(|e| NetworkingError::Protocol(format!("Failed to get device key: {}", e)))?;
		let identity = NetworkIdentity::from_device_key(&device_key).await?;

		// Convert identity to Iroh format
		let secret_key = identity.to_iroh_secret_key()?;
		let node_id = secret_key.public();

		// Create event channel
		let (event_sender, event_receiver) = mpsc::unbounded_channel();

		// Create registries
		let protocol_registry = Arc::new(RwLock::new(ProtocolRegistry::new()));
		let device_registry = Arc::new(RwLock::new(DeviceRegistry::new(
			device_manager,
			data_dir,
			logger.clone(),
		)?));

		Ok(Self {
			endpoint: None,
			identity,
			node_id,
			discovery: None,
			shutdown_sender: Arc::new(RwLock::new(None)),
			command_sender: None,
			protocol_registry,
			device_registry,
			event_sender,
			event_receiver: Arc::new(RwLock::new(Some(event_receiver))),
			active_connections: Arc::new(RwLock::new(std::collections::HashMap::new())),
			logger,
		})
	}

	/// Start the networking service
	pub async fn start(&mut self) -> Result<()> {
		// Create Iroh endpoint with discovery and relay configuration
		let secret_key = self.identity.to_iroh_secret_key()?;

		// Create discovery service - using local swarm discovery for now
		let discovery = LocalSwarmDiscovery::new(self.node_id).map_err(|e| {
			NetworkingError::Transport(format!("Failed to create discovery: {}", e))
		})?;

		// Create endpoint with discovery
		let endpoint = Endpoint::builder()
			.secret_key(secret_key)
			.alpns(vec![
				PAIRING_ALPN.to_vec(),
				FILE_TRANSFER_ALPN.to_vec(),
				MESSAGING_ALPN.to_vec(),
			])
			.relay_mode(iroh_net::relay::RelayMode::Default)
			.discovery(Box::new(discovery))
			.bind_addr_v4(std::net::SocketAddrV4::new(
				std::net::Ipv4Addr::UNSPECIFIED,
				0,
			))
			.bind_addr_v6(std::net::SocketAddrV6::new(
				std::net::Ipv6Addr::UNSPECIFIED,
				0,
				0,
				0,
			))
			.bind()
			.await
			.map_err(|e| NetworkingError::Transport(format!("Failed to create endpoint: {}", e)))?;

		// Store endpoint reference for other methods
		self.endpoint = Some(endpoint.clone());

		// Create and start event loop
		let event_loop = NetworkingEventLoop::new(
			endpoint,
			self.protocol_registry.clone(),
			self.device_registry.clone(),
			self.event_sender.clone(),
			self.identity.clone(),
			self.active_connections.clone(),
			self.logger.clone(),
		);

		// Store shutdown and command senders before starting
		let shutdown_sender = event_loop.shutdown_sender();
		let command_sender = event_loop.command_sender();

		// Start the event processing in background
		event_loop.start().await?;

		// Store senders for later use
		*self.shutdown_sender.write().await = Some(shutdown_sender);
		self.command_sender = Some(command_sender);

		// Load and attempt to reconnect to paired devices
		self.load_and_reconnect_devices().await?;

		// Start periodic reconnection attempts
		self.start_periodic_reconnection().await;

		Ok(())
	}

	/// Load paired devices from persistence and attempt reconnection
	async fn load_and_reconnect_devices(&mut self) -> Result<()> {
		let mut device_registry = self.device_registry.write().await;

		// Load paired devices from persistence
		let loaded_device_ids = device_registry.load_paired_devices().await?;
		self.logger
			.info(&format!(
				"Loaded {} paired devices from persistence",
				loaded_device_ids.len()
			))
			.await;

		// Get devices that should auto-reconnect
		let auto_reconnect_devices = device_registry.get_auto_reconnect_devices().await?;
		self.logger
			.info(&format!(
				"Found {} devices for auto-reconnection",
				auto_reconnect_devices.len()
			))
			.await;

		drop(device_registry); // Release the lock for async operations

		// Start background reconnection attempts
		self.start_background_reconnection(auto_reconnect_devices)
			.await;

		Ok(())
	}

	/// Start background reconnection attempts for paired devices
	async fn start_background_reconnection(
		&self,
		auto_reconnect_devices: Vec<(
			Uuid,
			crate::services::networking::device::PersistedPairedDevice,
		)>,
	) {
		for (device_id, persisted_device) in auto_reconnect_devices {
			let command_sender = self.command_sender.clone();
			let endpoint = self.endpoint.clone();
			let logger = self.logger.clone();

			// Spawn a background task for each device reconnection
			tokio::spawn(async move {
				Self::attempt_device_reconnection(
					device_id,
					persisted_device,
					command_sender,
					endpoint,
					logger,
				)
				.await;
			});
		}
	}

	/// Attempt to reconnect to a specific device
	async fn attempt_device_reconnection(
		device_id: Uuid,
		persisted_device: crate::services::networking::device::PersistedPairedDevice,
		command_sender: Option<tokio::sync::mpsc::UnboundedSender<EventLoopCommand>>,
		endpoint: Option<Endpoint>,
		logger: Arc<dyn NetworkLogger>,
	) {
		logger
			.info(&format!(
				"Starting reconnection attempts for device: {}",
				device_id
			))
			.await;

		if let (Some(endpoint), Some(sender)) = (endpoint, command_sender) {
			// Try to parse node ID from the persisted device
			if let Ok(node_id) = persisted_device
				.device_info
				.network_fingerprint
				.node_id
				.parse::<NodeId>()
			{
				// Build NodeAddr from persisted addresses
				let mut node_addr = NodeAddr::new(node_id);

				// Add direct addresses if available
				for addr_str in &persisted_device.last_seen_addresses {
					if let Ok(addr) = addr_str.parse() {
						node_addr = node_addr.with_direct_addresses([addr]);
					}
				}

				// Attempt connection with Iroh
				match endpoint.connect(node_addr, &[]).await {
					Ok(_conn) => {
						logger
							.info(&format!(
								"âœ… Successfully connected to device {}",
								device_id
							))
							.await;

						// Send connection established command
						let _ = sender
							.send(EventLoopCommand::ConnectionEstablished { device_id, node_id });
					}
					Err(e) => {
						logger
							.error(&format!(
								"âŒ Failed to connect to device {}: {}",
								device_id, e
							))
							.await;
					}
				}
			}
		}
	}

	/// Start periodic reconnection attempts for disconnected devices
	async fn start_periodic_reconnection(&self) {
		let device_registry = self.device_registry.clone();
		let command_sender = self.command_sender.clone();
		let endpoint = self.endpoint.clone();
		let logger = self.logger.clone();

		tokio::spawn(async move {
			let mut interval = tokio::time::interval(tokio::time::Duration::from_secs(30));

			loop {
				interval.tick().await;

				// Get disconnected devices that should be reconnected
				if let Ok(auto_reconnect_devices) = {
					let registry = device_registry.read().await;
					registry.get_auto_reconnect_devices().await
				} {
					// Only attempt reconnection for devices we haven't seen recently
					let now = chrono::Utc::now();
					for (device_id, persisted_device) in auto_reconnect_devices {
						// Skip if device was seen recently (within last 5 minutes)
						if let Some(last_connected) = persisted_device.last_connected_at {
							if now.signed_duration_since(last_connected)
								< chrono::Duration::minutes(5)
							{
								continue;
							}
						}

						// Check if device is currently disconnected in registry
						let is_disconnected = {
							let registry = device_registry.read().await;
							if let Some(device_state) = registry.get_device_state(device_id) {
								matches!(device_state, crate::services::networking::device::DeviceState::Disconnected { .. })
							} else {
								true // Not in registry, try to reconnect
							}
						};

						if is_disconnected {
							logger
								.info(&format!(
									"Attempting periodic reconnection to device: {}",
									device_id
								))
								.await;
							let cmd_sender = command_sender.clone();
							let ep = endpoint.clone();
							let logger_clone = logger.clone();
							tokio::spawn(async move {
								Self::attempt_device_reconnection(
									device_id,
									persisted_device,
									cmd_sender,
									ep,
									logger_clone,
								)
								.await;
							});
						}
					}
				}
			}
		});
	}

	/// Stop the networking service
	pub async fn shutdown(&self) -> Result<()> {
		if let Some(shutdown_sender) = self.shutdown_sender.write().await.take() {
			let _ = shutdown_sender.send(());
			// Wait a bit for graceful shutdown
			tokio::time::sleep(std::time::Duration::from_millis(100)).await;
		}
		Ok(())
	}

	/// Subscribe to network events
	pub async fn subscribe_events(&self) -> Option<mpsc::UnboundedReceiver<NetworkEvent>> {
		self.event_receiver.write().await.take()
	}

	/// Get our network identity
	pub fn identity(&self) -> &NetworkIdentity {
		&self.identity
	}

	/// Get our node ID
	pub fn node_id(&self) -> NodeId {
		self.node_id
	}

	/// Get connected devices
	pub async fn get_connected_devices(&self) -> Vec<DeviceInfo> {
		self.device_registry.read().await.get_connected_devices()
	}

	/// Get raw connected nodes directly from endpoint
	pub async fn get_raw_connected_nodes(&self) -> Vec<NodeId> {
		let connections = self.active_connections.read().await;
		connections.keys().cloned().collect()
	}

	/// Send a message to a device
	pub async fn send_message(&self, device_id: Uuid, protocol: &str, data: Vec<u8>) -> Result<()> {
		if let Some(command_sender) = &self.command_sender {
			let command = event_loop::EventLoopCommand::SendMessage {
				device_id,
				protocol: protocol.to_string(),
				data,
			};

			command_sender.send(command).map_err(|_| {
				NetworkingError::ConnectionFailed("Event loop not running".to_string())
			})?;

			Ok(())
		} else {
			Err(NetworkingError::ConnectionFailed(
				"Networking not started".to_string(),
			))
		}
	}

	/// Get protocol registry for registering new protocols
	pub fn protocol_registry(&self) -> Arc<RwLock<ProtocolRegistry>> {
		self.protocol_registry.clone()
	}

	/// Get device registry for device management
	pub fn device_registry(&self) -> Arc<RwLock<DeviceRegistry>> {
		self.device_registry.clone()
	}

	/// Publish a discovery record for pairing session
	pub async fn publish_discovery_record(&self, key: &[u8], value: Vec<u8>) -> Result<()> {
		// For pairing, we don't need to explicitly publish with LocalSwarmDiscovery
		// It automatically advertises our node on the local network
		// The pairing protocol will handle session-specific discovery via direct connection
		Ok(())
	}

	/// Query a discovery record for pairing session
	pub async fn query_discovery_record(&self, key: &[u8]) -> Result<Vec<NodeAddr>> {
		// With LocalSwarmDiscovery, we can't query specific records
		// Instead, we'll discover all local nodes and filter by pairing session
		// For now, return empty - pairing will use direct connection after discovery
		Ok(Vec::new())
	}

	/// Get currently connected nodes for direct pairing attempts
	pub async fn get_connected_nodes(&self) -> Vec<NodeId> {
		// Get connected nodes from device registry
		let registry = self.device_registry.read().await;
		registry.get_connected_nodes()
	}

	/// Get the local device ID
	pub fn device_id(&self) -> Uuid {
		self.identity.device_id()
	}

	/// Get the command sender for the event loop
	pub fn command_sender(&self) -> Option<&mpsc::UnboundedSender<event_loop::EventLoopCommand>> {
		self.command_sender.as_ref()
	}

	/// Send message to a specific node (bypassing device lookup)
	pub async fn send_message_to_node(
		&self,
		node_id: NodeId,
		protocol: &str,
		data: Vec<u8>,
	) -> Result<()> {
		if let Some(command_sender) = &self.command_sender {
			let command = event_loop::EventLoopCommand::SendMessageToNode {
				node_id,
				protocol: protocol.to_string(),
				data,
			};

			command_sender.send(command).map_err(|_| {
				NetworkingError::ConnectionFailed("Event loop not running".to_string())
			})?;

			Ok(())
		} else {
			Err(NetworkingError::ConnectionFailed(
				"Networking not started".to_string(),
			))
		}
	}

	/// Connect to a node at a specific address
	pub async fn connect_to_node(&self, node_addr: NodeAddr) -> Result<()> {
		if let Some(endpoint) = &self.endpoint {
			// Use pairing ALPN for initial connection during pairing
			let conn = endpoint
				.connect(node_addr.clone(), PAIRING_ALPN)
				.await
				.map_err(|e| {
					NetworkingError::ConnectionFailed(format!("Failed to connect: {}", e))
				})?;

			// Track the outbound connection
			let node_id = node_addr.node_id;
			{
				let mut connections = self.active_connections.write().await;
				connections.insert(node_id, conn);
				self.logger
					.info(&format!("Tracked outbound connection to {}", node_id))
					.await;
			}

			Ok(())
		} else {
			Err(NetworkingError::ConnectionFailed(
				"Networking not started".to_string(),
			))
		}
	}

	/// Get our node address for advertising
	pub async fn get_node_addr(&self) -> Result<NodeAddr> {
		if let Some(endpoint) = &self.endpoint {
			endpoint
				.node_addr()
				.await
				.map_err(|e| NetworkingError::Protocol(format!("Failed to get node addr: {}", e)))
		} else {
			Err(NetworkingError::ConnectionFailed(
				"Networking not started".to_string(),
			))
		}
	}

	/// Start pairing as an initiator (generates pairing code)
	pub async fn start_pairing_as_initiator(&self) -> Result<(String, u32)> {
		// Get pairing handler from protocol registry
		let registry = self.protocol_registry();
		let pairing_handler =
			registry
				.read()
				.await
				.get_handler("pairing")
				.ok_or(NetworkingError::Protocol(
					"Pairing protocol not registered".to_string(),
				))?;

		// Cast to pairing handler to access pairing-specific methods
		let pairing_handler = pairing_handler
			.as_any()
			.downcast_ref::<crate::services::networking::protocols::PairingProtocolHandler>()
			.ok_or(NetworkingError::Protocol(
				"Invalid pairing handler type".to_string(),
			))?;

		// Generate session ID
		let session_id = uuid::Uuid::new_v4();
		let pairing_code =
			crate::services::networking::protocols::pairing::PairingCode::from_session_id(
				session_id,
			);

		// Start pairing session
		pairing_handler
			.start_pairing_session_with_id(session_id, pairing_code.clone())
			.await?;

		// Register in device registry
		let initiator_device_id = self.device_id();
		let initiator_node_id = self.node_id();
		let device_registry = self.device_registry();
		{
			let mut registry = device_registry.write().await;
			registry.start_pairing(initiator_device_id, initiator_node_id, session_id)?;
		}

		// Get our node address for advertising
		let node_addr = self.get_node_addr().await?;

		self.logger
			.info(&format!("Node address: {:?}", node_addr))
			.await;
		self.logger
			.info(&format!(
				"Direct addresses: {:?}",
				node_addr.direct_addresses().collect::<Vec<_>>()
			))
			.await;
		self.logger
			.info(&format!("Relay URL: {:?}", node_addr.relay_url()))
			.await;

		// Create pairing advertisement
		let node_addr_info = crate::services::networking::protocols::pairing::types::NodeAddrInfo {
			node_id: self.node_id().to_string(),
			direct_addresses: node_addr
				.direct_addresses()
				.map(|addr| addr.to_string())
				.collect(),
			relay_url: node_addr.relay_url().map(|u| u.to_string()),
		};

		let advertisement = crate::services::networking::protocols::pairing::PairingAdvertisement {
			node_id: self.node_id().to_string(),
			node_addr_info,
			device_info: pairing_handler.get_device_info().await?,
			expires_at: chrono::Utc::now() + chrono::Duration::minutes(5),
			created_at: chrono::Utc::now(),
		};

		// Publish to discovery
		let key = session_id.as_bytes();
		let value = serde_json::to_vec(&advertisement)
			.map_err(|e| NetworkingError::Protocol(e.to_string()))?;

		self.publish_discovery_record(key, value.clone()).await?;

		// For local testing: also write the advertisement to a file
		// This simulates what a DHT would do for peer discovery
		if let Ok(temp_dir) = std::env::var("SPACEDRIVE_TEST_DIR") {
			let session_file = format!("{}/pairing_session_{}.json", temp_dir, session_id);
			if let Err(e) = std::fs::write(&session_file, &value) {
				self.logger
					.warn(&format!(
						"Warning: Could not write pairing session file: {}",
						e
					))
					.await;
			} else {
				self.logger
					.info(&format!("Wrote pairing session info to {}", session_file))
					.await;
			}
		}

		let expires_in = 300; // 5 minutes

		Ok((pairing_code.to_string(), expires_in))
	}

	/// Start pairing as a joiner (connects using pairing code)
	pub async fn start_pairing_as_joiner(&self, code: &str) -> Result<()> {
		// Parse BIP39 pairing code
		let pairing_code =
			crate::services::networking::protocols::pairing::PairingCode::from_string(code)?;
		let session_id = pairing_code.session_id();

		// Get pairing handler
		let registry = self.protocol_registry();
		let pairing_handler =
			registry
				.read()
				.await
				.get_handler("pairing")
				.ok_or(NetworkingError::Protocol(
					"Pairing protocol not registered".to_string(),
				))?;
		let pairing_handler = pairing_handler
			.as_any()
			.downcast_ref::<crate::services::networking::protocols::PairingProtocolHandler>()
			.ok_or(NetworkingError::Protocol(
				"Invalid pairing handler type".to_string(),
			))?;

		// Join pairing session
		pairing_handler
			.join_pairing_session(session_id, pairing_code)
			.await?;

		// With LocalSwarmDiscovery, peers should auto-discover on the local network
		// Wait a moment for discovery to happen
		self.logger
			.info("Waiting for peer discovery on local network...")
			.await;
		tokio::time::sleep(tokio::time::Duration::from_secs(2)).await;

		// Query discovery for initiator's advertisement (even though it returns empty with LocalSwarmDiscovery)
		let key = session_id.as_bytes();
		let mut node_addrs = self.query_discovery_record(key).await?;

		// For local testing: also check for file-based session advertisement
		if let Ok(temp_dir) = std::env::var("SPACEDRIVE_TEST_DIR") {
			let session_file = format!("{}/pairing_session_{}.json", temp_dir, session_id);
			if let Ok(data) = std::fs::read(&session_file) {
				if let Ok(advertisement) = serde_json::from_slice::<
					crate::services::networking::protocols::pairing::PairingAdvertisement,
				>(&data)
				{
					if let Ok(initiator_node_addr) = advertisement.node_addr() {
						self.logger
							.info("Found Initiator's session info, attempting connection...")
							.await;
						node_addrs.push(initiator_node_addr);
					}
				}
			}
		}

		// Try to connect to discovered nodes
		for node_addr in node_addrs {
			self.logger.info("Attempting to connect to node...").await;
			self.logger
				.info(&format!("Node address: {:?}", node_addr))
				.await;
			self.logger
				.info(&format!(
					"Direct addresses: {:?}",
					node_addr.direct_addresses().collect::<Vec<_>>()
				))
				.await;
			self.logger
				.info(&format!("Relay URL: {:?}", node_addr.relay_url()))
				.await;

			if let Err(e) = self.connect_to_node(node_addr.clone()).await {
				self.logger
					.error(&format!("Failed to connect to {:?}: {}", node_addr, e))
					.await;
			} else {
				self.logger.info("Successfully connected to peer!").await;
			}
		}

		// Wait a moment for connections to be properly tracked
		tokio::time::sleep(tokio::time::Duration::from_millis(500)).await;

		// Send pairing request to any connected nodes
		let connected_nodes = self.get_raw_connected_nodes().await;
		self.logger
			.debug(&format!(
				"Found {} raw connected nodes",
				connected_nodes.len()
			))
			.await;
		if !connected_nodes.is_empty() {
			self.logger
				.info(&format!(
					"Found {} connected nodes, sending pairing requests...",
					connected_nodes.len()
				))
				.await;
			for node_id in connected_nodes {
				// Get local device info
				let local_device_info = {
					let device_registry = self.device_registry();
					let registry = device_registry.read().await;
					registry.get_local_device_info().unwrap_or_else(|_| {
						crate::services::networking::device::DeviceInfo {
							device_id: self.device_id(),
							device_name: "Joiner Device".to_string(),
							device_type: crate::services::networking::device::DeviceType::Desktop,
							os_version: std::env::consts::OS.to_string(),
							app_version: env!("CARGO_PKG_VERSION").to_string(),
							network_fingerprint: self.identity().network_fingerprint(),
							last_seen: chrono::Utc::now(),
						}
					})
				};

				let pairing_request =
					crate::services::networking::protocols::pairing::messages::PairingMessage::PairingRequest {
						session_id,
						device_info: local_device_info,
						public_key: self.identity().public_key_bytes(),
					};

				// Send via Iroh stream using the pairing handler and wait for response
				if let Some(endpoint) = &self.endpoint {
					let registry = self.protocol_registry();
					let guard = registry.read().await;
					if let Some(handler) = guard.get_handler("pairing") {
						if let Some(pairing_handler) =
							handler.as_any().downcast_ref::<PairingProtocolHandler>()
						{
							self.logger
								.info(&format!("Sending pairing request to node {}", node_id))
								.await;
							match pairing_handler
								.send_pairing_message_to_node(endpoint, node_id, &pairing_request)
								.await
							{
								Ok(Some(response)) => {
									self.logger.info("Received response from Initiator!").await;
									// Process the response via the trait's handle_response method
									if let Ok(msg_bytes) = serde_json::to_vec(&response) {
										let device_id = self.device_id(); // Joiner's own device ID
										let _ = handler
											.handle_response(device_id, node_id, msg_bytes)
											.await;
									}
									// Stop sending more requests since we got a response
									break;
								}
								Ok(None) => {
									self.logger
										.warn("No response received from Initiator")
										.await;
								}
								Err(e) => {
									self.logger
										.error(&format!("Failed to send pairing request: {}", e))
										.await;
								}
							}
						}
					}
				}
			}
		}

		// Ensure pairing requests are sent with polling
		self.ensure_pairing_requests_sent(session_id).await?;

		Ok(())
	}

	/// Get current pairing status
	pub async fn get_pairing_status(
		&self,
	) -> Result<Vec<crate::services::networking::PairingSession>> {
		// Get pairing handler from protocol registry
		let registry = self.protocol_registry();
		let pairing_handler =
			registry
				.read()
				.await
				.get_handler("pairing")
				.ok_or(NetworkingError::Protocol(
					"Pairing protocol not registered".to_string(),
				))?;

		// Downcast to concrete pairing handler type to access sessions
		if let Some(pairing_handler) =
			pairing_handler
				.as_any()
				.downcast_ref::<crate::services::networking::protocols::PairingProtocolHandler>()
		{
			let sessions = pairing_handler.get_active_sessions().await;
			Ok(sessions)
		} else {
			Err(NetworkingError::Protocol(
				"Failed to downcast pairing handler".to_string(),
			))
		}
	}

	/// Enhanced pairing request sending with robust active polling
	async fn ensure_pairing_requests_sent(&self, session_id: uuid::Uuid) -> Result<()> {
		const MAX_WAIT_TIME: u64 = 15000; // 15 seconds
		const POLL_INTERVAL: u64 = 500; // Check every 500ms
		let start_time = std::time::Instant::now();

		loop {
			// First, check if the session has already advanced
			let registry = self.protocol_registry();
			let registry_guard = registry.read().await;
			if let Some(pairing_handler) = registry_guard.get_handler("pairing") {
				if let Some(handler) =
					pairing_handler
						.as_any()
						.downcast_ref::<crate::services::networking::protocols::PairingProtocolHandler>(
					) {
					let sessions = handler.get_active_sessions().await;
					if let Some(session) = sessions.iter().find(|s| s.id == session_id) {
						if !matches!(
							session.state,
							crate::services::networking::protocols::pairing::PairingState::Scanning
						) {
							return Ok(());
						}
					}
				}
			}
			drop(registry_guard);

			// Check for connected nodes and send the request
			let connected_nodes = self.get_raw_connected_nodes().await;
			if !connected_nodes.is_empty() {
				for node_id in &connected_nodes {
					let local_device_info = {
						let device_registry = self.device_registry();
						let registry = device_registry.read().await;
						registry.get_local_device_info().unwrap_or_else(|_| {
							crate::services::networking::device::DeviceInfo {
								device_id: self.device_id(),
								device_name: "Joiner's Test Device".to_string(),
								device_type:
									crate::services::networking::device::DeviceType::Desktop,
								os_version: std::env::consts::OS.to_string(),
								app_version: env!("CARGO_PKG_VERSION").to_string(),
								network_fingerprint: self.identity().network_fingerprint(),
								last_seen: chrono::Utc::now(),
							}
						})
					};

					let pairing_request =
						crate::services::networking::protocols::pairing::messages::PairingMessage::PairingRequest {
							session_id,
							device_info: local_device_info,
							public_key: self.identity().public_key_bytes(),
						};

					// Send via Iroh stream using the pairing handler and wait for response
					if let Some(endpoint) = &self.endpoint {
						let registry = self.protocol_registry();
						let guard = registry.read().await;
						if let Some(handler) = guard.get_handler("pairing") {
							if let Some(pairing_handler) =
								handler.as_any().downcast_ref::<PairingProtocolHandler>()
							{
								match pairing_handler
									.send_pairing_message_to_node(
										endpoint,
										*node_id,
										&pairing_request,
									)
									.await
								{
									Ok(Some(response)) => {
										self.logger
											.info("Received challenge response from Initiator!")
											.await;
										// Process the response via the trait's handle_response method
										if let Ok(msg_bytes) = serde_json::to_vec(&response) {
											let device_id = self.device_id(); // Joiner's own device ID
											let _ = handler
												.handle_response(device_id, *node_id, msg_bytes)
												.await;
										}
										// Return early since we got a response
										return Ok(());
									}
									Ok(None) => {
										self.logger
											.warn("No response received in ensure_pairing_requests_sent")
											.await;
									}
									Err(e) => {
										self.logger
											.error(&format!("Failed to send pairing request in ensure_pairing_requests_sent: {}", e))
											.await;
									}
								}
							}
						}
					}
				}
			}

			// Check for timeout
			if start_time.elapsed().as_millis() > MAX_WAIT_TIME as u128 {
				return Err(NetworkingError::Protocol(
					"Pairing timeout: Did not receive challenge from Initiator.".to_string(),
				));
			}

			tokio::time::sleep(tokio::time::Duration::from_millis(POLL_INTERVAL)).await;
		}
	}
}

// Ensure NetworkingService is Send + Sync for proper async usage
unsafe impl Send for NetworkingService {}
unsafe impl Sync for NetworkingService {}
```

## src/services/networking/core/event_loop.rs

```rust
//! Networking event loop for handling Iroh connections and messages

use crate::services::networking::{
	core::{NetworkEvent, FILE_TRANSFER_ALPN, MESSAGING_ALPN, PAIRING_ALPN},
	device::DeviceRegistry,
	protocols::ProtocolRegistry,
	utils::{logging::NetworkLogger, NetworkIdentity},
	NetworkingError, Result,
};
use iroh::net::endpoint::Connection;
use iroh::net::key::NodeId;
use iroh::net::{Endpoint, NodeAddr};
use std::sync::Arc;
use tokio::sync::{mpsc, RwLock};
use uuid::Uuid;

/// Commands that can be sent to the event loop
#[derive(Debug)]
pub enum EventLoopCommand {
	// Connection management
	ConnectionEstablished {
		device_id: Uuid,
		node_id: NodeId,
	},

	// Message sending
	SendMessage {
		device_id: Uuid,
		protocol: String,
		data: Vec<u8>,
	},
	SendMessageToNode {
		node_id: NodeId,
		protocol: String,
		data: Vec<u8>,
	},

	// Shutdown
	Shutdown,
}

/// Networking event loop that processes Iroh connections
pub struct NetworkingEventLoop {
	/// Iroh endpoint
	endpoint: Endpoint,

	/// Protocol registry for routing messages
	protocol_registry: Arc<RwLock<ProtocolRegistry>>,

	/// Device registry for managing device state
	device_registry: Arc<RwLock<DeviceRegistry>>,

	/// Event sender for broadcasting network events
	event_sender: mpsc::UnboundedSender<NetworkEvent>,

	/// Command receiver
	command_rx: mpsc::UnboundedReceiver<EventLoopCommand>,

	/// Command sender (for cloning)
	command_tx: mpsc::UnboundedSender<EventLoopCommand>,

	/// Shutdown receiver
	shutdown_rx: mpsc::UnboundedReceiver<()>,

	/// Shutdown sender (for cloning)
	shutdown_tx: mpsc::UnboundedSender<()>,

	/// Our network identity
	identity: NetworkIdentity,

	/// Active connections tracker
	active_connections: Arc<RwLock<std::collections::HashMap<NodeId, Connection>>>,

	/// Logger for event loop operations
	logger: Arc<dyn NetworkLogger>,
}

impl NetworkingEventLoop {
	/// Create a new networking event loop
	pub fn new(
		endpoint: Endpoint,
		protocol_registry: Arc<RwLock<ProtocolRegistry>>,
		device_registry: Arc<RwLock<DeviceRegistry>>,
		event_sender: mpsc::UnboundedSender<NetworkEvent>,
		identity: NetworkIdentity,
		active_connections: Arc<RwLock<std::collections::HashMap<NodeId, Connection>>>,
		logger: Arc<dyn NetworkLogger>,
	) -> Self {
		let (command_tx, command_rx) = mpsc::unbounded_channel();
		let (shutdown_tx, shutdown_rx) = mpsc::unbounded_channel();

		Self {
			endpoint,
			protocol_registry,
			device_registry,
			event_sender,
			command_rx,
			command_tx,
			shutdown_rx,
			shutdown_tx,
			identity,
			active_connections,
			logger,
		}
	}

	/// Get the command sender for sending commands to the event loop
	pub fn command_sender(&self) -> mpsc::UnboundedSender<EventLoopCommand> {
		self.command_tx.clone()
	}

	/// Get the shutdown sender
	pub fn shutdown_sender(&self) -> mpsc::UnboundedSender<()> {
		self.shutdown_tx.clone()
	}

	/// Start the event loop (consumes self)
	pub async fn start(mut self) -> Result<()> {
		// Spawn the event loop task
		let logger = self.logger.clone();
		tokio::spawn(async move {
			if let Err(e) = self.run().await {
				logger
					.error(&format!("Networking event loop error: {}", e))
					.await;
			}
		});

		Ok(())
	}

	/// Main event loop
	async fn run(&mut self) -> Result<()> {
		self.logger.info("ðŸš€ Networking event loop started").await;

		loop {
			tokio::select! {
				// Handle incoming connections
				Some(incoming) = self.endpoint.accept() => {
					let conn = match incoming.await {
						Ok(c) => c,
						Err(e) => {
							self.logger.error(&format!("Failed to establish connection: {}", e)).await;
							continue;
						}
					};

					// Handle the connection based on ALPN
					self.handle_connection(conn).await;
				}

				// Handle commands
				Some(cmd) = self.command_rx.recv() => {
					match cmd {
						EventLoopCommand::Shutdown => {
							self.logger.info("Shutting down networking event loop").await;
							break;
						}
						_ => self.handle_command(cmd).await,
					}
				}

				// Handle shutdown signal
				Some(_) = self.shutdown_rx.recv() => {
					self.logger.info("Received shutdown signal").await;
					break;
				}
			}
		}

		self.logger.info("Networking event loop stopped").await;
		Ok(())
	}

	/// Handle an incoming connection
	async fn handle_connection(&self, conn: Connection) {
		// Extract the remote node ID from the connection
		let remote_node_id = match iroh::net::endpoint::get_remote_node_id(&conn) {
			Ok(key) => key,
			Err(e) => {
				self.logger
					.error(&format!("Failed to get remote node ID: {}", e))
					.await;
				return;
			}
		};

		// Track the connection
		{
			let mut connections = self.active_connections.write().await;
			connections.insert(remote_node_id, conn.clone());
		}

		// For now, we'll need to detect ALPN from the first stream
		// TODO: Find the correct way to get ALPN from iroh Connection
		let alpn = PAIRING_ALPN; // Default to pairing, will be overridden based on stream detection

		self.logger
			.info(&format!("Incoming connection from {:?}", remote_node_id))
			.await;
		self.logger
			.debug("Detecting protocol from incoming streams...")
			.await;

		// Clone necessary components for the spawned task
		let protocol_registry = self.protocol_registry.clone();
		let device_registry = self.device_registry.clone();
		let event_sender = self.event_sender.clone();
		let active_connections = self.active_connections.clone();
		let logger = self.logger.clone();

		// Spawn a task to handle this connection
		tokio::spawn(async move {
			// Handle incoming connection by accepting streams and routing based on content
			Self::handle_incoming_connection(
				conn.clone(),
				protocol_registry,
				device_registry,
				event_sender,
				remote_node_id,
				logger.clone(),
			)
			.await;

			// Only remove connection if it's actually closed
			if conn.close_reason().is_some() {
				let mut connections = active_connections.write().await;
				connections.remove(&remote_node_id);
				logger
					.info(&format!(
						"Connection to {} removed (closed)",
						remote_node_id
					))
					.await;
			} else {
				logger
					.debug(&format!(
						"Connection to {} still active after stream handling",
						remote_node_id
					))
					.await;
			}
		});
	}

	/// Handle an incoming connection by detecting protocol from streams
	async fn handle_incoming_connection(
		conn: Connection,
		protocol_registry: Arc<RwLock<ProtocolRegistry>>,
		device_registry: Arc<RwLock<DeviceRegistry>>,
		event_sender: mpsc::UnboundedSender<NetworkEvent>,
		remote_node_id: NodeId,
		logger: Arc<dyn NetworkLogger>,
	) {
		loop {
			// Try to accept different types of streams
			tokio::select! {
				// Try bidirectional stream (pairing/messaging)
				bi_result = conn.accept_bi() => {
					match bi_result {
						Ok((send, recv)) => {
							logger.debug(&format!("Accepted bidirectional stream from {}", remote_node_id)).await;
							// For now, assume bidirectional streams are pairing
							let registry = protocol_registry.read().await;
							if let Some(handler) = registry.get_handler("pairing") {
								logger.debug("Directing bidirectional stream to pairing handler").await;
								handler.handle_stream(
									Box::new(send),
									Box::new(recv),
									remote_node_id,
								).await;
							}
						}
						Err(e) => {
							logger.error(&format!("Failed to accept bidirectional stream: {}", e)).await;
							break;
						}
					}
				}
				// Try unidirectional stream (file transfer)
				uni_result = conn.accept_uni() => {
					match uni_result {
						Ok(recv) => {
							logger.debug(&format!("Accepted unidirectional stream from {}", remote_node_id)).await;
							// Unidirectional streams are for file transfer
							let registry = protocol_registry.read().await;
							if let Some(handler) = registry.get_handler("file_transfer") {
								logger.debug("Directing unidirectional stream to file transfer handler").await;
								handler.handle_stream(
									Box::new(tokio::io::empty()), // No send stream for unidirectional
									Box::new(recv),
									remote_node_id,
								).await;
							}
						}
						Err(e) => {
							logger.error(&format!("Failed to accept unidirectional stream: {}", e)).await;
							break;
						}
					}
				}
			}
		}
	}

	/// Handle a command from the main thread
	async fn handle_command(&self, command: EventLoopCommand) {
		match command {
			EventLoopCommand::ConnectionEstablished { device_id, node_id } => {
				// Update device registry
				let mut registry = self.device_registry.write().await;
				if let Err(e) = registry.set_device_connected(device_id, node_id) {
					self.logger
						.error(&format!("Failed to update device connection state: {}", e))
						.await;
				}

				// Send connection event
				let _ = self
					.event_sender
					.send(NetworkEvent::ConnectionEstablished { device_id, node_id });
			}

			EventLoopCommand::SendMessage {
				device_id,
				protocol,
				data,
			} => {
				// Look up node ID for device
				let node_id = {
					let registry = self.device_registry.read().await;
					registry.get_node_id_for_device(device_id)
				};

				if let Some(node_id) = node_id {
					// Send to node
					self.send_to_node(node_id, &protocol, data).await;
				} else {
					self.logger
						.warn(&format!("No node ID found for device {}", device_id))
						.await;
				}
			}

			EventLoopCommand::SendMessageToNode {
				node_id,
				protocol,
				data,
			} => {
				self.send_to_node(node_id, &protocol, data).await;
			}

			EventLoopCommand::Shutdown => {
				// Handled in main loop
			}
		}
	}

	/// Send a message to a specific node
	async fn send_to_node(&self, node_id: NodeId, protocol: &str, data: Vec<u8>) {
		self.logger
			.debug(&format!(
				"Sending {} message to {} ({} bytes)",
				protocol,
				node_id,
				data.len()
			))
			.await;

		// Determine ALPN based on protocol
		let alpn = match protocol {
			"pairing" => PAIRING_ALPN,
			"file_transfer" => {
				self.logger
					.debug(&format!(
						"Using ALPN: {:?}",
						String::from_utf8_lossy(FILE_TRANSFER_ALPN)
					))
					.await;
				FILE_TRANSFER_ALPN
			}
			"messaging" => MESSAGING_ALPN,
			_ => {
				self.logger
					.error(&format!("Unknown protocol: {}", protocol))
					.await;
				return;
			}
		};

		// Create node address (Iroh will use existing connection if available)
		let node_addr = NodeAddr::new(node_id);

		// Connect with specific ALPN
		self.logger
			.debug(&format!(
				"Attempting to connect to {} with ALPN: {:?}",
				node_id,
				String::from_utf8_lossy(alpn)
			))
			.await;
		match self.endpoint.connect(node_addr, alpn).await {
			Ok(conn) => {
				self.logger
					.debug(&format!(
						"Successfully connected to {} with ALPN: {:?}",
						node_id,
						String::from_utf8_lossy(alpn)
					))
					.await;
				// Track the connection
				{
					let mut connections = self.active_connections.write().await;
					connections.insert(node_id, conn.clone());
				}

				// Open appropriate stream based on protocol
				match protocol {
					"pairing" | "messaging" => {
						// Bidirectional stream
						match conn.open_bi().await {
							Ok((mut send, _recv)) => {
								// For pairing, send with length prefix like send_pairing_message_to_node does
								if protocol == "pairing" {
									// Send message length first
									let len = data.len() as u32;
									if let Err(e) = send.write_all(&len.to_be_bytes()).await {
										self.logger
											.error(&format!(
												"Failed to write pairing message length: {}",
												e
											))
											.await;
										return;
									}
								}

								// Send the data
								if let Err(e) = send.write_all(&data).await {
									self.logger
										.error(&format!(
											"Failed to send {} message: {}",
											protocol, e
										))
										.await;
								}
								let _ = send.finish();
							}
							Err(e) => {
								self.logger
									.error(&format!("Failed to open {} stream: {}", protocol, e))
									.await;
							}
						}
					}
					"file_transfer" => {
						// Unidirectional stream
						self.logger
							.debug(&format!("Opening unidirectional stream to {}", node_id))
							.await;
						match conn.open_uni().await {
							Ok(mut send) => {
								self.logger.debug("Opened stream, sending data").await;
								// Send with the expected format for file transfer protocol
								// Transfer type: 0 for file metadata request
								if let Err(e) = send.write_all(&[0u8]).await {
									self.logger
										.error(&format!(
											"Failed to write file transfer type: {}",
											e
										))
										.await;
									return;
								}

								// Send message length (big-endian u32)
								let len = data.len() as u32;
								if let Err(e) = send.write_all(&len.to_be_bytes()).await {
									self.logger
										.error(&format!(
											"Failed to write file transfer message length: {}",
											e
										))
										.await;
									return;
								}

								// Send the actual message data
								if let Err(e) = send.write_all(&data).await {
									self.logger
										.error(&format!("Failed to send file transfer data: {}", e))
										.await;
								} else {
									self.logger
										.debug(&format!("Successfully sent {} bytes", data.len()))
										.await;
								}
								let _ = send.finish();
							}
							Err(e) => {
								self.logger
									.error(&format!("Failed to open stream: {}", e))
									.await;
							}
						}
					}
					_ => {}
				}
			}
			Err(e) => {
				self.logger
					.error(&format!("Failed to connect to {}: {}", node_id, e))
					.await;
			}
		}
	}
}
```

## src/services/networking/utils/logging.rs

```rust
//! Logging utilities for networking operations

use async_trait::async_trait;

/// Trait for network logging
#[async_trait]
pub trait NetworkLogger: Send + Sync {
    async fn info(&self, message: &str);
    async fn warn(&self, message: &str);
    async fn error(&self, message: &str);
    async fn debug(&self, message: &str);
}

/// Silent logger that discards all messages
pub struct SilentLogger;

#[async_trait]
impl NetworkLogger for SilentLogger {
    async fn info(&self, _message: &str) {}
    async fn warn(&self, _message: &str) {}
    async fn error(&self, _message: &str) {}
    async fn debug(&self, _message: &str) {}
}

/// Console logger that prints to stdout/stderr
pub struct ConsoleLogger;

#[async_trait]
impl NetworkLogger for ConsoleLogger {
    async fn info(&self, message: &str) {
        println!("[NETWORKING INFO] {}", message);
    }
    
    async fn warn(&self, message: &str) {
        eprintln!("[NETWORKING WARN] {}", message);
    }
    
    async fn error(&self, message: &str) {
        eprintln!("[NETWORKING ERROR] {}", message);
    }
    
    async fn debug(&self, message: &str) {
        println!("[NETWORKING DEBUG] {}", message);
    }
}```

## src/services/networking/utils/identity.rs

```rust
//! Network identity management - node ID and key generation

use crate::services::networking::{NetworkingError, Result};
use iroh::net::key::{NodeId, SecretKey};
use serde::{Deserialize, Serialize};
use uuid::Uuid;

/// Network identity containing keypair and node ID
#[derive(Clone)]
pub struct NetworkIdentity {
	secret_key: SecretKey,
	node_id: NodeId,
	// Keep Ed25519 keypair for backward compatibility
	ed25519_seed: [u8; 32],
}

impl NetworkIdentity {
	/// Create a new random network identity
	pub async fn new() -> Result<Self> {
		let secret_key = SecretKey::generate();
		let node_id = secret_key.public();
		
		// Generate Ed25519 seed for backward compatibility
		let ed25519_seed = rand::random();

		Ok(Self { 
			secret_key, 
			node_id,
			ed25519_seed,
		})
	}

	/// Create a deterministic network identity from device key
	pub async fn from_device_key(device_key: &[u8; 32]) -> Result<Self> {
		// Derive Ed25519 seed from master key using HKDF
		use hkdf::Hkdf;
		use sha2::Sha256;
		
		let hk = Hkdf::<Sha256>::new(None, device_key);
		let mut ed25519_seed = [0u8; 32];
		hk.expand(b"spacedrive-network-identity", &mut ed25519_seed)
			.map_err(|e| NetworkingError::Protocol(format!("Failed to derive network key: {}", e)))?;
		
		// Create Iroh secret key from the same seed
		let secret_key = SecretKey::from_bytes(&ed25519_seed);
		let node_id = secret_key.public();

		Ok(Self { 
			secret_key, 
			node_id,
			ed25519_seed,
		})
	}

	/// Convert to Iroh SecretKey
	pub fn to_iroh_secret_key(&self) -> Result<SecretKey> {
		Ok(self.secret_key.clone())
	}

	/// Get the node ID
	pub fn node_id(&self) -> NodeId {
		self.node_id
	}

	/// Get the public key bytes (for backward compatibility)
	pub fn public_key_bytes(&self) -> Vec<u8> {
		self.node_id.as_bytes().to_vec()
	}

	/// Get the keypair bytes (for backward compatibility)
	pub fn keypair_bytes(&self) -> &[u8; 32] {
		&self.ed25519_seed
	}

	/// Sign data with this identity
	pub fn sign(&self, data: &[u8]) -> Result<Vec<u8>> {
		// Use Ed25519 signing for backward compatibility
		use ed25519_dalek::{Signer, SigningKey};
		
		let signing_key = SigningKey::from_bytes(&self.ed25519_seed);
		let signature = signing_key.sign(data);
		Ok(signature.to_bytes().to_vec())
	}

	/// Verify signature with this identity's public key
	pub fn verify(&self, data: &[u8], signature: &[u8]) -> bool {
		// Use Ed25519 verification for backward compatibility
		use ed25519_dalek::{Signature, Verifier, VerifyingKey, SigningKey};
		
		let signing_key = SigningKey::from_bytes(&self.ed25519_seed);
		let verifying_key = signing_key.verifying_key();
		
		if let Ok(sig) = Signature::from_slice(signature) {
			verifying_key.verify(data, &sig).is_ok()
		} else {
			false
		}
	}

	/// Get a deterministic device ID from the network identity
	pub fn device_id(&self) -> Uuid {
		// Create a deterministic UUID from the node ID
		let node_id_bytes = self.node_id.as_bytes();
		
		// Use the first 16 bytes of the node ID hash to create a UUID
		let mut uuid_bytes = [0u8; 16];
		let hash = blake3::hash(node_id_bytes);
		uuid_bytes.copy_from_slice(&hash.as_bytes()[..16]);
		
		Uuid::from_bytes(uuid_bytes)
	}

	/// Get network fingerprint for device identification
	pub fn network_fingerprint(&self) -> NetworkFingerprint {
		let public_key_bytes = self.public_key_bytes();
		let public_key_hash = blake3::hash(&public_key_bytes);
		
		NetworkFingerprint {
			node_id: self.node_id.to_string(),
			public_key_hash: hex::encode(&public_key_hash.as_bytes()[..16]),
		}
	}
}

impl std::fmt::Debug for NetworkIdentity {
	fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
		f.debug_struct("NetworkIdentity")
			.field("node_id", &self.node_id)
			.finish()
	}
}

/// Serializable network fingerprint for device identification
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq, Hash)]
pub struct NetworkFingerprint {
	pub node_id: String,
	pub public_key_hash: String,
}

impl std::fmt::Display for NetworkFingerprint {
	fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
		write!(f, "{}:{}", &self.node_id[..8], &self.public_key_hash[..8])
	}
}

impl NetworkFingerprint {
	/// Create fingerprint from network identity
	pub fn from_identity(identity: &NetworkIdentity) -> Self {
		let public_key_bytes = identity.public_key_bytes();
		let public_key_hash = blake3::hash(&public_key_bytes).to_hex().to_string();

		Self {
			node_id: identity.node_id().to_string(),
			public_key_hash,
		}
	}

	/// Verify that this fingerprint matches the given identity
	pub fn verify(&self, identity: &NetworkIdentity) -> bool {
		let expected = Self::from_identity(identity);
		*self == expected
	}
}```

## src/services/networking/utils/mod.rs

```rust
//! Shared utilities for the networking system

pub mod identity;
pub mod logging;

pub use identity::NetworkIdentity;
pub use logging::{NetworkLogger, SilentLogger, ConsoleLogger};```

## src/services/networking/mod.rs

```rust
//! Spacedrive Networking v2 - Unified Architecture
//!
//! This is a complete redesign of the networking system that addresses the fundamental
//! issues in the original implementation:
//! - Single LibP2P swarm instead of multiple competing swarms
//! - Proper Send/Sync design for background task execution
//! - Centralized event system and state management
//! - Modular protocol handling
//!
//! Key components:
//! - `core`: Central networking engine with unified LibP2P swarm
//! - `protocols`: Modular protocol handlers (pairing, messaging, file transfer)
//! - `device`: Device registry and connection management
//! - `utils`: Shared utilities (identity, codecs, logging)

pub mod core;
pub mod protocols;
pub mod device;
pub mod utils;

// Re-export main types for easy access
pub use core::{NetworkingService, NetworkEvent};

// Compatibility alias for legacy code
pub use NetworkingService as NetworkingCore;
pub use device::{DeviceInfo, DeviceRegistry, DeviceState};
pub use protocols::{ProtocolHandler, ProtocolRegistry};
pub use utils::{NetworkIdentity, NetworkLogger, SilentLogger};

// Re-export specific protocol types for CLI compatibility
pub use protocols::pairing::{PairingState, PairingSession};

/// Main error type for networking operations
#[derive(Debug, thiserror::Error)]
pub enum NetworkingError {
    #[error("LibP2P error: {0}")]
    LibP2P(String),
    
    #[error("Protocol error: {0}")]
    Protocol(String),
    
    #[error("Device not found: {0}")]
    DeviceNotFound(uuid::Uuid),
    
    #[error("Connection failed: {0}")]
    ConnectionFailed(String),
    
    #[error("Authentication failed: {0}")]
    AuthenticationFailed(String),
    
    #[error("Timeout: {0}")]
    Timeout(String),
    
    #[error("IO error: {0}")]
    Io(#[from] std::io::Error),
    
    #[error("Serialization error: {0}")]
    Serialization(#[from] serde_json::Error),
    
    #[error("Transport error: {0}")]
    Transport(String),
}

pub type Result<T> = std::result::Result<T, NetworkingError>;

/// Initialize the new networking system
pub async fn init_networking(
    device_manager: std::sync::Arc<crate::device::DeviceManager>,
    library_key_manager: std::sync::Arc<crate::keys::library_key_manager::LibraryKeyManager>,
    data_dir: impl AsRef<std::path::Path>,
) -> Result<NetworkingService> {
    let logger = std::sync::Arc::new(utils::logging::ConsoleLogger);
    NetworkingService::new(device_manager, library_key_manager, data_dir, logger).await
}```

## src/services/networking/device/registry.rs

```rust
//! Device registry for centralized state management

use super::{DeviceConnection, DeviceInfo, DeviceState, DevicePersistence, PersistedPairedDevice, SessionKeys, TrustLevel};
use crate::device::DeviceManager;
use crate::services::networking::{NetworkingError, Result, utils::logging::NetworkLogger};
use chrono::{DateTime, Utc};
use iroh::net::NodeAddr;
use iroh::net::key::NodeId;
use std::collections::HashMap;
use std::path::Path;
use std::sync::Arc;
use uuid::Uuid;

/// Central registry for all device state and connections
pub struct DeviceRegistry {
	/// Reference to the device manager for local device info
	device_manager: Arc<DeviceManager>,

	/// Map of device ID to current state
	devices: HashMap<Uuid, DeviceState>,

	/// Map of node ID to device ID for quick lookup
	node_to_device: HashMap<NodeId, Uuid>,

	/// Map of session ID to device ID for pairing lookup
	session_to_device: HashMap<Uuid, Uuid>,

	/// Persistence manager for paired devices
	persistence: DevicePersistence,

	/// Logger for device registry operations
	logger: Arc<dyn NetworkLogger>,
}

impl DeviceRegistry {
	/// Create a new device registry
	pub fn new(device_manager: Arc<DeviceManager>, data_dir: impl AsRef<Path>, logger: Arc<dyn NetworkLogger>) -> Result<Self> {
		let persistence = DevicePersistence::new(data_dir)?;
		
		Ok(Self {
			device_manager,
			devices: HashMap::new(),
			node_to_device: HashMap::new(),
			session_to_device: HashMap::new(),
			persistence,
			logger,
		})
	}

	/// Load paired devices from persistence on startup
	pub async fn load_paired_devices(&mut self) -> Result<Vec<Uuid>> {
		let paired_devices = self.persistence.load_paired_devices().await?;
		let mut loaded_device_ids = Vec::new();

		for (device_id, persisted_device) in paired_devices {
			// Add device to registry in Paired state
			let state = DeviceState::Paired {
				info: persisted_device.device_info.clone(),
				session_keys: persisted_device.session_keys.clone(),
				paired_at: persisted_device.paired_at,
			};

			self.devices.insert(device_id, state);
			loaded_device_ids.push(device_id);
		}

		Ok(loaded_device_ids)
	}

	/// Get devices that should auto-reconnect
	pub async fn get_auto_reconnect_devices(&self) -> Result<Vec<(Uuid, PersistedPairedDevice)>> {
		self.persistence.get_auto_reconnect_devices().await
	}

	/// Add a discovered node
	pub fn add_discovered_node(
		&mut self,
		device_id: Uuid,
		node_id: NodeId,
		node_addr: NodeAddr,
	) {
		let state = DeviceState::Discovered {
			node_id,
			node_addr,
			discovered_at: Utc::now(),
		};

		self.devices.insert(device_id, state);
		self.node_to_device.insert(node_id, device_id);
	}

	/// Start pairing process for a device
	pub fn start_pairing(
		&mut self,
		device_id: Uuid,
		node_id: NodeId,
		session_id: Uuid,
	) -> Result<()> {
		let state = DeviceState::Pairing {
			node_id,
			session_id,
			started_at: Utc::now(),
		};

		self.devices.insert(device_id, state);
		self.node_to_device.insert(node_id, device_id);
		self.session_to_device.insert(session_id, device_id);

		Ok(())
	}

	pub fn get_device_name(&self) -> Result<String> {
		let config = self.device_manager.config().map_err(|e| {
			NetworkingError::Protocol(format!("Failed to get device config: {}", e))
		})?;
		Ok(config.name)
	}

	/// Complete pairing for a device
	pub async fn complete_pairing(
		&mut self,
		device_id: Uuid,
		info: DeviceInfo,
		session_keys: SessionKeys,
	) -> Result<()> {
		// Parse node ID from network fingerprint
		let addresses = if let Ok(node_id) = info.network_fingerprint.node_id.parse::<NodeId>() {
			// Add node-to-device mapping so device can be found for messaging
			self.node_to_device.insert(node_id, device_id);
			// Successfully added node-to-device mapping: node_id -> device_id
			
			// Get current addresses from discovered state if available
			match self.devices.get(&device_id) {
				Some(DeviceState::Discovered { node_addr, .. }) => {
					// Convert NodeAddr to string addresses
					node_addr.direct_addresses().map(|addr| addr.to_string()).collect()
				}
				Some(DeviceState::Pairing { .. }) => {
					vec![] // Pairing state doesn't have addresses
				}
				_ => vec![]
			}
		} else {
			// Warning: Failed to parse node ID from network fingerprint
			vec![]
		};

		let state = DeviceState::Paired {
			info: info.clone(),
			session_keys: session_keys.clone(),
			paired_at: Utc::now(),
		};

		self.devices.insert(device_id, state);

		// Persist the paired device for future reconnection
		if let Err(e) = self.persistence.add_paired_device(device_id, info.clone(), session_keys.clone(), addresses).await {
			self.logger.warn(&format!("âš ï¸ Failed to persist paired device {}: {}", device_id, e)).await;
			// Continue anyway - pairing succeeded even if persistence failed
		} else {
			self.logger.debug(&format!("âœ… Persisted paired device: {}", device_id)).await;
		}

		Ok(())
	}

	/// Mark device as connected
	pub async fn mark_connected(&mut self, device_id: Uuid, connection: DeviceConnection) -> Result<()> {
		let current_state = self
			.devices
			.get(&device_id)
			.ok_or_else(|| NetworkingError::DeviceNotFound(device_id))?;

		let (info, session_keys): (DeviceInfo, super::SessionKeys) = match current_state {
			DeviceState::Paired { info, session_keys, .. } => (info.clone(), session_keys.clone()),
			DeviceState::Disconnected { info, .. } => {
				// For disconnected devices, we need to find their session keys from a previous state
				// This is a limitation - we should store session keys with disconnected devices too
				return Err(NetworkingError::Protocol(
					"Cannot connect disconnected device without session keys".to_string(),
				));
			}
			DeviceState::Discovered { node_id, .. } => {
				// Need device info - this shouldn't happen normally
				return Err(NetworkingError::Protocol(
					"Cannot connect to unpaired device".to_string(),
				));
			}
			DeviceState::Connected { .. } => {
				return Err(NetworkingError::Protocol(
					"Device already connected".to_string(),
				));
			}
			DeviceState::Pairing { .. } => {
				return Err(NetworkingError::Protocol(
					"Device still pairing".to_string(),
				));
			}
		};

		let state = DeviceState::Connected {
			info,
			connection,
			session_keys,
			connected_at: Utc::now(),
		};

		self.devices.insert(device_id, state);

		// Update persistence - device connected successfully
		if let Err(e) = self.persistence.update_device_connection(device_id, true, None).await {
			self.logger.warn(&format!("âš ï¸ Failed to update device connection status {}: {}", device_id, e)).await;
		}

		Ok(())
	}

	/// Mark device as disconnected
	pub async fn mark_disconnected(
		&mut self,
		device_id: Uuid,
		reason: super::DisconnectionReason,
	) -> Result<()> {
		let current_state = self
			.devices
			.get(&device_id)
			.ok_or_else(|| NetworkingError::DeviceNotFound(device_id))?;

		let info = match current_state {
			DeviceState::Connected { info, .. } => info.clone(),
			DeviceState::Paired { info, .. } => info.clone(),
			_ => {
				return Err(NetworkingError::Protocol(
					"Cannot disconnect device that isn't connected".to_string(),
				));
			}
		};

		let state = DeviceState::Disconnected {
			info,
			last_seen: Utc::now(),
			reason,
		};

		self.devices.insert(device_id, state);

		// Update persistence - device disconnected
		if let Err(e) = self.persistence.update_device_connection(device_id, false, None).await {
			self.logger.warn(&format!("âš ï¸ Failed to update device disconnection status {}: {}", device_id, e)).await;
		}

		Ok(())
	}

	/// Get device state by device ID
	pub fn get_device_state(&self, device_id: Uuid) -> Option<&DeviceState> {
		self.devices.get(&device_id)
	}

	/// Get device ID by peer ID
	pub fn get_device_by_node(&self, node_id: NodeId) -> Option<Uuid> {
		self.node_to_device.get(&node_id).copied()
	}

	/// Get device ID by session ID
	pub fn get_device_by_session(&self, session_id: Uuid) -> Option<Uuid> {
		self.session_to_device.get(&session_id).copied()
	}

	/// Get all connected devices
	pub fn get_connected_devices(&self) -> Vec<DeviceInfo> {
		self.devices
			.values()
			.filter_map(|state| match state {
				DeviceState::Connected { info, .. } => Some(info.clone()),
				_ => None,
			})
			.collect()
	}

	/// Get all paired devices (including disconnected)
	pub fn get_paired_devices(&self) -> Vec<DeviceInfo> {
		self.devices
			.values()
			.filter_map(|state| match state {
				DeviceState::Paired { info, .. } => Some(info.clone()),
				DeviceState::Connected { info, .. } => Some(info.clone()),
				DeviceState::Disconnected { info, .. } => Some(info.clone()),
				_ => None,
			})
			.collect()
	}

	/// Remove a device from the registry
	pub fn remove_device(&mut self, device_id: Uuid) -> Result<()> {
		if let Some(state) = self.devices.remove(&device_id) {
			// Clean up mappings
			match &state {
				DeviceState::Discovered { node_id, .. } | DeviceState::Pairing { node_id, .. } => {
					self.node_to_device.remove(node_id);
				}
				DeviceState::Pairing { session_id, .. } => {
					self.session_to_device.remove(session_id);
				}
				_ => {}
			}
		}

		Ok(())
	}

	/// Get peer ID for a device
	pub fn get_node_by_device(&self, device_id: Uuid) -> Option<NodeId> {
		// Look through node_to_device map in reverse
		for (node_id, &dev_id) in &self.node_to_device {
			if dev_id == device_id {
				// Found node for device
				return Some(*node_id);
			}
		}
		// No peer found for device - check node_to_device mappings
		None
	}

	/// Get node ID for a device (alias for get_node_by_device)
	pub fn get_node_id_for_device(&self, device_id: Uuid) -> Option<NodeId> {
		self.get_node_by_device(device_id)
	}

	/// Get session keys for a device
	pub fn get_session_keys(&self, device_id: Uuid) -> Option<super::SessionKeys> {
		match self.devices.get(&device_id) {
			Some(DeviceState::Paired { session_keys, .. }) => {
				// Found session keys for paired device
				Some(session_keys.clone())
			}
			Some(DeviceState::Connected { session_keys, .. }) => {
				// Found session keys for connected device
				Some(session_keys.clone())
			}
			_ => {
				// Device not found or not paired/connected
				None
			}
		}
	}

	/// Get all currently connected peer IDs
	pub fn get_connected_nodes(&self) -> Vec<NodeId> {
		self.node_to_device.keys().cloned().collect()
	}


	/// Get our local device info
	pub fn get_local_device_info(&self) -> Result<DeviceInfo> {
		let device_id = self
			.device_manager
			.device_id()
			.map_err(|e| NetworkingError::Protocol(format!("Failed to get device ID: {}", e)))?;

		let config = self.device_manager.config().map_err(|e| {
			NetworkingError::Protocol(format!("Failed to get device config: {}", e))
		})?;
		let device_name = config.name;

		// TODO: Get actual values from device manager or system
		Ok(DeviceInfo {
			device_id,
			device_name,
			device_type: super::DeviceType::Desktop, // TODO: Detect actual device type
			os_version: std::env::consts::OS.to_string(),
			app_version: env!("CARGO_PKG_VERSION").to_string(),
			network_fingerprint:
				crate::services::networking::utils::identity::NetworkFingerprint {
					node_id: "placeholder".to_string(), // Will be filled in by caller
					public_key_hash: "placeholder".to_string(),
				},
			last_seen: Utc::now(),
		})
	}

	/// Clean up expired sessions and old disconnected devices
	pub fn cleanup_expired(&mut self) {
		let now = Utc::now();
		let mut to_remove = Vec::new();
		let mut session_mappings_to_remove = Vec::new();

		for (device_id, state) in &self.devices {
			match state {
				DeviceState::Pairing { started_at, session_id, .. } => {
					// Remove pairing sessions older than 10 minutes
					if now.signed_duration_since(*started_at).num_minutes() > 10 {
						to_remove.push(*device_id);
						session_mappings_to_remove.push(*session_id);
					}
				}
				DeviceState::Paired { paired_at, .. } => {
					// Remove session mappings for paired devices older than 1 hour
					// (pairing completed successfully, no need to keep session mapping)
					if now.signed_duration_since(*paired_at).num_hours() > 1 {
						// Find session ID to remove
						for (session_id, &dev_id) in &self.session_to_device {
							if dev_id == *device_id {
								session_mappings_to_remove.push(*session_id);
							}
						}
					}
				}
				DeviceState::Connected { .. } => {
					// Remove session mappings for connected devices
					// (no longer needed for active connections)
					for (session_id, &dev_id) in &self.session_to_device {
						if dev_id == *device_id {
							session_mappings_to_remove.push(*session_id);
						}
					}
				}
				DeviceState::Disconnected { last_seen, .. } => {
					// Remove disconnected devices older than 7 days
					if now.signed_duration_since(*last_seen).num_days() > 7 {
						to_remove.push(*device_id);
					}
				}
				_ => {}
			}
		}

		// Remove expired devices
		for device_id in to_remove {
			let _ = self.remove_device(device_id);
		}
		
		// Remove expired session mappings
		for session_id in session_mappings_to_remove {
			self.session_to_device.remove(&session_id);
		}
	}

	/// Set a device as connected with its node ID
	pub fn set_device_connected(&mut self, device_id: Uuid, node_id: NodeId) -> Result<()> {
		// Update the node_to_device mapping
		self.node_to_device.insert(node_id, device_id);
		
		// Get the current device state to preserve info
		if let Some(current_state) = self.devices.get(&device_id) {
			match current_state {
				DeviceState::Paired { info, session_keys, .. } => {
					let state = DeviceState::Connected {
						info: info.clone(),
						session_keys: session_keys.clone(),
						connected_at: Utc::now(),
						connection: DeviceConnection {
							addresses: Vec::new(), // Will be updated with actual addresses
							latency_ms: None,
							rx_bytes: 0,
							tx_bytes: 0,
						},
					};
					self.devices.insert(device_id, state);
				}
				_ => {
					return Err(NetworkingError::Protocol(
						"Device must be paired before connecting".to_string(),
					));
				}
			}
		} else {
			return Err(NetworkingError::DeviceNotFound(device_id));
		}
		
		Ok(())
	}
}
```

## src/services/networking/device/persistence.rs

```rust
//! Persistence for paired devices and their connection info

use super::{DeviceInfo, SessionKeys};
use crate::keys::device_key_manager::DeviceKeyManager;
use crate::services::networking::{NetworkingError, Result};
use aes_gcm::{
    aead::{Aead, AeadCore, KeyInit, OsRng},
    Aes256Gcm, Key, Nonce,
};
use chrono::{DateTime, Utc};
use hkdf::Hkdf;
use serde::{Deserialize, Serialize};
use sha2::Sha256;
use std::collections::HashMap;
use std::path::{Path, PathBuf};
use tokio::fs;
use uuid::Uuid;

/// Persisted paired device data (plain data structure)
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PersistedPairedDevice {
    pub device_info: DeviceInfo,
    pub session_keys: SessionKeys,
    pub paired_at: DateTime<Utc>,
    pub last_connected_at: Option<DateTime<Utc>>,
    pub last_seen_addresses: Vec<String>, // String representation of Multiaddr
    pub connection_attempts: u32,
    pub trust_level: TrustLevel,
}

/// Encrypted device data for disk storage
#[derive(Debug, Serialize, Deserialize)]
struct EncryptedDeviceData {
    /// Encrypted device data using AES-256-GCM
    ciphertext: Vec<u8>,
    /// Nonce for AES-GCM encryption
    nonce: Vec<u8>,
    /// Salt used for key derivation
    salt: Vec<u8>,
}

/// Trust level for persistent connections
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum TrustLevel {
    /// Device was manually paired and should auto-reconnect
    Trusted,
    /// Device connection failed multiple times, deprioritize
    Unreliable,
    /// Device manually disconnected, don't auto-reconnect
    Blocked,
}

impl Default for TrustLevel {
    fn default() -> Self {
        Self::Trusted
    }
}

/// Collection of all paired devices (encrypted on disk)
#[derive(Debug, Serialize, Deserialize)]
struct PersistedPairedDevices {
    devices: HashMap<Uuid, EncryptedDeviceData>,
    last_saved: DateTime<Utc>,
    /// Version for future migration support
    version: u32,
}

/// Device persistence manager
pub struct DevicePersistence {
    data_dir: PathBuf,
    devices_file: PathBuf,
    device_key_manager: DeviceKeyManager,
}

impl DevicePersistence {
    /// Create a new device persistence manager
    pub fn new(data_dir: impl AsRef<Path>) -> Result<Self> {
        let data_dir = data_dir.as_ref().to_path_buf();
        let networking_dir = data_dir.join("networking");
        let devices_file = networking_dir.join("paired_devices.json");
        
        let device_key_manager = DeviceKeyManager::new()
            .map_err(|e| NetworkingError::Protocol(format!("Failed to initialize master key manager: {}", e)))?;

        Ok(Self {
            data_dir: networking_dir,
            devices_file,
            device_key_manager,
        })
    }

    #[cfg(test)]
    /// Create a test persistence manager with a fixed key (for testing only)
    pub fn new_for_test(data_dir: impl AsRef<Path>) -> Result<Self> {
        let data_dir = data_dir.as_ref().to_path_buf();
        let networking_dir = data_dir.join("networking");
        let devices_file = networking_dir.join("paired_devices.json");
        
        const TEST_SERVICE: &str = "SpacedriveTest";
        const TEST_USERNAME: &str = "test_device_persistence";
        
        // Clean up any existing test key
        if let Ok(entry) = keyring::Entry::new(TEST_SERVICE, TEST_USERNAME) {
            let _ = entry.delete_credential();
        }
        
        // Create test master key manager and ensure it has a key
        let device_key_manager = DeviceKeyManager::new_for_test(TEST_SERVICE, TEST_USERNAME)
            .map_err(|e| NetworkingError::Protocol(format!("Failed to create test master key manager: {}", e)))?;
        
        // Ensure the test key exists
        let _ = device_key_manager.get_or_create_master_key()
            .map_err(|e| NetworkingError::Protocol(format!("Failed to create test master key: {}", e)))?;

        Ok(Self {
            data_dir: networking_dir,
            devices_file,
            device_key_manager,
        })
    }

    /// Derive encryption key from master key for device persistence
    fn derive_encryption_key(&self, salt: &[u8]) -> Result<[u8; 32]> {
        let master_key = self.device_key_manager.get_or_create_master_key()
            .map_err(|e| NetworkingError::Protocol(format!("Failed to get or create master key: {}", e)))?;
        
        let hk = Hkdf::<Sha256>::new(Some(salt), &master_key);
        let mut derived_key = [0u8; 32];
        hk.expand(b"spacedrive-device-persistence", &mut derived_key)
            .map_err(|e| NetworkingError::Protocol(format!("Key derivation failed: {}", e)))?;
        
        Ok(derived_key)
    }

    /// Encrypt device data using master key-derived encryption key
    fn encrypt_device_data(&self, device: &PersistedPairedDevice) -> Result<EncryptedDeviceData> {
        // Generate random salt for key derivation
        let mut salt = [0u8; 32];
        aes_gcm::aead::rand_core::RngCore::fill_bytes(&mut OsRng, &mut salt);
        
        // Derive encryption key
        let encryption_key = self.derive_encryption_key(&salt)?;
        let key = Key::<Aes256Gcm>::from_slice(&encryption_key);
        let cipher = Aes256Gcm::new(key);
        
        // Generate nonce
        let nonce = Aes256Gcm::generate_nonce(&mut OsRng);
        
        // Serialize and encrypt device data
        let plaintext = serde_json::to_vec(device)
            .map_err(|e| NetworkingError::Serialization(e))?;
        
        let ciphertext = cipher.encrypt(&nonce, plaintext.as_ref())
            .map_err(|e| NetworkingError::Protocol(format!("Encryption failed: {}", e)))?;
        
        Ok(EncryptedDeviceData {
            ciphertext,
            nonce: nonce.to_vec(),
            salt: salt.to_vec(),
        })
    }

    /// Decrypt device data using master key-derived encryption key
    fn decrypt_device_data(&self, encrypted_data: &EncryptedDeviceData) -> Result<PersistedPairedDevice> {
        // Derive encryption key using stored salt
        let encryption_key = self.derive_encryption_key(&encrypted_data.salt)?;
        let key = Key::<Aes256Gcm>::from_slice(&encryption_key);
        let cipher = Aes256Gcm::new(key);
        
        // Decrypt data
        let nonce = Nonce::from_slice(&encrypted_data.nonce);
        let plaintext = cipher.decrypt(nonce, encrypted_data.ciphertext.as_ref())
            .map_err(|e| NetworkingError::Protocol(format!("Decryption failed: {}", e)))?;
        
        // Deserialize device data
        let device: PersistedPairedDevice = serde_json::from_slice(&plaintext)
            .map_err(|e| NetworkingError::Serialization(e))?;
        
        Ok(device)
    }

    /// Save paired devices to disk (encrypted)
    pub async fn save_paired_devices(&self, devices: &HashMap<Uuid, PersistedPairedDevice>) -> Result<()> {
        // Ensure data directory exists
        if let Some(parent) = self.devices_file.parent() {
            fs::create_dir_all(parent).await.map_err(NetworkingError::Io)?;
        }

        // Encrypt each device individually
        let mut encrypted_devices = HashMap::new();
        for (device_id, device) in devices {
            let encrypted_data = self.encrypt_device_data(device)?;
            encrypted_devices.insert(*device_id, encrypted_data);
        }

        let persisted = PersistedPairedDevices {
            devices: encrypted_devices,
            last_saved: Utc::now(),
            version: 1, // Current version
        };

        // Write to temporary file first, then rename for atomic operation
        let temp_file = self.devices_file.with_extension("tmp");
        let json_data = serde_json::to_string_pretty(&persisted).map_err(|e| {
            NetworkingError::Serialization(e)
        })?;

        fs::write(&temp_file, json_data).await.map_err(NetworkingError::Io)?;
        fs::rename(&temp_file, &self.devices_file).await.map_err(NetworkingError::Io)?;

        println!("ðŸ” Saved {} paired devices (encrypted)", devices.len());
        Ok(())
    }

    /// Load paired devices from disk (decrypt)
    pub async fn load_paired_devices(&self) -> Result<HashMap<Uuid, PersistedPairedDevice>> {
        if !self.devices_file.exists() {
            return Ok(HashMap::new());
        }

        let json_data = fs::read_to_string(&self.devices_file).await.map_err(NetworkingError::Io)?;

        let persisted: PersistedPairedDevices = serde_json::from_str(&json_data).map_err(|e| {
            NetworkingError::Serialization(e)
        })?;

        // Check version compatibility
        if persisted.version != 1 {
            return Err(NetworkingError::Protocol(format!(
                "Unsupported device persistence version: {}",
                persisted.version
            )));
        }

        // Decrypt each device individually
        let mut devices = HashMap::new();
        for (device_id, encrypted_data) in persisted.devices {
            match self.decrypt_device_data(&encrypted_data) {
                Ok(device) => {
                    // Filter out devices with expired session keys
                    if !device.session_keys.is_expired() {
                        devices.insert(device_id, device);
                    }
                }
                Err(e) => {
                    eprintln!("âš ï¸ Failed to decrypt device {}: {}", device_id, e);
                    // Continue loading other devices even if one fails
                }
            }
        }

        println!("ðŸ”“ Loaded {} paired devices (decrypted)", devices.len());
        Ok(devices)
    }

    /// Add a newly paired device
    pub async fn add_paired_device(
        &self,
        device_id: Uuid,
        device_info: DeviceInfo,
        session_keys: SessionKeys,
        addresses: Vec<String>,
    ) -> Result<()> {
        let mut devices = self.load_paired_devices().await?;

        let paired_device = PersistedPairedDevice {
            device_info,
            session_keys,
            paired_at: Utc::now(),
            last_connected_at: None,
            last_seen_addresses: addresses,
            connection_attempts: 0,
            trust_level: TrustLevel::Trusted,
        };

        devices.insert(device_id, paired_device);
        self.save_paired_devices(&devices).await?;

        Ok(())
    }

    /// Update connection info for a device
    pub async fn update_device_connection(
        &self,
        device_id: Uuid,
        connected: bool,
        addresses: Option<Vec<String>>,
    ) -> Result<()> {
        let mut devices = self.load_paired_devices().await?;

        if let Some(device) = devices.get_mut(&device_id) {
            if connected {
                device.last_connected_at = Some(Utc::now());
                device.connection_attempts = 0; // Reset on successful connection
            } else {
                device.connection_attempts += 1;
                
                // Mark as unreliable after 5 failed attempts
                if device.connection_attempts >= 5 {
                    device.trust_level = TrustLevel::Unreliable;
                }
            }

            if let Some(addrs) = addresses {
                device.last_seen_addresses = addrs;
            }

            self.save_paired_devices(&devices).await?;
        }

        Ok(())
    }

    /// Remove a paired device
    pub async fn remove_paired_device(&self, device_id: Uuid) -> Result<bool> {
        let mut devices = self.load_paired_devices().await?;
        let removed = devices.remove(&device_id).is_some();
        
        if removed {
            self.save_paired_devices(&devices).await?;
        }

        Ok(removed)
    }

    /// Set device trust level
    pub async fn set_device_trust_level(&self, device_id: Uuid, trust_level: TrustLevel) -> Result<()> {
        let mut devices = self.load_paired_devices().await?;

        if let Some(device) = devices.get_mut(&device_id) {
            device.trust_level = trust_level;
            self.save_paired_devices(&devices).await?;
        }

        Ok(())
    }

    /// Get devices that should auto-reconnect
    pub async fn get_auto_reconnect_devices(&self) -> Result<Vec<(Uuid, PersistedPairedDevice)>> {
        let devices = self.load_paired_devices().await?;

        let auto_reconnect: Vec<(Uuid, PersistedPairedDevice)> = devices
            .into_iter()
            .filter(|(_, device)| {
                matches!(device.trust_level, TrustLevel::Trusted) && !device.session_keys.is_expired()
            })
            .collect();

        Ok(auto_reconnect)
    }

    /// Clean up expired devices
    pub async fn cleanup_expired_devices(&self) -> Result<usize> {
        let initial_devices = self.load_paired_devices().await?;
        let initial_count = initial_devices.len();

        // The load_paired_devices method already filters out expired devices
        // Just save the filtered result
        self.save_paired_devices(&initial_devices).await?;

        Ok(initial_count - initial_devices.len())
    }

    /// Clear all paired devices
    pub async fn clear_all_devices(&self) -> Result<()> {
        if self.devices_file.exists() {
            fs::remove_file(&self.devices_file).await.map_err(NetworkingError::Io)?;
        }
        Ok(())
    }

    /// Get file path
    pub fn devices_file_path(&self) -> &Path {
        &self.devices_file
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::services::networking::utils::identity::NetworkFingerprint;
    use tempfile::TempDir;

    async fn create_test_persistence() -> (DevicePersistence, TempDir) {
        let temp_dir = TempDir::new().expect("Failed to create temp dir");
        let persistence = DevicePersistence::new_for_test(temp_dir.path()).expect("Failed to create test persistence");
        (persistence, temp_dir)
    }

    fn create_test_device_info() -> DeviceInfo {
        DeviceInfo {
            device_id: Uuid::new_v4(),
            device_name: "Test Device".to_string(),
            device_type: super::super::DeviceType::Desktop,
            os_version: "Test OS 1.0".to_string(),
            app_version: "1.0.0".to_string(),
            network_fingerprint: NetworkFingerprint {
                node_id: "test_node_id".to_string(),
                public_key_hash: "test_hash".to_string(),
            },
            last_seen: Utc::now(),
        }
    }

    #[tokio::test]
    async fn test_add_and_load_paired_device() {
        let (persistence, _temp_dir) = create_test_persistence().await;

        let device_id = Uuid::new_v4();
        let device_info = create_test_device_info();
        let session_keys = SessionKeys::from_shared_secret(vec![1, 2, 3, 4]);
        let addresses = vec!["127.0.0.1:8080".to_string()];

        // Add paired device
        persistence
            .add_paired_device(device_id, device_info.clone(), session_keys.clone(), addresses.clone())
            .await
            .unwrap();

        // Load devices
        let devices = persistence.load_paired_devices().await.unwrap();

        assert_eq!(devices.len(), 1);
        assert!(devices.contains_key(&device_id));

        let loaded_device = &devices[&device_id];
        assert_eq!(loaded_device.device_info.device_id, device_info.device_id);
        assert_eq!(loaded_device.last_seen_addresses, addresses);
        assert!(matches!(loaded_device.trust_level, TrustLevel::Trusted));
    }

    #[tokio::test]
    async fn test_auto_reconnect_devices() {
        let (persistence, _temp_dir) = create_test_persistence().await;

        let device_id = Uuid::new_v4();
        let device_info = create_test_device_info();
        let session_keys = SessionKeys::from_shared_secret(vec![1, 2, 3, 4]);

        persistence
            .add_paired_device(device_id, device_info, session_keys, vec![])
            .await
            .unwrap();

        let auto_reconnect = persistence.get_auto_reconnect_devices().await.unwrap();
        assert_eq!(auto_reconnect.len(), 1);
        assert_eq!(auto_reconnect[0].0, device_id);
    }

    #[tokio::test]
    async fn test_trust_level_management() {
        let (persistence, _temp_dir) = create_test_persistence().await;

        let device_id = Uuid::new_v4();
        let device_info = create_test_device_info();
        let session_keys = SessionKeys::from_shared_secret(vec![1, 2, 3, 4]);

        persistence
            .add_paired_device(device_id, device_info, session_keys, vec![])
            .await
            .unwrap();

        // Block the device
        persistence
            .set_device_trust_level(device_id, TrustLevel::Blocked)
            .await
            .unwrap();

        // Should not appear in auto-reconnect list
        let auto_reconnect = persistence.get_auto_reconnect_devices().await.unwrap();
        assert_eq!(auto_reconnect.len(), 0);
    }

    #[tokio::test]
    async fn test_encryption_decryption() {
        let (persistence, _temp_dir) = create_test_persistence().await;

        let device_id = Uuid::new_v4();
        let device_info = create_test_device_info();
        let session_keys = SessionKeys::from_shared_secret(vec![1, 2, 3, 4]);

        // Add device (this will encrypt and save)
        persistence
            .add_paired_device(device_id, device_info.clone(), session_keys.clone(), vec!["127.0.0.1:8080".to_string()])
            .await
            .unwrap();

        // Load devices (this will decrypt)
        let loaded_devices = persistence.load_paired_devices().await.unwrap();

        assert_eq!(loaded_devices.len(), 1);
        assert!(loaded_devices.contains_key(&device_id));

        let loaded_device = &loaded_devices[&device_id];
        assert_eq!(loaded_device.device_info.device_id, device_info.device_id);
        assert_eq!(loaded_device.session_keys.shared_secret, session_keys.shared_secret);
    }

    #[tokio::test]
    async fn test_file_encryption_format() {
        let (persistence, temp_dir) = create_test_persistence().await;

        let device_id = Uuid::new_v4();
        let device_info = create_test_device_info();
        let session_keys = SessionKeys::from_shared_secret(vec![1, 2, 3, 4]);

        // Add device
        persistence
            .add_paired_device(device_id, device_info, session_keys, vec![])
            .await
            .unwrap();

        // Read the raw file - it should be encrypted (not contain plaintext session keys)
        let file_path = temp_dir.path().join("networking").join("paired_devices.json");
        let raw_content = tokio::fs::read_to_string(&file_path).await.unwrap();
        
        println!("Raw file content: {}", raw_content);
        
        // The file should not contain the plaintext session key bytes
        assert!(!raw_content.contains("\"shared_secret\":[1,2,3,4]"));
        
        // But should contain encrypted structure
        assert!(raw_content.contains("\"ciphertext\""));
        assert!(raw_content.contains("\"nonce\""));
        assert!(raw_content.contains("\"salt\""));
        assert!(raw_content.contains("\"version\": 1"));

        println!("âœ… Device data is properly encrypted on disk");
    }
}```

## src/services/networking/device/mod.rs

```rust
//! Device registry and connection management

pub mod connection;
pub mod persistence;
pub mod registry;

use chrono::{DateTime, Utc};
use iroh::net::NodeAddr;
use iroh::net::key::NodeId;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use uuid::Uuid;

// Note: The connection module has a more complex DeviceConnection for active connections
// This simpler one is used in DeviceState
#[derive(Debug, Clone)]
pub struct DeviceConnection {
	pub addresses: Vec<String>,  // Node addresses as strings
	pub latency_ms: Option<u32>,
	pub rx_bytes: u64,
	pub tx_bytes: u64,
}
pub use persistence::{DevicePersistence, PersistedPairedDevice, TrustLevel};
pub use registry::DeviceRegistry;

/// Information about a device on the network
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DeviceInfo {
	pub device_id: Uuid,
	pub device_name: String,
	pub device_type: DeviceType,
	pub os_version: String,
	pub app_version: String,
	pub network_fingerprint: crate::services::networking::utils::identity::NetworkFingerprint,
	pub last_seen: DateTime<Utc>,
}

/// Type of device
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum DeviceType {
	Desktop,
	Laptop,
	Mobile,
	Server,
	Other(String),
}

impl Default for DeviceType {
	fn default() -> Self {
		Self::Desktop
	}
}

/// State of a device in the registry
#[derive(Debug, Clone)]
pub enum DeviceState {
	/// Device discovered via Iroh discovery but not yet connected
	Discovered {
		node_id: NodeId,
		node_addr: NodeAddr,
		discovered_at: DateTime<Utc>,
	},
	/// Device currently in pairing process
	Pairing {
		node_id: NodeId,
		session_id: Uuid,
		started_at: DateTime<Utc>,
	},
	/// Device successfully paired but not currently connected
	Paired {
		info: DeviceInfo,
		session_keys: SessionKeys,
		paired_at: DateTime<Utc>,
	},
	/// Device currently connected and active
	Connected {
		info: DeviceInfo,
		connection: DeviceConnection,
		session_keys: SessionKeys,
		connected_at: DateTime<Utc>,
	},
	/// Device was connected but is now disconnected
	Disconnected {
		info: DeviceInfo,
		last_seen: DateTime<Utc>,
		reason: DisconnectionReason,
	},
}

/// Reason for disconnection
#[derive(Debug, Clone)]
pub enum DisconnectionReason {
	UserInitiated,
	NetworkError(String),
	Timeout,
	AuthenticationFailed,
	ProtocolError(String),
}

/// Session keys for encrypted communication
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SessionKeys {
	pub shared_secret: Vec<u8>,
	pub send_key: Vec<u8>,
	pub receive_key: Vec<u8>,
	pub created_at: DateTime<Utc>,
	pub expires_at: Option<DateTime<Utc>>,
}

impl SessionKeys {
	/// Generate new session keys from a shared secret
	pub fn from_shared_secret(shared_secret: Vec<u8>) -> Self {
		// Use HKDF to derive send/receive keys from shared secret
		use hkdf::Hkdf;
		use sha2::Sha256;

		let hk = Hkdf::<Sha256>::new(None, &shared_secret);
		let mut send_key = [0u8; 32];
		let mut receive_key = [0u8; 32];

		// Use the same salt for both keys to ensure initiator's send key 
		// matches joiner's receive key, enabling successful decryption
		hk.expand(b"spacedrive-symmetric-key", &mut send_key).unwrap();
		hk.expand(b"spacedrive-symmetric-key", &mut receive_key).unwrap();

		Self {
			shared_secret,
			send_key: send_key.to_vec(),
			receive_key: receive_key.to_vec(),
			created_at: Utc::now(),
			expires_at: Some(Utc::now() + chrono::Duration::hours(24)), // 24 hour expiry
		}
	}

	/// Check if keys are expired
	pub fn is_expired(&self) -> bool {
		if let Some(expires_at) = self.expires_at {
			Utc::now() > expires_at
		} else {
			false
		}
	}
}
```

## src/services/networking/device/connection.rs

```rust
//! Individual device connection handling

use super::{DeviceInfo, SessionKeys};
use crate::services::networking::{NetworkingError, Result};
use chrono::{DateTime, Utc};
use iroh::net::key::NodeId;
use std::sync::Arc;
use tokio::sync::{mpsc, RwLock};
use uuid::Uuid;

/// Represents an active connection to a remote device
#[derive(Debug, Clone)]
pub struct DeviceConnection {
	/// The node ID of the remote device
	pub node_id: NodeId,

	/// Device information
	pub device_info: DeviceInfo,

	/// Session keys for encryption
	pub session_keys: SessionKeys,

	/// Connection statistics
	pub stats: ConnectionStats,

	/// Channel for sending messages to this device
	pub message_sender: mpsc::UnboundedSender<OutgoingMessage>,
}

/// Statistics about a connection
#[derive(Debug, Clone)]
pub struct ConnectionStats {
	pub connected_at: DateTime<Utc>,
	pub bytes_sent: u64,
	pub bytes_received: u64,
	pub messages_sent: u64,
	pub messages_received: u64,
	pub last_activity: DateTime<Utc>,
}

impl Default for ConnectionStats {
	fn default() -> Self {
		let now = Utc::now();
		Self {
			connected_at: now,
			bytes_sent: 0,
			bytes_received: 0,
			messages_sent: 0,
			messages_received: 0,
			last_activity: now,
		}
	}
}

/// Message to be sent to a remote device
#[derive(Debug)]
pub struct OutgoingMessage {
	pub protocol: String,
	pub data: Vec<u8>,
	pub response_channel: Option<tokio::sync::oneshot::Sender<Result<Vec<u8>>>>,
}

impl DeviceConnection {
	/// Create a new device connection
	pub fn new(
		node_id: NodeId,
		device_info: DeviceInfo,
		session_keys: SessionKeys,
	) -> (Self, mpsc::UnboundedReceiver<OutgoingMessage>) {
		let (message_sender, message_receiver) = mpsc::unbounded_channel();

		let connection = Self {
			node_id,
			device_info,
			session_keys,
			stats: ConnectionStats::default(),
			message_sender,
		};

		(connection, message_receiver)
	}

	/// Send a message to this device
	pub async fn send_message(&self, protocol: &str, data: Vec<u8>) -> Result<()> {
		let message = OutgoingMessage {
			protocol: protocol.to_string(),
			data,
			response_channel: None,
		};

		self.message_sender
			.send(message)
			.map_err(|_| NetworkingError::ConnectionFailed("Connection closed".to_string()))?;

		Ok(())
	}

	/// Send a message and wait for a response
	pub async fn send_request(&self, protocol: &str, data: Vec<u8>) -> Result<Vec<u8>> {
		let (response_sender, response_receiver) = tokio::sync::oneshot::channel();

		let message = OutgoingMessage {
			protocol: protocol.to_string(),
			data,
			response_channel: Some(response_sender),
		};

		self.message_sender
			.send(message)
			.map_err(|_| NetworkingError::ConnectionFailed("Connection closed".to_string()))?;

		response_receiver
			.await
			.map_err(|_| NetworkingError::Timeout("Request timeout".to_string()))?
	}

	/// Check if the connection is still valid
	pub fn is_valid(&self) -> bool {
		!self.message_sender.is_closed() && !self.session_keys.is_expired()
	}

	/// Update connection statistics
	pub fn update_stats(&mut self, bytes_sent: u64, bytes_received: u64) {
		self.stats.bytes_sent += bytes_sent;
		self.stats.bytes_received += bytes_received;
		self.stats.last_activity = Utc::now();
	}

	/// Encrypt data using session keys
	pub fn encrypt(&self, data: &[u8]) -> Result<Vec<u8>> {
		// Simple XOR encryption for now - in production use proper AEAD
		let key = &self.session_keys.send_key;
		let mut encrypted = Vec::with_capacity(data.len());

		for (i, byte) in data.iter().enumerate() {
			encrypted.push(byte ^ key[i % key.len()]);
		}

		Ok(encrypted)
	}

	/// Decrypt data using session keys
	pub fn decrypt(&self, data: &[u8]) -> Result<Vec<u8>> {
		// Simple XOR decryption for now - in production use proper AEAD
		let key = &self.session_keys.receive_key;
		let mut decrypted = Vec::with_capacity(data.len());

		for (i, byte) in data.iter().enumerate() {
			decrypted.push(byte ^ key[i % key.len()]);
		}

		Ok(decrypted)
	}
}
```

## src/services/networking/protocols/pairing/types.rs

```rust
//! Pairing protocol types and state definitions

use crate::services::networking::{
    device::{DeviceInfo, SessionKeys},
    utils::identity::NetworkFingerprint,
};
use chrono::{DateTime, Utc};
use iroh::net::NodeAddr;
use iroh::net::key::NodeId;
use serde::{Deserialize, Serialize};
use uuid::Uuid;

/// Human-readable pairing code using BIP39 mnemonic words
#[derive(Debug, Clone)]
pub struct PairingCode {
    /// 256-bit cryptographic secret
    secret: [u8; 32],

    /// 12 words from BIP39 wordlist for user-friendly sharing
    words: [String; 12],

    /// Session ID derived from secret
    session_id: Uuid,

    /// Expiration timestamp
    expires_at: DateTime<Utc>,
}

impl PairingCode {
    /// Generate a new pairing code using BIP39 wordlist
    pub fn generate() -> crate::services::networking::Result<Self> {
        use rand::RngCore;

        let mut secret = [0u8; 32];
        rand::thread_rng().fill_bytes(&mut secret);

        // Convert secret to 12 BIP39 words using proper mnemonic encoding
        let words = Self::encode_to_bip39_words(&secret)?;

        // Derive session ID from secret
        let session_id = Self::derive_session_id(&secret);

        Ok(PairingCode {
            secret,
            words,
            session_id,
            expires_at: Utc::now() + chrono::Duration::minutes(5),
        })
    }

    /// Generate a pairing code from a session ID
    pub fn from_session_id(session_id: Uuid) -> Self {
        // Use the session ID as the BIP39 entropy source (16 bytes)
        let entropy = session_id.as_bytes();

        // Expand entropy to full 32-byte secret deterministically
        // This matches the logic in decode_from_bip39_words to ensure round-trip compatibility
        let mut hasher = blake3::Hasher::new();
        hasher.update(b"spacedrive-pairing-entropy-extension-v1");
        hasher.update(entropy);
        let derived_bytes = hasher.finalize();
        
        let mut secret = [0u8; 32];
        secret[..16].copy_from_slice(entropy);
        secret[16..].copy_from_slice(&derived_bytes.as_bytes()[..16]);

        // Generate BIP39 words from the entropy (first 16 bytes only, as per BIP39 standard)
        let words = Self::encode_to_bip39_words(&secret).unwrap_or_else(|_| {
            // Fallback to empty words if BIP39 fails
            [const { String::new() }; 12]
        });

        Self {
            secret,
            words,
            session_id, // Use the provided session_id directly
            expires_at: Utc::now() + chrono::Duration::minutes(5),
        }
    }

    /// Parse a pairing code from a BIP39 mnemonic string
    pub fn from_string(code: &str) -> crate::services::networking::Result<Self> {
        let words: Vec<String> = code.split_whitespace().map(|s| s.to_lowercase()).collect();

        if words.len() != 12 {
            return Err(crate::services::networking::NetworkingError::Protocol(
                "Invalid pairing code format - must be 12 BIP39 words".to_string(),
            ));
        }

        // Convert Vec to array
        let words_array: [String; 12] = words.try_into().map_err(|_| {
            crate::services::networking::NetworkingError::Protocol(
                "Failed to convert words to array".to_string(),
            )
        })?;

        Self::from_words(&words_array)
    }

    /// Create pairing code from BIP39 words
    pub fn from_words(words: &[String; 12]) -> crate::services::networking::Result<Self> {
        // Decode BIP39 words back to secret
        let secret = Self::decode_from_bip39_words(words)?;

        // Extract session ID directly from the first 16 bytes (entropy)
        let session_id = Uuid::from_bytes(secret[..16].try_into().map_err(|_| {
            crate::services::networking::NetworkingError::Protocol(
                "Failed to extract session ID from entropy".to_string(),
            )
        })?);

        Ok(PairingCode {
            secret,
            words: words.clone(),
            session_id,
            expires_at: Utc::now() + chrono::Duration::minutes(5),
        })
    }

    /// Get the session ID from this pairing code
    pub fn session_id(&self) -> Uuid {
        self.session_id
    }

    /// Get the cryptographic secret
    pub fn secret(&self) -> &[u8; 32] {
        &self.secret
    }

    /// Convert to display string
    pub fn to_string(&self) -> String {
        self.words.join(" ")
    }

    /// Check if the code has expired
    pub fn is_expired(&self) -> bool {
        Utc::now() > self.expires_at
    }

    /// Encode bytes to BIP39 words using proper mnemonic generation
    fn encode_to_bip39_words(secret: &[u8; 32]) -> crate::services::networking::Result<[String; 12]> {
        use bip39::{Language, Mnemonic};

        // For 12 words, we need 128 bits of entropy (standard BIP39)
        // Use the first 16 bytes from our 32-byte secret
        let entropy = &secret[..16];

        // Generate mnemonic from entropy
        let mnemonic = Mnemonic::from_entropy(entropy).map_err(|e| {
            crate::services::networking::NetworkingError::Protocol(format!(
                "BIP39 generation failed: {}",
                e
            ))
        })?;

        // Get the word list (should be exactly 12 words for 128 bits of entropy)
        let word_list: Vec<&str> = mnemonic.words().collect();

        if word_list.len() != 12 {
            return Err(crate::services::networking::NetworkingError::Protocol(
                format!("Expected 12 words, got {}", word_list.len()),
            ));
        }

        Ok([
            word_list[0].to_string(),
            word_list[1].to_string(),
            word_list[2].to_string(),
            word_list[3].to_string(),
            word_list[4].to_string(),
            word_list[5].to_string(),
            word_list[6].to_string(),
            word_list[7].to_string(),
            word_list[8].to_string(),
            word_list[9].to_string(),
            word_list[10].to_string(),
            word_list[11].to_string(),
        ])
    }

    /// Decode BIP39 words back to secret
    fn decode_from_bip39_words(words: &[String; 12]) -> crate::services::networking::Result<[u8; 32]> {
        use bip39::{Language, Mnemonic};

        // Join words with spaces to create mnemonic string
        let mnemonic_str = words.join(" ");

        // Parse the mnemonic
        let mnemonic = Mnemonic::parse_in(Language::English, &mnemonic_str).map_err(|e| {
            crate::services::networking::NetworkingError::Protocol(format!(
                "Invalid BIP39 mnemonic: {}",
                e
            ))
        })?;

        // Extract the entropy (should be 16 bytes for 12 words)
        let entropy = mnemonic.to_entropy();

        if entropy.len() != 16 {
            return Err(crate::services::networking::NetworkingError::Protocol(
                format!("Expected 16 bytes of entropy, got {}", entropy.len()),
            ));
        }

        // Reconstruct the full 32-byte secret
        // Use the 16 bytes of entropy and derive the remaining 16 bytes deterministically
        let mut full_secret = [0u8; 32];
        full_secret[..16].copy_from_slice(&entropy);

        // Derive the remaining 16 bytes using BLAKE3 for deterministic padding
        let mut hasher = blake3::Hasher::new();
        hasher.update(b"spacedrive-pairing-entropy-extension-v1");
        hasher.update(&entropy);
        let derived_bytes = hasher.finalize();
        full_secret[16..].copy_from_slice(&derived_bytes.as_bytes()[..16]);

        Ok(full_secret)
    }

    /// Derive session ID from secret
    fn derive_session_id(secret: &[u8; 32]) -> Uuid {
        // For pairing codes, derive session ID from the entropy that survives BIP39 round-trip
        // This ensures Initiator (who generates) and Joiner (who parses) get the same session ID
        // This is critical for DHT-based pairing where session IDs must match
        let hash = blake3::hash(&secret[..16]); // Use only the first 16 bytes (BIP39 entropy)
        let bytes = hash.as_bytes();

        let uuid_bytes = [
            bytes[0], bytes[1], bytes[2], bytes[3], bytes[4], bytes[5], bytes[6], bytes[7],
            bytes[8], bytes[9], bytes[10], bytes[11], bytes[12], bytes[13], bytes[14], bytes[15],
        ];

        Uuid::from_bytes(uuid_bytes)
    }
}

impl std::fmt::Display for PairingCode {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        write!(f, "{}", self.to_string())
    }
}

/// State of a pairing session
#[derive(Debug, Clone)]
pub struct PairingSession {
    pub id: Uuid,
    pub state: PairingState,
    pub remote_device_id: Option<Uuid>,
    pub remote_device_info: Option<DeviceInfo>,
    pub remote_public_key: Option<Vec<u8>>,
    pub shared_secret: Option<Vec<u8>>,
    pub created_at: DateTime<Utc>,
}

impl std::fmt::Display for PairingSession {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        write!(
            f,
            "PairingSession {{ id: {}, state: {}, remote_device_id: {:?}, shared_secret: {}, created_at: {} }}",
            self.id,
            self.state,
            self.remote_device_id,
            self.shared_secret.as_ref().map(|_| "[PRESENT]").unwrap_or("None"),
            self.created_at
        )
    }
}

/// States of the pairing process
#[derive(Debug, Clone)]
pub enum PairingState {
    Idle,
    GeneratingCode,
    Broadcasting,
    Scanning,
    WaitingForConnection,
    Connecting,
    Authenticating,
    ExchangingKeys,
    AwaitingConfirmation,
    EstablishingSession,
    ChallengeReceived {
        challenge: Vec<u8>,
    },
    ResponsePending {
        challenge: Vec<u8>,
        response_data: Vec<u8>,
        remote_node_id: Option<NodeId>,
    },
    ResponseSent,
    Completed,
    Failed {
        reason: String,
    },
}

impl std::fmt::Display for PairingState {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            PairingState::ResponsePending {
                challenge,
                response_data,
                ..
            } => {
                write!(
                    f,
                    "ResponsePending {{ challenge: [{}], response_data: [{}], .. }}",
                    if challenge.len() > 8 {
                        format!("{} items (truncated)", challenge.len())
                    } else {
                        challenge
                            .iter()
                            .map(|b| b.to_string())
                            .collect::<Vec<_>>()
                            .join(", ")
                    },
                    if response_data.len() > 8 {
                        format!("{} items (truncated)", response_data.len())
                    } else {
                        response_data
                            .iter()
                            .map(|b| b.to_string())
                            .collect::<Vec<_>>()
                            .join(", ")
                    }
                )
            }
            PairingState::ChallengeReceived { challenge } => {
                write!(
                    f,
                    "ChallengeReceived {{ challenge: [{}] }}",
                    if challenge.len() > 8 {
                        format!("{} items (truncated)", challenge.len())
                    } else {
                        challenge
                            .iter()
                            .map(|b| b.to_string())
                            .collect::<Vec<_>>()
                            .join(", ")
                    }
                )
            }
            _ => write!(f, "{:?}", self),
        }
    }
}

/// Role in the pairing process
#[derive(Debug, Clone, PartialEq)]
pub enum PairingRole {
    Initiator,
    Joiner,
}

/// Discovery advertisement for pairing session
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PairingAdvertisement {
    /// The node ID of the initiator (as string for serialization)
    pub node_id: String,
    /// The node address components for reconstruction
    pub node_addr_info: NodeAddrInfo,
    /// Device information of the initiator
    pub device_info: DeviceInfo,
    /// When this advertisement expires
    pub expires_at: DateTime<Utc>,
    /// When this advertisement was created
    pub created_at: DateTime<Utc>,
}

/// Serializable representation of NodeAddr
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct NodeAddrInfo {
    /// Node ID as string
    pub node_id: String,
    /// Direct socket addresses
    pub direct_addresses: Vec<String>,
    /// Relay URL if available
    pub relay_url: Option<String>,
}

impl PairingAdvertisement {
    /// Convert node ID string back to NodeId
    pub fn node_id(&self) -> crate::services::networking::Result<NodeId> {
        self.node_id.parse().map_err(|e| {
            crate::services::networking::NetworkingError::Protocol(format!(
                "Invalid node ID: {}",
                e
            ))
        })
    }

    /// Convert node address info back to NodeAddr
    pub fn node_addr(&self) -> crate::services::networking::Result<NodeAddr> {
        // Parse node ID
        let node_id = self.node_addr_info.node_id.parse::<NodeId>()
            .map_err(|e| crate::services::networking::NetworkingError::Protocol(
                format!("Invalid node ID in advertisement: {}", e)
            ))?;
        
        // Start with base NodeAddr
        let mut node_addr = NodeAddr::new(node_id);
        
        // Add direct addresses
        let mut direct_addrs = Vec::new();
        for addr_str in &self.node_addr_info.direct_addresses {
            if let Ok(addr) = addr_str.parse() {
                direct_addrs.push(addr);
            }
        }
        if !direct_addrs.is_empty() {
            node_addr = node_addr.with_direct_addresses(direct_addrs);
        }
        
        // Add relay URL if present
        if let Some(relay_url) = &self.node_addr_info.relay_url {
            if let Ok(url) = relay_url.parse() {
                node_addr = node_addr.with_relay_url(url);
            }
        }
        
        Ok(node_addr)
    }
}```

## src/services/networking/protocols/pairing/initiator.rs

```rust
//! Initiator-specific pairing logic

use super::{
    messages::PairingMessage,
    security::PairingSecurity,
    types::{PairingSession, PairingState},
    PairingProtocolHandler,
};
use crate::services::networking::{
    device::{DeviceInfo, SessionKeys},
    NetworkingError, Result,
};
use iroh::net::key::NodeId;
use uuid::Uuid;

impl PairingProtocolHandler {
    /// Handle an incoming pairing request (Initiator receives this from Joiner)
    pub(crate) async fn handle_pairing_request(
        &self,
        from_device: Uuid,
        session_id: Uuid,
        device_info: DeviceInfo,
        public_key: Vec<u8>,
    ) -> Result<Vec<u8>> {
        // Validate the public key format first
        super::security::PairingSecurity::validate_public_key(&public_key)?;
        self.log_info(&format!(
            "Received pairing request from device {} for session {}",
            from_device, session_id
        )).await;

        // Generate challenge
        let challenge = self.generate_challenge()?;
        self.log_debug(&format!(
            "Generated challenge of {} bytes for session {}",
            challenge.len(),
            session_id
        )).await;

        // Hold the write lock for the entire duration to prevent any scoping issues
        let mut sessions = self.active_sessions.write().await;
        self.log_debug(&format!("ðŸ” INITIATOR_HANDLER_DEBUG: Looking for session {} in {} total sessions", session_id, sessions.len())).await;
        
        if let Some(existing_session) = sessions.get_mut(&session_id) {
            self.log_debug(&format!("ðŸ” INITIATOR_HANDLER_DEBUG: Found existing session {} in state {:?}", session_id, existing_session.state)).await;
            self.log_debug(&format!("Transitioning existing session {} to ChallengeReceived", session_id)).await;
            
            // Update the existing session in place
            existing_session.state = PairingState::ChallengeReceived {
                challenge: challenge.clone(),
            };
            existing_session.remote_device_id = Some(from_device);
            existing_session.remote_device_info = Some(device_info.clone());
            existing_session.remote_public_key = Some(public_key.clone());
        } else {
            self.log_debug(&format!("ðŸ” INITIATOR_HANDLER_DEBUG: No existing session found for {}, creating new session", session_id)).await;
            self.log_debug(&format!("Creating new session {} for pairing request", session_id)).await;

            // Create new session only if none exists
            let session = PairingSession {
                id: session_id,
                state: PairingState::ChallengeReceived {
                    challenge: challenge.clone(),
                },
                remote_device_id: Some(from_device),
                remote_device_info: Some(device_info.clone()),
                remote_public_key: Some(public_key.clone()),
                shared_secret: None,
                created_at: chrono::Utc::now(),
            };

            sessions.insert(session_id, session);
        }
        // Write lock is automatically released here

        // Send challenge response with proper network fingerprint
        let local_device_info = self
            .get_device_info()
            .await
            .map_err(|e| {
                NetworkingError::Protocol(format!("Failed to get initiator device info: {}", e))
            })?;

        let response = PairingMessage::Challenge {
            session_id,
            challenge: challenge.clone(),
            device_info: local_device_info,
        };

        self.log_info(&format!(
            "Sending Challenge response for session {} with {} byte challenge",
            session_id,
            challenge.len()
        )).await;
        serde_json::to_vec(&response).map_err(|e| NetworkingError::Serialization(e))
    }

    /// Handle a pairing response (Initiator receives this from Joiner)
    pub(crate) async fn handle_pairing_response(
        &self,
        from_device: Uuid,
        session_id: Uuid,
        response: Vec<u8>,
        device_info: DeviceInfo,
    ) -> Result<Vec<u8>> {
        // Verify the response signature
        let session = self
            .active_sessions
            .read()
            .await
            .get(&session_id)
            .cloned()
            .ok_or_else(|| NetworkingError::Protocol("Session not found".to_string()))?;

        let challenge = match &session.state {
            PairingState::ChallengeReceived { challenge } => challenge.clone(),
            _ => {
                return Err(NetworkingError::Protocol(
                    "Invalid session state".to_string(),
                ))
            }
        };

        // Get the public key from the session (stored during pairing request)
        let device_public_key = session.remote_public_key
            .as_ref()
            .ok_or_else(|| NetworkingError::Protocol(
                "Device public key not found in session for signature verification".to_string()
            ))?
            .clone();

        // Validate inputs
        PairingSecurity::validate_challenge(&challenge)?;
        PairingSecurity::validate_signature(&response)?;
        PairingSecurity::validate_public_key(&device_public_key)?;

        // Verify the signature
        let signature_valid = PairingSecurity::verify_challenge_response(
            &device_public_key,
            &challenge,
            &response,
        )?;

        if !signature_valid {
            self.log_error(&format!(
                "Invalid signature for session {} from device {}",
                session_id, from_device
            )).await;
            
            // Mark session as failed
            if let Some(session) = self.active_sessions.write().await.get_mut(&session_id) {
                session.state = PairingState::Failed {
                    reason: "Invalid challenge signature".to_string(),
                };
            }
            
            return Err(NetworkingError::Protocol(
                "Challenge signature verification failed".to_string(),
            ));
        }

        self.log_info(&format!(
            "Signature verified successfully for session {} from device {}",
            session_id, from_device
        )).await;

        // Generate session keys using pairing code secret
        let shared_secret = self.generate_shared_secret(session_id).await?;
        let session_keys = SessionKeys::from_shared_secret(shared_secret.clone());

        // Complete pairing in device registry with proper lock scoping
        // Use the actual device ID from device_info to ensure consistency
        let actual_device_id = device_info.device_id;
        {
            let mut registry = self.device_registry.write().await;
            registry.complete_pairing(
                actual_device_id,
                device_info.clone(),
                session_keys.clone(),
            ).await?;
        } // Release write lock here

        // Get node ID for device connection with separate read lock
        let node_id = {
            let registry = self.device_registry.read().await;
            registry.get_node_id_for_device(actual_device_id)
                .unwrap_or_else(|| NodeId::from_bytes(&[0u8; 32]).unwrap())
        }; // Release read lock here

        // Mark device as connected since pairing is successful
        let simple_connection = crate::services::networking::device::DeviceConnection {
            addresses: vec![], // Will be filled in later
            latency_ms: None,
            rx_bytes: 0,
            tx_bytes: 0,
        };

        if let Err(e) = {
            let mut registry = self.device_registry.write().await;
            registry.mark_connected(actual_device_id, simple_connection).await
        }
        {
            self.log_warn(&format!(
                "Warning - failed to mark device as connected: {}",
                e
            )).await;
        } else {
            self.log_info(&format!(
                "Successfully marked device {} as connected",
                actual_device_id
            )).await;
        }

        // Update session
        if let Some(session) = self.active_sessions.write().await.get_mut(&session_id) {
            session.state = PairingState::Completed;
            session.shared_secret = Some(shared_secret);
            session.remote_device_id = Some(actual_device_id);
            self.log_info(&format!(
                "Session {} updated with shared secret and remote device ID {}",
                session_id, actual_device_id
            )).await;
        }

        // Send completion message
        let response = PairingMessage::Complete {
            session_id,
            success: true,
            reason: None,
        };

        serde_json::to_vec(&response).map_err(|e| NetworkingError::Serialization(e))
    }
}```

## src/services/networking/protocols/pairing/security.rs

```rust
//! Security utilities for pairing protocol

use crate::services::networking::{NetworkingError, Result};
// We'll use our own signature verification

/// Security operations for pairing protocol
pub struct PairingSecurity;

impl PairingSecurity {
	/// Verify a challenge response signature using ed25519 public key
	pub fn verify_challenge_response(
		public_key_bytes: &[u8],
		challenge: &[u8],
		signature: &[u8],
	) -> Result<bool> {
		// Validate inputs first
		Self::validate_public_key(public_key_bytes)?;
		Self::validate_challenge(challenge)?;
		Self::validate_signature(signature)?;

		// Use ed25519-dalek for verification
		use ed25519_dalek::{Signature, Verifier, VerifyingKey};

		// Public key should be 32 bytes for Ed25519
		if public_key_bytes.len() != 32 {
			return Err(NetworkingError::Protocol(
				"Invalid public key length for Ed25519".to_string(),
			));
		}

		let verifying_key = VerifyingKey::from_bytes(public_key_bytes.try_into().unwrap())
			.map_err(|e| NetworkingError::Protocol(format!("Invalid public key: {}", e)))?;

		let sig = Signature::from_slice(signature)
			.map_err(|e| NetworkingError::Protocol(format!("Invalid signature: {}", e)))?;

		Ok(verifying_key.verify(challenge, &sig).is_ok())
	}

	/// Validate device public key format (Ed25519 raw bytes)
	pub fn validate_public_key(public_key_bytes: &[u8]) -> Result<()> {
		// Ed25519 public keys are exactly 32 bytes
		if public_key_bytes.len() != 32 {
			return Err(NetworkingError::Protocol(format!(
				"Invalid public key length: expected 32 bytes, got {}",
				public_key_bytes.len()
			)));
		}

		// Basic validation - ensure it's not all zeros
		if public_key_bytes.iter().all(|&b| b == 0) {
			return Err(NetworkingError::Protocol(
				"Invalid public key: all zeros".to_string(),
			));
		}

		Ok(())
	}

	/// Validate challenge format and size
	pub fn validate_challenge(challenge: &[u8]) -> Result<()> {
		if challenge.len() != 32 {
			return Err(NetworkingError::Protocol(format!(
				"Invalid challenge length: expected 32 bytes, got {}",
				challenge.len()
			)));
		}

		// Ensure challenge isn't all zeros (weak challenge)
		if challenge.iter().all(|&b| b == 0) {
			return Err(NetworkingError::Protocol(
				"Invalid challenge: all zeros".to_string(),
			));
		}

		Ok(())
	}

	/// Validate signature format
	pub fn validate_signature(signature: &[u8]) -> Result<()> {
		if signature.len() != 64 {
			return Err(NetworkingError::Protocol(format!(
				"Invalid signature length: expected 64 bytes, got {}",
				signature.len()
			)));
		}

		// Basic validation - ensure it's not all zeros
		if signature.iter().all(|&b| b == 0) {
			return Err(NetworkingError::Protocol(
				"Invalid signature: all zeros".to_string(),
			));
		}

		Ok(())
	}
}

#[cfg(test)]
mod tests {
	use super::*;
	use ed25519_dalek::{ed25519::signature::Keypair, SigningKey};
	use rand::rngs::OsRng;

	#[test]
	fn test_validate_public_key() {
		// Create a real ed25519 keypair for testing
		let signing_key = SigningKey::from_bytes(&[1u8; 32]);
		let public_key_bytes = signing_key.verifying_key().to_bytes();

		// Valid key
		assert!(PairingSecurity::validate_public_key(&public_key_bytes).is_ok());

		// Invalid length (too short)
		assert!(PairingSecurity::validate_public_key(&[0u8; 20]).is_err());

		// Invalid length (too long)
		assert!(PairingSecurity::validate_public_key(&[0u8; 40]).is_err());

		// All zeros (invalid)
		let invalid_key = [0u8; 32];
		assert!(PairingSecurity::validate_public_key(&invalid_key).is_err());
	}

	#[test]
	fn test_validate_challenge() {
		// Valid challenge
		let valid_challenge = [1u8; 32];
		assert!(PairingSecurity::validate_challenge(&valid_challenge).is_ok());

		// Invalid length
		assert!(PairingSecurity::validate_challenge(&[1u8; 31]).is_err());
		assert!(PairingSecurity::validate_challenge(&[1u8; 33]).is_err());

		// All zeros (weak)
		let weak_challenge = [0u8; 32];
		assert!(PairingSecurity::validate_challenge(&weak_challenge).is_err());
	}

	#[test]
	fn test_validate_signature() {
		// Valid signature
		let valid_signature = [1u8; 64];
		assert!(PairingSecurity::validate_signature(&valid_signature).is_ok());

		// Invalid length
		assert!(PairingSecurity::validate_signature(&[1u8; 63]).is_err());
		assert!(PairingSecurity::validate_signature(&[1u8; 65]).is_err());

		// All zeros (invalid)
		let invalid_signature = [0u8; 64];
		assert!(PairingSecurity::validate_signature(&invalid_signature).is_err());
	}

	#[test]
	fn test_verify_challenge_response() {
		use ed25519_dalek::Signer;

		// Create a real keypair and sign a challenge
		let signing_key = SigningKey::from_bytes(&[3u8; 32]);
		let public_key_bytes = signing_key.verifying_key().to_bytes();
		let challenge = [2u8; 32];
		let signature = signing_key.sign(&challenge);

		println!("ðŸ” Testing REAL Ed25519 signature verification:");
		println!("   Public key: {} bytes", public_key_bytes.len());
		println!("   Challenge: {} bytes", challenge.len());
		println!("   Signature: {} bytes", signature.to_bytes().len());

		// Should verify successfully with REAL cryptographic verification
		let result = PairingSecurity::verify_challenge_response(
			&public_key_bytes,
			&challenge,
			&signature.to_bytes(),
		);
		assert!(result.is_ok());
		assert!(result.unwrap());

		println!("âœ… REAL cryptographic signature verification PASSED!");
	}

	#[test]
	fn test_verify_challenge_response_invalid_signature() {
		use ed25519_dalek::Signer;

		// Create a keypair and sign wrong data
		let signing_key = SigningKey::from_bytes(&[4u8; 32]);
		let public_key_bytes = signing_key.verifying_key().to_bytes();
		let challenge = [2u8; 32];
		let wrong_challenge = [3u8; 32];
		let signature = signing_key.sign(&wrong_challenge);

		println!("ðŸ”’ Testing REAL Ed25519 signature rejection:");
		println!("   Signed data: {:?}", &wrong_challenge[..4]);
		println!("   Verify data: {:?}", &challenge[..4]);

		// Should fail verification (this proves crypto is REALLY working!)
		let result = PairingSecurity::verify_challenge_response(
			&public_key_bytes,
			&challenge,
			&signature.to_bytes(),
		);
		assert!(result.is_ok());
		assert!(!result.unwrap()); // Should be false

		println!("âœ… REAL cryptographic signature rejection PASSED!");
		println!("   ðŸŽ¯ This proves we're doing REAL crypto verification!");
	}
}
```

## src/services/networking/protocols/pairing/persistence.rs

```rust
//! Session persistence for pairing protocol

use super::types::{PairingSession, PairingState};
use crate::services::networking::{NetworkingError, Result};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::path::{Path, PathBuf};
use tokio::fs;
use uuid::Uuid;

/// Serializable version of PairingSession for persistence
#[derive(Debug, Clone, Serialize, Deserialize)]
struct SerializablePairingSession {
    pub id: Uuid,
    pub state: SerializablePairingState,
    pub remote_device_id: Option<Uuid>,
    pub remote_public_key: Option<Vec<u8>>,
    pub shared_secret: Option<Vec<u8>>,
    pub created_at: chrono::DateTime<chrono::Utc>,
}

/// Serializable version of PairingState
#[derive(Debug, Clone, Serialize, Deserialize)]
enum SerializablePairingState {
    WaitingForConnection,
    Scanning,
    ChallengeReceived { challenge: Vec<u8> },
    ResponseSent,
    Completed,
    Failed { reason: String },
}

impl From<&PairingSession> for SerializablePairingSession {
    fn from(session: &PairingSession) -> Self {
        Self {
            id: session.id,
            state: match &session.state {
                PairingState::WaitingForConnection => SerializablePairingState::WaitingForConnection,
                PairingState::Scanning => SerializablePairingState::Scanning,
                PairingState::ChallengeReceived { challenge } => {
                    SerializablePairingState::ChallengeReceived {
                        challenge: challenge.clone(),
                    }
                }
                PairingState::ResponseSent => SerializablePairingState::ResponseSent,
                PairingState::Completed => SerializablePairingState::Completed,
                PairingState::Failed { reason } => SerializablePairingState::Failed {
                    reason: reason.clone(),
                },
                // Skip non-serializable states
                _ => SerializablePairingState::Failed {
                    reason: "State not serializable".to_string(),
                },
            },
            remote_device_id: session.remote_device_id,
            remote_public_key: session.remote_public_key.clone(),
            shared_secret: session.shared_secret.clone(),
            created_at: session.created_at,
        }
    }
}

impl From<SerializablePairingSession> for PairingSession {
    fn from(serializable: SerializablePairingSession) -> Self {
        Self {
            id: serializable.id,
            state: match serializable.state {
                SerializablePairingState::WaitingForConnection => PairingState::WaitingForConnection,
                SerializablePairingState::Scanning => PairingState::Scanning,
                SerializablePairingState::ChallengeReceived { challenge } => {
                    PairingState::ChallengeReceived { challenge }
                }
                SerializablePairingState::ResponseSent => PairingState::ResponseSent,
                SerializablePairingState::Completed => PairingState::Completed,
                SerializablePairingState::Failed { reason } => PairingState::Failed { reason },
            },
            remote_device_id: serializable.remote_device_id,
            remote_device_info: None, // Will be restored from device registry
            remote_public_key: serializable.remote_public_key,
            shared_secret: serializable.shared_secret,
            created_at: serializable.created_at,
        }
    }
}

/// Persisted pairing sessions data
#[derive(Debug, Serialize, Deserialize)]
struct PersistedPairingSessions {
    sessions: HashMap<Uuid, SerializablePairingSession>,
    last_saved: chrono::DateTime<chrono::Utc>,
}

/// Session persistence manager
pub struct PairingPersistence {
    data_dir: PathBuf,
    sessions_file: PathBuf,
}

impl PairingPersistence {
    /// Create a new persistence manager
    pub fn new(data_dir: impl AsRef<Path>) -> Self {
        let data_dir = data_dir.as_ref().to_path_buf();
        let networking_dir = data_dir.join("networking");
        let sessions_file = networking_dir.join("pairing_sessions.json");

        Self {
            data_dir: networking_dir,
            sessions_file,
        }
    }

    /// Save active sessions to disk
    pub async fn save_sessions(&self, sessions: &HashMap<Uuid, PairingSession>) -> Result<()> {
        // Ensure data directory exists
        if let Some(parent) = self.sessions_file.parent() {
            fs::create_dir_all(parent).await.map_err(NetworkingError::Io)?;
        }

        // Convert to serializable format, filtering out transient states
        let serializable_sessions: HashMap<Uuid, SerializablePairingSession> = sessions
            .iter()
            .filter_map(|(id, session)| {
                // Only persist certain states
                match &session.state {
                    PairingState::WaitingForConnection
                    | PairingState::Scanning
                    | PairingState::ChallengeReceived { .. }
                    | PairingState::ResponseSent
                    | PairingState::Completed => Some((*id, session.into())),
                    // Don't persist transient or failed states
                    _ => None,
                }
            })
            .collect();

        let persisted = PersistedPairingSessions {
            sessions: serializable_sessions,
            last_saved: chrono::Utc::now(),
        };

        // Write to temporary file first, then rename for atomic operation
        let temp_file = self.sessions_file.with_extension("tmp");
        let json_data = serde_json::to_string_pretty(&persisted).map_err(|e| {
            NetworkingError::Serialization(e)
        })?;

        fs::write(&temp_file, json_data).await.map_err(NetworkingError::Io)?;

        fs::rename(&temp_file, &self.sessions_file).await.map_err(NetworkingError::Io)?;

        Ok(())
    }

    /// Load sessions from disk
    pub async fn load_sessions(&self) -> Result<HashMap<Uuid, PairingSession>> {
        if !self.sessions_file.exists() {
            return Ok(HashMap::new());
        }

        let json_data = fs::read_to_string(&self.sessions_file).await.map_err(NetworkingError::Io)?;

        let persisted: PersistedPairingSessions = serde_json::from_str(&json_data).map_err(|e| {
            NetworkingError::Serialization(e)
        })?;

        // Filter out expired sessions (older than 1 hour)
        let now = chrono::Utc::now();
        let max_age = chrono::Duration::hours(1);

        let sessions: HashMap<Uuid, PairingSession> = persisted
            .sessions
            .into_iter()
            .filter_map(|(id, serializable)| {
                let age = now.signed_duration_since(serializable.created_at);
                if age <= max_age {
                    Some((id, serializable.into()))
                } else {
                    None
                }
            })
            .collect();

        Ok(sessions)
    }

    /// Clean up expired sessions from disk
    pub async fn cleanup_expired_sessions(&self) -> Result<usize> {
        let sessions = self.load_sessions().await?;
        let initial_count = sessions.len();

        // Save the filtered sessions back
        self.save_sessions(&sessions).await?;

        Ok(initial_count - sessions.len())
    }

    /// Delete all persisted sessions
    pub async fn clear_all_sessions(&self) -> Result<()> {
        if self.sessions_file.exists() {
            fs::remove_file(&self.sessions_file).await.map_err(NetworkingError::Io)?;
        }
        Ok(())
    }

    /// Get the path to the sessions file
    pub fn sessions_file_path(&self) -> &Path {
        &self.sessions_file
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use tempfile::TempDir;

    async fn create_test_persistence() -> (PairingPersistence, TempDir) {
        let temp_dir = TempDir::new().expect("Failed to create temp dir");
        let persistence = PairingPersistence::new(temp_dir.path());
        (persistence, temp_dir)
    }

    #[tokio::test]
    async fn test_save_and_load_sessions() {
        let (persistence, _temp_dir) = create_test_persistence().await;

        // Create test sessions
        let mut sessions = HashMap::new();
        let session_id = Uuid::new_v4();
        let session = PairingSession {
            id: session_id,
            state: PairingState::WaitingForConnection,
            remote_device_id: Some(Uuid::new_v4()),
            remote_device_info: None,
            remote_public_key: None,
            shared_secret: Some(vec![1, 2, 3, 4]),
            created_at: chrono::Utc::now(),
        };
        sessions.insert(session_id, session);

        // Save sessions
        persistence.save_sessions(&sessions).await.unwrap();

        // Load sessions
        let loaded_sessions = persistence.load_sessions().await.unwrap();

        assert_eq!(loaded_sessions.len(), 1);
        assert!(loaded_sessions.contains_key(&session_id));

        let loaded_session = &loaded_sessions[&session_id];
        assert_eq!(loaded_session.id, session_id);
        assert!(matches!(loaded_session.state, PairingState::WaitingForConnection));
    }

    #[tokio::test]
    async fn test_load_nonexistent_file() {
        let (persistence, _temp_dir) = create_test_persistence().await;

        let sessions = persistence.load_sessions().await.unwrap();
        assert!(sessions.is_empty());
    }

    #[tokio::test]
    async fn test_clear_sessions() {
        let (persistence, _temp_dir) = create_test_persistence().await;

        // Create and save sessions
        let mut sessions = HashMap::new();
        sessions.insert(Uuid::new_v4(), PairingSession {
            id: Uuid::new_v4(),
            state: PairingState::Completed,
            remote_device_id: None,
            remote_device_info: None,
            remote_public_key: None,
            shared_secret: None,
            created_at: chrono::Utc::now(),
        });

        persistence.save_sessions(&sessions).await.unwrap();
        assert!(persistence.sessions_file.exists());

        // Clear sessions
        persistence.clear_all_sessions().await.unwrap();
        assert!(!persistence.sessions_file.exists());
    }
}```

## src/services/networking/protocols/pairing/mod.rs

```rust
//! Pairing protocol handler

pub mod initiator;
pub mod joiner;
pub mod messages;
pub mod persistence;
pub mod security;
pub mod types;

// Re-export main types
pub use messages::PairingMessage;
pub use types::{PairingAdvertisement, PairingCode, PairingRole, PairingSession, PairingState};

use super::{ProtocolEvent, ProtocolHandler};
use crate::services::networking::{
    device::{DeviceInfo, DeviceRegistry, SessionKeys},
    utils::{identity::NetworkFingerprint, logging::NetworkLogger, NetworkIdentity},
    NetworkingError, Result,
};
use async_trait::async_trait;
use iroh::net::NodeAddr;
use iroh::net::key::NodeId;
use persistence::PairingPersistence;
use security::PairingSecurity;
use std::collections::HashMap;
use std::path::PathBuf;
use std::sync::Arc;
use tokio::sync::RwLock;
use uuid::Uuid;
use blake3;

/// Pairing protocol handler
pub struct PairingProtocolHandler {
    /// Network identity for signing
    identity: NetworkIdentity,

    /// Device registry for state management
    device_registry: Arc<RwLock<DeviceRegistry>>,

    /// Active pairing sessions
    active_sessions: Arc<RwLock<HashMap<Uuid, PairingSession>>>,

    /// Pairing codes for active sessions (session_id -> pairing_code)
    pairing_codes: Arc<RwLock<HashMap<Uuid, PairingCode>>>,

    /// Logger for structured logging
    logger: Arc<dyn NetworkLogger>,

    /// Command sender for dispatching commands to the NetworkingEventLoop
    command_sender: tokio::sync::mpsc::UnboundedSender<crate::services::networking::core::event_loop::EventLoopCommand>,

    /// Current pairing role
    role: Option<PairingRole>,

    /// Session persistence manager
    persistence: Option<Arc<PairingPersistence>>,
}

impl PairingProtocolHandler {
    /// Create a new pairing protocol handler
    pub fn new(
        identity: NetworkIdentity,
        device_registry: Arc<RwLock<DeviceRegistry>>,
        logger: Arc<dyn NetworkLogger>,
        command_sender: tokio::sync::mpsc::UnboundedSender<crate::services::networking::core::event_loop::EventLoopCommand>,
    ) -> Self {
        Self {
            identity,
            device_registry,
            active_sessions: Arc::new(RwLock::new(HashMap::new())),
            pairing_codes: Arc::new(RwLock::new(HashMap::new())),
            logger,
            command_sender,
            role: None,
            persistence: None,
        }
    }

    /// Create a new pairing protocol handler with persistence
    pub fn new_with_persistence(
        identity: NetworkIdentity,
        device_registry: Arc<RwLock<DeviceRegistry>>,
        logger: Arc<dyn NetworkLogger>,
        command_sender: tokio::sync::mpsc::UnboundedSender<crate::services::networking::core::event_loop::EventLoopCommand>,
        data_dir: PathBuf,
    ) -> Self {
        let persistence = Arc::new(PairingPersistence::new(data_dir));
        Self {
            identity,
            device_registry,
            active_sessions: Arc::new(RwLock::new(HashMap::new())),
            pairing_codes: Arc::new(RwLock::new(HashMap::new())),
            logger,
            command_sender,
            role: None,
            persistence: Some(persistence),
        }
    }

    /// Initialize sessions from persistence (call after construction)
    pub async fn load_persisted_sessions(&self) -> Result<usize> {
        if let Some(persistence) = &self.persistence {
            let sessions = persistence.load_sessions().await?;
            let count = sessions.len();

            if count > 0 {
                *self.active_sessions.write().await = sessions;
                self.log_info(&format!("Loaded {} persisted pairing sessions", count)).await;
            }

            Ok(count)
        } else {
            Ok(0)
        }
    }

    /// Save current sessions to persistence
    async fn save_sessions_to_persistence(&self) -> Result<()> {
        if let Some(persistence) = &self.persistence {
            let sessions = self.active_sessions.read().await;
            persistence.save_sessions(&sessions).await?;
        }
        Ok(())
    }

    /// Log info message with role prefix
    async fn log_info(&self, message: &str) {
        let role_prefix = match &self.role {
            Some(PairingRole::Initiator) => "[INITIATOR]",
            Some(PairingRole::Joiner) => "[JOINER]",
            None => "[PAIRING]",
        };
        self.logger.info(&format!("{} {}", role_prefix, message)).await;
    }

    /// Log debug message with role prefix
    async fn log_debug(&self, message: &str) {
        let role_prefix = match &self.role {
            Some(PairingRole::Initiator) => "[INITIATOR]",
            Some(PairingRole::Joiner) => "[JOINER]",
            None => "[PAIRING]",
        };
        self.logger.debug(&format!("{} {}", role_prefix, message)).await;
    }

    /// Log warning message with role prefix
    async fn log_warn(&self, message: &str) {
        let role_prefix = match &self.role {
            Some(PairingRole::Initiator) => "[INITIATOR]",
            Some(PairingRole::Joiner) => "[JOINER]",
            None => "[PAIRING]",
        };
        self.logger.warn(&format!("{} {}", role_prefix, message)).await;
    }

    /// Log error message with role prefix
    async fn log_error(&self, message: &str) {
        let role_prefix = match &self.role {
            Some(PairingRole::Initiator) => "[INITIATOR]",
            Some(PairingRole::Joiner) => "[JOINER]",
            None => "[PAIRING]",
        };
        self.logger.error(&format!("{} {}", role_prefix, message)).await;
    }

    /// Start a new pairing session as initiator
    /// Returns the session ID which should be advertised via DHT by the caller
    pub async fn start_pairing_session(&self) -> Result<Uuid> {
        let session_id = Uuid::new_v4();
        let pairing_code = PairingCode::from_session_id(session_id);
        self.start_pairing_session_with_id(session_id, pairing_code).await?;
        Ok(session_id)
    }

    /// Start a new pairing session with a specific session ID and pairing code
    pub async fn start_pairing_session_with_id(&self, session_id: Uuid, pairing_code: PairingCode) -> Result<()> {
        let session = PairingSession {
            id: session_id,
            state: PairingState::WaitingForConnection,
            remote_device_id: None,
            remote_device_info: None,
            remote_public_key: None,
            shared_secret: None,
            created_at: chrono::Utc::now(),
        };

        self.active_sessions
            .write()
            .await
            .insert(session_id, session);

        // Store the pairing code for this session
        self.pairing_codes
            .write()
            .await
            .insert(session_id, pairing_code);

        // Save to persistence
        self.save_sessions_to_persistence().await?;

        self.log_info(&format!("Started pairing session: {}", session_id)).await;
        Ok(())
    }

    /// Join an existing pairing session with a specific session ID and pairing code
    /// This allows a joiner to participate in an initiator's session
    pub async fn join_pairing_session(&self, session_id: Uuid, pairing_code: PairingCode) -> Result<()> {

        // Check if session already exists to prevent conflicts
        {
            let sessions = self.active_sessions.read().await;
            if let Some(existing_session) = sessions.get(&session_id) {
                return Err(NetworkingError::Protocol(format!(
                    "Session {} already exists in state {:?}",
                    session_id, existing_session.state
                )));
            }
        }

        // Create new scanning session for the joiner
        let session = PairingSession {
            id: session_id,
            state: PairingState::Scanning, // Joiner starts in scanning state
            remote_device_id: None,
            remote_device_info: None,
            remote_public_key: None,
            shared_secret: None,
            created_at: chrono::Utc::now(),
        };

        // Insert the session
        {
            let mut sessions = self.active_sessions.write().await;
            sessions.insert(session_id, session);
        }

        // Store the pairing code for this session
        self.pairing_codes
            .write()
            .await
            .insert(session_id, pairing_code);

        // Save to persistence
        self.save_sessions_to_persistence().await?;

        self.log_info(&format!(
            "Joined pairing session: {} (state: Scanning)",
            session_id
        )).await;

        // Verify session was created correctly
        let sessions = self.active_sessions.read().await;
        if let Some(created_session) = sessions.get(&session_id) {
            if matches!(created_session.state, PairingState::Scanning) {
                self.log_debug(&format!(
                    "Pairing session verified in Scanning state: {}",
                    session_id
                )).await;
            } else {
                return Err(NetworkingError::Protocol(format!(
                    "Session {} created in wrong state: {:?}",
                    session_id, created_session.state
                )));
            }
        } else {
            return Err(NetworkingError::Protocol(format!(
                "Failed to verify session creation: {}",
                session_id
            )));
        }

        Ok(())
    }

    /// Get device info for advertising in DHT records
    pub async fn get_device_info(&self) -> Result<DeviceInfo> {
        // Get device info from device registry (which uses device manager)
        let mut device_info = self.device_registry.read().await.get_local_device_info()?;

        // Update network fingerprint with current identity
        device_info.network_fingerprint = self.identity.network_fingerprint();
        device_info.last_seen = chrono::Utc::now();

        Ok(device_info)
    }

    /// Cancel a pairing session
    pub async fn cancel_session(&self, session_id: Uuid) -> Result<()> {
        self.active_sessions.write().await.remove(&session_id);
        self.pairing_codes.write().await.remove(&session_id);
        self.save_sessions_to_persistence().await?;
        Ok(())
    }

    /// Get active pairing sessions
    pub async fn get_active_sessions(&self) -> Vec<PairingSession> {
        let sessions = {
            let read_guard = self.active_sessions.read().await;
            read_guard.values().cloned().collect::<Vec<_>>()
        };
        sessions
    }

    /// Clean up expired pairing sessions
    pub async fn cleanup_expired_sessions(&self) -> Result<usize> {
        let now = chrono::Utc::now();
        let timeout_duration = chrono::Duration::minutes(10); // 10 minute timeout

        let mut sessions = self.active_sessions.write().await;
        let mut pairing_codes = self.pairing_codes.write().await;
        let initial_count = sessions.len();

        // Collect session IDs to remove first
        let mut sessions_to_remove = Vec::new();
        for (session_id, session) in sessions.iter() {
            let age = now.signed_duration_since(session.created_at);
            if age > timeout_duration {
                sessions_to_remove.push(*session_id);
            }
        }

        // Remove expired sessions and their pairing codes
        for session_id in &sessions_to_remove {
            sessions.remove(session_id);
            pairing_codes.remove(session_id);
        }

        let cleaned_count = sessions_to_remove.len();
        if cleaned_count > 0 {
            self.log_info(&format!("Cleaned up {} expired pairing sessions", cleaned_count)).await;
        }

        Ok(cleaned_count)
    }

    /// Start a background task to periodically clean up expired sessions
    pub fn start_cleanup_task(handler: Arc<Self>) {
        let logger = handler.logger.clone();
        tokio::spawn(async move {
            let mut interval = tokio::time::interval(tokio::time::Duration::from_secs(60)); // Check every minute

            loop {
                interval.tick().await;

                if let Err(e) = handler.cleanup_expired_sessions().await {
                    logger.error(&format!("Error during session cleanup: {}", e)).await;
                }
            }
        });
    }

    /// Start the background task for managing pairing state transitions
    pub fn start_state_machine_task(handler: Arc<Self>) {
        tokio::spawn(async move {
            // Check the state every 200 milliseconds
            let mut interval = tokio::time::interval(tokio::time::Duration::from_millis(200));

            loop {
                interval.tick().await;
                if let Err(e) = handler.process_state_transitions().await {
                    handler.log_error(&format!("State machine error: {}", e)).await;
                }
            }
        });
    }

    /// The core logic of the state machine - processes state transitions for all active sessions
    async fn process_state_transitions(&self) -> Result<()> {
        // Get a write lock because we may need to modify session states
        let mut sessions = self.active_sessions.write().await;

        for session in sessions.values_mut() {
            // Match on the current state to decide the next action
            match &session.state {
                // This is the critical missing logic - handle ResponsePending state
                PairingState::ResponsePending { response_data, remote_node_id, .. } => {
                    if let Some(node_id) = remote_node_id {
                        self.log_info(&format!(
                            "State Machine: Found ResponsePending for session {}, sending response to node {}",
                            session.id, node_id
                        )).await;

                        // Create the command to send the message
                        let command = crate::services::networking::core::event_loop::EventLoopCommand::SendMessageToNode {
                            node_id: *node_id,
                            protocol: "pairing".to_string(),
                            data: response_data.clone(),
                        };

                        // Send the command to the NetworkingEventLoop
                        if self.command_sender.send(command).is_ok() {
                            // Transition the state to prevent re-sending
                            session.state = PairingState::ResponseSent;
                            self.log_info(&format!(
                                "State Machine: Response sent for session {}, transitioned to ResponseSent",
                                session.id
                            )).await;
                        } else {
                            self.log_error("State Machine: Failed to send command to event loop.").await;
                            session.state = PairingState::Failed { 
                                reason: "Internal channel closed".to_string() 
                            };
                        }
                    } else {
                        self.log_error(&format!(
                            "State Machine: Session {} in ResponsePending but no remote node ID",
                            session.id
                        )).await;
                        session.state = PairingState::Failed { 
                            reason: "No remote node ID for response".to_string() 
                        };
                    }
                }

                // Optional: Add logic to time out sessions stuck in scanning for too long
                PairingState::Scanning => {
                    let age = chrono::Utc::now().signed_duration_since(session.created_at);
                    if age > chrono::Duration::minutes(5) { // 5 minute timeout for scanning
                        self.log_warn(&format!(
                            "State Machine: Session {} timed out while scanning, marking as failed",
                            session.id
                        )).await;
                        session.state = PairingState::Failed { 
                            reason: "Scanning timeout".to_string() 
                        };
                    }
                }

                // No action needed for other states in this loop
                _ => {
                    // Other states are handled elsewhere or don't need periodic processing
                }
            }
        }

        Ok(())
    }


    fn generate_challenge(&self) -> Result<Vec<u8>> {
        use rand::RngCore;
        let mut challenge = vec![0u8; 32];
        rand::thread_rng().fill_bytes(&mut challenge);
        Ok(challenge)
    }

    /// Generate shared secret for a pairing session using the pairing code secret
    async fn generate_shared_secret(&self, session_id: Uuid) -> Result<Vec<u8>> {
        let pairing_codes = self.pairing_codes.read().await;
        let pairing_code = pairing_codes.get(&session_id)
            .ok_or_else(|| NetworkingError::Protocol(
                format!("No pairing code found for session {}", session_id)
            ))?;
        
        // Use the pairing code secret as the shared secret
        Ok(pairing_code.secret().to_vec())
    }
    
    /// Handle a pairing message received over stream
    async fn handle_pairing_message(&self, message: PairingMessage, remote_node_id: NodeId) -> Result<Option<Vec<u8>>> {
        match message {
            PairingMessage::PairingRequest { session_id, device_info, public_key } => {
                // Generate a temporary device ID based on node ID
                let from_device = self.get_device_id_for_node(remote_node_id).await;
                let response = self.handle_pairing_request(from_device, session_id, device_info, public_key).await?;
                Ok(Some(response))
            }
            PairingMessage::Challenge { session_id, challenge, device_info } => {
                let response = self.handle_pairing_challenge(session_id, challenge, device_info).await?;
                Ok(Some(response))
            }
            PairingMessage::Response { session_id, response, device_info } => {
                let from_device = self.get_device_id_for_node(remote_node_id).await;
                let response = self.handle_pairing_response(from_device, session_id, response, device_info).await?;
                Ok(Some(response))
            }
            PairingMessage::Complete { session_id, success, reason } => {
                let from_device = self.get_device_id_for_node(remote_node_id).await;
                self.handle_completion(session_id, success, reason, from_device, remote_node_id).await?;
                Ok(None) // No response needed
            }
        }
    }
    
    /// Get or create a device ID for a node
    async fn get_device_id_for_node(&self, node_id: NodeId) -> Uuid {
        let registry = self.device_registry.read().await;
        registry.get_device_by_node(node_id)
            .unwrap_or_else(|| {
                // Generate a deterministic UUID from the node ID
                let mut hasher = blake3::Hasher::new();
                hasher.update(b"spacedrive-device-id");
                hasher.update(node_id.as_bytes());
                let hash = hasher.finalize();
                let mut uuid_bytes = [0u8; 16];
                uuid_bytes.copy_from_slice(&hash.as_bytes()[..16]);
                Uuid::from_bytes(uuid_bytes)
            })
    }
    
    /// Send a pairing message to a specific node using Iroh streams
    pub async fn send_pairing_message_to_node(
        &self,
        endpoint: &iroh::net::Endpoint,
        node_id: NodeId,
        message: &PairingMessage,
    ) -> Result<Option<PairingMessage>> {
        use tokio::io::{AsyncReadExt, AsyncWriteExt};
        
        // Create node address and connect
        let node_addr = NodeAddr::new(node_id);
        let conn = endpoint.connect(node_addr, crate::services::networking::core::PAIRING_ALPN).await
            .map_err(|e| NetworkingError::ConnectionFailed(format!("Failed to connect: {}", e)))?;
        
        // Open a bidirectional stream
        let (mut send, mut recv) = conn.open_bi().await
            .map_err(|e| NetworkingError::ConnectionFailed(format!("Failed to open stream: {}", e)))?;
        
        // Serialize the message
        let msg_data = serde_json::to_vec(message)
            .map_err(|e| NetworkingError::Serialization(e))?;
        
        // Send message length
        let len = msg_data.len() as u32;
        send.write_all(&len.to_be_bytes()).await
            .map_err(|e| NetworkingError::Transport(format!("Failed to write length: {}", e)))?;
        
        // Send message
        send.write_all(&msg_data).await
            .map_err(|e| NetworkingError::Transport(format!("Failed to write message: {}", e)))?;
        
        // Flush
        send.flush().await
            .map_err(|e| NetworkingError::Transport(format!("Failed to flush: {}", e)))?;
        
        // Read response length
        let mut len_buf = [0u8; 4];
        match recv.read_exact(&mut len_buf).await {
            Ok(_) => {
                let resp_len = u32::from_be_bytes(len_buf) as usize;
                
                // Read response
                let mut resp_buf = vec![0u8; resp_len];
                recv.read_exact(&mut resp_buf).await
                    .map_err(|e| NetworkingError::Transport(format!("Failed to read response: {}", e)))?;
                
                // Deserialize response
                let response: PairingMessage = serde_json::from_slice(&resp_buf)
                    .map_err(|e| NetworkingError::Serialization(e))?;
                
                Ok(Some(response))
            }
            Err(_) => Ok(None), // No response
        }
    }
}

#[async_trait]
impl ProtocolHandler for PairingProtocolHandler {
    fn protocol_name(&self) -> &str {
        "pairing"
    }

    async fn handle_stream(
        &self,
        mut send: Box<dyn tokio::io::AsyncWrite + Send + Unpin>,
        mut recv: Box<dyn tokio::io::AsyncRead + Send + Unpin>,
        remote_node_id: NodeId,
    ) {
        use tokio::io::{AsyncReadExt, AsyncWriteExt};
        
        // Read the message length (4 bytes)
        let mut len_buf = [0u8; 4];
        if let Err(e) = recv.read_exact(&mut len_buf).await {
            self.logger.error(&format!("Failed to read message length: {}", e)).await;
            return;
        }
        let msg_len = u32::from_be_bytes(len_buf) as usize;
        
        // Read the message
        let mut msg_buf = vec![0u8; msg_len];
        if let Err(e) = recv.read_exact(&mut msg_buf).await {
            self.logger.error(&format!("Failed to read message: {}", e)).await;
            return;
        }
        
        // Deserialize and handle the message
        let message: PairingMessage = match serde_json::from_slice(&msg_buf) {
            Ok(msg) => msg,
            Err(e) => {
                self.logger.error(&format!("Failed to deserialize pairing message: {}", e)).await;
                return;
            }
        };
        
        // Process the message and get response
        let response = match self.handle_pairing_message(message, remote_node_id).await {
            Ok(resp) => resp,
            Err(e) => {
                self.logger.error(&format!("Failed to handle pairing message: {}", e)).await;
                return;
            }
        };
        
        // Send response if any
        if let Some(response_data) = response {
            // Write message length
            let len = response_data.len() as u32;
            if let Err(e) = send.write_all(&len.to_be_bytes()).await {
                self.logger.error(&format!("Failed to write response length: {}", e)).await;
                return;
            }
            
            // Write message
            if let Err(e) = send.write_all(&response_data).await {
                self.logger.error(&format!("Failed to write response: {}", e)).await;
                return;
            }
            
            // Flush the stream
            let _ = send.flush().await;
        }
    }

    async fn handle_request(&self, from_device: Uuid, request_data: Vec<u8>) -> Result<Vec<u8>> {
        let message: PairingMessage =
            serde_json::from_slice(&request_data).map_err(|e| NetworkingError::Serialization(e))?;

        let result = match message {
            // Initiator handles these messages
            PairingMessage::PairingRequest {
                session_id,
                device_info,
                public_key,
            } => {
                self.handle_pairing_request(from_device, session_id, device_info, public_key)
                    .await
            }
            PairingMessage::Response {
                session_id,
                response,
                device_info,
            } => {
                self.handle_pairing_response(from_device, session_id, response, device_info)
                    .await
            }
            // These are handled by handle_response, not handle_request
            PairingMessage::Challenge { .. } | PairingMessage::Complete { .. } => {
                self.log_warn("Received Challenge or Complete in handle_request - this should be handled by handle_response").await;
                Ok(Vec::new())
            }
        };

        // Handle errors by marking session as failed
        if let Err(ref error) = result {
            // Try to extract session ID from the original message for error tracking
            if let Ok(message) = serde_json::from_slice::<PairingMessage>(&request_data) {
                let session_id = match message {
                    PairingMessage::PairingRequest { session_id, .. } => Some(session_id),
                    PairingMessage::Challenge { session_id, .. } => Some(session_id),
                    PairingMessage::Response { session_id, .. } => Some(session_id),
                    PairingMessage::Complete { session_id, .. } => Some(session_id),
                };

                if let Some(session_id) = session_id {
                    // Mark session as failed
                    if let Some(session) = self.active_sessions.write().await.get_mut(&session_id) {
                        session.state = PairingState::Failed {
                            reason: error.to_string(),
                        };
                        self.log_error(&format!("Marked pairing session {} as failed: {}", session_id, error)).await;
                    }
                }
            }
        }

        result
    }

    async fn handle_response(
        &self,
        from_device: Uuid,
        from_node: NodeId,
        response_data: Vec<u8>,
    ) -> Result<()> {
        self.log_debug(&format!(
            "handle_response called with {} bytes from device {}",
            response_data.len(),
            from_device
        )).await;

        // Parse the response message
        let message: PairingMessage = serde_json::from_slice(&response_data)
            .map_err(|e| NetworkingError::Serialization(e))?;

        self.log_debug("Parsed message type successfully").await;

        // Process the response based on the message type
        match message {
            // Joiner handles these messages
            PairingMessage::Challenge {
                session_id,
                challenge,
                device_info,
            } => {
                self.log_info(&format!(
                    "Received challenge for session {} with {} byte challenge",
                    session_id,
                    challenge.len()
                )).await;

                // Check session state before processing
                {
                    let sessions = self.active_sessions.read().await;
                    if let Some(session) = sessions.get(&session_id) {
                        self.log_debug(&format!(
                            "Session {} state before challenge processing: {}",
                            session_id, session.state
                        )).await;
                    } else {
                        self.log_debug(&format!("No session found for {}", session_id)).await;
                    }
                }

                self.log_debug("About to call handle_pairing_challenge...").await;

                // Call the existing handle_pairing_challenge method
                match self
                    .handle_pairing_challenge(session_id, challenge.clone(), device_info)
                    .await
                {
                    Ok(response_data) => {
                        self.log_debug(&format!("handle_pairing_challenge succeeded, generated {} byte response", response_data.len())).await;

                        // Check session state after handle_pairing_challenge
                        {
                            let sessions = self.active_sessions.read().await;
                            if let Some(session) = sessions.get(&session_id) {
                                self.log_debug(&format!(
                                    "Session {} state after handle_pairing_challenge: {}",
                                    session_id, session.state
                                )).await;
                            }
                        }

                        // Use the node ID directly from the method parameter (this is Initiator's node ID)
                        let remote_node_id = Some(from_node);
                        self.log_debug(&format!(
                            "Using node ID from method parameter: {:?}",
                            from_node
                        )).await;

                        // Update the session state to ResponsePending so the unified pairing flow can send it
                        {
                            let mut sessions = self.active_sessions.write().await;
                            if let Some(session) = sessions.get_mut(&session_id) {
                                session.state = PairingState::ResponsePending {
                                    challenge: challenge.clone(),
                                    response_data: response_data.clone(),
                                    remote_node_id,
                                };
                                self.log_debug(&format!(
                                    "Session {} updated to ResponsePending state",
                                    session_id
                                )).await;
                            } else {
                                self.log_error(&format!("ERROR: Session {} not found when trying to update to ResponsePending", session_id)).await;
                            }
                        }

                        // Verify state change
                        {
                            let sessions = self.active_sessions.read().await;
                            if let Some(session) = sessions.get(&session_id) {
                                self.log_debug(&format!(
                                    "Session {} final state: {}",
                                    session_id, session.state
                                )).await;
                            }
                        }

                        self.log_info(&format!(
                            "Challenge response ready to send for session {}",
                            session_id
                        )).await;
                    }
                    Err(e) => {
                        self.log_error(&format!(
                            "handle_pairing_challenge FAILED for session {}: {}",
                            session_id, e
                        )).await;
                    }
                }
            }
            PairingMessage::Complete {
                session_id,
                success,
                reason,
            } => {
                self.handle_completion(session_id, success, reason, from_device, from_node).await?;
            }
            // These are handled by handle_request, not handle_response
            PairingMessage::PairingRequest { .. } | PairingMessage::Response { .. } => {
                self.log_warn("Received PairingRequest or Response in handle_response - this should be handled by handle_request").await;
            }
        }

        self.log_debug("handle_response completed").await;
        Ok(())
    }

    async fn handle_event(&self, event: ProtocolEvent) -> Result<()> {
        match event {
            ProtocolEvent::DeviceDisconnected { device_id } => {
                // Clean up any active sessions for this device
                let mut sessions = self.active_sessions.write().await;
                sessions.retain(|_, session| session.remote_device_id != Some(device_id));
            }
            _ => {}
        }

        Ok(())
    }

    fn as_any(&self) -> &dyn std::any::Any {
        self
    }
}```

## src/services/networking/protocols/pairing/messages.rs

```rust
//! Pairing protocol message definitions

use crate::services::networking::device::DeviceInfo;
use serde::{Deserialize, Serialize};
use uuid::Uuid;

/// Messages exchanged during the pairing protocol
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum PairingMessage {
    // Pairing request with device info
    PairingRequest {
        session_id: Uuid,
        device_info: DeviceInfo,
        public_key: Vec<u8>,
    },
    // Pairing challenge
    Challenge {
        session_id: Uuid,
        challenge: Vec<u8>,
        device_info: DeviceInfo,  // Initiator's device info
    },
    // Pairing response with signed challenge
    Response {
        session_id: Uuid,
        response: Vec<u8>,
        device_info: DeviceInfo,
    },
    // Pairing completion
    Complete {
        session_id: Uuid,
        success: bool,
        reason: Option<String>,
    },
}```

## src/services/networking/protocols/pairing/joiner.rs

```rust
//! Joiner-specific pairing logic

use super::{
    messages::PairingMessage,
    types::{PairingSession, PairingState},
    PairingProtocolHandler,
};
use crate::services::networking::{
    device::{DeviceInfo, SessionKeys},
    NetworkingError, Result,
};
use iroh::net::key::NodeId;
use uuid::Uuid;

impl PairingProtocolHandler {
    /// Handle a pairing challenge (Joiner receives this from Initiator)
    pub(crate) async fn handle_pairing_challenge(
        &self,
        session_id: Uuid,
        challenge: Vec<u8>,
        initiator_device_info: DeviceInfo,
    ) -> Result<Vec<u8>> {
        self.log_info(&format!(
            "handle_pairing_challenge ENTRY - session {} with {} bytes",
            session_id,
            challenge.len()
        )).await;

        // Sign the challenge
        self.log_debug("About to sign challenge...").await;
        let signature = match self.identity.sign(&challenge) {
            Ok(sig) => {
                self.log_debug(&format!(
                    "Successfully signed challenge, signature is {} bytes",
                    sig.len()
                )).await;
                sig
            }
            Err(e) => {
                self.log_error(&format!("FAILED to sign challenge: {}", e)).await;
                return Err(e);
            }
        };

        // Get local device info with proper network fingerprint
        self.log_debug("About to get local device info...").await;
        let device_info = match self.get_device_info().await {
            Ok(info) => {
                self.log_debug(&format!(
                    "Successfully got local device info for device {} with node_id {}",
                    info.device_id, info.network_fingerprint.node_id
                )).await;
                info
            }
            Err(e) => {
                self.log_error(&format!("FAILED to get local device info: {}", e)).await;
                return Err(e);
            }
        };

        // Complete pairing immediately after successful challenge response since crypto exchange is done
        self.log_debug("About to complete pairing after challenge response...").await;
        
        // Generate shared secret and complete pairing
        let shared_secret = self.generate_shared_secret(session_id).await?;
        let session_keys = SessionKeys::from_shared_secret(shared_secret.clone());
        
        // Complete pairing in device registry
        let actual_device_id = initiator_device_info.device_id;
        {
            let mut registry = self.device_registry.write().await;
            if let Err(e) = registry.complete_pairing(
                actual_device_id,
                initiator_device_info.clone(),
                session_keys.clone(),
            ).await {
                self.log_error(&format!("Failed to complete pairing in registry: {}", e)).await;
                return Err(e);
            }
        }
        
        // Update session state to completed
        {
            let mut sessions = self.active_sessions.write().await;
            if let Some(session) = sessions.get_mut(&session_id) {
                self.log_debug(&format!(
                    "Found session {}, updating state from {:?} to Completed",
                    session_id, session.state
                )).await;
                session.state = PairingState::Completed;
                session.remote_device_id = Some(initiator_device_info.device_id);
                session.remote_device_info = Some(initiator_device_info.clone());
                session.shared_secret = Some(shared_secret.clone());
                self.log_info(&format!(
                    "Session {} completed with shared secret for {}",
                    session_id, initiator_device_info.device_name
                )).await;
            } else {
                self.log_error(&format!(
                    "ERROR: Session {} not found when trying to complete",
                    session_id
                )).await;
            }
        }

        // Send response
        self.log_debug("About to create response message...").await;
        let response = PairingMessage::Response {
            session_id,
            response: signature,
            device_info,
        };

        self.log_debug("About to serialize response...").await;
        let serialized = serde_json::to_vec(&response).map_err(|e| {
            NetworkingError::Serialization(e)
        })?;

        self.log_info(&format!(
            "handle_pairing_challenge SUCCESS - returning {} bytes",
            serialized.len()
        )).await;
        Ok(serialized)
    }

    /// Handle completion message (Joiner receives this from Initiator)
    pub(crate) async fn handle_completion(
        &self,
        session_id: Uuid,
        success: bool,
        reason: Option<String>,
        from_device: Uuid,
        from_node: NodeId,
    ) -> Result<()> {
        self.log_info(&format!(
            "Received completion message for session {} - success: {}",
            session_id, success
        )).await;

        if success {
            // Generate shared secret and complete pairing on joiner's side
            match self.generate_shared_secret(session_id).await {
                Ok(shared_secret) => {
                    self.log_debug(&format!(
                        "Generated shared secret of {} bytes",
                        shared_secret.len()
                    )).await;

                    // Create session keys
                    let session_keys =
                        SessionKeys::from_shared_secret(shared_secret.clone());

                    // Get Initiator's device info from the session state (received in Challenge message)
                    let initiator_device_info = {
                        let sessions = self.active_sessions.read().await;
                        if let Some(session) = sessions.get(&session_id) {
                            // Use the stored device info from the Challenge message
                            if let Some(device_info) = &session.remote_device_info {
                                device_info.clone()
                            } else {
                                // Fallback if no device info stored (shouldn't happen in normal flow)
                                self.log_warn("No remote device info stored in session, using fallback").await;
                                crate::services::networking::device::DeviceInfo {
                                    device_id: from_device,
                                    device_name: format!("Remote Device {}", &from_device.to_string()[..8]),
                                    device_type: crate::services::networking::device::DeviceType::Desktop,
                                    os_version: "Unknown".to_string(),
                                    app_version: "Unknown".to_string(),
                                    network_fingerprint: crate::services::networking::utils::identity::NetworkFingerprint {
                                        node_id: from_node.to_string(),
                                        public_key_hash: "unknown".to_string(),
                                    },
                                    last_seen: chrono::Utc::now(),
                                }
                            }
                        } else {
                            return Err(crate::services::networking::NetworkingError::Protocol(
                                "Session not found when completing pairing".to_string()
                            ));
                        }
                    };

                    // Complete pairing in device registry
                    // Use the actual device ID from device_info to ensure consistency
                    let actual_device_id = initiator_device_info.device_id;
                    let pairing_result = {
                        let mut registry = self.device_registry.write().await;
                        registry.complete_pairing(
                            actual_device_id,
                            initiator_device_info.clone(),
                            session_keys.clone(),
                        ).await
                    }; // Release write lock here

                    match pairing_result {
                        Ok(()) => {
                            // Update session state FIRST before any other operations that might fail
                            {
                                let mut sessions = self.active_sessions.write().await;
                                if let Some(session) = sessions.get_mut(&session_id) {
                                    session.state = PairingState::Completed;
                                    session.shared_secret = Some(shared_secret.clone());
                                    session.remote_device_id = Some(actual_device_id);
                                }
                            }

                            self.log_info("Successfully completed pairing").await;

                            // Mark Initiator as connected (optional - pairing already completed)
                            let initiator_node_id = Some(from_node); // Use node from completion message

                            if let Some(node_id) = initiator_node_id {
                                let simple_connection = crate::services::networking::device::DeviceConnection {
                                    addresses: vec![], // Will be filled in later
                                    latency_ms: None,
                                    rx_bytes: 0,
                                    tx_bytes: 0,
                                };

                                let _mark_result = {
                                    let mut registry = self.device_registry.write().await;
                                    registry.mark_connected(actual_device_id, simple_connection).await
                                };
                            }
                        }
                        Err(e) => {
                            self.log_error(&format!("Failed to complete pairing in device registry: {}", e)).await;
                        }
                    }
                }
                Err(e) => {
                    self.log_error(&format!("Failed to generate shared secret: {}", e)).await;
                    let mut sessions = self.active_sessions.write().await;
                    if let Some(session) = sessions.get_mut(&session_id) {
                        session.state = PairingState::Failed {
                            reason: format!("Failed to generate shared secret: {}", e),
                        };
                    }
                }
            }
        } else {
            // Pairing failed
            let failure_reason = reason.unwrap_or_else(|| "Pairing failed".to_string());
            let mut sessions = self.active_sessions.write().await;
            if let Some(session) = sessions.get_mut(&session_id) {
                session.state = PairingState::Failed {
                    reason: failure_reason.clone(),
                };
                self.log_error(&format!(
                    "Session {} marked as failed: {}",
                    session_id, failure_reason
                )).await;
            } else {
                self.log_error(&format!(
                    "ERROR: Session {} not found when processing completion",
                    session_id
                )).await;
            }
        }

        Ok(())
    }
}```

## src/services/networking/protocols/file_transfer.rs

```rust
//! File transfer protocol for cross-device file operations

use crate::services::networking::utils::logging::NetworkLogger;
use crate::services::networking::{NetworkingError, Result};
use async_trait::async_trait;
use iroh::net::key::NodeId;
use serde::{Deserialize, Serialize};
use std::{
	collections::HashMap,
	path::PathBuf,
	sync::{Arc, RwLock},
	time::{Duration, SystemTime},
};
use tokio::{fs::File, io::AsyncReadExt};
use uuid::Uuid;

// Encryption imports
use chacha20poly1305::{
	aead::{Aead, AeadCore, KeyInit, OsRng},
	ChaCha20Poly1305, Nonce,
};
use hkdf::Hkdf;
use sha2::Sha256;

/// Session keys for device-to-device encryption
#[derive(Debug, Clone)]
pub struct SessionKeys {
	pub send_key: Vec<u8>,    // 32-byte HKDF-derived send key
	pub receive_key: Vec<u8>, // 32-byte HKDF-derived receive key
}

/// File transfer protocol handler
pub struct FileTransferProtocolHandler {
	/// Active transfer sessions
	sessions: Arc<RwLock<HashMap<Uuid, TransferSession>>>,
	/// Protocol configuration
	config: TransferConfig,
	/// Device registry for session keys
	device_registry:
		Option<Arc<tokio::sync::RwLock<crate::services::networking::device::DeviceRegistry>>>,
	/// Logger for protocol operations
	logger: Arc<dyn NetworkLogger>,
}

/// Configuration for file transfers
#[derive(Debug, Clone)]
pub struct TransferConfig {
	/// Default chunk size for file streaming
	pub chunk_size: u32,
	/// Maximum concurrent transfers
	pub max_concurrent_transfers: u32,
	/// Transfer timeout
	pub transfer_timeout: Duration,
	/// Enable integrity verification
	pub verify_checksums: bool,
}

impl Default for TransferConfig {
	fn default() -> Self {
		Self {
			chunk_size: 64 * 1024, // 64KB chunks
			max_concurrent_transfers: 10,
			transfer_timeout: Duration::from_secs(300), // 5 minutes
			verify_checksums: true,
		}
	}
}

/// Active transfer session
#[derive(Debug, Clone)]
pub struct TransferSession {
	pub id: Uuid,
	pub file_metadata: FileMetadata,
	pub mode: TransferMode,
	pub state: TransferState,
	pub created_at: SystemTime,
	pub bytes_transferred: u64,
	pub chunks_received: Vec<u32>,
	pub source_device: Option<Uuid>,
	pub destination_device: Option<Uuid>,
	pub destination_path: String,
}

/// Transfer state machine
#[derive(Debug, Clone, PartialEq)]
pub enum TransferState {
	/// Waiting for transfer to be accepted
	Pending,
	/// Transfer in progress
	Active,
	/// Transfer completed successfully
	Completed,
	/// Transfer failed
	Failed(String),
	/// Transfer cancelled
	Cancelled,
}

/// Transfer modes for different use cases
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum TransferMode {
	/// Trusted device copy (automatic, uses session keys)
	TrustedCopy,
	/// Ephemeral sharing (requires consent, uses ephemeral keys)
	EphemeralShare {
		ephemeral_pubkey: [u8; 32],
		sender_identity: String,
	},
}

/// File metadata for transfer operations
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct FileMetadata {
	pub name: String,
	pub size: u64,
	pub modified: Option<SystemTime>,
	pub is_directory: bool,
	pub checksum: Option<String>, // ContentHashGenerator hash
	pub mime_type: Option<String>,
}

/// Universal message types for file operations
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum FileTransferMessage {
	/// Request to initiate file transfer
	TransferRequest {
		transfer_id: Uuid,
		file_metadata: FileMetadata,
		transfer_mode: TransferMode,
		chunk_size: u32,
		total_chunks: u32,
		destination_path: String,
	},

	/// Response to transfer request
	TransferResponse {
		transfer_id: Uuid,
		accepted: bool,
		reason: Option<String>,
		supported_resume: bool,
	},

	/// File data chunk
	FileChunk {
		transfer_id: Uuid,
		chunk_index: u32,
		data: Vec<u8>,            // Encrypted data
		nonce: [u8; 12],          // ChaCha20-Poly1305 nonce
		chunk_checksum: [u8; 32], // Checksum of original (unencrypted) data
	},

	/// Acknowledge received chunk
	ChunkAck {
		transfer_id: Uuid,
		chunk_index: u32,
		next_expected: u32,
	},

	/// Transfer completion notification
	TransferComplete {
		transfer_id: Uuid,
		final_checksum: String, // ContentHashGenerator hash
		total_bytes: u64,
	},

	/// Transfer error or cancellation
	TransferError {
		transfer_id: Uuid,
		error_type: TransferErrorType,
		message: String,
		recoverable: bool,
	},

	/// Final acknowledgment from receiver after getting TransferComplete
	TransferFinalAck { transfer_id: Uuid },
}

/// Types of transfer errors
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum TransferErrorType {
	NetworkError,
	FileSystemError,
	PermissionDenied,
	InsufficientSpace,
	ChecksumMismatch,
	Timeout,
	Cancelled,
	ProtocolError,
}

impl FileTransferProtocolHandler {
	/// Create a new file transfer protocol handler
	pub fn new(config: TransferConfig, logger: Arc<dyn NetworkLogger>) -> Self {
		Self {
			sessions: Arc::new(RwLock::new(HashMap::new())),
			config,
			device_registry: None,
			logger,
		}
	}

	/// Helper function to create a truncated version of FileTransferMessage for logging
	fn truncate_message_for_logging(message: &FileTransferMessage) -> String {
		match message {
			FileTransferMessage::TransferRequest { transfer_id, file_metadata, transfer_mode, chunk_size, total_chunks, destination_path } => {
				format!("TransferRequest {{ transfer_id: {}, file_metadata: FileMetadata {{ name: \"{}\", size: {}, is_directory: {}, checksum: {:?}, .. }}, transfer_mode: {:?}, chunk_size: {}, total_chunks: {}, destination_path: \"{}\" }}", 
					transfer_id, file_metadata.name, file_metadata.size, file_metadata.is_directory, 
					file_metadata.checksum.as_ref().map(|c| &c[..16]).unwrap_or("None"), 
					transfer_mode, chunk_size, total_chunks, destination_path)
			},
			FileTransferMessage::FileChunk { transfer_id, chunk_index, data, nonce, chunk_checksum } => {
				format!("FileChunk {{ transfer_id: {}, chunk_index: {}, data: [{} bytes], nonce: [{} bytes], chunk_checksum: [{} bytes] }}", 
					transfer_id, chunk_index, data.len(), nonce.len(), chunk_checksum.len())
			},
			FileTransferMessage::TransferComplete { transfer_id, final_checksum, total_bytes } => {
				format!("TransferComplete {{ transfer_id: {}, final_checksum: \"{}\", total_bytes: {} }}", 
					transfer_id, 
					if final_checksum.len() > 16 { format!("{}...", &final_checksum[..16]) } else { final_checksum.clone() }, 
					total_bytes)
			},
			FileTransferMessage::TransferResponse { transfer_id, accepted, reason, supported_resume } => {
				format!("TransferResponse {{ transfer_id: {}, accepted: {}, reason: {:?}, supported_resume: {} }}", 
					transfer_id, accepted, reason, supported_resume)
			},
			FileTransferMessage::ChunkAck { transfer_id, chunk_index, next_expected } => {
				format!("ChunkAck {{ transfer_id: {}, chunk_index: {}, next_expected: {} }}", 
					transfer_id, chunk_index, next_expected)
			},
			FileTransferMessage::TransferError { transfer_id, error_type, message, recoverable } => {
				format!("TransferError {{ transfer_id: {}, error_type: {:?}, message: \"{}\", recoverable: {} }}", 
					transfer_id, error_type, message, recoverable)
			},
			FileTransferMessage::TransferFinalAck { transfer_id } => {
				format!("TransferFinalAck {{ transfer_id: {} }}", transfer_id)
			}
		}
	}

	/// Set the device registry for session key lookup
	pub fn set_device_registry(
		&mut self,
		device_registry: Arc<
			tokio::sync::RwLock<crate::services::networking::device::DeviceRegistry>,
		>,
	) {
		self.device_registry = Some(device_registry);
	}

	/// Derive chunk encryption key from session keys
	fn derive_chunk_key(
		&self,
		session_send_key: &[u8],
		transfer_id: &Uuid,
		chunk_index: u32,
	) -> Result<[u8; 32]> {
		let hk = Hkdf::<Sha256>::new(None, session_send_key);
		let info = format!("spacedrive-chunk-{}-{}", transfer_id, chunk_index);
		let mut key = [0u8; 32];
		hk.expand(info.as_bytes(), &mut key)
			.map_err(|e| NetworkingError::Protocol(format!("Key derivation failed: {}", e)))?;
		Ok(key)
	}

	/// Encrypt chunk data using ChaCha20-Poly1305
	pub fn encrypt_chunk(
		&self,
		session_send_key: &[u8],
		transfer_id: &Uuid,
		chunk_index: u32,
		data: &[u8],
	) -> Result<(Vec<u8>, [u8; 12])> {
		// Derive chunk-specific key
		let chunk_key = self.derive_chunk_key(session_send_key, transfer_id, chunk_index)?;

		// Create cipher
		let cipher = ChaCha20Poly1305::new_from_slice(&chunk_key)
			.map_err(|e| NetworkingError::Protocol(format!("Cipher creation failed: {}", e)))?;

		// Generate nonce
		let nonce = ChaCha20Poly1305::generate_nonce(&mut OsRng);

		// Encrypt data
		let ciphertext = cipher
			.encrypt(&nonce, data)
			.map_err(|e| NetworkingError::Protocol(format!("Encryption failed: {}", e)))?;

		Ok((ciphertext, nonce.into()))
	}

	/// Decrypt chunk data using ChaCha20-Poly1305
	fn decrypt_chunk(
		&self,
		session_receive_key: &[u8],
		transfer_id: &Uuid,
		chunk_index: u32,
		encrypted_data: &[u8],
		nonce: &[u8; 12],
	) -> Result<Vec<u8>> {
		// Derive same chunk-specific key (using receive key)
		let chunk_key = self.derive_chunk_key(session_receive_key, transfer_id, chunk_index)?;

		// Create cipher
		let cipher = ChaCha20Poly1305::new_from_slice(&chunk_key)
			.map_err(|e| NetworkingError::Protocol(format!("Cipher creation failed: {}", e)))?;

		// Decrypt data
		let nonce = Nonce::from_slice(nonce);
		let plaintext = cipher
			.decrypt(nonce, encrypted_data)
			.map_err(|e| NetworkingError::Protocol(format!("Decryption failed: {}", e)))?;

		Ok(plaintext)
	}

	/// Get session keys for a device from the device registry
	pub async fn get_session_keys_for_device(&self, device_id: Uuid) -> Result<SessionKeys> {
		let device_registry = self
			.device_registry
			.as_ref()
			.ok_or_else(|| NetworkingError::Protocol("Device registry not set".to_string()))?;

		let registry_guard = device_registry.read().await;
		let session_keys = registry_guard.get_session_keys(device_id).ok_or_else(|| {
			NetworkingError::Protocol(format!("No session keys found for device {}", device_id))
		})?;

		Ok(SessionKeys {
			send_key: session_keys.send_key,
			receive_key: session_keys.receive_key,
		})
	}

	/// Create with default configuration
	pub fn new_default(logger: Arc<dyn NetworkLogger>) -> Self {
		Self::new(TransferConfig::default(), logger)
	}

	/// Initiate a file transfer to a device
	pub async fn initiate_transfer(
		&self,
		target_device: Uuid,
		file_path: PathBuf,
		transfer_mode: TransferMode,
	) -> Result<Uuid> {
		// Read file metadata
		let metadata = tokio::fs::metadata(&file_path).await.map_err(|e| {
			NetworkingError::file_system_error(format!("Failed to read file metadata: {}", e))
		})?;

		let file_metadata = FileMetadata {
			name: file_path
				.file_name()
				.unwrap_or_default()
				.to_string_lossy()
				.to_string(),
			size: metadata.len(),
			modified: metadata.modified().ok(),
			is_directory: metadata.is_dir(),
			checksum: if self.config.verify_checksums {
				Some(self.calculate_file_checksum(&file_path).await?)
			} else {
				None
			},
			mime_type: None, // TODO: Add MIME type detection
		};

		let transfer_id = Uuid::new_v4();
		let session = TransferSession {
			id: transfer_id,
			file_metadata: file_metadata.clone(),
			mode: transfer_mode.clone(),
			state: TransferState::Pending,
			created_at: SystemTime::now(),
			bytes_transferred: 0,
			chunks_received: Vec::new(),
			source_device: None, // Will be set when we know our device ID
			destination_device: Some(target_device),
			destination_path: "/tmp".to_string(), // Default destination, will be set by caller
		};

		// Store session
		{
			let mut sessions = self.sessions.write().unwrap();
			sessions.insert(transfer_id, session);
		}

		Ok(transfer_id)
	}

	/// Get transfer session by ID
	pub fn get_session(&self, transfer_id: &Uuid) -> Option<TransferSession> {
		let sessions = self.sessions.read().unwrap();
		sessions.get(transfer_id).cloned()
	}

	/// Update transfer session state
	pub fn update_session_state(&self, transfer_id: &Uuid, state: TransferState) -> Result<()> {
		let mut sessions = self.sessions.write().unwrap();
		if let Some(session) = sessions.get_mut(transfer_id) {
			session.state = state;
			Ok(())
		} else {
			Err(NetworkingError::transfer_not_found_error(*transfer_id))
		}
	}

	/// Record chunk received
	pub fn record_chunk_received(
		&self,
		transfer_id: &Uuid,
		chunk_index: u32,
		bytes: u64,
	) -> Result<()> {
		let mut sessions = self.sessions.write().unwrap();
		if let Some(session) = sessions.get_mut(transfer_id) {
			session.chunks_received.push(chunk_index);
			session.bytes_transferred += bytes;
			Ok(())
		} else {
			Err(NetworkingError::transfer_not_found_error(*transfer_id))
		}
	}

	/// Calculate file checksum using ContentHashGenerator
	async fn calculate_file_checksum(&self, path: &PathBuf) -> Result<String> {
		crate::domain::content_identity::ContentHashGenerator::generate_content_hash(path)
			.await
			.map_err(|e| {
				NetworkingError::file_system_error(format!(
					"Failed to generate content hash: {}",
					e
				))
			})
	}

	/// Calculate file checksum as bytes for compatibility
	async fn calculate_file_checksum_bytes(&self, path: &PathBuf) -> Result<[u8; 32]> {
		// Generate the content hash and then hash it again for 32-byte output
		let content_hash = self.calculate_file_checksum(path).await?;
		let mut hasher = blake3::Hasher::new();
		hasher.update(content_hash.as_bytes());
		Ok(hasher.finalize().into())
	}

	/// Handle transfer request message
	async fn handle_transfer_request(
		&self,
		from_device: Uuid,
		request: FileTransferMessage,
	) -> Result<FileTransferMessage> {
		if let FileTransferMessage::TransferRequest {
			transfer_id,
			file_metadata,
			transfer_mode,
			destination_path,
			..
		} = request
		{
			// For trusted devices, auto-accept transfers
			let accepted = match transfer_mode {
				TransferMode::TrustedCopy => true,
				TransferMode::EphemeralShare { .. } => {
					// For ephemeral shares, would need user consent
					// For now, auto-accept but this should trigger UI prompt
					true
				}
			};

			if accepted {
				// Create session for incoming transfer
				let session = TransferSession {
					id: transfer_id,
					file_metadata,
					mode: transfer_mode,
					state: TransferState::Active,
					created_at: SystemTime::now(),
					bytes_transferred: 0,
					chunks_received: Vec::new(),
					source_device: Some(from_device),
					destination_device: None, // We are the destination
					destination_path,
				};

				let mut sessions = self.sessions.write().unwrap();
				sessions.insert(transfer_id, session);
			}

			Ok(FileTransferMessage::TransferResponse {
				transfer_id,
				accepted,
				reason: if accepted {
					None
				} else {
					Some("User declined".to_string())
				},
				supported_resume: true,
			})
		} else {
			Err(NetworkingError::Protocol(
				"Invalid transfer request message".to_string(),
			))
		}
	}

	/// Handle file chunk message
	async fn handle_file_chunk(
		&self,
		from_device: Uuid,
		chunk: FileTransferMessage,
	) -> Result<FileTransferMessage> {
		if let FileTransferMessage::FileChunk {
			transfer_id,
			chunk_index,
			data,
			nonce,
			chunk_checksum,
		} = chunk
		{
			// Get session keys for decryption
			let session_keys = self.get_session_keys_for_device(from_device).await?;

			// Decrypt chunk data
			let decrypted_data = self.decrypt_chunk(
				&session_keys.receive_key,
				&transfer_id,
				chunk_index,
				&data,
				&nonce,
			)?;

			// Verify chunk checksum (of decrypted data)
			if self.config.verify_checksums {
				let calculated_checksum = blake3::hash(&decrypted_data);
				if calculated_checksum.as_bytes() != &chunk_checksum {
					return Ok(FileTransferMessage::TransferError {
						transfer_id,
						error_type: TransferErrorType::ChecksumMismatch,
						message: format!("Chunk {} checksum mismatch", chunk_index),
						recoverable: true,
					});
				}
			}

			// Record chunk received (using decrypted size)
			self.record_chunk_received(&transfer_id, chunk_index, decrypted_data.len() as u64)?;

			// Write decrypted chunk to file
			self.write_chunk_to_file(&transfer_id, chunk_index, &decrypted_data)
				.await
				.map_err(|e| {
					NetworkingError::Protocol(format!("Failed to write chunk to file: {}", e))
				})?;

			// Calculate next expected chunk
			let next_expected = {
				let sessions = self.sessions.read().unwrap();
				if let Some(session) = sessions.get(&transfer_id) {
					let mut received_chunks = session.chunks_received.clone();
					received_chunks.sort();

					// Find the first missing chunk
					let mut next = 0;
					for &chunk in &received_chunks {
						if chunk == next {
							next += 1;
						} else {
							break;
						}
					}
					next
				} else {
					return Err(NetworkingError::transfer_not_found_error(transfer_id));
				}
			};

			Ok(FileTransferMessage::ChunkAck {
				transfer_id,
				chunk_index,
				next_expected,
			})
		} else {
			Err(NetworkingError::Protocol(
				"Invalid file chunk message".to_string(),
			))
		}
	}

	/// Handle transfer completion
	async fn handle_transfer_complete(
		&self,
		from_device: Uuid,
		completion: FileTransferMessage,
	) -> Result<FileTransferMessage> {
		if let FileTransferMessage::TransferComplete {
			transfer_id,
			final_checksum,
			total_bytes,
		} = completion
		{
			// Verify final checksum if configured
			if self.config.verify_checksums {
				// Get the received file path
				let received_file_path = {
					let sessions = self.sessions.read().unwrap();
					if let Some(session) = sessions.get(&transfer_id) {
						let destination_path = PathBuf::from(&session.destination_path);
						destination_path.join(&session.file_metadata.name)
					} else {
						return Err(NetworkingError::transfer_not_found_error(transfer_id));
					}
				};

				// Calculate checksum of received file
				let received_checksum = self.calculate_file_checksum(&received_file_path).await?;

				// Compare with sender's checksum
				if received_checksum != final_checksum {
					self.update_session_state(
						&transfer_id,
						TransferState::Failed(format!(
							"Final checksum mismatch: expected {}, got {}",
							final_checksum, received_checksum
						)),
					)?;

					return Ok(FileTransferMessage::TransferError {
						transfer_id,
						error_type: TransferErrorType::ChecksumMismatch,
						message: "Final file checksum verification failed".to_string(),
						recoverable: false,
					});
				}

				println!("âœ… File checksum verified: {}", received_checksum);
			}

			// Mark transfer as completed
			self.update_session_state(&transfer_id, TransferState::Completed)?;

			println!(
				"âœ… File transfer {} completed: {} bytes",
				transfer_id, total_bytes
			);

			// Return final acknowledgment
			Ok(FileTransferMessage::TransferFinalAck { transfer_id })
		} else {
			Err(NetworkingError::Protocol(
				"Invalid transfer complete message".to_string(),
			))
		}
	}

	/// Get active transfers
	pub fn get_active_transfers(&self) -> Vec<TransferSession> {
		let sessions = self.sessions.read().unwrap();
		sessions
			.values()
			.filter(|session| {
				matches!(
					session.state,
					TransferState::Active | TransferState::Pending
				)
			})
			.cloned()
			.collect()
	}

	/// Cancel a transfer
	pub fn cancel_transfer(&self, transfer_id: &Uuid) -> Result<()> {
		self.update_session_state(transfer_id, TransferState::Cancelled)
	}

	/// Clean up completed/failed transfers older than specified duration
	pub fn cleanup_old_transfers(&self, max_age: Duration) {
		let mut sessions = self.sessions.write().unwrap();
		let cutoff = SystemTime::now() - max_age;

		sessions.retain(|_, session| match session.state {
			TransferState::Active | TransferState::Pending => true,
			_ => session.created_at > cutoff,
		});
	}

	/// Write a file chunk to the destination file
	async fn write_chunk_to_file(
		&self,
		transfer_id: &Uuid,
		chunk_index: u32,
		data: &[u8],
	) -> std::result::Result<(), String> {
		use tokio::io::{AsyncSeekExt, AsyncWriteExt};

		// Get session info to determine file path and chunk size
		let (file_path, chunk_size) = {
			let sessions = self.sessions.read().unwrap();
			let session = sessions
				.get(transfer_id)
				.ok_or_else(|| "Transfer session not found".to_string())?;

			// Use the destination path from the transfer request (already includes filename)
			let file_path = PathBuf::from(&session.destination_path);

			(file_path, 64 * 1024u32) // 64KB chunk size
		};

		// Ensure parent directory exists
		if let Some(parent) = file_path.parent() {
			tokio::fs::create_dir_all(parent)
				.await
				.map_err(|e| format!("Failed to create parent directory: {}", e))?;
		}

		// Open file for writing (create if doesn't exist)
		let mut file = tokio::fs::OpenOptions::new()
			.create(true)
			.write(true)
			.open(&file_path)
			.await
			.map_err(|e| format!("Failed to open file for writing: {}", e))?;

		// Calculate file offset for this chunk
		let offset = chunk_index as u64 * chunk_size as u64;

		// Seek to the correct position and write the chunk
		file.seek(std::io::SeekFrom::Start(offset))
			.await
			.map_err(|e| format!("Failed to seek in file: {}", e))?;
		file.write_all(data)
			.await
			.map_err(|e| format!("Failed to write chunk data: {}", e))?;
		file.flush()
			.await
			.map_err(|e| format!("Failed to flush file: {}", e))?;

		// Note: Using println for chunk writing as this is detailed debug info
		// that might be too verbose for standard logging

		Ok(())
	}

	/// Handle incoming transfer request
	async fn handle_incoming_transfer_request(
		&self,
		device_id: Uuid,
		transfer_id: Uuid,
		file_metadata: FileMetadata,
		destination_path: String,
	) -> Result<()> {
		self.logger
			.info(&format!(
				"Handling transfer request for file: {} ({} bytes) -> {}",
				file_metadata.name, file_metadata.size, destination_path
			))
			.await;

		// Create new transfer session
		let session = TransferSession {
			id: transfer_id,
			file_metadata: file_metadata.clone(),
			mode: TransferMode::TrustedCopy,
			state: TransferState::Pending,
			created_at: SystemTime::now(),
			bytes_transferred: 0,
			chunks_received: Vec::new(),
			source_device: Some(device_id),
			destination_device: None,
			destination_path: destination_path.clone(),
		};

		// Store session
		{
			let mut sessions = self.sessions.write().unwrap();
			sessions.insert(transfer_id, session);
		}

		// Accept the transfer (for trusted devices, auto-accept)
		self.update_session_state(&transfer_id, TransferState::Active)?;
		self.logger
			.info(&format!(
				"Auto-accepted transfer {} from trusted device {}",
				transfer_id, device_id
			))
			.await;

		Ok(())
	}

	/// Handle incoming file chunk
	async fn handle_incoming_file_chunk(
		&self,
		transfer_id: Uuid,
		chunk_index: u32,
		encrypted_data: Vec<u8>,
		nonce: [u8; 12],
		chunk_checksum: [u8; 32],
	) -> Result<()> {
		self.logger
			.debug(&format!(
				"Handling file chunk {} for transfer {}",
				chunk_index, transfer_id
			))
			.await;

		// Get the source device ID from the session
		let source_device_id = {
			let sessions = self.sessions.read().unwrap();
			if let Some(session) = sessions.get(&transfer_id) {
				session.source_device.ok_or_else(|| {
					NetworkingError::Protocol("No source device for transfer".to_string())
				})?
			} else {
				return Err(NetworkingError::Protocol(
					"Transfer session not found".to_string(),
				));
			}
		};

		// Get session keys for decryption
		let session_keys = if let Some(device_registry) = &self.device_registry {
			let registry = device_registry.read().await;
			registry.get_session_keys(source_device_id).ok_or_else(|| {
				NetworkingError::Protocol(format!(
					"No session keys for device {}",
					source_device_id
				))
			})?
		} else {
			return Err(NetworkingError::Protocol(
				"Device registry not available".to_string(),
			));
		};

		// Decrypt chunk data
		let chunk_data = self.decrypt_chunk(
			&session_keys.receive_key,
			&transfer_id,
			chunk_index,
			&encrypted_data,
			&nonce,
		)?;

		self.logger
			.debug(&format!(
				"Decrypted chunk {} ({} bytes -> {} bytes)",
				chunk_index,
				encrypted_data.len(),
				chunk_data.len()
			))
			.await;

		// Verify chunk checksum (of decrypted data)
		let calculated_checksum = blake3::hash(&chunk_data);
		if calculated_checksum.as_bytes() != &chunk_checksum {
			self.logger
				.error(&format!(
					"Chunk {} checksum mismatch after decryption",
					chunk_index
				))
				.await;
			return Err(NetworkingError::Protocol(format!(
				"Chunk {} checksum mismatch after decryption",
				chunk_index
			)));
		}

		self.logger
			.debug(&format!("Checksum verified for chunk {}", chunk_index))
			.await;

		// Write chunk to file
		if let Err(e) = self
			.write_chunk_to_file(&transfer_id, chunk_index, &chunk_data)
			.await
		{
			return Err(NetworkingError::Protocol(format!(
				"Failed to write chunk {}: {}",
				chunk_index, e
			)));
		}

		// Update session progress
		{
			let mut sessions = self.sessions.write().unwrap();
			if let Some(session) = sessions.get_mut(&transfer_id) {
				session.bytes_transferred += chunk_data.len() as u64;
				session.chunks_received.push(chunk_index);
				session.chunks_received.sort();
			}
		}

		self.logger
			.debug(&format!(
				"Successfully processed chunk {} for transfer {}",
				chunk_index, transfer_id
			))
			.await;
		Ok(())
	}

	/// Handle incoming transfer completion
	async fn handle_incoming_transfer_complete(
		&self,
		transfer_id: Uuid,
		final_checksum: String,
		total_bytes: u64,
	) -> Result<()> {
		let truncated_checksum = if final_checksum.len() > 16 {
			format!("{}...", &final_checksum[..16])
		} else {
			final_checksum.clone()
		};
		self.logger
			.info(&format!(
				"Handling transfer completion for transfer {} ({} bytes, checksum: {})",
				transfer_id,
				total_bytes,
				truncated_checksum
			))
			.await;

		// Mark transfer as completed
		self.update_session_state(&transfer_id, TransferState::Completed)?;

		// TODO: Verify final file checksum
		self.logger
			.info(&format!("Transfer {} completed successfully", transfer_id))
			.await;
		Ok(())
	}
}

#[async_trait]
impl super::ProtocolHandler for FileTransferProtocolHandler {
	fn protocol_name(&self) -> &str {
		"file_transfer"
	}

	async fn handle_stream(
		&self,
		mut send: Box<dyn tokio::io::AsyncWrite + Send + Unpin>,
		mut recv: Box<dyn tokio::io::AsyncRead + Send + Unpin>,
		remote_node_id: NodeId,
	) {
		use tokio::io::{AsyncReadExt, AsyncWriteExt};

		self.logger
			.debug(&format!(
				"FILE_TRANSFER: handle_stream called from node {}",
				remote_node_id
			))
			.await;

		// Read transfer type (1 byte)
		let mut transfer_type = [0u8; 1];
		if let Err(e) = recv.read_exact(&mut transfer_type).await {
			self.logger
				.error(&format!("Failed to read transfer type: {}", e))
				.await;
			return;
		}

		self.logger
			.debug(&format!(
				"FILE_TRANSFER: Received transfer type: {}",
				transfer_type[0]
			))
			.await;

		match transfer_type[0] {
			0 => {
				// File metadata request
				// Read message length
				let mut len_buf = [0u8; 4];
				if let Err(e) = recv.read_exact(&mut len_buf).await {
					self.logger
						.error(&format!("Failed to read message length: {}", e))
						.await;
					return;
				}
				let msg_len = u32::from_be_bytes(len_buf) as usize;

				// Read message
				let mut msg_buf = vec![0u8; msg_len];
				if let Err(e) = recv.read_exact(&mut msg_buf).await {
					self.logger
						.error(&format!("Failed to read message: {}", e))
						.await;
					return;
				}

				// Deserialize and handle
				if let Ok(message) = rmp_serde::from_slice::<FileTransferMessage>(&msg_buf) {
					self.logger
						.debug(&format!("Received file transfer message: {}", Self::truncate_message_for_logging(&message)))
						.await;

					// Get device ID from node ID using device registry
					let device_id = if let Some(device_registry) = &self.device_registry {
						let registry = device_registry.read().await;
						registry
							.get_device_by_node(remote_node_id)
							.unwrap_or_else(|| {
								// Note: Can't use await in closure, this should be refactored
								eprintln!("Warning: Could not find device ID for node {}, using random ID", remote_node_id);
								uuid::Uuid::new_v4()
							})
					} else {
						// Note: Need to await this call properly
						eprintln!("Warning: Device registry not available, using random device ID");
						uuid::Uuid::new_v4()
					};

					// Process the message based on type
					match message {
						FileTransferMessage::TransferRequest {
							transfer_id,
							file_metadata,
							destination_path,
							..
						} => {
							// Handle transfer request
							if let Err(e) = self
								.handle_incoming_transfer_request(
									device_id,
									transfer_id,
									file_metadata,
									destination_path,
								)
								.await
							{
								self.logger
									.error(&format!("Failed to handle transfer request: {}", e))
									.await;
							}
						}
						FileTransferMessage::FileChunk {
							transfer_id,
							chunk_index,
							data,
							nonce,
							chunk_checksum,
						} => {
							// Handle file chunk
							if let Err(e) = self
								.handle_incoming_file_chunk(
									transfer_id,
									chunk_index,
									data,
									nonce,
									chunk_checksum,
								)
								.await
							{
								self.logger
									.error(&format!("Failed to handle file chunk: {}", e))
									.await;
							}
						}
						FileTransferMessage::TransferComplete {
							transfer_id,
							final_checksum,
							total_bytes,
						} => {
							// Handle transfer completion
							if let Err(e) = self
								.handle_incoming_transfer_complete(
									transfer_id,
									final_checksum,
									total_bytes,
								)
								.await
							{
								self.logger
									.error(&format!("Failed to handle transfer completion: {}", e))
									.await;
							}
						}
						_ => {
							self.logger
								.warn("Received unexpected file transfer message type")
								.await;
						}
					}
				}
			}
			1 => {
				// File data stream
				// This would be a raw file transfer
				// For now, just read and discard
				let mut buffer = vec![0u8; 8192];
				while let Ok(n) = recv.read(&mut buffer).await {
					if n == 0 {
						break;
					}
					// Process file data chunk
				}
			}
			_ => {
				self.logger
					.error(&format!("Unknown transfer type: {}", transfer_type[0]))
					.await;
			}
		}
	}

	async fn handle_request(&self, from_device: Uuid, request_data: Vec<u8>) -> Result<Vec<u8>> {
		// Deserialize the request
		let request: FileTransferMessage = rmp_serde::from_slice(&request_data).map_err(|e| {
			NetworkingError::Protocol(format!("Failed to deserialize request: {}", e))
		})?;

		let response = match request {
			FileTransferMessage::TransferRequest { .. } => {
				self.handle_transfer_request(from_device, request).await?
			}
			FileTransferMessage::FileChunk { .. } => {
				self.handle_file_chunk(from_device, request).await?
			}
			FileTransferMessage::TransferComplete { .. } => {
				self.handle_transfer_complete(from_device, request).await?
			}
			_ => {
				return Err(NetworkingError::Protocol(
					"Unsupported request message type".to_string(),
				));
			}
		};

		// Serialize the response
		rmp_serde::to_vec(&response)
			.map_err(|e| NetworkingError::Protocol(format!("Failed to serialize response: {}", e)))
	}

	async fn handle_response(
		&self,
		from_device: Uuid,
		_from_node: NodeId,
		response_data: Vec<u8>,
	) -> Result<()> {
		// Deserialize the response
		let response: FileTransferMessage = rmp_serde::from_slice(&response_data).map_err(|e| {
			NetworkingError::Protocol(format!("Failed to deserialize response: {}", e))
		})?;

		match response {
			FileTransferMessage::TransferResponse {
				transfer_id,
				accepted,
				reason,
				..
			} => {
				if accepted {
					self.update_session_state(&transfer_id, TransferState::Active)?;
					self.logger
						.info(&format!(
							"Transfer {} accepted by device {}",
							transfer_id, from_device
						))
						.await;
				} else {
					let reason = reason.unwrap_or_else(|| "No reason given".to_string());
					self.update_session_state(&transfer_id, TransferState::Failed(reason.clone()))?;
					self.logger
						.warn(&format!(
							"Transfer {} rejected by device {}: {}",
							transfer_id, from_device, reason
						))
						.await;
				}
			}
			FileTransferMessage::ChunkAck {
				transfer_id,
				chunk_index,
				next_expected,
			} => {
				self.logger
					.debug(&format!(
						"Chunk {} acknowledged for transfer {}, next expected: {}",
						chunk_index, transfer_id, next_expected
					))
					.await;
				// TODO: Continue sending next chunks
			}
			FileTransferMessage::TransferError {
				transfer_id,
				error_type,
				message,
				..
			} => {
				self.update_session_state(&transfer_id, TransferState::Failed(message.clone()))?;
				self.logger
					.error(&format!(
						"Transfer {} error: {:?} - {}",
						transfer_id, error_type, message
					))
					.await;
			}
			FileTransferMessage::TransferFinalAck { transfer_id } => {
				self.logger
					.info(&format!(
						"Transfer {} fully acknowledged by receiver",
						transfer_id
					))
					.await;
				// The sender can now consider the transfer fully and cleanly closed
			}
			_ => {
				return Err(NetworkingError::Protocol(
					"Unsupported response message type".to_string(),
				));
			}
		}

		Ok(())
	}

	async fn handle_event(&self, event: super::ProtocolEvent) -> Result<()> {
		match event {
			super::ProtocolEvent::DeviceConnected { device_id } => {
				self.logger
					.info(&format!(
						"Device {} connected - file transfer available",
						device_id
					))
					.await;
			}
			super::ProtocolEvent::DeviceDisconnected { device_id } => {
				self.logger
					.info(&format!(
						"Device {} disconnected - pausing active transfers",
						device_id
					))
					.await;
				// TODO: Pause transfers to this device
			}
			super::ProtocolEvent::ConnectionFailed { device_id, reason } => {
				self.logger
					.warn(&format!(
						"Connection to device {} failed: {} - cancelling transfers",
						device_id, reason
					))
					.await;
				// TODO: Cancel transfers to this device
			}
			_ => {}
		}
		Ok(())
	}

	fn as_any(&self) -> &dyn std::any::Any {
		self
	}
}

/// Error extensions for file transfer
impl NetworkingError {
	pub fn transfer_not_found(transfer_id: Uuid) -> Self {
		Self::Protocol(format!("Transfer not found: {}", transfer_id))
	}

	pub fn file_system(message: String) -> Self {
		Self::Protocol(format!("File system error: {}", message))
	}
}

// Custom error variants for file transfer
impl NetworkingError {
	pub fn transfer_not_found_error(transfer_id: Uuid) -> Self {
		Self::Protocol(format!("Transfer not found: {}", transfer_id))
	}

	pub fn file_system_error(message: String) -> Self {
		Self::Protocol(format!("File system error: {}", message))
	}
}

#[cfg(test)]
mod tests {
	use super::*;
	use crate::services::networking::protocols::ProtocolHandler;
	use crate::services::networking::utils::logging::SilentLogger;

	#[tokio::test]
	async fn test_file_transfer_handler_creation() {
		let logger = Arc::new(SilentLogger);
		let handler = FileTransferProtocolHandler::new_default(logger);
		assert_eq!(handler.protocol_name(), "file_transfer");
		assert!(handler.get_active_transfers().is_empty());
	}

	#[tokio::test]
	async fn test_transfer_session_lifecycle() {
		let logger = Arc::new(SilentLogger);
		let handler = FileTransferProtocolHandler::new_default(logger);
		let transfer_id = Uuid::new_v4();

		// Initially no session
		assert!(handler.get_session(&transfer_id).is_none());

		// Update state should fail for non-existent session
		assert!(handler
			.update_session_state(&transfer_id, TransferState::Active)
			.is_err());
	}
}
```

## src/services/networking/protocols/registry.rs

```rust
//! Protocol registry for managing protocol handlers

use super::{ProtocolEvent, ProtocolHandler};
use iroh::net::key::NodeId;
use crate::services::networking::{NetworkingError, Result};
use std::collections::HashMap;
use std::sync::Arc;
use uuid::Uuid;

/// Registry for protocol handlers
pub struct ProtocolRegistry {
	handlers: HashMap<String, Arc<dyn ProtocolHandler>>,
}

impl ProtocolRegistry {
	/// Create a new protocol registry
	pub fn new() -> Self {
		Self {
			handlers: HashMap::new(),
		}
	}

	/// Register a protocol handler
	pub fn register_handler(&mut self, handler: Arc<dyn ProtocolHandler>) -> Result<()> {
		let protocol_name = handler.protocol_name().to_string();

		if self.handlers.contains_key(&protocol_name) {
			return Err(NetworkingError::Protocol(format!(
				"Protocol {} already registered",
				protocol_name
			)));
		}

		self.handlers.insert(protocol_name, handler);
		Ok(())
	}

	/// Unregister a protocol handler
	pub fn unregister_handler(&mut self, protocol_name: &str) -> Result<()> {
		self.handlers.remove(protocol_name).ok_or_else(|| {
			NetworkingError::Protocol(format!("Protocol {} not found", protocol_name))
		})?;

		Ok(())
	}

	/// Get a protocol handler by name
	pub fn get_handler(&self, protocol_name: &str) -> Option<Arc<dyn ProtocolHandler>> {
		self.handlers.get(protocol_name).cloned()
	}

	/// Handle an incoming request
	pub async fn handle_request(
		&self,
		protocol_name: &str,
		from_device: Uuid,
		request_data: Vec<u8>,
	) -> Result<Vec<u8>> {
		let handler = self.get_handler(protocol_name).ok_or_else(|| {
			NetworkingError::Protocol(format!("No handler for protocol {}", protocol_name))
		})?;

		handler.handle_request(from_device, request_data).await
	}

	/// Handle an incoming response
	pub async fn handle_response(
		&self,
		protocol_name: &str,
		from_device: Uuid,
		from_node: NodeId,
		response_data: Vec<u8>,
	) -> Result<()> {
		let handler = self.get_handler(protocol_name).ok_or_else(|| {
			NetworkingError::Protocol(format!("No handler for protocol {}", protocol_name))
		})?;

		handler.handle_response(from_device, from_node, response_data).await
	}

	/// Broadcast an event to all protocol handlers
	pub async fn broadcast_event(&self, event: ProtocolEvent) -> Result<()> {
		for handler in self.handlers.values() {
			if let Err(e) = handler.handle_event(event.clone()).await {
				// Log error but continue with other handlers
				eprintln!(
					"Protocol {} error handling event: {}",
					handler.protocol_name(),
					e
				);
			}
		}

		Ok(())
	}

	/// Get list of registered protocol names
	pub fn get_protocol_names(&self) -> Vec<String> {
		self.handlers.keys().cloned().collect()
	}

	/// Get the number of registered handlers
	pub fn handler_count(&self) -> usize {
		self.handlers.len()
	}
}

impl Default for ProtocolRegistry {
	fn default() -> Self {
		Self::new()
	}
}
```

## src/services/networking/protocols/mod.rs

```rust
//! Protocol handling system for different message types

pub mod file_transfer;
pub mod messaging;
pub mod pairing;
pub mod registry;

use crate::services::networking::{NetworkingError, Result};
use async_trait::async_trait;
use iroh::net::key::NodeId;
use std::collections::HashMap;
use uuid::Uuid;

pub use file_transfer::{FileTransferMessage, FileTransferProtocolHandler, FileMetadata, TransferMode, TransferSession};
pub use messaging::MessagingProtocolHandler;
pub use pairing::{PairingMessage, PairingProtocolHandler, PairingSession, PairingState};
pub use registry::ProtocolRegistry;

/// Trait for handling specific protocols over Iroh streams
#[async_trait]
pub trait ProtocolHandler: Send + Sync {
	/// Get the protocol name
	fn protocol_name(&self) -> &str;

	/// Handle an incoming stream (bidirectional or unidirectional)
	async fn handle_stream(
		&self,
		send: Box<dyn tokio::io::AsyncWrite + Send + Unpin>,
		recv: Box<dyn tokio::io::AsyncRead + Send + Unpin>,
		remote_node_id: NodeId,
	);

	/// Handle an incoming request (legacy compatibility)
	async fn handle_request(&self, from_device: Uuid, request_data: Vec<u8>) -> Result<Vec<u8>>;

	/// Handle an incoming response (legacy compatibility)
	async fn handle_response(&self, from_device: Uuid, from_node: NodeId, response_data: Vec<u8>) -> Result<()>;

	/// Handle protocol-specific events
	async fn handle_event(&self, event: ProtocolEvent) -> Result<()>;

	/// Enable downcasting to concrete types
	fn as_any(&self) -> &dyn std::any::Any;
}

/// Events that protocols can receive
#[derive(Debug, Clone)]
pub enum ProtocolEvent {
	/// Device connected
	DeviceConnected { device_id: Uuid },

	/// Device disconnected
	DeviceDisconnected { device_id: Uuid },

	/// Connection failed
	ConnectionFailed { device_id: Uuid, reason: String },

	/// Custom protocol event
	Custom {
		protocol: String,
		data: HashMap<String, serde_json::Value>,
	},
}```

## src/services/networking/protocols/messaging.rs

```rust
//! Basic messaging protocol handler

use super::{ProtocolEvent, ProtocolHandler};
use crate::services::networking::{NetworkingError, Result};
use iroh::net::key::NodeId;
use async_trait::async_trait;
use serde::{Deserialize, Serialize};
use uuid::Uuid;

/// Basic messaging protocol handler
pub struct MessagingProtocolHandler;

/// Basic message types
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum Message {
	/// Ping message for connection testing
	Ping {
		timestamp: chrono::DateTime<chrono::Utc>,
		payload: Option<Vec<u8>>,
	},
	/// Pong response
	Pong {
		timestamp: chrono::DateTime<chrono::Utc>,
		original_timestamp: chrono::DateTime<chrono::Utc>,
	},
	/// Generic data message
	Data {
		message_id: Uuid,
		content_type: String,
		payload: Vec<u8>,
	},
	/// Acknowledgment message
	Ack {
		message_id: Uuid,
		success: bool,
		error: Option<String>,
	},
}

impl MessagingProtocolHandler {
	/// Create a new messaging protocol handler
	pub fn new() -> Self {
		Self
	}

	async fn handle_ping(
		&self,
		_from_device: Uuid,
		timestamp: chrono::DateTime<chrono::Utc>,
		_payload: Option<Vec<u8>>,
	) -> Result<Vec<u8>> {
		let response = Message::Pong {
			timestamp: chrono::Utc::now(),
			original_timestamp: timestamp,
		};

		serde_json::to_vec(&response).map_err(|e| NetworkingError::Serialization(e))
	}

	async fn handle_pong(
		&self,
		_from_device: Uuid,
		_timestamp: chrono::DateTime<chrono::Utc>,
		original_timestamp: chrono::DateTime<chrono::Utc>,
	) -> Result<Vec<u8>> {
		let now = chrono::Utc::now();
		let rtt = now.signed_duration_since(original_timestamp);

		println!("Ping RTT: {}ms", rtt.num_milliseconds());

		// Return empty response for pong
		Ok(Vec::new())
	}

	async fn handle_data(
		&self,
		from_device: Uuid,
		message_id: Uuid,
		content_type: String,
		payload: Vec<u8>,
	) -> Result<Vec<u8>> {
		// Process the data message
		println!(
			"Received data message from {}: {} ({} bytes)",
			from_device,
			content_type,
			payload.len()
		);

		// Send acknowledgment
		let response = Message::Ack {
			message_id,
			success: true,
			error: None,
		};

		serde_json::to_vec(&response).map_err(|e| NetworkingError::Serialization(e))
	}

	async fn handle_ack(
		&self,
		_from_device: Uuid,
		message_id: Uuid,
		success: bool,
		error: Option<String>,
	) -> Result<Vec<u8>> {
		if success {
			println!("Message {} acknowledged successfully", message_id);
		} else {
			println!("Message {} failed: {:?}", message_id, error);
		}

		// Return empty response for ack
		Ok(Vec::new())
	}
}

impl Default for MessagingProtocolHandler {
	fn default() -> Self {
		Self::new()
	}
}

#[async_trait]
impl ProtocolHandler for MessagingProtocolHandler {
	fn protocol_name(&self) -> &str {
		"messaging"
	}

	async fn handle_stream(
		&self,
		mut send: Box<dyn tokio::io::AsyncWrite + Send + Unpin>,
		mut recv: Box<dyn tokio::io::AsyncRead + Send + Unpin>,
		remote_node_id: NodeId,
	) {
		use tokio::io::{AsyncReadExt, AsyncWriteExt};
		
		// Simple request-response messaging over streams
		loop {
			// Read message length (4 bytes)
			let mut len_buf = [0u8; 4];
			match recv.read_exact(&mut len_buf).await {
				Ok(_) => {},
				Err(_) => break, // Connection closed
			}
			let msg_len = u32::from_be_bytes(len_buf) as usize;
			
			// Read message
			let mut msg_buf = vec![0u8; msg_len];
			if let Err(e) = recv.read_exact(&mut msg_buf).await {
				eprintln!("Failed to read message: {}", e);
				break;
			}
			
			// Deserialize and handle
			match serde_json::from_slice::<Message>(&msg_buf) {
				Ok(message) => {
					// Process message based on type
					let response = match message {
						Message::Ping { timestamp, payload } => {
							let pong = Message::Pong {
								timestamp: chrono::Utc::now(),
								original_timestamp: timestamp,
							};
							serde_json::to_vec(&pong).unwrap_or_default()
						}
						Message::Data { message_id, .. } => {
							let ack = Message::Ack {
								message_id,
								success: true,
								error: None,
							};
							serde_json::to_vec(&ack).unwrap_or_default()
						}
						_ => Vec::new(), // No response for Pong/Ack
					};
					
					// Send response if any
					if !response.is_empty() {
						let len = response.len() as u32;
						if send.write_all(&len.to_be_bytes()).await.is_err() {
							break;
						}
						if send.write_all(&response).await.is_err() {
							break;
						}
						let _ = send.flush().await;
					}
				}
				Err(e) => {
					eprintln!("Failed to deserialize message: {}", e);
					break;
				}
			}
		}
	}

	async fn handle_request(&self, from_device: Uuid, request_data: Vec<u8>) -> Result<Vec<u8>> {
		let message: Message =
			serde_json::from_slice(&request_data).map_err(|e| NetworkingError::Serialization(e))?;

		match message {
			Message::Ping { timestamp, payload } => {
				self.handle_ping(from_device, timestamp, payload).await
			}
			Message::Pong {
				timestamp,
				original_timestamp,
			} => {
				self.handle_pong(from_device, timestamp, original_timestamp)
					.await
			}
			Message::Data {
				message_id,
				content_type,
				payload,
			} => {
				self.handle_data(from_device, message_id, content_type, payload)
					.await
			}
			Message::Ack {
				message_id,
				success,
				error,
			} => {
				self.handle_ack(from_device, message_id, success, error)
					.await
			}
		}
	}

	async fn handle_response(&self, _from_device: Uuid, _from_node: NodeId, _response_data: Vec<u8>) -> Result<()> {
		// Messaging protocol handles responses in handle_request
		Ok(())
	}

	async fn handle_event(&self, _event: ProtocolEvent) -> Result<()> {
		// Basic messaging doesn't need special event handling
		Ok(())
	}

	fn as_any(&self) -> &dyn std::any::Any {
		self
	}
}
```

## src/services/file_sharing.rs

```rust
//! File sharing service providing high-level file transfer operations

use crate::{
	context::CoreContext,
	operations::files::copy::{CopyOptions, FileCopyJob},
	services::networking::protocols::file_transfer::FileMetadata,
	shared::types::SdPath,
};
use serde::{Deserialize, Serialize};
use std::{path::PathBuf, sync::Arc, time::SystemTime};
use uuid::Uuid;

/// File sharing service
pub struct FileSharingService {
	context: Arc<CoreContext>,
}

/// Sharing target specification
#[derive(Debug, Clone)]
pub enum SharingTarget {
	/// Share with a specific paired device
	PairedDevice(Uuid),
	/// Discover and share with nearby devices
	NearbyDevices,
	/// Share with a specific device (may or may not be paired)
	SpecificDevice(DeviceInfo),
}

/// Device information for sharing
#[derive(Debug, Clone)]
pub struct DeviceInfo {
	pub device_id: Uuid,
	pub device_name: String,
	pub is_paired: bool,
	pub last_seen: Option<SystemTime>,
}

/// Transfer identifier for tracking operations
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum TransferId {
	/// Job system ID for cross-device copies with library ID
	JobId { job_id: Uuid, library_id: Uuid },
	/// Spacedrop session ID for ephemeral shares
	SpacedropId(Uuid),
}

/// Options for file sharing operations
#[derive(Debug, Clone)]
pub struct SharingOptions {
	/// Destination path on target device
	pub destination_path: PathBuf,
	/// Whether to overwrite existing files
	pub overwrite: bool,
	/// Whether to preserve file timestamps
	pub preserve_timestamps: bool,
	/// Sender name for display
	pub sender_name: String,
	/// Optional message to include with share
	pub message: Option<String>,
}

impl Default for SharingOptions {
	fn default() -> Self {
		Self {
			destination_path: PathBuf::from("/tmp/spacedrive"),
			overwrite: false,
			preserve_timestamps: true,
			sender_name: "Spacedrive User".to_string(),
			message: None,
		}
	}
}

/// Errors that can occur during file sharing
#[derive(Debug, thiserror::Error)]
pub enum SharingError {
	#[error("Networking not available")]
	NetworkingUnavailable,

	#[error("Device not found: {0}")]
	DeviceNotFound(Uuid),

	#[error("File not found: {0}")]
	FileNotFound(PathBuf),

	#[error("Permission denied: {0}")]
	PermissionDenied(String),

	#[error("Transfer failed: {0}")]
	TransferFailed(String),

	#[error("Invalid sharing target")]
	InvalidTarget,

	#[error("Job system error: {0}")]
	JobError(String),

	#[error("Network error: {0}")]
	NetworkError(String),
}

impl FileSharingService {
	/// Create a new file sharing service
	pub fn new(context: Arc<CoreContext>) -> Self {
		Self { context }
	}

	/// Share files with automatic protocol selection based on device relationship
	pub async fn share_files(
		&self,
		files: Vec<PathBuf>,
		target: SharingTarget,
		options: SharingOptions,
	) -> Result<Vec<TransferId>, SharingError> {
		// Validate files exist
		for file in &files {
			if !file.exists() {
				return Err(SharingError::FileNotFound(file.clone()));
			}
		}

		match target {
			SharingTarget::PairedDevice(device_id) => {
				// Use cross-device copy for trusted devices
				self.copy_to_paired_device(files, device_id, options).await
			}
			SharingTarget::NearbyDevices => {
				// Use Spacedrop for discovery-based sharing
				self.initiate_spacedrop(files, options).await
			}
			SharingTarget::SpecificDevice(device_info) => {
				// Check if device is paired, choose protocol accordingly
				if device_info.is_paired {
					self.copy_to_paired_device(files, device_info.device_id, options)
						.await
				} else {
					self.share_via_spacedrop(files, vec![device_info], options)
						.await
				}
			}
		}
	}

	/// Share files with a paired device
	pub async fn share_with_device(
		&self,
		files: Vec<PathBuf>,
		device_id: Uuid,
		destination_path: Option<PathBuf>,
	) -> Result<TransferId, SharingError> {
		// Get networking service from context
		let _networking = self
			.context
			.get_networking()
			.await
			.ok_or(SharingError::NetworkingUnavailable)?;

		// Get the current library to access its job manager
		let library = self
			.context
			.library_manager
			.get_primary_library()
			.await
			.ok_or(SharingError::JobError(
				"No active library for job dispatch".to_string(),
			))?;

		// Create and dispatch the FileCopyJob
		let job_manager = library.jobs();
		let sources = files.into_iter().map(SdPath::local).collect();
		let destination = SdPath::new(device_id, destination_path.unwrap_or_default());
		let copy_job = FileCopyJob::from_paths(sources, destination);

		let handle = job_manager
			.dispatch(copy_job)
			.await
			.map_err(|e| SharingError::JobError(e.to_string()))?;

		let transfer_id = TransferId::JobId {
			job_id: handle.id().into(),
			library_id: library.id(),
		};

		Ok(transfer_id)
	}

	/// Copy files to a paired device (trusted, automatic)
	async fn copy_to_paired_device(
		&self,
		files: Vec<PathBuf>,
		device_id: Uuid,
		options: SharingOptions,
	) -> Result<Vec<TransferId>, SharingError> {
		let library = self
			.context
			.library_manager
			.get_primary_library()
			.await
			.ok_or(SharingError::JobError(
				"No active library for job dispatch".to_string(),
			))?;

		let job_manager = library.jobs();

		// Create SdPath objects for sources
		let sources: Vec<SdPath> = files.into_iter().map(|path| SdPath::local(path)).collect();

		let destination = SdPath::new(device_id, options.destination_path);

		// Create FileCopyJob for cross-device operation
		let copy_job = FileCopyJob::from_paths(sources, destination).with_options(CopyOptions {
			overwrite: options.overwrite,
			verify_checksum: true,
			preserve_timestamps: options.preserve_timestamps,
			delete_after_copy: false,
			move_mode: None,
			copy_method: crate::operations::files::copy::input::CopyMethod::Auto,
		});

		// Submit job to job system
		let handle = job_manager
			.dispatch(copy_job)
			.await
			.map_err(|e| SharingError::JobError(e.to_string()))?;

		Ok(vec![TransferId::JobId {
			job_id: handle.id().into(),
			library_id: library.id(),
		}])
	}

	/// Share files via Spacedrop (ephemeral, requires consent)
	async fn initiate_spacedrop(
		&self,
		files: Vec<PathBuf>,
		options: SharingOptions,
	) -> Result<Vec<TransferId>, SharingError> {
		let _networking = self
			.context
			.get_networking()
			.await
			.ok_or(SharingError::NetworkingUnavailable)?;

		let mut transfer_ids = Vec::new();

		for file_path in files {
			let _file_metadata = self.create_file_metadata(&file_path).await?;

			// TODO: Implement Spacedrop protocol
			// For now, simulate the process
			let transfer_id = Uuid::new_v4();

			transfer_ids.push(TransferId::SpacedropId(transfer_id));
		}

		Ok(transfer_ids)
	}

	/// Share files via Spacedrop with specific devices
	async fn share_via_spacedrop(
		&self,
		files: Vec<PathBuf>,
		_target_devices: Vec<DeviceInfo>,
		options: SharingOptions,
	) -> Result<Vec<TransferId>, SharingError> {
		// For now, use the same implementation as general Spacedrop
		self.initiate_spacedrop(files, options).await
	}

	/// Create file metadata for sharing
	pub async fn create_file_metadata(
		&self,
		file_path: &PathBuf,
	) -> Result<FileMetadata, SharingError> {
		let metadata = tokio::fs::metadata(file_path)
			.await
			.map_err(|_e| SharingError::FileNotFound(file_path.clone()))?;

		Ok(FileMetadata {
			name: file_path
				.file_name()
				.unwrap_or_default()
				.to_string_lossy()
				.to_string(),
			size: metadata.len(),
			modified: metadata.modified().ok(),
			is_directory: metadata.is_dir(),
			checksum: None,  // Will be calculated during transfer
			mime_type: None, // TODO: Add MIME type detection
		})
	}

	/// Get nearby devices available for sharing
	pub async fn get_nearby_devices(&self) -> Result<Vec<DeviceInfo>, SharingError> {
		let _networking = self
			.context
			.get_networking()
			.await
			.ok_or(SharingError::NetworkingUnavailable)?;

		// TODO: Implement device discovery
		// For now, return empty list
		Ok(Vec::new())
	}

	/// Get paired devices
	pub async fn get_paired_devices(&self) -> Result<Vec<DeviceInfo>, SharingError> {
		// TODO: Get paired devices from device manager
		// For now, return empty list
		Ok(Vec::new())
	}

	/// Get status of a transfer
	pub async fn get_transfer_status(
		&self,
		transfer_id: &TransferId,
	) -> Result<TransferStatus, SharingError> {
		match transfer_id {
			TransferId::JobId { job_id, library_id } => {
				let library = self
					.context
					.library_manager
					.get_library(*library_id)
					.await
					.ok_or(SharingError::JobError(format!(
						"Library {} not found",
						library_id
					)))?;

				let job_manager = library.jobs();

				// Query job system for status
				let job_info = job_manager
					.get_job_info(*job_id)
					.await
					.map_err(|e| SharingError::JobError(e.to_string()))?;

				if let Some(info) = job_info {
					let state = match info.status {
						crate::infrastructure::jobs::types::JobStatus::Queued => {
							TransferState::Pending
						}
						crate::infrastructure::jobs::types::JobStatus::Running => {
							TransferState::Active
						}
						crate::infrastructure::jobs::types::JobStatus::Paused => {
							TransferState::Active
						}
						crate::infrastructure::jobs::types::JobStatus::Completed => {
							TransferState::Completed
						}
						crate::infrastructure::jobs::types::JobStatus::Failed => {
							TransferState::Failed
						}
						crate::infrastructure::jobs::types::JobStatus::Cancelled => {
							TransferState::Cancelled
						}
					};

					Ok(TransferStatus {
						id: transfer_id.clone(),
						state,
						progress: TransferProgress {
							bytes_transferred: 0, // TODO: Extract from job progress
							total_bytes: 0,       // TODO: Extract from job progress
							files_transferred: 0, // TODO: Extract from job progress
							total_files: 0,       // TODO: Extract from job progress
							estimated_remaining: None,
						},
						error: info.error_message,
					})
				} else {
					Err(SharingError::TransferFailed("Job not found".to_string()))
				}
			}
			TransferId::SpacedropId(_session_id) => {
				// TODO: Query Spacedrop protocol for status
				Ok(TransferStatus {
					id: transfer_id.clone(),
					state: TransferState::Pending,
					progress: TransferProgress {
						bytes_transferred: 0,
						total_bytes: 0,
						files_transferred: 0,
						total_files: 0,
						estimated_remaining: None,
					},
					error: None,
				})
			}
		}
	}

	/// Cancel a transfer
	pub async fn cancel_transfer(&self, transfer_id: &TransferId) -> Result<(), SharingError> {
		match transfer_id {
			TransferId::JobId { job_id, library_id } => {
				let library = self
					.context
					.library_manager
					.get_library(*library_id)
					.await
					.ok_or(SharingError::JobError(format!(
						"Library {} not found",
						library_id
					)))?;

				let job_manager = library.jobs();

				// Get the job handle and cancel it
				if let Some(_job_handle) = job_manager.get_job((*job_id).into()).await {
					// TODO: Implement cancel functionality on JobHandle
					Ok(())
				} else {
					Err(SharingError::TransferFailed("Job not found".to_string()))
				}
			}
			TransferId::SpacedropId(_session_id) => {
				// TODO: Cancel Spacedrop session
				Ok(())
			}
		}
	}

	/// Get all active transfers
	pub async fn get_active_transfers(&self) -> Result<Vec<TransferStatus>, SharingError> {
		let library = self
			.context
			.library_manager
			.get_primary_library()
			.await
			.ok_or(SharingError::JobError(
				"No active library for job dispatch".to_string(),
			))?;

		let job_manager = library.jobs();

		// Get all running jobs
		let running_jobs = job_manager.list_running_jobs().await;
		let mut transfers = Vec::new();

		for job_info in running_jobs {
			// Only include file copy jobs as transfers
			if job_info.name == "file_copy" {
				let state = match job_info.status {
					crate::infrastructure::jobs::types::JobStatus::Queued => TransferState::Pending,
					crate::infrastructure::jobs::types::JobStatus::Running => TransferState::Active,
					crate::infrastructure::jobs::types::JobStatus::Paused => TransferState::Active,
					crate::infrastructure::jobs::types::JobStatus::Completed => {
						TransferState::Completed
					}
					crate::infrastructure::jobs::types::JobStatus::Failed => TransferState::Failed,
					crate::infrastructure::jobs::types::JobStatus::Cancelled => {
						TransferState::Cancelled
					}
				};

				transfers.push(TransferStatus {
					id: TransferId::JobId {
						job_id: job_info.id,
						library_id: library.id(),
					},
					state,
					progress: TransferProgress {
						bytes_transferred: 0, // TODO: Extract from job progress
						total_bytes: 0,       // TODO: Extract from job progress
						files_transferred: 0, // TODO: Extract from job progress
						total_files: 0,       // TODO: Extract from job progress
						estimated_remaining: None,
					},
					error: job_info.error_message,
				});
			}
		}

		Ok(transfers)
	}
}

/// Transfer status information
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TransferStatus {
	pub id: TransferId,
	pub state: TransferState,
	pub progress: TransferProgress,
	pub error: Option<String>,
}

/// Transfer state
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum TransferState {
	Pending,
	Active,
	Completed,
	Failed,
	Cancelled,
}

/// Transfer progress information
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TransferProgress {
	pub bytes_transferred: u64,
	pub total_bytes: u64,
	pub files_transferred: usize,
	pub total_files: usize,
	pub estimated_remaining: Option<std::time::Duration>,
}

#[cfg(test)]
mod tests {
	use super::*;
	use crate::{
		device::DeviceManager, infrastructure::events::EventBus,
		keys::library_key_manager::LibraryKeyManager, library::LibraryManager,
	};
	use tempfile::tempdir;

	#[tokio::test]
	async fn test_file_sharing_service_creation() {
		let events = Arc::new(EventBus::default());
		let device_manager = Arc::new(DeviceManager::init().unwrap());
		let library_manager = Arc::new(LibraryManager::new_with_dir(
			std::env::temp_dir().join("test_libraries"),
			events.clone(),
		));
		let volume_manager = Arc::new(crate::volume::VolumeManager::new(
			uuid::Uuid::new_v4(), // Test device ID
			crate::volume::VolumeDetectionConfig::default(),
			events.clone(),
		));
		let library_key_manager = Arc::new(LibraryKeyManager::new().unwrap());
		let context = Arc::new(CoreContext::new(
			events,
			device_manager,
			library_manager,
			volume_manager,
			library_key_manager,
		));

		let _file_sharing = FileSharingService::new(context);
	}

	#[tokio::test]
	async fn test_sharing_options_default() {
		let options = SharingOptions::default();
		assert_eq!(options.sender_name, "Spacedrive User");
		assert!(!options.overwrite);
		assert!(options.preserve_timestamps);
		assert!(options.message.is_none());
	}

	#[tokio::test]
	async fn test_create_file_metadata() {
		let events = Arc::new(EventBus::default());
		let device_manager = Arc::new(DeviceManager::init().unwrap());
		let library_manager = Arc::new(LibraryManager::new_with_dir(
			std::env::temp_dir().join("test_libraries"),
			events.clone(),
		));
		let volume_manager = Arc::new(crate::volume::VolumeManager::new(
			uuid::Uuid::new_v4(), // Test device ID
			crate::volume::VolumeDetectionConfig::default(),
			events.clone(),
		));
		let library_key_manager = Arc::new(LibraryKeyManager::new().unwrap());
		let context = Arc::new(CoreContext::new(
			events,
			device_manager,
			library_manager,
			volume_manager,
			library_key_manager,
		));
		let file_sharing = FileSharingService::new(context);

		// Create a temporary file
		let temp_dir = tempdir().unwrap();
		let file_path = temp_dir.path().join("test.txt");
		tokio::fs::write(&file_path, b"test content").await.unwrap();

		let metadata = file_sharing.create_file_metadata(&file_path).await.unwrap();
		assert_eq!(metadata.name, "test.txt");
		assert_eq!(metadata.size, 12);
		assert!(!metadata.is_directory);
	}
}
```

## src/services/mod.rs

```rust
//! Background services management

use crate::{
	context::CoreContext, infrastructure::events::EventBus,
	keys::library_key_manager::LibraryKeyManager,
};
use anyhow::Result;
use std::sync::Arc;
use tokio::sync::RwLock;
use tracing::info;

pub mod device;
pub mod file_sharing;
pub mod location_watcher;
pub mod networking;
pub mod volume_monitor;

use device::DeviceService;
use file_sharing::FileSharingService;
use location_watcher::{LocationWatcher, LocationWatcherConfig};
use networking::NetworkingService;
use volume_monitor::{VolumeMonitorService, VolumeMonitorConfig};

/// Container for all background services
pub struct Services {
	/// File system watcher for locations
	pub location_watcher: Arc<LocationWatcher>,
	/// File sharing service
	pub file_sharing: Arc<FileSharingService>,
	/// Device management service
	pub device: Arc<DeviceService>,
	/// Networking service for device connections
	pub networking: Option<Arc<NetworkingService>>,
	/// Volume monitoring service
	pub volume_monitor: Option<Arc<VolumeMonitorService>>,
	/// Library key manager
	pub library_key_manager: Arc<LibraryKeyManager>,
	/// Shared context for all services
	context: Arc<CoreContext>,
}

impl Services {
	/// Create new services container with context
	pub fn new(context: Arc<CoreContext>) -> Self {
		info!("Initializing background services");

		let location_watcher_config = LocationWatcherConfig::default();
		let location_watcher = Arc::new(LocationWatcher::new(
			location_watcher_config,
			context.events.clone(),
		));
		let file_sharing = Arc::new(FileSharingService::new(context.clone()));
		let device = Arc::new(DeviceService::new(context.clone()));
		let library_key_manager = context.library_key_manager.clone();

		Self {
			location_watcher,
			file_sharing,
			device,
			networking: None, // Initialized separately when needed
			volume_monitor: None, // Initialized after library manager is available
			library_key_manager,
			context,
		}
	}

	/// Get the shared context
	pub fn context(&self) -> Arc<CoreContext> {
		self.context.clone()
	}

	/// Start all services
	pub async fn start_all(&self) -> Result<()> {
		info!("Starting all background services");

		self.location_watcher.start().await?;
		
		// Start volume monitor if initialized
		if let Some(monitor) = &self.volume_monitor {
			monitor.start().await?;
		}

		// Networking service is already started during initialization

		// TODO: Start other services
		// self.jobs.start().await?;
		// self.thumbnails.start().await?;

		Ok(())
	}

	/// Stop all services gracefully
	pub async fn stop_all(&self) -> Result<()> {
		info!("Stopping all background services");

		self.location_watcher.stop().await?;
		
		// Stop volume monitor if initialized
		if let Some(monitor) = &self.volume_monitor {
			monitor.stop().await?;
		}

		// Stop networking service if initialized
		if let Some(networking) = &self.networking {
			networking
				.shutdown()
				.await
				.map_err(|e| anyhow::anyhow!("Failed to stop networking: {}", e))?;
		}

		Ok(())
	}

	/// Initialize networking service
	pub async fn init_networking(
		&mut self,
		device_manager: std::sync::Arc<crate::device::DeviceManager>,
		library_key_manager: std::sync::Arc<crate::keys::library_key_manager::LibraryKeyManager>,
		data_dir: impl AsRef<std::path::Path>,
	) -> Result<()> {
		use crate::services::networking::{NetworkingService, utils::logging::ConsoleLogger};

		info!("Initializing networking service");
		let logger = std::sync::Arc::new(ConsoleLogger);
		let networking_service =
			NetworkingService::new(device_manager, library_key_manager, data_dir, logger)
				.await
				.map_err(|e| anyhow::anyhow!("Failed to create networking service: {}", e))?;

		self.networking = Some(Arc::new(networking_service));
		Ok(())
	}

	/// Start networking service after initialization
	pub async fn start_networking(&self) -> Result<()> {
		if let Some(networking) = &self.networking {
			// Create a temporary mutable reference to start the service
			// This is safe because start() is only called once during initialization
			let networking_ptr =
				Arc::as_ptr(networking) as *mut crate::services::networking::NetworkingService;
			unsafe {
				(*networking_ptr)
					.start()
					.await
					.map_err(|e| anyhow::anyhow!("Failed to start networking service: {}", e))?;
			}
		}
		Ok(())
	}

	/// Get networking service if initialized
	pub fn networking(&self) -> Option<Arc<NetworkingService>> {
		self.networking.clone()
	}

	/// Initialize volume monitor service
	pub fn init_volume_monitor(
		&mut self,
		volume_manager: Arc<crate::volume::VolumeManager>,
		library_manager: std::sync::Weak<crate::library::LibraryManager>,
	) {
		info!("Initializing volume monitor service");
		
		let config = VolumeMonitorConfig::default();
		let volume_monitor = Arc::new(VolumeMonitorService::new(
			volume_manager,
			library_manager,
			config,
		));
		
		self.volume_monitor = Some(volume_monitor);
	}

	/// Start volume monitor service
	pub async fn start_volume_monitor(&self) -> Result<()> {
		if let Some(monitor) = &self.volume_monitor {
			monitor.start().await?;
		}
		Ok(())
	}

	/// Stop volume monitor service
	pub async fn stop_volume_monitor(&self) -> Result<()> {
		if let Some(monitor) = &self.volume_monitor {
			monitor.stop().await?;
		}
		Ok(())
	}
}

/// Trait for background services
#[async_trait::async_trait]
pub trait Service: Send + Sync {
	/// Start the service
	async fn start(&self) -> Result<()>;

	/// Stop the service gracefully
	async fn stop(&self) -> Result<()>;

	/// Check if the service is running
	fn is_running(&self) -> bool;

	/// Get service name for logging
	fn name(&self) -> &'static str;
}
```

## src/services/volume_monitor.rs

```rust
//! Volume monitoring service
//!
//! Periodically refreshes volume information and updates tracked volumes in the database.

use crate::{
    context::CoreContext,
    infrastructure::events::EventBus,
    library::LibraryManager,
    services::Service,
    volume::VolumeManager,
};
use anyhow::Result;
use std::sync::{Arc, Weak};
use std::time::Duration;
use tokio::sync::RwLock;
use tracing::{debug, error, info, warn};

/// Configuration for volume monitoring
#[derive(Debug, Clone)]
pub struct VolumeMonitorConfig {
    /// How often to refresh volume information (in seconds)
    pub refresh_interval_secs: u64,
    /// Whether to update tracked volumes in the database
    pub update_tracked_volumes: bool,
}

impl Default for VolumeMonitorConfig {
    fn default() -> Self {
        Self {
            refresh_interval_secs: 30,
            update_tracked_volumes: true,
        }
    }
}

/// Background service that monitors volume state changes
pub struct VolumeMonitorService {
    volume_manager: Arc<VolumeManager>,
    library_manager: Weak<LibraryManager>,
    config: VolumeMonitorConfig,
    running: RwLock<bool>,
    handle: RwLock<Option<tokio::task::JoinHandle<()>>>,
}

impl VolumeMonitorService {
    /// Create a new volume monitor service
    pub fn new(
        volume_manager: Arc<VolumeManager>,
        library_manager: Weak<LibraryManager>,
        config: VolumeMonitorConfig,
    ) -> Self {
        Self {
            volume_manager,
            library_manager,
            config,
            running: RwLock::new(false),
            handle: RwLock::new(None),
        }
    }

    /// Monitor volumes and update tracked volumes in libraries
    async fn monitor_loop(
        volume_manager: Arc<VolumeManager>,
        library_manager: Weak<LibraryManager>,
        config: VolumeMonitorConfig,
        running: Arc<RwLock<bool>>,
    ) {
        let mut interval = tokio::time::interval(Duration::from_secs(config.refresh_interval_secs));
        
        while *running.read().await {
            interval.tick().await;
            
            // Refresh all volumes
            if let Err(e) = volume_manager.refresh_volumes().await {
                error!("Failed to refresh volumes: {}", e);
                continue;
            }
            
            // Update tracked volumes if enabled and library manager is available
            if config.update_tracked_volumes {
                if let Some(lib_manager) = library_manager.upgrade() {
                    debug!("Updating tracked volumes across libraries");
                    
                    // Get all open libraries
                    let libraries = lib_manager.get_open_libraries().await;
                    
                    for library in &libraries {
                        // Get tracked volumes for this library
                        match volume_manager.get_tracked_volumes(&library).await {
                            Ok(tracked_volumes) => {
                                for tracked in tracked_volumes {
                                    // Check if volume is still present
                                    if let Some(current_volume) = volume_manager
                                        .get_volume(&tracked.fingerprint)
                                        .await
                                    {
                                        // Update volume state if changed
                                        if tracked.is_online != current_volume.is_mounted {
                                            if let Err(e) = volume_manager
                                                .update_tracked_volume_state(
                                                    &library,
                                                    &tracked.fingerprint,
                                                    &current_volume,
                                                )
                                                .await
                                            {
                                                error!(
                                                    "Failed to update tracked volume {} in library {}: {}",
                                                    tracked.fingerprint,
                                                    library.id(),
                                                    e
                                                );
                                            } else {
                                                debug!(
                                                    "Updated tracked volume {} in library {} (online: {} -> {})",
                                                    tracked.fingerprint,
                                                    library.id(),
                                                    tracked.is_online,
                                                    current_volume.is_mounted
                                                );
                                            }
                                        }
                                    } else {
                                        // Volume no longer detected but still tracked
                                        debug!(
                                            "Tracked volume {} not detected in library {}",
                                            tracked.fingerprint,
                                            library.id()
                                        );
                                    }
                                }
                            }
                            Err(e) => {
                                error!(
                                    "Failed to get tracked volumes for library {}: {}",
                                    library.id(),
                                    e
                                );
                            }
                        }
                    }
                    
                    // Check for new external volumes to auto-track
                    let all_volumes = volume_manager.get_all_volumes().await;
                    for volume in all_volumes {
                        // Only consider external volumes
                        if matches!(volume.mount_type, crate::volume::types::MountType::External) {
                            for library in &libraries {
                                // Check if auto-tracking is enabled
                                let config = library.config().await;
                                if config.settings.auto_track_external_volumes {
                                    // Check if not already tracked
                                    if !volume_manager
                                        .is_volume_tracked(&library, &volume.fingerprint)
                                        .await
                                        .unwrap_or(false)
                                    {
                                        // Auto-track the external volume
                                        match volume_manager
                                            .track_volume(&library, &volume.fingerprint, None)
                                            .await
                                        {
                                            Ok(_) => {
                                                info!(
                                                    "Auto-tracked external volume '{}' in library '{}'",
                                                    volume.name,
                                                    library.name().await
                                                );
                                            }
                                            Err(e) => {
                                                debug!(
                                                    "Failed to auto-track external volume '{}': {}",
                                                    volume.name, e
                                                );
                                            }
                                        }
                                    }
                                }
                            }
                        }
                    }
                } else {
                    debug!("Library manager not available, skipping tracked volume updates");
                }
            }
        }
        
        info!("Volume monitoring stopped");
    }
}

#[async_trait::async_trait]
impl Service for VolumeMonitorService {
    async fn start(&self) -> Result<()> {
        let mut running = self.running.write().await;
        if *running {
            warn!("Volume monitor service already running");
            return Ok(());
        }
        
        *running = true;
        
        let volume_manager = self.volume_manager.clone();
        let library_manager = self.library_manager.clone();
        let config = self.config.clone();
        let running_flag = Arc::new(RwLock::new(*running));
        
        let handle = tokio::spawn(Self::monitor_loop(
            volume_manager,
            library_manager,
            config,
            running_flag,
        ));
        
        *self.handle.write().await = Some(handle);
        
        info!(
            "Volume monitor service started (refresh every {}s)",
            self.config.refresh_interval_secs
        );
        
        Ok(())
    }
    
    async fn stop(&self) -> Result<()> {
        *self.running.write().await = false;
        
        if let Some(handle) = self.handle.write().await.take() {
            handle.abort();
        }
        
        info!("Volume monitor service stopped");
        Ok(())
    }
    
    fn is_running(&self) -> bool {
        // Use blocking read since this is a sync method
        *self.running.blocking_read()
    }
    
    fn name(&self) -> &'static str {
        "volume_monitor"
    }
}```

## src/context.rs

```rust
//! Shared context providing access to core application components.

//! Shared context providing access to core application components.

use crate::{
	device::DeviceManager, infrastructure::events::EventBus,
	keys::library_key_manager::LibraryKeyManager, library::LibraryManager,
	infrastructure::actions::manager::ActionManager,
	services::networking::NetworkingService, volume::VolumeManager,
};
use std::sync::Arc;
use tokio::sync::RwLock;

/// Shared context providing access to core application components.
#[derive(Clone)]
pub struct CoreContext {
	pub events: Arc<EventBus>,
	pub device_manager: Arc<DeviceManager>,
	pub library_manager: Arc<LibraryManager>,
	pub volume_manager: Arc<VolumeManager>,
	pub library_key_manager: Arc<LibraryKeyManager>,
	// This is wrapped in an RwLock to allow it to be set after initialization
	pub action_manager: Arc<RwLock<Option<Arc<ActionManager>>>>,
	pub networking: Arc<RwLock<Option<Arc<NetworkingService>>>>,
}

impl CoreContext {
	/// Create a new context with the given components
	pub fn new(
		events: Arc<EventBus>,
		device_manager: Arc<DeviceManager>,
		library_manager: Arc<LibraryManager>,
		volume_manager: Arc<VolumeManager>,
		library_key_manager: Arc<LibraryKeyManager>,
	) -> Self {
		Self {
			events,
			device_manager,
			library_manager,
			volume_manager,
			library_key_manager,
			action_manager: Arc::new(RwLock::new(None)),
			networking: Arc::new(RwLock::new(None)),
		}
	}

	/// Helper method for services to get the networking service
	pub async fn get_networking(&self) -> Option<Arc<NetworkingService>> {
		self.networking.read().await.clone()
	}

	/// Method for Core to set networking after it's initialized
	pub async fn set_networking(&self, networking: Arc<NetworkingService>) {
		*self.networking.write().await = Some(networking);
	}

	/// Helper method to get the action manager
	pub async fn get_action_manager(&self) -> Option<Arc<ActionManager>> {
		self.action_manager.read().await.clone()
	}

	/// Method for Core to set action manager after it's initialized
	pub async fn set_action_manager(&self, action_manager: Arc<ActionManager>) {
		*self.action_manager.write().await = Some(action_manager);
	}
}
```

